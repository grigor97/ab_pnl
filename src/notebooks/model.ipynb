{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simulate_bivariate_abpnl(n: int) -> dict: \n",
    "    def f1(x: np.array) -> np.array:\n",
    "        return x**(-1) + 10*x\n",
    "    def f2(z: np.array) -> np.array:\n",
    "        return z**3\n",
    "    \n",
    "    x = np.random.uniform(0.1, 1.1, n)\n",
    "    noise = np.random.uniform(0, 5, n)\n",
    "    \n",
    "    z = f1(x) + noise\n",
    "    y = f2(z)\n",
    "    df = pd.DataFrame({'x1': x, 'x2': y})\n",
    "    sim_data = {'df': df, 'noise': noise}\n",
    "    \n",
    "    return sim_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def centering(M):\n",
    "    n = M.shape[0]\n",
    "    mat_ones = torch.ones((n, n))\n",
    "    idendity = torch.eye(n)\n",
    "    H = idendity - mat_ones/n\n",
    "    \n",
    "    C = torch.matmul(M, H)\n",
    "    return C\n",
    "    \n",
    "    \n",
    "def gaussian_grammat(x, sigma2=None):\n",
    "    xxT = torch.squeeze(torch.matmul(x, x.T))\n",
    "    x2 = torch.diag(xxT)\n",
    "    xnorm = x2 - xxT + (x2 - xxT).T\n",
    "    \n",
    "    if sigma2 is None:\n",
    "        sigma2 = torch.median(xnorm[xnorm != 0])\n",
    "        \n",
    "    if sigma2 == 0:\n",
    "        sigma2 += 1e-16\n",
    "        \n",
    "    Kx = torch.exp(-xnorm/sigma2)\n",
    "    \n",
    "    return Kx\n",
    "    \n",
    "def HSIC(x, y):\n",
    "    gram_x = gaussian_grammat(x)\n",
    "    gram_y = gaussian_grammat(y)\n",
    "    \n",
    "    c = x.shape[0]**2\n",
    "    hsic = torch.trace(torch.matmul(centering(gram_x), centering(gram_y)))/c\n",
    "    \n",
    "    return hsic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.0002)\n",
      "tensor(0.1009)\n"
     ]
    }
   ],
   "source": [
    "x = torch.Tensor(np.random.normal(size=1000).reshape((-1, 1, 1)))\n",
    "y = torch.Tensor(np.random.normal(size=1000).reshape((-1, 1, 1)))\n",
    "print(HSIC(x, y))\n",
    "print(HSIC(x, x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDataset(Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "        \n",
    "        data = self.data[idx, :]\n",
    "        \n",
    "        return data[:-1].reshape((-1, 1)), data[-1].reshape((-1, 1))\n",
    "    \n",
    "\n",
    "class Network(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super().__init__()\n",
    "        self.network = nn.Sequential(\n",
    "            nn.Linear(input_dim, 5),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Linear(5, 5),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Linear(5, 5),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Linear(5, 5),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Linear(5, input_dim)\n",
    "#             nn.LeakyReLU()\n",
    "        )\n",
    "        \n",
    "        self.encode = nn.Sequential(\n",
    "            nn.Linear(input_dim, 5),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Linear(5, 5),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Linear(5, 5),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Linear(5, 5),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Linear(5, input_dim)\n",
    "#             nn.LeakyReLU()\n",
    "        )\n",
    "        self.decode = nn.Sequential(\n",
    "            nn.Linear(input_dim, 5),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Linear(5, 5),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Linear(5, 5),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Linear(5, 5),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Linear(5, input_dim)\n",
    "#             nn.LeakyReLU()\n",
    "        )\n",
    "        \n",
    "    def forward(self, x, y):\n",
    "        g1_x = self.network(x)\n",
    "        g3_y = self.encode(y)\n",
    "        y_approx = self.decode(g3_y)\n",
    "        \n",
    "        assert y.shape == y_approx.shape\n",
    "        \n",
    "        return [g1_x, y_approx, g3_y]\n",
    "    \n",
    "def train_model(train_loader, test_loader, num_epochs, input_dim, log_every_batch = 10):\n",
    "    device = 0 if torch.cuda.is_available() else 'cpu'\n",
    "    model = Network(input_dim).to(device)\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.01, betas=(0.9, 0.999))\n",
    "\n",
    "    train_loss_avgs = []\n",
    "    test_loss_avgs = []\n",
    "    \n",
    "    min_loss = 10000\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        train_loss_trace = []\n",
    "\n",
    "        for batch, (x, y) in enumerate(train_loader):\n",
    "            x = x.to(device)\n",
    "            x = x.float()\n",
    "            y = y.to(device)\n",
    "            y = y.float()\n",
    "\n",
    "            g1_x, y_approx, g3_y = model.forward(x, y)\n",
    "            noise = g3_y - g1_x\n",
    "\n",
    "            loss = lamb*F.mse_loss(y_approx, y) + (1-lamb)*HSIC(x, noise)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            train_loss_trace.append(loss.detach().item())\n",
    "            if batch % log_every_batch == 0:\n",
    "                print(f'Training: epoch {epoch} batch {batch} loss {loss}')\n",
    "\n",
    "        model.eval()\n",
    "        test_loss_trace = []\n",
    "        for batch, (x, y) in enumerate(test_loader):\n",
    "            x = x.to(device)\n",
    "            x = x.float()\n",
    "            y = y.to(device)\n",
    "            y = y.float()\n",
    "\n",
    "            g1_x, y_approx, g3_y = model.forward(x, y)\n",
    "            noise = g3_y - g1_x\n",
    "\n",
    "            loss = lamb*F.mse_loss(y_approx, y) + (1-lamb)*HSIC(x, noise)\n",
    "\n",
    "            test_loss_trace.append(loss.detach().item())\n",
    "            if batch % log_every_batch == 0:\n",
    "                print(f'Test: epoch {epoch} batch {batch} loss {loss}')\n",
    "\n",
    "        train_avg = np.mean(train_loss_trace)\n",
    "        test_avg = np.mean(test_loss_trace)\n",
    "        \n",
    "        if test_avg < min_loss:\n",
    "            min_loss = test_avg\n",
    "\n",
    "        train_loss_avgs.append(train_avg)\n",
    "        test_loss_avgs.append(test_avg)\n",
    "        print(f'epoch {epoch} finished - avarage train loss {train_avg} ',\n",
    "             f'avarage test loss {test_avg}')\n",
    "        \n",
    "    return train_loss_avgs, test_loss_avgs, min_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 1000\n",
    "\n",
    "batch_size = 32\n",
    "lamb = 0.5\n",
    "num_epochs = 200\n",
    "\n",
    "num_trials = 9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = simulate_bivariate_abpnl(n)\n",
    "df = data['df']\n",
    "df = (df-df.mean())/df.std()\n",
    "noise = data['noise']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_final_median_loss(df, num_trials):\n",
    "    rand_seed = np.random.randint(0, 1000000)\n",
    "    random.seed(rand_seed)\n",
    "    np.random.seed(rand_seed)\n",
    "    torch.manual_seed(rand_seed)\n",
    "\n",
    "    input_dim = df.shape[1] - 1\n",
    "\n",
    "    train, test = train_test_split(df, test_size=0.1, random_state=10, shuffle=True)\n",
    "\n",
    "    train = np.array(train)\n",
    "    test = np.array(test)\n",
    "\n",
    "    train = MyDataset(train)\n",
    "    test = MyDataset(test)\n",
    "    \n",
    "    train_loader = DataLoader(train, batch_size=batch_size, shuffle=True, \n",
    "                              num_workers=0, pin_memory=True)\n",
    "    test_loader = DataLoader(test, batch_size=batch_size, shuffle=False,\n",
    "                             num_workers=0, pin_memory=True)\n",
    "    \n",
    "    losses = []\n",
    "    for trial in range(num_trials):\n",
    "        train_loss_avgs, test_loss_avgs, min_loss = train_model(train_loader, test_loader, num_epochs, input_dim)\n",
    "        losses.append(min_loss)\n",
    "    \n",
    "    median_loss = np.median(losses)\n",
    "    return median_loss, losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for b, (x, y) in enumerate(train_loader):\n",
    "#     print(b)\n",
    "#     print(x.shape)\n",
    "#     print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "median_loss, losses = get_final_median_loss(df, num_trials)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.011459972709417343"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "median_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.011936204391531646,\n",
       " 0.011919335345737636,\n",
       " 0.011285196291282773,\n",
       " 0.012202803627587855,\n",
       " 0.010931709315627813,\n",
       " 0.011369480635039508,\n",
       " 0.011665846221148968,\n",
       " 0.01126380858477205,\n",
       " 0.011459972709417343]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>x2</th>\n",
       "      <th>x1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.199210</td>\n",
       "      <td>-1.242773</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.099672</td>\n",
       "      <td>1.030061</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-1.093608</td>\n",
       "      <td>-0.196601</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.788205</td>\n",
       "      <td>1.525318</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.431947</td>\n",
       "      <td>0.611544</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         x2        x1\n",
       "0 -0.199210 -1.242773\n",
       "1  0.099672  1.030061\n",
       "2 -1.093608 -0.196601\n",
       "3  1.788205  1.525318\n",
       "4  0.431947  0.611544"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[['x2', 'x1']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "median_loss_back, losses_back = get_final_median_loss(df[['x2', 'x1']], num_trials)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.012396272097248584"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "median_loss_back"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.01262878708075732,\n",
       " 0.012263277196325362,\n",
       " 0.012966789654456079,\n",
       " 0.012680116342380643,\n",
       " 0.012383790162857622,\n",
       " 0.012234172376338392,\n",
       " 0.012616099382285029,\n",
       " 0.012191358720883727,\n",
       " 0.012396272097248584]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "losses_back"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim = df.shape[1] - 1\n",
    "\n",
    "train, test = train_test_split(df, test_size=0.1, random_state=10, shuffle=True)\n",
    "\n",
    "train = np.array(train)\n",
    "test = np.array(test)\n",
    "\n",
    "train = MyDataset(train)\n",
    "test = MyDataset(test)\n",
    "\n",
    "train_loader = DataLoader(train, batch_size=batch_size, shuffle=True, \n",
    "                          num_workers=0, pin_memory=True)\n",
    "test_loader = DataLoader(test, batch_size=batch_size, shuffle=False,\n",
    "                         num_workers=0, pin_memory=True)\n",
    "\n",
    "train_loss_avgs, test_loss_avgs, min_loss = train_model(num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize=(12,8))\n",
    "ax.plot(train_loss_avgs, label='train')\n",
    "ax.plot(test_loss_avgs, label='test')\n",
    "ax.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize=(12,8))\n",
    "ax.plot(train_loss_avgs[100:], label='train')\n",
    "ax.plot(test_loss_avgs[100:], label='test')\n",
    "ax.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml1",
   "language": "python",
   "name": "ml1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
