{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simulate_bivariate_abpnl(n: int) -> dict: \n",
    "    def f1(x: np.array) -> np.array:\n",
    "        return x**(-1) + 10*x\n",
    "    def f2(z: np.array) -> np.array:\n",
    "        return z**3\n",
    "    \n",
    "    x = np.random.uniform(0.1, 1.1, n)\n",
    "    noise = np.random.uniform(0, 5, n)\n",
    "    \n",
    "    z = f1(x) + noise\n",
    "    y = f2(z)\n",
    "    df = pd.DataFrame({'x1': x, 'x2': y})\n",
    "    sim_data = {'df': df, 'noise': noise}\n",
    "    \n",
    "    return sim_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def centering(M):\n",
    "    n = M.shape[0]\n",
    "    mat_ones = torch.ones((n, n))\n",
    "    idendity = torch.eye(n)\n",
    "    H = idendity - mat_ones/n\n",
    "    \n",
    "    C = torch.matmul(M, H)\n",
    "    return C\n",
    "    \n",
    "    \n",
    "def gaussian_grammat(x, sigma2=None):\n",
    "    xxT = torch.squeeze(torch.matmul(x, x.T))\n",
    "    x2 = torch.diag(xxT)\n",
    "    xnorm = x2 - xxT + (x2 - xxT).T\n",
    "    \n",
    "    if sigma2 is None:\n",
    "        sigma2 = torch.median(xnorm[xnorm != 0])\n",
    "        \n",
    "    if sigma2 == 0:\n",
    "        sigma2 += 1e-16\n",
    "        \n",
    "    Kx = torch.exp(-xnorm/sigma2)\n",
    "    \n",
    "    return Kx\n",
    "    \n",
    "def HSIC(x, y):\n",
    "    gram_x = gaussian_grammat(x)\n",
    "    gram_y = gaussian_grammat(y)\n",
    "    \n",
    "    c = x.shape[0]**2\n",
    "    hsic = torch.trace(torch.matmul(centering(gram_x), centering(gram_y)))/c\n",
    "    \n",
    "    return hsic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.0002)\n",
      "tensor(0.1009)\n"
     ]
    }
   ],
   "source": [
    "x = torch.Tensor(np.random.normal(size=1000).reshape((-1, 1, 1)))\n",
    "y = torch.Tensor(np.random.normal(size=1000).reshape((-1, 1, 1)))\n",
    "print(HSIC(x, y))\n",
    "print(HSIC(x, x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDataset(Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "        \n",
    "        data = self.data[idx, :]\n",
    "        \n",
    "        return data[:-1].reshape((-1, 1)), data[-1].reshape((-1, 1))\n",
    "    \n",
    "\n",
    "class Network(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super().__init__()\n",
    "        self.network = nn.Sequential(\n",
    "            nn.Linear(input_dim, 5),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Linear(5, 5),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Linear(5, 5),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Linear(5, 5),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Linear(5, input_dim)\n",
    "#             nn.LeakyReLU()\n",
    "        )\n",
    "        \n",
    "        self.encode = nn.Sequential(\n",
    "            nn.Linear(input_dim, 5),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Linear(5, 5),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Linear(5, 5),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Linear(5, 5),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Linear(5, input_dim)\n",
    "#             nn.LeakyReLU()\n",
    "        )\n",
    "        self.decode = nn.Sequential(\n",
    "            nn.Linear(input_dim, 5),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Linear(5, 5),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Linear(5, 5),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Linear(5, 5),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Linear(5, input_dim)\n",
    "#             nn.LeakyReLU()\n",
    "        )\n",
    "        \n",
    "    def forward(self, x, y):\n",
    "        g1_x = self.network(x)\n",
    "        g3_y = self.encode(y)\n",
    "        y_approx = self.decode(g3_y)\n",
    "        \n",
    "        assert y.shape == y_approx.shape\n",
    "        \n",
    "        return [g1_x, y_approx, g3_y]\n",
    "    \n",
    "def train_model(train_loader, test_loader, num_epochs, input_dim, log_every_batch = 10):\n",
    "    device = 0 if torch.cuda.is_available() else 'cpu'\n",
    "    model = Network(input_dim).to(device)\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.01, betas=(0.9, 0.999))\n",
    "\n",
    "    train_loss_avgs = []\n",
    "    test_loss_avgs = []\n",
    "    \n",
    "    min_loss = 10000\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        train_loss_trace = []\n",
    "\n",
    "        for batch, (x, y) in enumerate(train_loader):\n",
    "            x = x.to(device)\n",
    "            x = x.float()\n",
    "            y = y.to(device)\n",
    "            y = y.float()\n",
    "\n",
    "            g1_x, y_approx, g3_y = model.forward(x, y)\n",
    "            noise = g3_y - g1_x\n",
    "\n",
    "            loss = lamb*F.mse_loss(y_approx, y) + (1-lamb)*HSIC(x, noise)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            train_loss_trace.append(loss.detach().item())\n",
    "            if batch % log_every_batch == 0:\n",
    "                print(f'Training: epoch {epoch} batch {batch} loss {loss}')\n",
    "\n",
    "        model.eval()\n",
    "        test_loss_trace = []\n",
    "        for batch, (x, y) in enumerate(test_loader):\n",
    "            x = x.to(device)\n",
    "            x = x.float()\n",
    "            y = y.to(device)\n",
    "            y = y.float()\n",
    "\n",
    "            g1_x, y_approx, g3_y = model.forward(x, y)\n",
    "            noise = g3_y - g1_x\n",
    "\n",
    "            loss = lamb*F.mse_loss(y_approx, y) + (1-lamb)*HSIC(x, noise)\n",
    "\n",
    "            test_loss_trace.append(loss.detach().item())\n",
    "            if batch % log_every_batch == 0:\n",
    "                print(f'Test: epoch {epoch} batch {batch} loss {loss}')\n",
    "\n",
    "        train_avg = np.mean(train_loss_trace)\n",
    "        test_avg = np.mean(test_loss_trace)\n",
    "        \n",
    "        if test_avg < min_loss:\n",
    "            min_loss = test_avg\n",
    "\n",
    "        train_loss_avgs.append(train_avg)\n",
    "        test_loss_avgs.append(test_avg)\n",
    "        print(f'epoch {epoch} finished - avarage train loss {train_avg} ',\n",
    "             f'avarage test loss {test_avg}')\n",
    "        \n",
    "    return train_loss_avgs, test_loss_avgs, min_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 1000\n",
    "\n",
    "batch_size = 32\n",
    "lamb = 0.5\n",
    "num_epochs = 200\n",
    "\n",
    "num_trials = 9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = simulate_bivariate_abpnl(n)\n",
    "df = data['df']\n",
    "df = (df-df.mean())/df.std()\n",
    "noise = data['noise']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_final_median_loss(df, num_trials):\n",
    "    rand_seed = np.random.randint(0, 1000000)\n",
    "    random.seed(rand_seed)\n",
    "    np.random.seed(rand_seed)\n",
    "    torch.manual_seed(rand_seed)\n",
    "\n",
    "    input_dim = df.shape[1] - 1\n",
    "\n",
    "    train, test = train_test_split(df, test_size=0.1, random_state=10, shuffle=True)\n",
    "\n",
    "    train = np.array(train)\n",
    "    test = np.array(test)\n",
    "\n",
    "    train = MyDataset(train)\n",
    "    test = MyDataset(test)\n",
    "    \n",
    "    train_loader = DataLoader(train, batch_size=batch_size, shuffle=True, \n",
    "                              num_workers=0, pin_memory=True)\n",
    "    test_loader = DataLoader(test, batch_size=batch_size, shuffle=False,\n",
    "                             num_workers=0, pin_memory=True)\n",
    "    \n",
    "    losses = []\n",
    "    for trial in range(num_trials):\n",
    "        train_loss_avgs, test_loss_avgs, min_loss = train_model(train_loader, test_loader, num_epochs, input_dim)\n",
    "        losses.append(min_loss)\n",
    "    \n",
    "    median_loss = np.median(losses)\n",
    "    return median_loss, losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for b, (x, y) in enumerate(train_loader):\n",
    "#     print(b)\n",
    "#     print(x.shape)\n",
    "#     print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training: epoch 0 batch 0 loss 0.6096925735473633\n",
      "Training: epoch 0 batch 10 loss 0.6297626495361328\n",
      "Training: epoch 0 batch 20 loss 0.3950054943561554\n",
      "Test: epoch 0 batch 0 loss 0.3975762724876404\n",
      "epoch 0 finished - avarage train loss 0.5092814030318424  avarage test loss 0.43740154430270195\n",
      "Training: epoch 1 batch 0 loss 0.6242485642433167\n",
      "Training: epoch 1 batch 10 loss 0.49413564801216125\n",
      "Training: epoch 1 batch 20 loss 0.4532685875892639\n",
      "Test: epoch 1 batch 0 loss 0.39700934290885925\n",
      "epoch 1 finished - avarage train loss 0.513841894166223  avarage test loss 0.43810373172163963\n",
      "Training: epoch 2 batch 0 loss 0.9149767756462097\n",
      "Training: epoch 2 batch 10 loss 0.41105225682258606\n",
      "Training: epoch 2 batch 20 loss 0.5483582615852356\n",
      "Test: epoch 2 batch 0 loss 0.32973232865333557\n",
      "epoch 2 finished - avarage train loss 0.4945511047182412  avarage test loss 0.3701705299317837\n",
      "Training: epoch 3 batch 0 loss 0.4995259642601013\n",
      "Training: epoch 3 batch 10 loss 0.3498347997665405\n",
      "Training: epoch 3 batch 20 loss 0.10414598882198334\n",
      "Test: epoch 3 batch 0 loss 0.055326905101537704\n",
      "epoch 3 finished - avarage train loss 0.18766453374048758  avarage test loss 0.06595918070524931\n",
      "Training: epoch 4 batch 0 loss 0.05240282043814659\n",
      "Training: epoch 4 batch 10 loss 0.017512882128357887\n",
      "Training: epoch 4 batch 20 loss 0.0074036940932273865\n",
      "Test: epoch 4 batch 0 loss 0.02631581947207451\n",
      "epoch 4 finished - avarage train loss 0.017815107335175932  avarage test loss 0.024087429977953434\n",
      "Training: epoch 5 batch 0 loss 0.02575329877436161\n",
      "Training: epoch 5 batch 10 loss 0.014631632715463638\n",
      "Training: epoch 5 batch 20 loss 0.003599695861339569\n",
      "Test: epoch 5 batch 0 loss 0.015579762868583202\n",
      "epoch 5 finished - avarage train loss 0.011947403758250433  avarage test loss 0.01745182555168867\n",
      "Training: epoch 6 batch 0 loss 0.0062810759991407394\n",
      "Training: epoch 6 batch 10 loss 0.006243860349059105\n",
      "Training: epoch 6 batch 20 loss 0.005050013307482004\n",
      "Test: epoch 6 batch 0 loss 0.01566474884748459\n",
      "epoch 6 finished - avarage train loss 0.01028148206111429  avarage test loss 0.021387493703514338\n",
      "Training: epoch 7 batch 0 loss 0.007251725997775793\n",
      "Training: epoch 7 batch 10 loss 0.005112660117447376\n",
      "Training: epoch 7 batch 20 loss 0.011270598508417606\n",
      "Test: epoch 7 batch 0 loss 0.011494634672999382\n",
      "epoch 7 finished - avarage train loss 0.008448489451909373  avarage test loss 0.01744019240140915\n",
      "Training: epoch 8 batch 0 loss 0.009437700733542442\n",
      "Training: epoch 8 batch 10 loss 0.003995063249021769\n",
      "Training: epoch 8 batch 20 loss 0.010022463276982307\n",
      "Test: epoch 8 batch 0 loss 0.012508628889918327\n",
      "epoch 8 finished - avarage train loss 0.00828789518985512  avarage test loss 0.014496736461296678\n",
      "Training: epoch 9 batch 0 loss 0.013571548275649548\n",
      "Training: epoch 9 batch 10 loss 0.0107651986181736\n",
      "Training: epoch 9 batch 20 loss 0.005394582636654377\n",
      "Test: epoch 9 batch 0 loss 0.015421569347381592\n",
      "epoch 9 finished - avarage train loss 0.00837206376459578  avarage test loss 0.01607510889880359\n",
      "Training: epoch 10 batch 0 loss 0.008412312716245651\n",
      "Training: epoch 10 batch 10 loss 0.0061289588920772076\n",
      "Training: epoch 10 batch 20 loss 0.004204584285616875\n",
      "Test: epoch 10 batch 0 loss 0.014879359863698483\n",
      "epoch 10 finished - avarage train loss 0.008954976837889388  avarage test loss 0.02175392536446452\n",
      "Training: epoch 11 batch 0 loss 0.007943605072796345\n",
      "Training: epoch 11 batch 10 loss 0.004640597850084305\n",
      "Training: epoch 11 batch 20 loss 0.006100424099713564\n",
      "Test: epoch 11 batch 0 loss 0.013342794962227345\n",
      "epoch 11 finished - avarage train loss 0.008407724250493378  avarage test loss 0.01859918620903045\n",
      "Training: epoch 12 batch 0 loss 0.004118334501981735\n",
      "Training: epoch 12 batch 10 loss 0.006564321927726269\n",
      "Training: epoch 12 batch 20 loss 0.0076105124317109585\n",
      "Test: epoch 12 batch 0 loss 0.01431797444820404\n",
      "epoch 12 finished - avarage train loss 0.00828277913789297  avarage test loss 0.01739553001243621\n",
      "Training: epoch 13 batch 0 loss 0.01216545607894659\n",
      "Training: epoch 13 batch 10 loss 0.009690068662166595\n",
      "Training: epoch 13 batch 20 loss 0.00644290866330266\n",
      "Test: epoch 13 batch 0 loss 0.013118698261678219\n",
      "epoch 13 finished - avarage train loss 0.007830536129466933  avarage test loss 0.014957279432564974\n",
      "Training: epoch 14 batch 0 loss 0.003165735164657235\n",
      "Training: epoch 14 batch 10 loss 0.004111621528863907\n",
      "Training: epoch 14 batch 20 loss 0.009227131493389606\n",
      "Test: epoch 14 batch 0 loss 0.014208500273525715\n",
      "epoch 14 finished - avarage train loss 0.006069390408309369  avarage test loss 0.01793945033568889\n",
      "Training: epoch 15 batch 0 loss 0.003913239575922489\n",
      "Training: epoch 15 batch 10 loss 0.0039010443724691868\n",
      "Training: epoch 15 batch 20 loss 0.005346293561160564\n",
      "Test: epoch 15 batch 0 loss 0.015482030808925629\n",
      "epoch 15 finished - avarage train loss 0.008205753156979537  avarage test loss 0.018538475036621094\n",
      "Training: epoch 16 batch 0 loss 0.005993532948195934\n",
      "Training: epoch 16 batch 10 loss 0.006473619025200605\n",
      "Training: epoch 16 batch 20 loss 0.00916537456214428\n",
      "Test: epoch 16 batch 0 loss 0.010988283902406693\n",
      "epoch 16 finished - avarage train loss 0.007702553283487414  avarage test loss 0.01896068616770208\n",
      "Training: epoch 17 batch 0 loss 0.009327770210802555\n",
      "Training: epoch 17 batch 10 loss 0.010558129288256168\n",
      "Training: epoch 17 batch 20 loss 0.00666468171402812\n",
      "Test: epoch 17 batch 0 loss 0.013816882856190205\n",
      "epoch 17 finished - avarage train loss 0.008656362930698127  avarage test loss 0.015499320346862078\n",
      "Training: epoch 18 batch 0 loss 0.01232624240219593\n",
      "Training: epoch 18 batch 10 loss 0.007781890220940113\n",
      "Training: epoch 18 batch 20 loss 0.0044740233570337296\n",
      "Test: epoch 18 batch 0 loss 0.012724095955491066\n",
      "epoch 18 finished - avarage train loss 0.008569002902167368  avarage test loss 0.014347855118103325\n",
      "Training: epoch 19 batch 0 loss 0.004763106815516949\n",
      "Training: epoch 19 batch 10 loss 0.003936245106160641\n",
      "Training: epoch 19 batch 20 loss 0.0032194529194384813\n",
      "Test: epoch 19 batch 0 loss 0.012335783801972866\n",
      "epoch 19 finished - avarage train loss 0.008277273889438346  avarage test loss 0.015161649440415204\n",
      "Training: epoch 20 batch 0 loss 0.004111245274543762\n",
      "Training: epoch 20 batch 10 loss 0.006039176601916552\n",
      "Training: epoch 20 batch 20 loss 0.008812876418232918\n",
      "Test: epoch 20 batch 0 loss 0.017823657020926476\n",
      "epoch 20 finished - avarage train loss 0.008583475289673641  avarage test loss 0.020592809189110994\n",
      "Training: epoch 21 batch 0 loss 0.0075081889517605305\n",
      "Training: epoch 21 batch 10 loss 0.0027391095645725727\n",
      "Training: epoch 21 batch 20 loss 0.015732351690530777\n",
      "Test: epoch 21 batch 0 loss 0.015375738963484764\n",
      "epoch 21 finished - avarage train loss 0.007568710114293057  avarage test loss 0.017114903777837753\n",
      "Training: epoch 22 batch 0 loss 0.009784705936908722\n",
      "Training: epoch 22 batch 10 loss 0.005614538211375475\n",
      "Training: epoch 22 batch 20 loss 0.012466499581933022\n",
      "Test: epoch 22 batch 0 loss 0.02107664756476879\n",
      "epoch 22 finished - avarage train loss 0.007803441812123718  avarage test loss 0.020073344698175788\n",
      "Training: epoch 23 batch 0 loss 0.010219625197350979\n",
      "Training: epoch 23 batch 10 loss 0.01092111598700285\n",
      "Training: epoch 23 batch 20 loss 0.006282494403421879\n",
      "Test: epoch 23 batch 0 loss 0.016284547746181488\n",
      "epoch 23 finished - avarage train loss 0.007191043852924787  avarage test loss 0.01869447308126837\n",
      "Training: epoch 24 batch 0 loss 0.006011248100548983\n",
      "Training: epoch 24 batch 10 loss 0.006536718457937241\n",
      "Training: epoch 24 batch 20 loss 0.004547337535768747\n",
      "Test: epoch 24 batch 0 loss 0.020592639222741127\n",
      "epoch 24 finished - avarage train loss 0.006768669571791743  avarage test loss 0.017056192504242063\n",
      "Training: epoch 25 batch 0 loss 0.008289938792586327\n",
      "Training: epoch 25 batch 10 loss 0.006522870156913996\n",
      "Training: epoch 25 batch 20 loss 0.008928283117711544\n",
      "Test: epoch 25 batch 0 loss 0.015224173665046692\n",
      "epoch 25 finished - avarage train loss 0.007605641301528647  avarage test loss 0.018572965171188116\n",
      "Training: epoch 26 batch 0 loss 0.016237469390034676\n",
      "Training: epoch 26 batch 10 loss 0.009094749577343464\n",
      "Training: epoch 26 batch 20 loss 0.011975813657045364\n",
      "Test: epoch 26 batch 0 loss 0.019984183833003044\n",
      "epoch 26 finished - avarage train loss 0.009742855543977228  avarage test loss 0.018404347589239478\n",
      "Training: epoch 27 batch 0 loss 0.013549068942666054\n",
      "Training: epoch 27 batch 10 loss 0.0035915132611989975\n",
      "Training: epoch 27 batch 20 loss 0.00682426244020462\n",
      "Test: epoch 27 batch 0 loss 0.02167210541665554\n",
      "epoch 27 finished - avarage train loss 0.009900527869768697  avarage test loss 0.02060193568468094\n",
      "Training: epoch 28 batch 0 loss 0.007029044441878796\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training: epoch 28 batch 10 loss 0.013228220865130424\n",
      "Training: epoch 28 batch 20 loss 0.006070566363632679\n",
      "Test: epoch 28 batch 0 loss 0.017024841159582138\n",
      "epoch 28 finished - avarage train loss 0.013882953689658436  avarage test loss 0.018758974270895123\n",
      "Training: epoch 29 batch 0 loss 0.007323897443711758\n",
      "Training: epoch 29 batch 10 loss 0.002524087205529213\n",
      "Training: epoch 29 batch 20 loss 0.008286064490675926\n",
      "Test: epoch 29 batch 0 loss 0.016069408506155014\n",
      "epoch 29 finished - avarage train loss 0.007599582100800913  avarage test loss 0.016017678892239928\n",
      "Training: epoch 30 batch 0 loss 0.0026284032501280308\n",
      "Training: epoch 30 batch 10 loss 0.0025577035266906023\n",
      "Training: epoch 30 batch 20 loss 0.004371720366179943\n",
      "Test: epoch 30 batch 0 loss 0.015329928137362003\n",
      "epoch 30 finished - avarage train loss 0.007316525383242245  avarage test loss 0.020567144732922316\n",
      "Training: epoch 31 batch 0 loss 0.007602439261972904\n",
      "Training: epoch 31 batch 10 loss 0.014409455470740795\n",
      "Training: epoch 31 batch 20 loss 0.006367184221744537\n",
      "Test: epoch 31 batch 0 loss 0.01556001603603363\n",
      "epoch 31 finished - avarage train loss 0.010003540809426842  avarage test loss 0.015080713317729533\n",
      "Training: epoch 32 batch 0 loss 0.0049377260729670525\n",
      "Training: epoch 32 batch 10 loss 0.00984808336943388\n",
      "Training: epoch 32 batch 20 loss 0.012147244065999985\n",
      "Test: epoch 32 batch 0 loss 0.01343381218612194\n",
      "epoch 32 finished - avarage train loss 0.011093139616322929  avarage test loss 0.015695461886934936\n",
      "Training: epoch 33 batch 0 loss 0.009861234575510025\n",
      "Training: epoch 33 batch 10 loss 0.014210074208676815\n",
      "Training: epoch 33 batch 20 loss 0.0030681733042001724\n",
      "Test: epoch 33 batch 0 loss 0.010307811200618744\n",
      "epoch 33 finished - avarage train loss 0.009529123998022285  avarage test loss 0.012397407554090023\n",
      "Training: epoch 34 batch 0 loss 0.005687672179192305\n",
      "Training: epoch 34 batch 10 loss 0.0046555278822779655\n",
      "Training: epoch 34 batch 20 loss 0.0032538266386836767\n",
      "Test: epoch 34 batch 0 loss 0.009239878505468369\n",
      "epoch 34 finished - avarage train loss 0.008605975312469849  avarage test loss 0.011936204391531646\n",
      "Training: epoch 35 batch 0 loss 0.008253775537014008\n",
      "Training: epoch 35 batch 10 loss 0.005644974298775196\n",
      "Training: epoch 35 batch 20 loss 0.006963463965803385\n",
      "Test: epoch 35 batch 0 loss 0.014729958958923817\n",
      "epoch 35 finished - avarage train loss 0.008933607769873122  avarage test loss 0.018918307730928063\n",
      "Training: epoch 36 batch 0 loss 0.006741415709257126\n",
      "Training: epoch 36 batch 10 loss 0.03415827080607414\n",
      "Training: epoch 36 batch 20 loss 0.02160288207232952\n",
      "Test: epoch 36 batch 0 loss 0.01458317507058382\n",
      "epoch 36 finished - avarage train loss 0.016949971174371654  avarage test loss 0.020362803945317864\n",
      "Training: epoch 37 batch 0 loss 0.014447048306465149\n",
      "Training: epoch 37 batch 10 loss 0.018669474869966507\n",
      "Training: epoch 37 batch 20 loss 0.013565420173108578\n",
      "Test: epoch 37 batch 0 loss 0.018197674304246902\n",
      "epoch 37 finished - avarage train loss 0.011975752037211225  avarage test loss 0.022282013203948736\n",
      "Training: epoch 38 batch 0 loss 0.015688268467783928\n",
      "Training: epoch 38 batch 10 loss 0.007285573519766331\n",
      "Training: epoch 38 batch 20 loss 0.004418438300490379\n",
      "Test: epoch 38 batch 0 loss 0.009611316956579685\n",
      "epoch 38 finished - avarage train loss 0.009439552812997637  avarage test loss 0.015120340278372169\n",
      "Training: epoch 39 batch 0 loss 0.01096273958683014\n",
      "Training: epoch 39 batch 10 loss 0.010271086357533932\n",
      "Training: epoch 39 batch 20 loss 0.006289622746407986\n",
      "Test: epoch 39 batch 0 loss 0.009525373578071594\n",
      "epoch 39 finished - avarage train loss 0.0087413615065402  avarage test loss 0.012396236532367766\n",
      "Training: epoch 40 batch 0 loss 0.005333839450031519\n",
      "Training: epoch 40 batch 10 loss 0.008504221215844154\n",
      "Training: epoch 40 batch 20 loss 0.005039081908762455\n",
      "Test: epoch 40 batch 0 loss 0.009407379664480686\n",
      "epoch 40 finished - avarage train loss 0.0072973927338447035  avarage test loss 0.01307641842868179\n",
      "Training: epoch 41 batch 0 loss 0.009559600614011288\n",
      "Training: epoch 41 batch 10 loss 0.003320879302918911\n",
      "Training: epoch 41 batch 20 loss 0.010407185181975365\n",
      "Test: epoch 41 batch 0 loss 0.008770141750574112\n",
      "epoch 41 finished - avarage train loss 0.008457359295852226  avarage test loss 0.012762498343363404\n",
      "Training: epoch 42 batch 0 loss 0.0068162912502884865\n",
      "Training: epoch 42 batch 10 loss 0.006202606484293938\n",
      "Training: epoch 42 batch 20 loss 0.005586718674749136\n",
      "Test: epoch 42 batch 0 loss 0.012729214504361153\n",
      "epoch 42 finished - avarage train loss 0.008276340138199258  avarage test loss 0.014540401520207524\n",
      "Training: epoch 43 batch 0 loss 0.00364086520858109\n",
      "Training: epoch 43 batch 10 loss 0.004584646318107843\n",
      "Training: epoch 43 batch 20 loss 0.0022446142975240946\n",
      "Test: epoch 43 batch 0 loss 0.009087568148970604\n",
      "epoch 43 finished - avarage train loss 0.006823303747986411  avarage test loss 0.014460161910392344\n",
      "Training: epoch 44 batch 0 loss 0.008672928437590599\n",
      "Training: epoch 44 batch 10 loss 0.005511222407221794\n",
      "Training: epoch 44 batch 20 loss 0.001747140777297318\n",
      "Test: epoch 44 batch 0 loss 0.014268058352172375\n",
      "epoch 44 finished - avarage train loss 0.008641488741730052  avarage test loss 0.015700267627835274\n",
      "Training: epoch 45 batch 0 loss 0.009228512644767761\n",
      "Training: epoch 45 batch 10 loss 0.006144543644040823\n",
      "Training: epoch 45 batch 20 loss 0.003518073121085763\n",
      "Test: epoch 45 batch 0 loss 0.012385386042296886\n",
      "epoch 45 finished - avarage train loss 0.006614161976452531  avarage test loss 0.013875217409804463\n",
      "Training: epoch 46 batch 0 loss 0.01242559589445591\n",
      "Training: epoch 46 batch 10 loss 0.006134325172752142\n",
      "Training: epoch 46 batch 20 loss 0.005727660842239857\n",
      "Test: epoch 46 batch 0 loss 0.00911792367696762\n",
      "epoch 46 finished - avarage train loss 0.006954378472512652  avarage test loss 0.014373730402439833\n",
      "Training: epoch 47 batch 0 loss 0.00766880065202713\n",
      "Training: epoch 47 batch 10 loss 0.005323824007064104\n",
      "Training: epoch 47 batch 20 loss 0.008804285898804665\n",
      "Test: epoch 47 batch 0 loss 0.012226085178554058\n",
      "epoch 47 finished - avarage train loss 0.009505182673255431  avarage test loss 0.013733495492488146\n",
      "Training: epoch 48 batch 0 loss 0.014795455150306225\n",
      "Training: epoch 48 batch 10 loss 0.0035341614857316017\n",
      "Training: epoch 48 batch 20 loss 0.003736576996743679\n",
      "Test: epoch 48 batch 0 loss 0.014943433925509453\n",
      "epoch 48 finished - avarage train loss 0.008098827538498003  avarage test loss 0.016150415875017643\n",
      "Training: epoch 49 batch 0 loss 0.012706292793154716\n",
      "Training: epoch 49 batch 10 loss 0.01466467697173357\n",
      "Training: epoch 49 batch 20 loss 0.008086979389190674\n",
      "Test: epoch 49 batch 0 loss 0.011625056155025959\n",
      "epoch 49 finished - avarage train loss 0.008452850474237368  avarage test loss 0.013467997079715133\n",
      "Training: epoch 50 batch 0 loss 0.0060553159564733505\n",
      "Training: epoch 50 batch 10 loss 0.008128337562084198\n",
      "Training: epoch 50 batch 20 loss 0.003241088707000017\n",
      "Test: epoch 50 batch 0 loss 0.011478081345558167\n",
      "epoch 50 finished - avarage train loss 0.007346967219150272  avarage test loss 0.015258873347193003\n",
      "Training: epoch 51 batch 0 loss 0.005218966864049435\n",
      "Training: epoch 51 batch 10 loss 0.008031315170228481\n",
      "Training: epoch 51 batch 20 loss 0.009667163714766502\n",
      "Test: epoch 51 batch 0 loss 0.015719469636678696\n",
      "epoch 51 finished - avarage train loss 0.010390685036264617  avarage test loss 0.016371325124055147\n",
      "Training: epoch 52 batch 0 loss 0.008096158504486084\n",
      "Training: epoch 52 batch 10 loss 0.006049129646271467\n",
      "Training: epoch 52 batch 20 loss 0.004863339941948652\n",
      "Test: epoch 52 batch 0 loss 0.013603436760604382\n",
      "epoch 52 finished - avarage train loss 0.0065507335299304845  avarage test loss 0.014524214551784098\n",
      "Training: epoch 53 batch 0 loss 0.0014734811848029494\n",
      "Training: epoch 53 batch 10 loss 0.012516655959188938\n",
      "Training: epoch 53 batch 20 loss 0.007383256684988737\n",
      "Test: epoch 53 batch 0 loss 0.011603690683841705\n",
      "epoch 53 finished - avarage train loss 0.006819474908266345  avarage test loss 0.013186071650125086\n",
      "Training: epoch 54 batch 0 loss 0.0037295478396117687\n",
      "Training: epoch 54 batch 10 loss 0.007091409992426634\n",
      "Training: epoch 54 batch 20 loss 0.002670254558324814\n",
      "Test: epoch 54 batch 0 loss 0.011840442195534706\n",
      "epoch 54 finished - avarage train loss 0.006954862455163023  avarage test loss 0.012998139136470854\n",
      "Training: epoch 55 batch 0 loss 0.0057500991970300674\n",
      "Training: epoch 55 batch 10 loss 0.004444957245141268\n",
      "Training: epoch 55 batch 20 loss 0.005192325916141272\n",
      "Test: epoch 55 batch 0 loss 0.0150093212723732\n",
      "epoch 55 finished - avarage train loss 0.00955243388608355  avarage test loss 0.015298061771318316\n",
      "Training: epoch 56 batch 0 loss 0.006642381194978952\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training: epoch 56 batch 10 loss 0.006797271780669689\n",
      "Training: epoch 56 batch 20 loss 0.0014953997451812029\n",
      "Test: epoch 56 batch 0 loss 0.013074476271867752\n",
      "epoch 56 finished - avarage train loss 0.007330689659534857  avarage test loss 0.013878089608624578\n",
      "Training: epoch 57 batch 0 loss 0.008442454040050507\n",
      "Training: epoch 57 batch 10 loss 0.008689203299582005\n",
      "Training: epoch 57 batch 20 loss 0.008260846138000488\n",
      "Test: epoch 57 batch 0 loss 0.015359115786850452\n",
      "epoch 57 finished - avarage train loss 0.007994303351332402  avarage test loss 0.01717477012425661\n",
      "Training: epoch 58 batch 0 loss 0.0066860574297606945\n",
      "Training: epoch 58 batch 10 loss 0.010626566596329212\n",
      "Training: epoch 58 batch 20 loss 0.004048692528158426\n",
      "Test: epoch 58 batch 0 loss 0.010818760842084885\n",
      "epoch 58 finished - avarage train loss 0.012675982434302568  avarage test loss 0.015534335281699896\n",
      "Training: epoch 59 batch 0 loss 0.0075156730599701405\n",
      "Training: epoch 59 batch 10 loss 0.008390998467803001\n",
      "Training: epoch 59 batch 20 loss 0.004710060544312\n",
      "Test: epoch 59 batch 0 loss 0.010902171023190022\n",
      "epoch 59 finished - avarage train loss 0.008861662434606716  avarage test loss 0.013558031409047544\n",
      "Training: epoch 60 batch 0 loss 0.005447505507618189\n",
      "Training: epoch 60 batch 10 loss 0.0053792293183505535\n",
      "Training: epoch 60 batch 20 loss 0.004556300118565559\n",
      "Test: epoch 60 batch 0 loss 0.014630351215600967\n",
      "epoch 60 finished - avarage train loss 0.006987641469157976  avarage test loss 0.015111113199964166\n",
      "Training: epoch 61 batch 0 loss 0.006511338986456394\n",
      "Training: epoch 61 batch 10 loss 0.006694605574011803\n",
      "Training: epoch 61 batch 20 loss 0.0030483726877719164\n",
      "Test: epoch 61 batch 0 loss 0.016582168638706207\n",
      "epoch 61 finished - avarage train loss 0.007430393818830107  avarage test loss 0.018251769244670868\n",
      "Training: epoch 62 batch 0 loss 0.009097078815102577\n",
      "Training: epoch 62 batch 10 loss 0.011122438125312328\n",
      "Training: epoch 62 batch 20 loss 0.005547048524022102\n",
      "Test: epoch 62 batch 0 loss 0.012199323624372482\n",
      "epoch 62 finished - avarage train loss 0.008151417782787105  avarage test loss 0.015557780163362622\n",
      "Training: epoch 63 batch 0 loss 0.006145354826003313\n",
      "Training: epoch 63 batch 10 loss 0.010798219591379166\n",
      "Training: epoch 63 batch 20 loss 0.007931488566100597\n",
      "Test: epoch 63 batch 0 loss 0.010999110527336597\n",
      "epoch 63 finished - avarage train loss 0.008898231265102995  avarage test loss 0.017957558971829712\n",
      "Training: epoch 64 batch 0 loss 0.01997782476246357\n",
      "Training: epoch 64 batch 10 loss 0.005994269624352455\n",
      "Training: epoch 64 batch 20 loss 0.0055784545838832855\n",
      "Test: epoch 64 batch 0 loss 0.015967540442943573\n",
      "epoch 64 finished - avarage train loss 0.010570424528599813  avarage test loss 0.022056386340409517\n",
      "Training: epoch 65 batch 0 loss 0.012611787766218185\n",
      "Training: epoch 65 batch 10 loss 0.009515061974525452\n",
      "Training: epoch 65 batch 20 loss 0.007688337005674839\n",
      "Test: epoch 65 batch 0 loss 0.014692923054099083\n",
      "epoch 65 finished - avarage train loss 0.010503300610158977  avarage test loss 0.016231671208515763\n",
      "Training: epoch 66 batch 0 loss 0.0034333504736423492\n",
      "Training: epoch 66 batch 10 loss 0.003611217252910137\n",
      "Training: epoch 66 batch 20 loss 0.0029369574040174484\n",
      "Test: epoch 66 batch 0 loss 0.013289553113281727\n",
      "epoch 66 finished - avarage train loss 0.006686740177522959  avarage test loss 0.015511842560954392\n",
      "Training: epoch 67 batch 0 loss 0.006376173347234726\n",
      "Training: epoch 67 batch 10 loss 0.009152337908744812\n",
      "Training: epoch 67 batch 20 loss 0.004154408350586891\n",
      "Test: epoch 67 batch 0 loss 0.013441881164908409\n",
      "epoch 67 finished - avarage train loss 0.007196478256634597  avarage test loss 0.01711723452899605\n",
      "Training: epoch 68 batch 0 loss 0.007521045859903097\n",
      "Training: epoch 68 batch 10 loss 0.006788464263081551\n",
      "Training: epoch 68 batch 20 loss 0.0035959952510893345\n",
      "Test: epoch 68 batch 0 loss 0.01503050047904253\n",
      "epoch 68 finished - avarage train loss 0.0062489605845947715  avarage test loss 0.01755529921501875\n",
      "Training: epoch 69 batch 0 loss 0.004287202842533588\n",
      "Training: epoch 69 batch 10 loss 0.007312246132642031\n",
      "Training: epoch 69 batch 20 loss 0.005347692407667637\n",
      "Test: epoch 69 batch 0 loss 0.0161665678024292\n",
      "epoch 69 finished - avarage train loss 0.00840822555092645  avarage test loss 0.016660518711432815\n",
      "Training: epoch 70 batch 0 loss 0.008521253243088722\n",
      "Training: epoch 70 batch 10 loss 0.005254068877547979\n",
      "Training: epoch 70 batch 20 loss 0.0018905730685219169\n",
      "Test: epoch 70 batch 0 loss 0.01611972600221634\n",
      "epoch 70 finished - avarage train loss 0.007023484128976947  avarage test loss 0.016703049885109067\n",
      "Training: epoch 71 batch 0 loss 0.0034991996362805367\n",
      "Training: epoch 71 batch 10 loss 0.008963150903582573\n",
      "Training: epoch 71 batch 20 loss 0.002531765727326274\n",
      "Test: epoch 71 batch 0 loss 0.020933188498020172\n",
      "epoch 71 finished - avarage train loss 0.007659965447664004  avarage test loss 0.026652065571397543\n",
      "Training: epoch 72 batch 0 loss 0.02207200974225998\n",
      "Training: epoch 72 batch 10 loss 0.013557292520999908\n",
      "Training: epoch 72 batch 20 loss 0.006398186553269625\n",
      "Test: epoch 72 batch 0 loss 0.014642678201198578\n",
      "epoch 72 finished - avarage train loss 0.009999210400314167  avarage test loss 0.015605719294399023\n",
      "Training: epoch 73 batch 0 loss 0.004263053648173809\n",
      "Training: epoch 73 batch 10 loss 0.010317671112716198\n",
      "Training: epoch 73 batch 20 loss 0.005049681290984154\n",
      "Test: epoch 73 batch 0 loss 0.016906961798667908\n",
      "epoch 73 finished - avarage train loss 0.0073578338829222425  avarage test loss 0.01854900410398841\n",
      "Training: epoch 74 batch 0 loss 0.007368359249085188\n",
      "Training: epoch 74 batch 10 loss 0.006499926559627056\n",
      "Training: epoch 74 batch 20 loss 0.013787122443318367\n",
      "Test: epoch 74 batch 0 loss 0.016643289476633072\n",
      "epoch 74 finished - avarage train loss 0.008545031193001517  avarage test loss 0.016980149783194065\n",
      "Training: epoch 75 batch 0 loss 0.007487442810088396\n",
      "Training: epoch 75 batch 10 loss 0.009472174569964409\n",
      "Training: epoch 75 batch 20 loss 0.004406869877129793\n",
      "Test: epoch 75 batch 0 loss 0.014249956235289574\n",
      "epoch 75 finished - avarage train loss 0.009533271542750299  avarage test loss 0.014518453390337527\n",
      "Training: epoch 76 batch 0 loss 0.010569565929472446\n",
      "Training: epoch 76 batch 10 loss 0.004614782053977251\n",
      "Training: epoch 76 batch 20 loss 0.011580347083508968\n",
      "Test: epoch 76 batch 0 loss 0.016264135017991066\n",
      "epoch 76 finished - avarage train loss 0.00782951527949551  avarage test loss 0.016662803711369634\n",
      "Training: epoch 77 batch 0 loss 0.006963097956031561\n",
      "Training: epoch 77 batch 10 loss 0.008273236453533173\n",
      "Training: epoch 77 batch 20 loss 0.005111860577017069\n",
      "Test: epoch 77 batch 0 loss 0.01652311347424984\n",
      "epoch 77 finished - avarage train loss 0.006671623375009874  avarage test loss 0.01897330512292683\n",
      "Training: epoch 78 batch 0 loss 0.007614757400006056\n",
      "Training: epoch 78 batch 10 loss 0.008570385165512562\n",
      "Training: epoch 78 batch 20 loss 0.0040014926344156265\n",
      "Test: epoch 78 batch 0 loss 0.014324268326163292\n",
      "epoch 78 finished - avarage train loss 0.010382524587149763  avarage test loss 0.01507067121565342\n",
      "Training: epoch 79 batch 0 loss 0.0033692747820168734\n",
      "Training: epoch 79 batch 10 loss 0.008724154904484749\n",
      "Training: epoch 79 batch 20 loss 0.0032097315415740013\n",
      "Test: epoch 79 batch 0 loss 0.015492458827793598\n",
      "epoch 79 finished - avarage train loss 0.006805115467708172  avarage test loss 0.015345363994129002\n",
      "Training: epoch 80 batch 0 loss 0.006115306168794632\n",
      "Training: epoch 80 batch 10 loss 0.00932934507727623\n",
      "Training: epoch 80 batch 20 loss 0.004951434209942818\n",
      "Test: epoch 80 batch 0 loss 0.014123165979981422\n",
      "epoch 80 finished - avarage train loss 0.006979275117467703  avarage test loss 0.018724559573456645\n",
      "Training: epoch 81 batch 0 loss 0.005946232005953789\n",
      "Training: epoch 81 batch 10 loss 0.008573467843234539\n",
      "Training: epoch 81 batch 20 loss 0.007405686192214489\n",
      "Test: epoch 81 batch 0 loss 0.010600696317851543\n",
      "epoch 81 finished - avarage train loss 0.00753785897014213  avarage test loss 0.013124082121066749\n",
      "Training: epoch 82 batch 0 loss 0.004935771226882935\n",
      "Training: epoch 82 batch 10 loss 0.003854035632684827\n",
      "Training: epoch 82 batch 20 loss 0.007536747492849827\n",
      "Test: epoch 82 batch 0 loss 0.013855630531907082\n",
      "epoch 82 finished - avarage train loss 0.00737965924279957  avarage test loss 0.014307107659988105\n",
      "Training: epoch 83 batch 0 loss 0.007642956916242838\n",
      "Training: epoch 83 batch 10 loss 0.007018546108156443\n",
      "Training: epoch 83 batch 20 loss 0.006338310427963734\n",
      "Test: epoch 83 batch 0 loss 0.013623080216348171\n",
      "epoch 83 finished - avarage train loss 0.007494594259508725  avarage test loss 0.014120172359980643\n",
      "Training: epoch 84 batch 0 loss 0.00429306086152792\n",
      "Training: epoch 84 batch 10 loss 0.006547140888869762\n",
      "Training: epoch 84 batch 20 loss 0.0018288468709215522\n",
      "Test: epoch 84 batch 0 loss 0.014196420088410378\n",
      "epoch 84 finished - avarage train loss 0.00837433031069693  avarage test loss 0.013968799612484872\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training: epoch 85 batch 0 loss 0.003067314624786377\n",
      "Training: epoch 85 batch 10 loss 0.0016412180848419666\n",
      "Training: epoch 85 batch 20 loss 0.006302244495600462\n",
      "Test: epoch 85 batch 0 loss 0.01343118492513895\n",
      "epoch 85 finished - avarage train loss 0.007219912042712857  avarage test loss 0.013513531535863876\n",
      "Training: epoch 86 batch 0 loss 0.0022852581460028887\n",
      "Training: epoch 86 batch 10 loss 0.007489379961043596\n",
      "Training: epoch 86 batch 20 loss 0.003887064289301634\n",
      "Test: epoch 86 batch 0 loss 0.01869245059788227\n",
      "epoch 86 finished - avarage train loss 0.007066436946905893  avarage test loss 0.01924050599336624\n",
      "Training: epoch 87 batch 0 loss 0.010701825842261314\n",
      "Training: epoch 87 batch 10 loss 0.009670414961874485\n",
      "Training: epoch 87 batch 20 loss 0.004877867177128792\n",
      "Test: epoch 87 batch 0 loss 0.016665775328874588\n",
      "epoch 87 finished - avarage train loss 0.012751248745440409  avarage test loss 0.017708923434838653\n",
      "Training: epoch 88 batch 0 loss 0.008153417147696018\n",
      "Training: epoch 88 batch 10 loss 0.004050869029015303\n",
      "Training: epoch 88 batch 20 loss 0.005826531443744898\n",
      "Test: epoch 88 batch 0 loss 0.014796057716012001\n",
      "epoch 88 finished - avarage train loss 0.007509282958725917  avarage test loss 0.0165601649787277\n",
      "Training: epoch 89 batch 0 loss 0.004288136027753353\n",
      "Training: epoch 89 batch 10 loss 0.008577097207307816\n",
      "Training: epoch 89 batch 20 loss 0.0067711928859353065\n",
      "Test: epoch 89 batch 0 loss 0.014148819260299206\n",
      "epoch 89 finished - avarage train loss 0.00862388967151015  avarage test loss 0.014343376737087965\n",
      "Training: epoch 90 batch 0 loss 0.006074432283639908\n",
      "Training: epoch 90 batch 10 loss 0.00382126378826797\n",
      "Training: epoch 90 batch 20 loss 0.004599302541464567\n",
      "Test: epoch 90 batch 0 loss 0.013391140848398209\n",
      "epoch 90 finished - avarage train loss 0.007345449267726006  avarage test loss 0.013867422821931541\n",
      "Training: epoch 91 batch 0 loss 0.006334179081022739\n",
      "Training: epoch 91 batch 10 loss 0.004406922031193972\n",
      "Training: epoch 91 batch 20 loss 0.006270634941756725\n",
      "Test: epoch 91 batch 0 loss 0.012468710541725159\n",
      "epoch 91 finished - avarage train loss 0.008122977527693427  avarage test loss 0.014990906463935971\n",
      "Training: epoch 92 batch 0 loss 0.00704285129904747\n",
      "Training: epoch 92 batch 10 loss 0.011288726702332497\n",
      "Training: epoch 92 batch 20 loss 0.007759889122098684\n",
      "Test: epoch 92 batch 0 loss 0.02396123670041561\n",
      "epoch 92 finished - avarage train loss 0.010083356390482393  avarage test loss 0.02562617277726531\n",
      "Training: epoch 93 batch 0 loss 0.034610167145729065\n",
      "Training: epoch 93 batch 10 loss 0.01358381099998951\n",
      "Training: epoch 93 batch 20 loss 0.007440178655087948\n",
      "Test: epoch 93 batch 0 loss 0.013383051380515099\n",
      "epoch 93 finished - avarage train loss 0.011479459333651024  avarage test loss 0.016188109759241343\n",
      "Training: epoch 94 batch 0 loss 0.009779773652553558\n",
      "Training: epoch 94 batch 10 loss 0.005353487096726894\n",
      "Training: epoch 94 batch 20 loss 0.006096929777413607\n",
      "Test: epoch 94 batch 0 loss 0.014138299971818924\n",
      "epoch 94 finished - avarage train loss 0.007405967002028021  avarage test loss 0.014407219714485109\n",
      "Training: epoch 95 batch 0 loss 0.0022187726572155952\n",
      "Training: epoch 95 batch 10 loss 0.010375871323049068\n",
      "Training: epoch 95 batch 20 loss 0.013919317163527012\n",
      "Test: epoch 95 batch 0 loss 0.017204713076353073\n",
      "epoch 95 finished - avarage train loss 0.008799532043008968  avarage test loss 0.01792113180272281\n",
      "Training: epoch 96 batch 0 loss 0.004127555061131716\n",
      "Training: epoch 96 batch 10 loss 0.004105526953935623\n",
      "Training: epoch 96 batch 20 loss 0.0042952862568199635\n",
      "Test: epoch 96 batch 0 loss 0.02266806736588478\n",
      "epoch 96 finished - avarage train loss 0.006776538709628171  avarage test loss 0.02334217820316553\n",
      "Training: epoch 97 batch 0 loss 0.01439393125474453\n",
      "Training: epoch 97 batch 10 loss 0.004003818146884441\n",
      "Training: epoch 97 batch 20 loss 0.006175030954182148\n",
      "Test: epoch 97 batch 0 loss 0.014621899463236332\n",
      "epoch 97 finished - avarage train loss 0.009754810875667066  avarage test loss 0.01497541880235076\n",
      "Training: epoch 98 batch 0 loss 0.00503279734402895\n",
      "Training: epoch 98 batch 10 loss 0.007248622365295887\n",
      "Training: epoch 98 batch 20 loss 0.007268485613167286\n",
      "Test: epoch 98 batch 0 loss 0.015253045596182346\n",
      "epoch 98 finished - avarage train loss 0.0067015629149331105  avarage test loss 0.016718412283807993\n",
      "Training: epoch 99 batch 0 loss 0.005992562044411898\n",
      "Training: epoch 99 batch 10 loss 0.007612983230501413\n",
      "Training: epoch 99 batch 20 loss 0.007091968320310116\n",
      "Test: epoch 99 batch 0 loss 0.012467567808926105\n",
      "epoch 99 finished - avarage train loss 0.0065854110209078625  avarage test loss 0.013232648372650146\n",
      "Training: epoch 100 batch 0 loss 0.0044888402335345745\n",
      "Training: epoch 100 batch 10 loss 0.004697531461715698\n",
      "Training: epoch 100 batch 20 loss 0.005462646484375\n",
      "Test: epoch 100 batch 0 loss 0.012752849608659744\n",
      "epoch 100 finished - avarage train loss 0.007496660213981723  avarage test loss 0.013704121112823486\n",
      "Training: epoch 101 batch 0 loss 0.00392756424844265\n",
      "Training: epoch 101 batch 10 loss 0.006407144945114851\n",
      "Training: epoch 101 batch 20 loss 0.005753029137849808\n",
      "Test: epoch 101 batch 0 loss 0.012644228525459766\n",
      "epoch 101 finished - avarage train loss 0.007986336818029141  avarage test loss 0.013332161353901029\n",
      "Training: epoch 102 batch 0 loss 0.0098186070099473\n",
      "Training: epoch 102 batch 10 loss 0.003869303036481142\n",
      "Training: epoch 102 batch 20 loss 0.008865629322826862\n",
      "Test: epoch 102 batch 0 loss 0.011933914385735989\n",
      "epoch 102 finished - avarage train loss 0.008412787409756204  avarage test loss 0.013109787716530263\n",
      "Training: epoch 103 batch 0 loss 0.006882179994136095\n",
      "Training: epoch 103 batch 10 loss 0.008538785390555859\n",
      "Training: epoch 103 batch 20 loss 0.005279559176415205\n",
      "Test: epoch 103 batch 0 loss 0.013717682100832462\n",
      "epoch 103 finished - avarage train loss 0.00949392298332833  avarage test loss 0.0147377485409379\n",
      "Training: epoch 104 batch 0 loss 0.006733444985002279\n",
      "Training: epoch 104 batch 10 loss 0.0075709400698542595\n",
      "Training: epoch 104 batch 20 loss 0.008516773581504822\n",
      "Test: epoch 104 batch 0 loss 0.016308294609189034\n",
      "epoch 104 finished - avarage train loss 0.008146243868395686  avarage test loss 0.01898864912800491\n",
      "Training: epoch 105 batch 0 loss 0.008832570165395737\n",
      "Training: epoch 105 batch 10 loss 0.0067188935354352\n",
      "Training: epoch 105 batch 20 loss 0.0019354030955582857\n",
      "Test: epoch 105 batch 0 loss 0.010848921723663807\n",
      "epoch 105 finished - avarage train loss 0.006833485022572608  avarage test loss 0.01303251797799021\n",
      "Training: epoch 106 batch 0 loss 0.006371002644300461\n",
      "Training: epoch 106 batch 10 loss 0.003614847082644701\n",
      "Training: epoch 106 batch 20 loss 0.005024057347327471\n",
      "Test: epoch 106 batch 0 loss 0.012702446430921555\n",
      "epoch 106 finished - avarage train loss 0.006463319416446933  avarage test loss 0.013313626404851675\n",
      "Training: epoch 107 batch 0 loss 0.005601974204182625\n",
      "Training: epoch 107 batch 10 loss 0.004758161958307028\n",
      "Training: epoch 107 batch 20 loss 0.003741564229130745\n",
      "Test: epoch 107 batch 0 loss 0.011918608099222183\n",
      "epoch 107 finished - avarage train loss 0.007272388962707643  avarage test loss 0.012992009054869413\n",
      "Training: epoch 108 batch 0 loss 0.008940771222114563\n",
      "Training: epoch 108 batch 10 loss 0.008540607057511806\n",
      "Training: epoch 108 batch 20 loss 0.0023995591327548027\n",
      "Test: epoch 108 batch 0 loss 0.011730511672794819\n",
      "epoch 108 finished - avarage train loss 0.008015386880664476  avarage test loss 0.014137248042970896\n",
      "Training: epoch 109 batch 0 loss 0.00497153215110302\n",
      "Training: epoch 109 batch 10 loss 0.007311576511710882\n",
      "Training: epoch 109 batch 20 loss 0.006343410350382328\n",
      "Test: epoch 109 batch 0 loss 0.014245560392737389\n",
      "epoch 109 finished - avarage train loss 0.00770149913873395  avarage test loss 0.016127723967656493\n",
      "Training: epoch 110 batch 0 loss 0.0035159504041075706\n",
      "Training: epoch 110 batch 10 loss 0.006142532918602228\n",
      "Training: epoch 110 batch 20 loss 0.008700747042894363\n",
      "Test: epoch 110 batch 0 loss 0.01107410155236721\n",
      "epoch 110 finished - avarage train loss 0.006962026959156682  avarage test loss 0.014257431728765368\n",
      "Training: epoch 111 batch 0 loss 0.004688409622758627\n",
      "Training: epoch 111 batch 10 loss 0.00486210361123085\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training: epoch 111 batch 20 loss 0.00674485694617033\n",
      "Test: epoch 111 batch 0 loss 0.011576288379728794\n",
      "epoch 111 finished - avarage train loss 0.009991497973558205  avarage test loss 0.012920779874548316\n",
      "Training: epoch 112 batch 0 loss 0.006207141559571028\n",
      "Training: epoch 112 batch 10 loss 0.0031442337203770876\n",
      "Training: epoch 112 batch 20 loss 0.0027438243851065636\n",
      "Test: epoch 112 batch 0 loss 0.013711882755160332\n",
      "epoch 112 finished - avarage train loss 0.007324010334459358  avarage test loss 0.014969486044719815\n",
      "Training: epoch 113 batch 0 loss 0.003258145647123456\n",
      "Training: epoch 113 batch 10 loss 0.004091388080269098\n",
      "Training: epoch 113 batch 20 loss 0.007794233039021492\n",
      "Test: epoch 113 batch 0 loss 0.013024095445871353\n",
      "epoch 113 finished - avarage train loss 0.0077181877728937  avarage test loss 0.01391446846537292\n",
      "Training: epoch 114 batch 0 loss 0.013074064627289772\n",
      "Training: epoch 114 batch 10 loss 0.004599754232913256\n",
      "Training: epoch 114 batch 20 loss 0.007596455980092287\n",
      "Test: epoch 114 batch 0 loss 0.012645206414163113\n",
      "epoch 114 finished - avarage train loss 0.006905663959232384  avarage test loss 0.013416623929515481\n",
      "Training: epoch 115 batch 0 loss 0.004723655059933662\n",
      "Training: epoch 115 batch 10 loss 0.0034056762233376503\n",
      "Training: epoch 115 batch 20 loss 0.005703869741410017\n",
      "Test: epoch 115 batch 0 loss 0.01145749632269144\n",
      "epoch 115 finished - avarage train loss 0.006413713796064258  avarage test loss 0.012837087269872427\n",
      "Training: epoch 116 batch 0 loss 0.006133581977337599\n",
      "Training: epoch 116 batch 10 loss 0.003341850358992815\n",
      "Training: epoch 116 batch 20 loss 0.003708166303113103\n",
      "Test: epoch 116 batch 0 loss 0.0121832937002182\n",
      "epoch 116 finished - avarage train loss 0.008287857021686846  avarage test loss 0.013262872933410108\n",
      "Training: epoch 117 batch 0 loss 0.008898078463971615\n",
      "Training: epoch 117 batch 10 loss 0.004782543517649174\n",
      "Training: epoch 117 batch 20 loss 0.003912340849637985\n",
      "Test: epoch 117 batch 0 loss 0.011844856664538383\n",
      "epoch 117 finished - avarage train loss 0.008212339823487503  avarage test loss 0.012979127350263298\n",
      "Training: epoch 118 batch 0 loss 0.004201832693070173\n",
      "Training: epoch 118 batch 10 loss 0.006873280741274357\n",
      "Training: epoch 118 batch 20 loss 0.007384633645415306\n",
      "Test: epoch 118 batch 0 loss 0.012853449210524559\n",
      "epoch 118 finished - avarage train loss 0.006893811105140324  avarage test loss 0.013637957163155079\n",
      "Training: epoch 119 batch 0 loss 0.005429956130683422\n",
      "Training: epoch 119 batch 10 loss 0.002017452148720622\n",
      "Training: epoch 119 batch 20 loss 0.0057182651944458485\n",
      "Test: epoch 119 batch 0 loss 0.012291470542550087\n",
      "epoch 119 finished - avarage train loss 0.006861633492697929  avarage test loss 0.013333125971257687\n",
      "Training: epoch 120 batch 0 loss 0.0031393817625939846\n",
      "Training: epoch 120 batch 10 loss 0.0032793728169053793\n",
      "Training: epoch 120 batch 20 loss 0.0028542426880449057\n",
      "Test: epoch 120 batch 0 loss 0.012297112494707108\n",
      "epoch 120 finished - avarage train loss 0.007776923598079332  avarage test loss 0.013357662945054471\n",
      "Training: epoch 121 batch 0 loss 0.009672149084508419\n",
      "Training: epoch 121 batch 10 loss 0.004742949269711971\n",
      "Training: epoch 121 batch 20 loss 0.006286927033215761\n",
      "Test: epoch 121 batch 0 loss 0.014804939739406109\n",
      "epoch 121 finished - avarage train loss 0.007777481516502027  avarage test loss 0.015883360989391804\n",
      "Training: epoch 122 batch 0 loss 0.006148629356175661\n",
      "Training: epoch 122 batch 10 loss 0.005353896878659725\n",
      "Training: epoch 122 batch 20 loss 0.005722516681998968\n",
      "Test: epoch 122 batch 0 loss 0.011855311691761017\n",
      "epoch 122 finished - avarage train loss 0.007732158799752079  avarage test loss 0.013124532182700932\n",
      "Training: epoch 123 batch 0 loss 0.0031173815950751305\n",
      "Training: epoch 123 batch 10 loss 0.0033485163003206253\n",
      "Training: epoch 123 batch 20 loss 0.010327957570552826\n",
      "Test: epoch 123 batch 0 loss 0.0122652817517519\n",
      "epoch 123 finished - avarage train loss 0.006761031827471894  avarage test loss 0.013131331303156912\n",
      "Training: epoch 124 batch 0 loss 0.005447707138955593\n",
      "Training: epoch 124 batch 10 loss 0.0031137578189373016\n",
      "Training: epoch 124 batch 20 loss 0.011230588890612125\n",
      "Test: epoch 124 batch 0 loss 0.013649220578372478\n",
      "epoch 124 finished - avarage train loss 0.007705132677701527  avarage test loss 0.014087404240854084\n",
      "Training: epoch 125 batch 0 loss 0.007998799905180931\n",
      "Training: epoch 125 batch 10 loss 0.010797233320772648\n",
      "Training: epoch 125 batch 20 loss 0.0015908703207969666\n",
      "Test: epoch 125 batch 0 loss 0.012080933898687363\n",
      "epoch 125 finished - avarage train loss 0.006056507270590499  avarage test loss 0.013162159826606512\n",
      "Training: epoch 126 batch 0 loss 0.003571738488972187\n",
      "Training: epoch 126 batch 10 loss 0.004324492998421192\n",
      "Training: epoch 126 batch 20 loss 0.008168315514922142\n",
      "Test: epoch 126 batch 0 loss 0.019352968782186508\n",
      "epoch 126 finished - avarage train loss 0.006541582964489172  avarage test loss 0.020669011864811182\n",
      "Training: epoch 127 batch 0 loss 0.012498440220952034\n",
      "Training: epoch 127 batch 10 loss 0.011590978130698204\n",
      "Training: epoch 127 batch 20 loss 0.009318280033767223\n",
      "Test: epoch 127 batch 0 loss 0.012499065138399601\n",
      "epoch 127 finished - avarage train loss 0.010201375522040602  avarage test loss 0.013571537449024618\n",
      "Training: epoch 128 batch 0 loss 0.005664435215294361\n",
      "Training: epoch 128 batch 10 loss 0.008067387156188488\n",
      "Training: epoch 128 batch 20 loss 0.006094302516430616\n",
      "Test: epoch 128 batch 0 loss 0.013161604292690754\n",
      "epoch 128 finished - avarage train loss 0.0069061318685397  avarage test loss 0.013851590687409043\n",
      "Training: epoch 129 batch 0 loss 0.0047440677881240845\n",
      "Training: epoch 129 batch 10 loss 0.0024285472463816404\n",
      "Training: epoch 129 batch 20 loss 0.003645449411123991\n",
      "Test: epoch 129 batch 0 loss 0.01391199603676796\n",
      "epoch 129 finished - avarage train loss 0.007422783086879243  avarage test loss 0.014926372095942497\n",
      "Training: epoch 130 batch 0 loss 0.00855706911534071\n",
      "Training: epoch 130 batch 10 loss 0.006601843051612377\n",
      "Training: epoch 130 batch 20 loss 0.003812234615907073\n",
      "Test: epoch 130 batch 0 loss 0.014793128706514835\n",
      "epoch 130 finished - avarage train loss 0.007478106582280377  avarage test loss 0.016345151467248797\n",
      "Training: epoch 131 batch 0 loss 0.00987650640308857\n",
      "Training: epoch 131 batch 10 loss 0.005422305315732956\n",
      "Training: epoch 131 batch 20 loss 0.009999539703130722\n",
      "Test: epoch 131 batch 0 loss 0.013472862541675568\n",
      "epoch 131 finished - avarage train loss 0.006275631820558217  avarage test loss 0.014351200545206666\n",
      "Training: epoch 132 batch 0 loss 0.004266916774213314\n",
      "Training: epoch 132 batch 10 loss 0.0071114543825387955\n",
      "Training: epoch 132 batch 20 loss 0.002795664593577385\n",
      "Test: epoch 132 batch 0 loss 0.012792643159627914\n",
      "epoch 132 finished - avarage train loss 0.006637416956625108  avarage test loss 0.01360050670336932\n",
      "Training: epoch 133 batch 0 loss 0.006301100365817547\n",
      "Training: epoch 133 batch 10 loss 0.011249478906393051\n",
      "Training: epoch 133 batch 20 loss 0.009158449247479439\n",
      "Test: epoch 133 batch 0 loss 0.010846246033906937\n",
      "epoch 133 finished - avarage train loss 0.006651133086114865  avarage test loss 0.012687145033851266\n",
      "Training: epoch 134 batch 0 loss 0.010318259708583355\n",
      "Training: epoch 134 batch 10 loss 0.00932165328413248\n",
      "Training: epoch 134 batch 20 loss 0.005477108061313629\n",
      "Test: epoch 134 batch 0 loss 0.013163574039936066\n",
      "epoch 134 finished - avarage train loss 0.0065618861338187905  avarage test loss 0.014287973521277308\n",
      "Training: epoch 135 batch 0 loss 0.0085371695458889\n",
      "Training: epoch 135 batch 10 loss 0.012142511084675789\n",
      "Training: epoch 135 batch 20 loss 0.006572981830686331\n",
      "Test: epoch 135 batch 0 loss 0.009620294906198978\n",
      "epoch 135 finished - avarage train loss 0.007734791166951944  avarage test loss 0.013798007974401116\n",
      "Training: epoch 136 batch 0 loss 0.007635601330548525\n",
      "Training: epoch 136 batch 10 loss 0.0193052738904953\n",
      "Training: epoch 136 batch 20 loss 0.01328294351696968\n",
      "Test: epoch 136 batch 0 loss 0.011422256007790565\n",
      "epoch 136 finished - avarage train loss 0.010429463365340027  avarage test loss 0.014458408928476274\n",
      "Training: epoch 137 batch 0 loss 0.004902173765003681\n",
      "Training: epoch 137 batch 10 loss 0.006320119835436344\n",
      "Training: epoch 137 batch 20 loss 0.010975848883390427\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test: epoch 137 batch 0 loss 0.01284522470086813\n",
      "epoch 137 finished - avarage train loss 0.00845813770489446  avarage test loss 0.014432856813073158\n",
      "Training: epoch 138 batch 0 loss 0.0072881076484918594\n",
      "Training: epoch 138 batch 10 loss 0.004568950738757849\n",
      "Training: epoch 138 batch 20 loss 0.008810457773506641\n",
      "Test: epoch 138 batch 0 loss 0.01151240337640047\n",
      "epoch 138 finished - avarage train loss 0.007129306318880669  avarage test loss 0.01301903702551499\n",
      "Training: epoch 139 batch 0 loss 0.003001964883878827\n",
      "Training: epoch 139 batch 10 loss 0.002516345586627722\n",
      "Training: epoch 139 batch 20 loss 0.012995224446058273\n",
      "Test: epoch 139 batch 0 loss 0.01187150925397873\n",
      "epoch 139 finished - avarage train loss 0.00699883645625207  avarage test loss 0.014599537942558527\n",
      "Training: epoch 140 batch 0 loss 0.005104733631014824\n",
      "Training: epoch 140 batch 10 loss 0.0043325843289494514\n",
      "Training: epoch 140 batch 20 loss 0.004867259878665209\n",
      "Test: epoch 140 batch 0 loss 0.01439550518989563\n",
      "epoch 140 finished - avarage train loss 0.007325280273076275  avarage test loss 0.016769059235230088\n",
      "Training: epoch 141 batch 0 loss 0.01088474877178669\n",
      "Training: epoch 141 batch 10 loss 0.011134968139231205\n",
      "Training: epoch 141 batch 20 loss 0.00514042004942894\n",
      "Test: epoch 141 batch 0 loss 0.01180285681039095\n",
      "epoch 141 finished - avarage train loss 0.011051324469133698  avarage test loss 0.016292312764562666\n",
      "Training: epoch 142 batch 0 loss 0.003694530576467514\n",
      "Training: epoch 142 batch 10 loss 0.005507933907210827\n",
      "Training: epoch 142 batch 20 loss 0.004133621230721474\n",
      "Test: epoch 142 batch 0 loss 0.015214934013783932\n",
      "epoch 142 finished - avarage train loss 0.00680163179138868  avarage test loss 0.01621852209791541\n",
      "Training: epoch 143 batch 0 loss 0.004204026889055967\n",
      "Training: epoch 143 batch 10 loss 0.008657518774271011\n",
      "Training: epoch 143 batch 20 loss 0.0028618359938263893\n",
      "Test: epoch 143 batch 0 loss 0.013220918364822865\n",
      "epoch 143 finished - avarage train loss 0.007932041153504417  avarage test loss 0.013553367462009192\n",
      "Training: epoch 144 batch 0 loss 0.0049906461499631405\n",
      "Training: epoch 144 batch 10 loss 0.005675261840224266\n",
      "Training: epoch 144 batch 20 loss 0.005310468841344118\n",
      "Test: epoch 144 batch 0 loss 0.01117632258683443\n",
      "epoch 144 finished - avarage train loss 0.008078818787531606  avarage test loss 0.014116379315964878\n",
      "Training: epoch 145 batch 0 loss 0.007804242894053459\n",
      "Training: epoch 145 batch 10 loss 0.0053710537031292915\n",
      "Training: epoch 145 batch 20 loss 0.003930993843823671\n",
      "Test: epoch 145 batch 0 loss 0.015336242504417896\n",
      "epoch 145 finished - avarage train loss 0.006631560495187496  avarage test loss 0.016621857415884733\n",
      "Training: epoch 146 batch 0 loss 0.00453563779592514\n",
      "Training: epoch 146 batch 10 loss 0.0032170861959457397\n",
      "Training: epoch 146 batch 20 loss 0.006417245604097843\n",
      "Test: epoch 146 batch 0 loss 0.015592724084854126\n",
      "epoch 146 finished - avarage train loss 0.006891429175397959  avarage test loss 0.016100098146125674\n",
      "Training: epoch 147 batch 0 loss 0.007607069332152605\n",
      "Training: epoch 147 batch 10 loss 0.005395259242504835\n",
      "Training: epoch 147 batch 20 loss 0.012402204796671867\n",
      "Test: epoch 147 batch 0 loss 0.012766124680638313\n",
      "epoch 147 finished - avarage train loss 0.007580327953950599  avarage test loss 0.013959323521703482\n",
      "Training: epoch 148 batch 0 loss 0.003226308850571513\n",
      "Training: epoch 148 batch 10 loss 0.006384118460118771\n",
      "Training: epoch 148 batch 20 loss 0.0071928235702216625\n",
      "Test: epoch 148 batch 0 loss 0.013556249439716339\n",
      "epoch 148 finished - avarage train loss 0.0059713682898416606  avarage test loss 0.013977278606034815\n",
      "Training: epoch 149 batch 0 loss 0.002260724315419793\n",
      "Training: epoch 149 batch 10 loss 0.005573502741754055\n",
      "Training: epoch 149 batch 20 loss 0.006103242747485638\n",
      "Test: epoch 149 batch 0 loss 0.012223048135638237\n",
      "epoch 149 finished - avarage train loss 0.007133822872078624  avarage test loss 0.014592489576898515\n",
      "Training: epoch 150 batch 0 loss 0.006410631351172924\n",
      "Training: epoch 150 batch 10 loss 0.00909189973026514\n",
      "Training: epoch 150 batch 20 loss 0.00874109473079443\n",
      "Test: epoch 150 batch 0 loss 0.015330642461776733\n",
      "epoch 150 finished - avarage train loss 0.009387782236946553  avarage test loss 0.017244253773242235\n",
      "Training: epoch 151 batch 0 loss 0.011830483563244343\n",
      "Training: epoch 151 batch 10 loss 0.005212938413023949\n",
      "Training: epoch 151 batch 20 loss 0.006237896624952555\n",
      "Test: epoch 151 batch 0 loss 0.013407313264906406\n",
      "epoch 151 finished - avarage train loss 0.008715615503425742  avarage test loss 0.0151769844815135\n",
      "Training: epoch 152 batch 0 loss 0.006646262481808662\n",
      "Training: epoch 152 batch 10 loss 0.003611490363255143\n",
      "Training: epoch 152 batch 20 loss 0.005358162801712751\n",
      "Test: epoch 152 batch 0 loss 0.01235476229339838\n",
      "epoch 152 finished - avarage train loss 0.006914312734493408  avarage test loss 0.013947207713499665\n",
      "Training: epoch 153 batch 0 loss 0.005243069492280483\n",
      "Training: epoch 153 batch 10 loss 0.006587119773030281\n",
      "Training: epoch 153 batch 20 loss 0.005933867301791906\n",
      "Test: epoch 153 batch 0 loss 0.01337787602096796\n",
      "epoch 153 finished - avarage train loss 0.007639521965757012  avarage test loss 0.014356577536091208\n",
      "Training: epoch 154 batch 0 loss 0.010587482713162899\n",
      "Training: epoch 154 batch 10 loss 0.004081360995769501\n",
      "Training: epoch 154 batch 20 loss 0.006977724842727184\n",
      "Test: epoch 154 batch 0 loss 0.014892364852130413\n",
      "epoch 154 finished - avarage train loss 0.007675154736779374  avarage test loss 0.015160399954766035\n",
      "Training: epoch 155 batch 0 loss 0.00833066925406456\n",
      "Training: epoch 155 batch 10 loss 0.0071115754544734955\n",
      "Training: epoch 155 batch 20 loss 0.0031356026884168386\n",
      "Test: epoch 155 batch 0 loss 0.013337494805455208\n",
      "epoch 155 finished - avarage train loss 0.007817375112389183  avarage test loss 0.013700014213100076\n",
      "Training: epoch 156 batch 0 loss 0.007047739811241627\n",
      "Training: epoch 156 batch 10 loss 0.0063309138640761375\n",
      "Training: epoch 156 batch 20 loss 0.002573073375970125\n",
      "Test: epoch 156 batch 0 loss 0.017615482211112976\n",
      "epoch 156 finished - avarage train loss 0.00714634317133961  avarage test loss 0.01808768417686224\n",
      "Training: epoch 157 batch 0 loss 0.0095061045140028\n",
      "Training: epoch 157 batch 10 loss 0.026304736733436584\n",
      "Training: epoch 157 batch 20 loss 0.0069790193811059\n",
      "Test: epoch 157 batch 0 loss 0.015190331265330315\n",
      "epoch 157 finished - avarage train loss 0.012394829380229629  avarage test loss 0.015813055098988116\n",
      "Training: epoch 158 batch 0 loss 0.006074456498026848\n",
      "Training: epoch 158 batch 10 loss 0.003648608224466443\n",
      "Training: epoch 158 batch 20 loss 0.005155374761670828\n",
      "Test: epoch 158 batch 0 loss 0.013715981505811214\n",
      "epoch 158 finished - avarage train loss 0.006830452861071661  avarage test loss 0.014271859196014702\n",
      "Training: epoch 159 batch 0 loss 0.006124484818428755\n",
      "Training: epoch 159 batch 10 loss 0.006483886856585741\n",
      "Training: epoch 159 batch 20 loss 0.009431783109903336\n",
      "Test: epoch 159 batch 0 loss 0.01569385826587677\n",
      "epoch 159 finished - avarage train loss 0.00746235647238791  avarage test loss 0.016829736065119505\n",
      "Training: epoch 160 batch 0 loss 0.006688323803246021\n",
      "Training: epoch 160 batch 10 loss 0.00756869139149785\n",
      "Training: epoch 160 batch 20 loss 0.008913824334740639\n",
      "Test: epoch 160 batch 0 loss 0.016703665256500244\n",
      "epoch 160 finished - avarage train loss 0.00883802840614627  avarage test loss 0.01694904826581478\n",
      "Training: epoch 161 batch 0 loss 0.012148238718509674\n",
      "Training: epoch 161 batch 10 loss 0.008398731239140034\n",
      "Training: epoch 161 batch 20 loss 0.009337320923805237\n",
      "Test: epoch 161 batch 0 loss 0.01673143170773983\n",
      "epoch 161 finished - avarage train loss 0.007639485270844708  avarage test loss 0.017970118205994368\n",
      "Training: epoch 162 batch 0 loss 0.011151737533509731\n",
      "Training: epoch 162 batch 10 loss 0.0026835270691663027\n",
      "Training: epoch 162 batch 20 loss 0.003036373294889927\n",
      "Test: epoch 162 batch 0 loss 0.014093774370849133\n",
      "epoch 162 finished - avarage train loss 0.006843317043164681  avarage test loss 0.014764443621970713\n",
      "Training: epoch 163 batch 0 loss 0.00542065966874361\n",
      "Training: epoch 163 batch 10 loss 0.007151523604989052\n",
      "Training: epoch 163 batch 20 loss 0.007298019248992205\n",
      "Test: epoch 163 batch 0 loss 0.012105670757591724\n",
      "epoch 163 finished - avarage train loss 0.007809844052675983  avarage test loss 0.013228291762061417\n",
      "Training: epoch 164 batch 0 loss 0.0035210696514695883\n",
      "Training: epoch 164 batch 10 loss 0.007022543344646692\n",
      "Training: epoch 164 batch 20 loss 0.0057782139629125595\n",
      "Test: epoch 164 batch 0 loss 0.012333856895565987\n",
      "epoch 164 finished - avarage train loss 0.006514894117697559  avarage test loss 0.013131075538694859\n",
      "Training: epoch 165 batch 0 loss 0.0043500796891748905\n",
      "Training: epoch 165 batch 10 loss 0.006212209351360798\n",
      "Training: epoch 165 batch 20 loss 0.006153855938464403\n",
      "Test: epoch 165 batch 0 loss 0.01205348502844572\n",
      "epoch 165 finished - avarage train loss 0.006747673699182683  avarage test loss 0.013007713598199189\n",
      "Training: epoch 166 batch 0 loss 0.004783818032592535\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training: epoch 166 batch 10 loss 0.004865226801484823\n",
      "Training: epoch 166 batch 20 loss 0.004754478577524424\n",
      "Test: epoch 166 batch 0 loss 0.013626975007355213\n",
      "epoch 166 finished - avarage train loss 0.0068833176207182736  avarage test loss 0.01493771467357874\n",
      "Training: epoch 167 batch 0 loss 0.010791530832648277\n",
      "Training: epoch 167 batch 10 loss 0.006730285473167896\n",
      "Training: epoch 167 batch 20 loss 0.003615425666794181\n",
      "Test: epoch 167 batch 0 loss 0.011174621991813183\n",
      "epoch 167 finished - avarage train loss 0.006451316876336932  avarage test loss 0.014578257803805172\n",
      "Training: epoch 168 batch 0 loss 0.007235953584313393\n",
      "Training: epoch 168 batch 10 loss 0.005193388555198908\n",
      "Training: epoch 168 batch 20 loss 0.0023512032348662615\n",
      "Test: epoch 168 batch 0 loss 0.012482371181249619\n",
      "epoch 168 finished - avarage train loss 0.006951431679571497  avarage test loss 0.014209383865818381\n",
      "Training: epoch 169 batch 0 loss 0.0022699974942952394\n",
      "Training: epoch 169 batch 10 loss 0.004573494661599398\n",
      "Training: epoch 169 batch 20 loss 0.012703336775302887\n",
      "Test: epoch 169 batch 0 loss 0.011914437636733055\n",
      "epoch 169 finished - avarage train loss 0.006452082158548051  avarage test loss 0.014167225104756653\n",
      "Training: epoch 170 batch 0 loss 0.01131989061832428\n",
      "Training: epoch 170 batch 10 loss 0.00569729832932353\n",
      "Training: epoch 170 batch 20 loss 0.0052713812328875065\n",
      "Test: epoch 170 batch 0 loss 0.011424023658037186\n",
      "epoch 170 finished - avarage train loss 0.006928620506720296  avarage test loss 0.013267860864289105\n",
      "Training: epoch 171 batch 0 loss 0.005151289515197277\n",
      "Training: epoch 171 batch 10 loss 0.0070337289944291115\n",
      "Training: epoch 171 batch 20 loss 0.006182428915053606\n",
      "Test: epoch 171 batch 0 loss 0.012463093735277653\n",
      "epoch 171 finished - avarage train loss 0.006662578799697603  avarage test loss 0.013663272024132311\n",
      "Training: epoch 172 batch 0 loss 0.0033179938327521086\n",
      "Training: epoch 172 batch 10 loss 0.010036702267825603\n",
      "Training: epoch 172 batch 20 loss 0.0033929466735571623\n",
      "Test: epoch 172 batch 0 loss 0.011566905304789543\n",
      "epoch 172 finished - avarage train loss 0.006601757939969157  avarage test loss 0.013648642227053642\n",
      "Training: epoch 173 batch 0 loss 0.015358788892626762\n",
      "Training: epoch 173 batch 10 loss 0.006492739077657461\n",
      "Training: epoch 173 batch 20 loss 0.004571090452373028\n",
      "Test: epoch 173 batch 0 loss 0.013492821715772152\n",
      "epoch 173 finished - avarage train loss 0.0079185226370163  avarage test loss 0.014243099954910576\n",
      "Training: epoch 174 batch 0 loss 0.006655677687376738\n",
      "Training: epoch 174 batch 10 loss 0.004109400324523449\n",
      "Training: epoch 174 batch 20 loss 0.002703593811020255\n",
      "Test: epoch 174 batch 0 loss 0.013249393552541733\n",
      "epoch 174 finished - avarage train loss 0.0058423847417700394  avarage test loss 0.01713651977479458\n",
      "Training: epoch 175 batch 0 loss 0.005384616553783417\n",
      "Training: epoch 175 batch 10 loss 0.012014045380055904\n",
      "Training: epoch 175 batch 20 loss 0.007891430519521236\n",
      "Test: epoch 175 batch 0 loss 0.014586705714464188\n",
      "epoch 175 finished - avarage train loss 0.010571839376192155  avarage test loss 0.01990479533560574\n",
      "Training: epoch 176 batch 0 loss 0.008942249231040478\n",
      "Training: epoch 176 batch 10 loss 0.009369143284857273\n",
      "Training: epoch 176 batch 20 loss 0.006238461006432772\n",
      "Test: epoch 176 batch 0 loss 0.01398941595107317\n",
      "epoch 176 finished - avarage train loss 0.006555491761336553  avarage test loss 0.01569266035221517\n",
      "Training: epoch 177 batch 0 loss 0.009177875705063343\n",
      "Training: epoch 177 batch 10 loss 0.002877474296838045\n",
      "Training: epoch 177 batch 20 loss 0.018614865839481354\n",
      "Test: epoch 177 batch 0 loss 0.014683463610708714\n",
      "epoch 177 finished - avarage train loss 0.00782930661506694  avarage test loss 0.016094054793938994\n",
      "Training: epoch 178 batch 0 loss 0.006908772513270378\n",
      "Training: epoch 178 batch 10 loss 0.009144468232989311\n",
      "Training: epoch 178 batch 20 loss 0.010323939844965935\n",
      "Test: epoch 178 batch 0 loss 0.017888927832245827\n",
      "epoch 178 finished - avarage train loss 0.010488668135527906  avarage test loss 0.020046279532834888\n",
      "Training: epoch 179 batch 0 loss 0.012747141532599926\n",
      "Training: epoch 179 batch 10 loss 0.007753931917250156\n",
      "Training: epoch 179 batch 20 loss 0.005382722243666649\n",
      "Test: epoch 179 batch 0 loss 0.013811685144901276\n",
      "epoch 179 finished - avarage train loss 0.00804742334956496  avarage test loss 0.019480676157400012\n",
      "Training: epoch 180 batch 0 loss 0.004141103010624647\n",
      "Training: epoch 180 batch 10 loss 0.010405199602246284\n",
      "Training: epoch 180 batch 20 loss 0.011525730602443218\n",
      "Test: epoch 180 batch 0 loss 0.02523099072277546\n",
      "epoch 180 finished - avarage train loss 0.013634143766529602  avarage test loss 0.022531326860189438\n",
      "Training: epoch 181 batch 0 loss 0.008890094235539436\n",
      "Training: epoch 181 batch 10 loss 0.004300171975046396\n",
      "Training: epoch 181 batch 20 loss 0.004696632269769907\n",
      "Test: epoch 181 batch 0 loss 0.017823070287704468\n",
      "epoch 181 finished - avarage train loss 0.00788101778718932  avarage test loss 0.016046401928178966\n",
      "Training: epoch 182 batch 0 loss 0.004501732997596264\n",
      "Training: epoch 182 batch 10 loss 0.00419128080829978\n",
      "Training: epoch 182 batch 20 loss 0.0060206810012459755\n",
      "Test: epoch 182 batch 0 loss 0.016921646893024445\n",
      "epoch 182 finished - avarage train loss 0.006885273903929468  avarage test loss 0.01660633401479572\n",
      "Training: epoch 183 batch 0 loss 0.00601820508018136\n",
      "Training: epoch 183 batch 10 loss 0.003986867144703865\n",
      "Training: epoch 183 batch 20 loss 0.007061761803925037\n",
      "Test: epoch 183 batch 0 loss 0.025422334671020508\n",
      "epoch 183 finished - avarage train loss 0.0071630456067364795  avarage test loss 0.02832549437880516\n",
      "Training: epoch 184 batch 0 loss 0.021495336666703224\n",
      "Training: epoch 184 batch 10 loss 0.016818568110466003\n",
      "Training: epoch 184 batch 20 loss 0.009779802523553371\n",
      "Test: epoch 184 batch 0 loss 0.020327873528003693\n",
      "epoch 184 finished - avarage train loss 0.02177873961948629  avarage test loss 0.020748432725667953\n",
      "Training: epoch 185 batch 0 loss 0.013418560847640038\n",
      "Training: epoch 185 batch 10 loss 0.013969093561172485\n",
      "Training: epoch 185 batch 20 loss 0.008072976022958755\n",
      "Test: epoch 185 batch 0 loss 0.014974558725953102\n",
      "epoch 185 finished - avarage train loss 0.010110782571779242  avarage test loss 0.01917876908555627\n",
      "Training: epoch 186 batch 0 loss 0.005264107137918472\n",
      "Training: epoch 186 batch 10 loss 0.009109859354794025\n",
      "Training: epoch 186 batch 20 loss 0.010576937347650528\n",
      "Test: epoch 186 batch 0 loss 0.01550950575619936\n",
      "epoch 186 finished - avarage train loss 0.00993122266412809  avarage test loss 0.019477798836305737\n",
      "Training: epoch 187 batch 0 loss 0.007193916942924261\n",
      "Training: epoch 187 batch 10 loss 0.0045223659835755825\n",
      "Training: epoch 187 batch 20 loss 0.004063980188220739\n",
      "Test: epoch 187 batch 0 loss 0.013275207951664925\n",
      "epoch 187 finished - avarage train loss 0.006371507517479617  avarage test loss 0.016051765764132142\n",
      "Training: epoch 188 batch 0 loss 0.008388455025851727\n",
      "Training: epoch 188 batch 10 loss 0.004754321649670601\n",
      "Training: epoch 188 batch 20 loss 0.006745361257344484\n",
      "Test: epoch 188 batch 0 loss 0.012802859768271446\n",
      "epoch 188 finished - avarage train loss 0.006948204405991168  avarage test loss 0.014297979301773012\n",
      "Training: epoch 189 batch 0 loss 0.004693236667662859\n",
      "Training: epoch 189 batch 10 loss 0.009838531725108624\n",
      "Training: epoch 189 batch 20 loss 0.0032551144249737263\n",
      "Test: epoch 189 batch 0 loss 0.017441410571336746\n",
      "epoch 189 finished - avarage train loss 0.006993580866476585  avarage test loss 0.020239593693986535\n",
      "Training: epoch 190 batch 0 loss 0.009312208741903305\n",
      "Training: epoch 190 batch 10 loss 0.024657929316163063\n",
      "Training: epoch 190 batch 20 loss 0.005727407988160849\n",
      "Test: epoch 190 batch 0 loss 0.011767190881073475\n",
      "epoch 190 finished - avarage train loss 0.014034838865286317  avarage test loss 0.013186119962483644\n",
      "Training: epoch 191 batch 0 loss 0.0049461182206869125\n",
      "Training: epoch 191 batch 10 loss 0.003043541219085455\n",
      "Training: epoch 191 batch 20 loss 0.004308083560317755\n",
      "Test: epoch 191 batch 0 loss 0.013244962319731712\n",
      "epoch 191 finished - avarage train loss 0.006966732418293069  avarage test loss 0.014231552486307919\n",
      "Training: epoch 192 batch 0 loss 0.006663871463388205\n",
      "Training: epoch 192 batch 10 loss 0.004646243527531624\n",
      "Training: epoch 192 batch 20 loss 0.004337251652032137\n",
      "Test: epoch 192 batch 0 loss 0.01157454028725624\n",
      "epoch 192 finished - avarage train loss 0.006195482195772487  avarage test loss 0.013106369529850781\n",
      "Training: epoch 193 batch 0 loss 0.0030100401490926743\n",
      "Training: epoch 193 batch 10 loss 0.008673105388879776\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training: epoch 193 batch 20 loss 0.004025574307888746\n",
      "Test: epoch 193 batch 0 loss 0.012488899752497673\n",
      "epoch 193 finished - avarage train loss 0.008619983624345783  avarage test loss 0.014870290295220912\n",
      "Training: epoch 194 batch 0 loss 0.0028436980210244656\n",
      "Training: epoch 194 batch 10 loss 0.0028607239946722984\n",
      "Training: epoch 194 batch 20 loss 0.006662387400865555\n",
      "Test: epoch 194 batch 0 loss 0.01465694047510624\n",
      "epoch 194 finished - avarage train loss 0.005850844839908953  avarage test loss 0.014621231239289045\n",
      "Training: epoch 195 batch 0 loss 0.0033004714641720057\n",
      "Training: epoch 195 batch 10 loss 0.010966929607093334\n",
      "Training: epoch 195 batch 20 loss 0.010875822044909\n",
      "Test: epoch 195 batch 0 loss 0.015237969346344471\n",
      "epoch 195 finished - avarage train loss 0.006818210706114769  avarage test loss 0.015270020230673254\n",
      "Training: epoch 196 batch 0 loss 0.009511258453130722\n",
      "Training: epoch 196 batch 10 loss 0.0047199176624417305\n",
      "Training: epoch 196 batch 20 loss 0.00581118930131197\n",
      "Test: epoch 196 batch 0 loss 0.01285996288061142\n",
      "epoch 196 finished - avarage train loss 0.007348260721834055  avarage test loss 0.015292092750314623\n",
      "Training: epoch 197 batch 0 loss 0.005784435197710991\n",
      "Training: epoch 197 batch 10 loss 0.0057670907117426395\n",
      "Training: epoch 197 batch 20 loss 0.005438648629933596\n",
      "Test: epoch 197 batch 0 loss 0.015056593343615532\n",
      "epoch 197 finished - avarage train loss 0.007851641809824726  avarage test loss 0.015028543886728585\n",
      "Training: epoch 198 batch 0 loss 0.00669250451028347\n",
      "Training: epoch 198 batch 10 loss 0.00302127399481833\n",
      "Training: epoch 198 batch 20 loss 0.0063087353482842445\n",
      "Test: epoch 198 batch 0 loss 0.012838440015912056\n",
      "epoch 198 finished - avarage train loss 0.006650795582039603  avarage test loss 0.01336997514590621\n",
      "Training: epoch 199 batch 0 loss 0.007603972218930721\n",
      "Training: epoch 199 batch 10 loss 0.005689452402293682\n",
      "Training: epoch 199 batch 20 loss 0.013515536673367023\n",
      "Test: epoch 199 batch 0 loss 0.01869087852537632\n",
      "epoch 199 finished - avarage train loss 0.009232671433610135  avarage test loss 0.01999306189827621\n",
      "Training: epoch 0 batch 0 loss 0.43935054540634155\n",
      "Training: epoch 0 batch 10 loss 0.3354731500148773\n",
      "Training: epoch 0 batch 20 loss 0.5201213359832764\n",
      "Test: epoch 0 batch 0 loss 0.40999317169189453\n",
      "epoch 0 finished - avarage train loss 0.5204413656530709  avarage test loss 0.4615175575017929\n",
      "Training: epoch 1 batch 0 loss 0.5026913285255432\n",
      "Training: epoch 1 batch 10 loss 0.420503705739975\n",
      "Training: epoch 1 batch 20 loss 0.5013077855110168\n",
      "Test: epoch 1 batch 0 loss 0.4016149342060089\n",
      "epoch 1 finished - avarage train loss 0.5086908186304158  avarage test loss 0.45405101031064987\n",
      "Training: epoch 2 batch 0 loss 0.4322177767753601\n",
      "Training: epoch 2 batch 10 loss 0.7704222202301025\n",
      "Training: epoch 2 batch 20 loss 0.6037047505378723\n",
      "Test: epoch 2 batch 0 loss 0.3939392864704132\n",
      "epoch 2 finished - avarage train loss 0.49987604957202386  avarage test loss 0.441229946911335\n",
      "Training: epoch 3 batch 0 loss 0.471120148897171\n",
      "Training: epoch 3 batch 10 loss 0.47980937361717224\n",
      "Training: epoch 3 batch 20 loss 0.4904352128505707\n",
      "Test: epoch 3 batch 0 loss 0.40090611577033997\n",
      "epoch 3 finished - avarage train loss 0.5081483988926329  avarage test loss 0.44769492000341415\n",
      "Training: epoch 4 batch 0 loss 0.6652676463127136\n",
      "Training: epoch 4 batch 10 loss 0.560565173625946\n",
      "Training: epoch 4 batch 20 loss 0.3394967019557953\n",
      "Test: epoch 4 batch 0 loss 0.3965877294540405\n",
      "epoch 4 finished - avarage train loss 0.5373372322526472  avarage test loss 0.4449501223862171\n",
      "Training: epoch 5 batch 0 loss 0.6546134352684021\n",
      "Training: epoch 5 batch 10 loss 0.6942608952522278\n",
      "Training: epoch 5 batch 20 loss 0.49784204363822937\n",
      "Test: epoch 5 batch 0 loss 0.39342260360717773\n",
      "epoch 5 finished - avarage train loss 0.49793166024931546  avarage test loss 0.44826890528202057\n",
      "Training: epoch 6 batch 0 loss 0.6278696060180664\n",
      "Training: epoch 6 batch 10 loss 0.32576653361320496\n",
      "Training: epoch 6 batch 20 loss 0.3963552713394165\n",
      "Test: epoch 6 batch 0 loss 0.40128570795059204\n",
      "epoch 6 finished - avarage train loss 0.5473144855992548  avarage test loss 0.4456316977739334\n",
      "Training: epoch 7 batch 0 loss 0.33518436551094055\n",
      "Training: epoch 7 batch 10 loss 0.5004656314849854\n",
      "Training: epoch 7 batch 20 loss 0.46994468569755554\n",
      "Test: epoch 7 batch 0 loss 0.3993711769580841\n",
      "epoch 7 finished - avarage train loss 0.5153328319048059  avarage test loss 0.4537464529275894\n",
      "Training: epoch 8 batch 0 loss 0.6820119023323059\n",
      "Training: epoch 8 batch 10 loss 0.424130380153656\n",
      "Training: epoch 8 batch 20 loss 0.6419389247894287\n",
      "Test: epoch 8 batch 0 loss 0.3970748782157898\n",
      "epoch 8 finished - avarage train loss 0.5406965443800236  avarage test loss 0.45182202011346817\n",
      "Training: epoch 9 batch 0 loss 0.3652588427066803\n",
      "Training: epoch 9 batch 10 loss 0.529764711856842\n",
      "Training: epoch 9 batch 20 loss 0.6320043802261353\n",
      "Test: epoch 9 batch 0 loss 0.4071902334690094\n",
      "epoch 9 finished - avarage train loss 0.5325638949871063  avarage test loss 0.45066263154149055\n",
      "Training: epoch 10 batch 0 loss 0.35913708806037903\n",
      "Training: epoch 10 batch 10 loss 0.5521228909492493\n",
      "Training: epoch 10 batch 20 loss 0.3402940630912781\n",
      "Test: epoch 10 batch 0 loss 0.4001690149307251\n",
      "epoch 10 finished - avarage train loss 0.5079471407265499  avarage test loss 0.4466743916273117\n",
      "Training: epoch 11 batch 0 loss 0.6940249800682068\n",
      "Training: epoch 11 batch 10 loss 0.5397838354110718\n",
      "Training: epoch 11 batch 20 loss 0.3869902193546295\n",
      "Test: epoch 11 batch 0 loss 0.4019399881362915\n",
      "epoch 11 finished - avarage train loss 0.5017381700976141  avarage test loss 0.44837381690740585\n",
      "Training: epoch 12 batch 0 loss 0.4893147945404053\n",
      "Training: epoch 12 batch 10 loss 0.3963644802570343\n",
      "Training: epoch 12 batch 20 loss 0.50130695104599\n",
      "Test: epoch 12 batch 0 loss 0.3978577256202698\n",
      "epoch 12 finished - avarage train loss 0.5061314013497583  avarage test loss 0.4453677088022232\n",
      "Training: epoch 13 batch 0 loss 0.5117403864860535\n",
      "Training: epoch 13 batch 10 loss 0.5748705267906189\n",
      "Training: epoch 13 batch 20 loss 0.42310160398483276\n",
      "Test: epoch 13 batch 0 loss 0.4006391763687134\n",
      "epoch 13 finished - avarage train loss 0.5096000124668253  avarage test loss 0.44767919927835464\n",
      "Training: epoch 14 batch 0 loss 0.39218205213546753\n",
      "Training: epoch 14 batch 10 loss 0.40392547845840454\n",
      "Training: epoch 14 batch 20 loss 0.7016482353210449\n",
      "Test: epoch 14 batch 0 loss 0.40240320563316345\n",
      "epoch 14 finished - avarage train loss 0.49900955115926676  avarage test loss 0.45131170004606247\n",
      "Training: epoch 15 batch 0 loss 0.7084541916847229\n",
      "Training: epoch 15 batch 10 loss 0.48112961649894714\n",
      "Training: epoch 15 batch 20 loss 0.6454228758811951\n",
      "Test: epoch 15 batch 0 loss 0.3987945318222046\n",
      "epoch 15 finished - avarage train loss 0.5119623439065342  avarage test loss 0.44541608542203903\n",
      "Training: epoch 16 batch 0 loss 0.6611855626106262\n",
      "Training: epoch 16 batch 10 loss 0.6068907976150513\n",
      "Training: epoch 16 batch 20 loss 0.6476176381111145\n",
      "Test: epoch 16 batch 0 loss 0.3968128561973572\n",
      "epoch 16 finished - avarage train loss 0.502974296438283  avarage test loss 0.4422408938407898\n",
      "Training: epoch 17 batch 0 loss 0.6363222002983093\n",
      "Training: epoch 17 batch 10 loss 0.3949386477470398\n",
      "Training: epoch 17 batch 20 loss 0.5417522192001343\n",
      "Test: epoch 17 batch 0 loss 0.3935265839099884\n",
      "epoch 17 finished - avarage train loss 0.5712440239972082  avarage test loss 0.43790579587221146\n",
      "Training: epoch 18 batch 0 loss 0.6082831025123596\n",
      "Training: epoch 18 batch 10 loss 0.3859829604625702\n",
      "Training: epoch 18 batch 20 loss 0.3941113352775574\n",
      "Test: epoch 18 batch 0 loss 0.39276987314224243\n",
      "epoch 18 finished - avarage train loss 0.5198120341218752  avarage test loss 0.4354199692606926\n",
      "Training: epoch 19 batch 0 loss 0.6550053954124451\n",
      "Training: epoch 19 batch 10 loss 0.3880978226661682\n",
      "Training: epoch 19 batch 20 loss 0.5156574845314026\n",
      "Test: epoch 19 batch 0 loss 0.39788535237312317\n",
      "epoch 19 finished - avarage train loss 0.5042777261857329  avarage test loss 0.4387693405151367\n",
      "Training: epoch 20 batch 0 loss 0.42187246680259705\n",
      "Training: epoch 20 batch 10 loss 0.4670371413230896\n",
      "Training: epoch 20 batch 20 loss 0.43283069133758545\n",
      "Test: epoch 20 batch 0 loss 0.40362030267715454\n",
      "epoch 20 finished - avarage train loss 0.5134028391591434  avarage test loss 0.4494919627904892\n",
      "Training: epoch 21 batch 0 loss 0.4823676347732544\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training: epoch 21 batch 10 loss 0.3743513822555542\n",
      "Training: epoch 21 batch 20 loss 0.38933756947517395\n",
      "Test: epoch 21 batch 0 loss 0.32428911328315735\n",
      "epoch 21 finished - avarage train loss 0.5240507064194515  avarage test loss 0.37029119953513145\n",
      "Training: epoch 22 batch 0 loss 0.3354743719100952\n",
      "Training: epoch 22 batch 10 loss 0.18322016298770905\n",
      "Training: epoch 22 batch 20 loss 0.07413285225629807\n",
      "Test: epoch 22 batch 0 loss 0.035791296511888504\n",
      "epoch 22 finished - avarage train loss 0.17275815816788836  avarage test loss 0.04469374008476734\n",
      "Training: epoch 23 batch 0 loss 0.03797227889299393\n",
      "Training: epoch 23 batch 10 loss 0.024402134120464325\n",
      "Training: epoch 23 batch 20 loss 0.015694815665483475\n",
      "Test: epoch 23 batch 0 loss 0.011585839092731476\n",
      "epoch 23 finished - avarage train loss 0.02128817709483977  avarage test loss 0.013902810867875814\n",
      "Training: epoch 24 batch 0 loss 0.0080183707177639\n",
      "Training: epoch 24 batch 10 loss 0.007463338319212198\n",
      "Training: epoch 24 batch 20 loss 0.014853205531835556\n",
      "Test: epoch 24 batch 0 loss 0.01907362788915634\n",
      "epoch 24 finished - avarage train loss 0.0123389415635631  avarage test loss 0.022663091775029898\n",
      "Training: epoch 25 batch 0 loss 0.017422953620553017\n",
      "Training: epoch 25 batch 10 loss 0.008829599246382713\n",
      "Training: epoch 25 batch 20 loss 0.008442128077149391\n",
      "Test: epoch 25 batch 0 loss 0.008336181752383709\n",
      "epoch 25 finished - avarage train loss 0.012256045777607581  avarage test loss 0.012804209720343351\n",
      "Training: epoch 26 batch 0 loss 0.007256954908370972\n",
      "Training: epoch 26 batch 10 loss 0.002269004238769412\n",
      "Training: epoch 26 batch 20 loss 0.0054943133145570755\n",
      "Test: epoch 26 batch 0 loss 0.010321798734366894\n",
      "epoch 26 finished - avarage train loss 0.006831529405738773  avarage test loss 0.01480036717839539\n",
      "Training: epoch 27 batch 0 loss 0.0107715530321002\n",
      "Training: epoch 27 batch 10 loss 0.018252791836857796\n",
      "Training: epoch 27 batch 20 loss 0.0037353348452597857\n",
      "Test: epoch 27 batch 0 loss 0.011498620733618736\n",
      "epoch 27 finished - avarage train loss 0.006382208634649628  avarage test loss 0.012810462270863354\n",
      "Training: epoch 28 batch 0 loss 0.006503624841570854\n",
      "Training: epoch 28 batch 10 loss 0.009502576664090157\n",
      "Training: epoch 28 batch 20 loss 0.0163847915828228\n",
      "Test: epoch 28 batch 0 loss 0.0100626265630126\n",
      "epoch 28 finished - avarage train loss 0.00858211029192497  avarage test loss 0.01334501733072102\n",
      "Training: epoch 29 batch 0 loss 0.0033341264352202415\n",
      "Training: epoch 29 batch 10 loss 0.009691021405160427\n",
      "Training: epoch 29 batch 20 loss 0.005443125497549772\n",
      "Test: epoch 29 batch 0 loss 0.007798309437930584\n",
      "epoch 29 finished - avarage train loss 0.008182138790665516  avarage test loss 0.013271522708237171\n",
      "Training: epoch 30 batch 0 loss 0.004652698524296284\n",
      "Training: epoch 30 batch 10 loss 0.003405860858038068\n",
      "Training: epoch 30 batch 20 loss 0.005979497916996479\n",
      "Test: epoch 30 batch 0 loss 0.009877655655145645\n",
      "epoch 30 finished - avarage train loss 0.006729710453735857  avarage test loss 0.011919335345737636\n",
      "Training: epoch 31 batch 0 loss 0.0022501733619719744\n",
      "Training: epoch 31 batch 10 loss 0.0032987729646265507\n",
      "Training: epoch 31 batch 20 loss 0.007180390879511833\n",
      "Test: epoch 31 batch 0 loss 0.014683887362480164\n",
      "epoch 31 finished - avarage train loss 0.0064781662638716654  avarage test loss 0.01700124773196876\n",
      "Training: epoch 32 batch 0 loss 0.01253717951476574\n",
      "Training: epoch 32 batch 10 loss 0.004646617919206619\n",
      "Training: epoch 32 batch 20 loss 0.003417269792407751\n",
      "Test: epoch 32 batch 0 loss 0.011884750798344612\n",
      "epoch 32 finished - avarage train loss 0.0075415720930323005  avarage test loss 0.012914364458993077\n",
      "Training: epoch 33 batch 0 loss 0.0031773436348885298\n",
      "Training: epoch 33 batch 10 loss 0.005634247325360775\n",
      "Training: epoch 33 batch 20 loss 0.006304015871137381\n",
      "Test: epoch 33 batch 0 loss 0.009499923326075077\n",
      "epoch 33 finished - avarage train loss 0.006364760438686815  avarage test loss 0.01391349930781871\n",
      "Training: epoch 34 batch 0 loss 0.0015813184436410666\n",
      "Training: epoch 34 batch 10 loss 0.008154552429914474\n",
      "Training: epoch 34 batch 20 loss 0.003962600138038397\n",
      "Test: epoch 34 batch 0 loss 0.017118122428655624\n",
      "epoch 34 finished - avarage train loss 0.008144211136446944  avarage test loss 0.018753826851025224\n",
      "Training: epoch 35 batch 0 loss 0.00610717898234725\n",
      "Training: epoch 35 batch 10 loss 0.01259641908109188\n",
      "Training: epoch 35 batch 20 loss 0.011313313618302345\n",
      "Test: epoch 35 batch 0 loss 0.012137094512581825\n",
      "epoch 35 finished - avarage train loss 0.008008970180526376  avarage test loss 0.013119280221872032\n",
      "Training: epoch 36 batch 0 loss 0.002254626015201211\n",
      "Training: epoch 36 batch 10 loss 0.004736486356705427\n",
      "Training: epoch 36 batch 20 loss 0.006363378372043371\n",
      "Test: epoch 36 batch 0 loss 0.017322560772299767\n",
      "epoch 36 finished - avarage train loss 0.008006896028808993  avarage test loss 0.018176423385739326\n",
      "Training: epoch 37 batch 0 loss 0.01543836947530508\n",
      "Training: epoch 37 batch 10 loss 0.00846116989850998\n",
      "Training: epoch 37 batch 20 loss 0.011157340370118618\n",
      "Test: epoch 37 batch 0 loss 0.01076614297926426\n",
      "epoch 37 finished - avarage train loss 0.009079051629543818  avarage test loss 0.013893093564547598\n",
      "Training: epoch 38 batch 0 loss 0.005299593787640333\n",
      "Training: epoch 38 batch 10 loss 0.01172468438744545\n",
      "Training: epoch 38 batch 20 loss 0.013344627805054188\n",
      "Test: epoch 38 batch 0 loss 0.017834162339568138\n",
      "epoch 38 finished - avarage train loss 0.007629016751487707  avarage test loss 0.02066043787635863\n",
      "Training: epoch 39 batch 0 loss 0.013370892964303493\n",
      "Training: epoch 39 batch 10 loss 0.012640117667615414\n",
      "Training: epoch 39 batch 20 loss 0.007738039363175631\n",
      "Test: epoch 39 batch 0 loss 0.011895522475242615\n",
      "epoch 39 finished - avarage train loss 0.007669138200259928  avarage test loss 0.012911707977764308\n",
      "Training: epoch 40 batch 0 loss 0.014590258710086346\n",
      "Training: epoch 40 batch 10 loss 0.005502298008650541\n",
      "Training: epoch 40 batch 20 loss 0.0037690007593482733\n",
      "Test: epoch 40 batch 0 loss 0.0113817248493433\n",
      "epoch 40 finished - avarage train loss 0.009047332500781992  avarage test loss 0.013485694129485637\n",
      "Training: epoch 41 batch 0 loss 0.0053102364763617516\n",
      "Training: epoch 41 batch 10 loss 0.009711616672575474\n",
      "Training: epoch 41 batch 20 loss 0.007033948786556721\n",
      "Test: epoch 41 batch 0 loss 0.013324342668056488\n",
      "epoch 41 finished - avarage train loss 0.009832907179049376  avarage test loss 0.01441685063764453\n",
      "Training: epoch 42 batch 0 loss 0.00815115962177515\n",
      "Training: epoch 42 batch 10 loss 0.005071432329714298\n",
      "Training: epoch 42 batch 20 loss 0.004307589493691921\n",
      "Test: epoch 42 batch 0 loss 0.010457785800099373\n",
      "epoch 42 finished - avarage train loss 0.008326004106744096  avarage test loss 0.014420313644222915\n",
      "Training: epoch 43 batch 0 loss 0.012106197886168957\n",
      "Training: epoch 43 batch 10 loss 0.002225210890173912\n",
      "Training: epoch 43 batch 20 loss 0.008564927615225315\n",
      "Test: epoch 43 batch 0 loss 0.01214690413326025\n",
      "epoch 43 finished - avarage train loss 0.00809404110246948  avarage test loss 0.013182687805965543\n",
      "Training: epoch 44 batch 0 loss 0.009012800641357899\n",
      "Training: epoch 44 batch 10 loss 0.005378838162869215\n",
      "Training: epoch 44 batch 20 loss 0.018747085705399513\n",
      "Test: epoch 44 batch 0 loss 0.012241635471582413\n",
      "epoch 44 finished - avarage train loss 0.008539596010513347  avarage test loss 0.013102285214699805\n",
      "Training: epoch 45 batch 0 loss 0.0068410723470151424\n",
      "Training: epoch 45 batch 10 loss 0.005368499085307121\n",
      "Training: epoch 45 batch 20 loss 0.0033987194765359163\n",
      "Test: epoch 45 batch 0 loss 0.012901787646114826\n",
      "epoch 45 finished - avarage train loss 0.0073247672989964485  avarage test loss 0.014336845371872187\n",
      "Training: epoch 46 batch 0 loss 0.007102631032466888\n",
      "Training: epoch 46 batch 10 loss 0.0048960004933178425\n",
      "Training: epoch 46 batch 20 loss 0.009321874938905239\n",
      "Test: epoch 46 batch 0 loss 0.012252065353095531\n",
      "epoch 46 finished - avarage train loss 0.007669210385788104  avarage test loss 0.012924803304485977\n",
      "Training: epoch 47 batch 0 loss 0.004644784610718489\n",
      "Training: epoch 47 batch 10 loss 0.0026954342611134052\n",
      "Training: epoch 47 batch 20 loss 0.00839931983500719\n",
      "Test: epoch 47 batch 0 loss 0.011625264771282673\n",
      "epoch 47 finished - avarage train loss 0.006951158352453133  avarage test loss 0.013727339683100581\n",
      "Training: epoch 48 batch 0 loss 0.00898178480565548\n",
      "Training: epoch 48 batch 10 loss 0.007551192305982113\n",
      "Training: epoch 48 batch 20 loss 0.004216423723846674\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test: epoch 48 batch 0 loss 0.013037040829658508\n",
      "epoch 48 finished - avarage train loss 0.007445603143423796  avarage test loss 0.013379609561525285\n",
      "Training: epoch 49 batch 0 loss 0.004970148205757141\n",
      "Training: epoch 49 batch 10 loss 0.00485119828954339\n",
      "Training: epoch 49 batch 20 loss 0.00558371189981699\n",
      "Test: epoch 49 batch 0 loss 0.014497744850814342\n",
      "epoch 49 finished - avarage train loss 0.0059693603663994324  avarage test loss 0.015048023196868598\n",
      "Training: epoch 50 batch 0 loss 0.004866295028477907\n",
      "Training: epoch 50 batch 10 loss 0.007704854477196932\n",
      "Training: epoch 50 batch 20 loss 0.004078450612723827\n",
      "Test: epoch 50 batch 0 loss 0.011328138411045074\n",
      "epoch 50 finished - avarage train loss 0.007294508090628119  avarage test loss 0.013051434652879834\n",
      "Training: epoch 51 batch 0 loss 0.008301738649606705\n",
      "Training: epoch 51 batch 10 loss 0.006065080873668194\n",
      "Training: epoch 51 batch 20 loss 0.003310699947178364\n",
      "Test: epoch 51 batch 0 loss 0.015395985916256905\n",
      "epoch 51 finished - avarage train loss 0.007735320455234112  avarage test loss 0.016621909337118268\n",
      "Training: epoch 52 batch 0 loss 0.00507855461910367\n",
      "Training: epoch 52 batch 10 loss 0.0038609285838901997\n",
      "Training: epoch 52 batch 20 loss 0.007600110024213791\n",
      "Test: epoch 52 batch 0 loss 0.012838570401072502\n",
      "epoch 52 finished - avarage train loss 0.00786480186346533  avarage test loss 0.016103064408525825\n",
      "Training: epoch 53 batch 0 loss 0.01113157719373703\n",
      "Training: epoch 53 batch 10 loss 0.006522921845316887\n",
      "Training: epoch 53 batch 20 loss 0.004519218113273382\n",
      "Test: epoch 53 batch 0 loss 0.014385620132088661\n",
      "epoch 53 finished - avarage train loss 0.007927898702950313  avarage test loss 0.014279973227530718\n",
      "Training: epoch 54 batch 0 loss 0.006502147298306227\n",
      "Training: epoch 54 batch 10 loss 0.012837184593081474\n",
      "Training: epoch 54 batch 20 loss 0.011485448107123375\n",
      "Test: epoch 54 batch 0 loss 0.018744274973869324\n",
      "epoch 54 finished - avarage train loss 0.010864346464774731  avarage test loss 0.0190429724752903\n",
      "Training: epoch 55 batch 0 loss 0.00564274238422513\n",
      "Training: epoch 55 batch 10 loss 0.010165814310312271\n",
      "Training: epoch 55 batch 20 loss 0.007835837081074715\n",
      "Test: epoch 55 batch 0 loss 0.015245798975229263\n",
      "epoch 55 finished - avarage train loss 0.008546064495398053  avarage test loss 0.015877641853876412\n",
      "Training: epoch 56 batch 0 loss 0.0063493140041828156\n",
      "Training: epoch 56 batch 10 loss 0.009804636240005493\n",
      "Training: epoch 56 batch 20 loss 0.006316083949059248\n",
      "Test: epoch 56 batch 0 loss 0.01256545353680849\n",
      "epoch 56 finished - avarage train loss 0.007027637871817268  avarage test loss 0.013173624989576638\n",
      "Training: epoch 57 batch 0 loss 0.0053744446486234665\n",
      "Training: epoch 57 batch 10 loss 0.004790257662534714\n",
      "Training: epoch 57 batch 20 loss 0.00299113430082798\n",
      "Test: epoch 57 batch 0 loss 0.012423058971762657\n",
      "epoch 57 finished - avarage train loss 0.007833376591061723  avarage test loss 0.01408226671628654\n",
      "Training: epoch 58 batch 0 loss 0.01006346195936203\n",
      "Training: epoch 58 batch 10 loss 0.0230270903557539\n",
      "Training: epoch 58 batch 20 loss 0.008225614205002785\n",
      "Test: epoch 58 batch 0 loss 0.012012390419840813\n",
      "epoch 58 finished - avarage train loss 0.009499510541429808  avarage test loss 0.01673120097257197\n",
      "Training: epoch 59 batch 0 loss 0.010490946471691132\n",
      "Training: epoch 59 batch 10 loss 0.006634996272623539\n",
      "Training: epoch 59 batch 20 loss 0.007500852923840284\n",
      "Test: epoch 59 batch 0 loss 0.013190356083214283\n",
      "epoch 59 finished - avarage train loss 0.008697542088941253  avarage test loss 0.014954648446291685\n",
      "Training: epoch 60 batch 0 loss 0.008299053646624088\n",
      "Training: epoch 60 batch 10 loss 0.006099377758800983\n",
      "Training: epoch 60 batch 20 loss 0.005165657959878445\n",
      "Test: epoch 60 batch 0 loss 0.014446264132857323\n",
      "epoch 60 finished - avarage train loss 0.007743642440643804  avarage test loss 0.014086359762586653\n",
      "Training: epoch 61 batch 0 loss 0.0037944933865219355\n",
      "Training: epoch 61 batch 10 loss 0.00726728281006217\n",
      "Training: epoch 61 batch 20 loss 0.013535767793655396\n",
      "Test: epoch 61 batch 0 loss 0.014164820313453674\n",
      "epoch 61 finished - avarage train loss 0.008045408971093852  avarage test loss 0.01377416611649096\n",
      "Training: epoch 62 batch 0 loss 0.003007776802405715\n",
      "Training: epoch 62 batch 10 loss 0.00673029525205493\n",
      "Training: epoch 62 batch 20 loss 0.004650960676372051\n",
      "Test: epoch 62 batch 0 loss 0.015008249320089817\n",
      "epoch 62 finished - avarage train loss 0.005756688296216829  avarage test loss 0.015483769937418401\n",
      "Training: epoch 63 batch 0 loss 0.0019280124688521028\n",
      "Training: epoch 63 batch 10 loss 0.003961155656725168\n",
      "Training: epoch 63 batch 20 loss 0.004250398371368647\n",
      "Test: epoch 63 batch 0 loss 0.018752895295619965\n",
      "epoch 63 finished - avarage train loss 0.0071067658196813585  avarage test loss 0.019881371641531587\n",
      "Training: epoch 64 batch 0 loss 0.013540617190301418\n",
      "Training: epoch 64 batch 10 loss 0.016947729513049126\n",
      "Training: epoch 64 batch 20 loss 0.006797376088798046\n",
      "Test: epoch 64 batch 0 loss 0.012106725946068764\n",
      "epoch 64 finished - avarage train loss 0.010908765911028302  avarage test loss 0.015311582712456584\n",
      "Training: epoch 65 batch 0 loss 0.011105384677648544\n",
      "Training: epoch 65 batch 10 loss 0.006028422154486179\n",
      "Training: epoch 65 batch 20 loss 0.005615030415356159\n",
      "Test: epoch 65 batch 0 loss 0.011111053638160229\n",
      "epoch 65 finished - avarage train loss 0.006536525906994939  avarage test loss 0.01259346294682473\n",
      "Training: epoch 66 batch 0 loss 0.0025004809722304344\n",
      "Training: epoch 66 batch 10 loss 0.006352764088660479\n",
      "Training: epoch 66 batch 20 loss 0.01132238656282425\n",
      "Test: epoch 66 batch 0 loss 0.011837074533104897\n",
      "epoch 66 finished - avarage train loss 0.008332292725942258  avarage test loss 0.012897828710265458\n",
      "Training: epoch 67 batch 0 loss 0.003939732443541288\n",
      "Training: epoch 67 batch 10 loss 0.0054525332525372505\n",
      "Training: epoch 67 batch 20 loss 0.006322567816823721\n",
      "Test: epoch 67 batch 0 loss 0.014124661684036255\n",
      "epoch 67 finished - avarage train loss 0.006820663769633092  avarage test loss 0.015176820568740368\n",
      "Training: epoch 68 batch 0 loss 0.008848574012517929\n",
      "Training: epoch 68 batch 10 loss 0.008685611188411713\n",
      "Training: epoch 68 batch 20 loss 0.003372820094227791\n",
      "Test: epoch 68 batch 0 loss 0.009927543811500072\n",
      "epoch 68 finished - avarage train loss 0.008285716432949593  avarage test loss 0.012175503594335169\n",
      "Training: epoch 69 batch 0 loss 0.0033023948781192303\n",
      "Training: epoch 69 batch 10 loss 0.0036766191478818655\n",
      "Training: epoch 69 batch 20 loss 0.003935184329748154\n",
      "Test: epoch 69 batch 0 loss 0.012981407344341278\n",
      "epoch 69 finished - avarage train loss 0.006500802299906981  avarage test loss 0.01440249988809228\n",
      "Training: epoch 70 batch 0 loss 0.005360761657357216\n",
      "Training: epoch 70 batch 10 loss 0.007358147297054529\n",
      "Training: epoch 70 batch 20 loss 0.00760055985301733\n",
      "Test: epoch 70 batch 0 loss 0.011843252927064896\n",
      "epoch 70 finished - avarage train loss 0.011432143570534114  avarage test loss 0.013441254617646337\n",
      "Training: epoch 71 batch 0 loss 0.007563662715256214\n",
      "Training: epoch 71 batch 10 loss 0.0054090432822704315\n",
      "Training: epoch 71 batch 20 loss 0.007982390001416206\n",
      "Test: epoch 71 batch 0 loss 0.011160213500261307\n",
      "epoch 71 finished - avarage train loss 0.008670122908620998  avarage test loss 0.01245440065395087\n",
      "Training: epoch 72 batch 0 loss 0.004819212481379509\n",
      "Training: epoch 72 batch 10 loss 0.006715672556310892\n",
      "Training: epoch 72 batch 20 loss 0.007054330315440893\n",
      "Test: epoch 72 batch 0 loss 0.01262234803289175\n",
      "epoch 72 finished - avarage train loss 0.007523200964426686  avarage test loss 0.01384798577055335\n",
      "Training: epoch 73 batch 0 loss 0.013831605203449726\n",
      "Training: epoch 73 batch 10 loss 0.004554151091724634\n",
      "Training: epoch 73 batch 20 loss 0.013560100458562374\n",
      "Test: epoch 73 batch 0 loss 0.011011757887899876\n",
      "epoch 73 finished - avarage train loss 0.00891113114254228  avarage test loss 0.012364371912553906\n",
      "Training: epoch 74 batch 0 loss 0.006857701111584902\n",
      "Training: epoch 74 batch 10 loss 0.004536663647741079\n",
      "Training: epoch 74 batch 20 loss 0.004741842392832041\n",
      "Test: epoch 74 batch 0 loss 0.012664246372878551\n",
      "epoch 74 finished - avarage train loss 0.008617142777376133  avarage test loss 0.013575810240581632\n",
      "Training: epoch 75 batch 0 loss 0.003105394309386611\n",
      "Training: epoch 75 batch 10 loss 0.004188464488834143\n",
      "Training: epoch 75 batch 20 loss 0.006171933840960264\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test: epoch 75 batch 0 loss 0.01132759265601635\n",
      "epoch 75 finished - avarage train loss 0.006259573904540518  avarage test loss 0.012532009393908083\n",
      "Training: epoch 76 batch 0 loss 0.003851512912660837\n",
      "Training: epoch 76 batch 10 loss 0.003354616230353713\n",
      "Training: epoch 76 batch 20 loss 0.0033830907195806503\n",
      "Test: epoch 76 batch 0 loss 0.010341673158109188\n",
      "epoch 76 finished - avarage train loss 0.005658190312056706  avarage test loss 0.012421235849615186\n",
      "Training: epoch 77 batch 0 loss 0.008286562748253345\n",
      "Training: epoch 77 batch 10 loss 0.002716462127864361\n",
      "Training: epoch 77 batch 20 loss 0.008611632511019707\n",
      "Test: epoch 77 batch 0 loss 0.011490238830447197\n",
      "epoch 77 finished - avarage train loss 0.006439331445264919  avarage test loss 0.012383054359816015\n",
      "Training: epoch 78 batch 0 loss 0.003940608352422714\n",
      "Training: epoch 78 batch 10 loss 0.002881307154893875\n",
      "Training: epoch 78 batch 20 loss 0.004263891372829676\n",
      "Test: epoch 78 batch 0 loss 0.010725810192525387\n",
      "epoch 78 finished - avarage train loss 0.006428686789525994  avarage test loss 0.012414808035828173\n",
      "Training: epoch 79 batch 0 loss 0.00216247932985425\n",
      "Training: epoch 79 batch 10 loss 0.008384463377296925\n",
      "Training: epoch 79 batch 20 loss 0.005731949582695961\n",
      "Test: epoch 79 batch 0 loss 0.011645168997347355\n",
      "epoch 79 finished - avarage train loss 0.008094121412984255  avarage test loss 0.013097246992401779\n",
      "Training: epoch 80 batch 0 loss 0.005749225616455078\n",
      "Training: epoch 80 batch 10 loss 0.005603602156043053\n",
      "Training: epoch 80 batch 20 loss 0.002948527690023184\n",
      "Test: epoch 80 batch 0 loss 0.013252335600554943\n",
      "epoch 80 finished - avarage train loss 0.007828425190507853  avarage test loss 0.014779430814087391\n",
      "Training: epoch 81 batch 0 loss 0.004496088717132807\n",
      "Training: epoch 81 batch 10 loss 0.007814271375536919\n",
      "Training: epoch 81 batch 20 loss 0.008388041518628597\n",
      "Test: epoch 81 batch 0 loss 0.01185246929526329\n",
      "epoch 81 finished - avarage train loss 0.008040861973282078  avarage test loss 0.013016476412303746\n",
      "Training: epoch 82 batch 0 loss 0.004071427974849939\n",
      "Training: epoch 82 batch 10 loss 0.00830982904881239\n",
      "Training: epoch 82 batch 20 loss 0.0063261291943490505\n",
      "Test: epoch 82 batch 0 loss 0.011570781469345093\n",
      "epoch 82 finished - avarage train loss 0.005870851238482985  avarage test loss 0.01273123617284\n",
      "Training: epoch 83 batch 0 loss 0.00996218342334032\n",
      "Training: epoch 83 batch 10 loss 0.011400820687413216\n",
      "Training: epoch 83 batch 20 loss 0.005408394616097212\n",
      "Test: epoch 83 batch 0 loss 0.010105308145284653\n",
      "epoch 83 finished - avarage train loss 0.007451711892921092  avarage test loss 0.013001815532334149\n",
      "Training: epoch 84 batch 0 loss 0.00512466998770833\n",
      "Training: epoch 84 batch 10 loss 0.0019180423114448786\n",
      "Training: epoch 84 batch 20 loss 0.007049374748021364\n",
      "Test: epoch 84 batch 0 loss 0.014409903436899185\n",
      "epoch 84 finished - avarage train loss 0.007083752537788502  avarage test loss 0.015626769280061126\n",
      "Training: epoch 85 batch 0 loss 0.005104027688503265\n",
      "Training: epoch 85 batch 10 loss 0.005320093594491482\n",
      "Training: epoch 85 batch 20 loss 0.008727937005460262\n",
      "Test: epoch 85 batch 0 loss 0.01179758831858635\n",
      "epoch 85 finished - avarage train loss 0.009769443037181065  avarage test loss 0.013255828409455717\n",
      "Training: epoch 86 batch 0 loss 0.004356445744633675\n",
      "Training: epoch 86 batch 10 loss 0.009509364143013954\n",
      "Training: epoch 86 batch 20 loss 0.008988034911453724\n",
      "Test: epoch 86 batch 0 loss 0.01694915071129799\n",
      "epoch 86 finished - avarage train loss 0.009346932405605912  avarage test loss 0.022033976390957832\n",
      "Training: epoch 87 batch 0 loss 0.010628551244735718\n",
      "Training: epoch 87 batch 10 loss 0.003705085488036275\n",
      "Training: epoch 87 batch 20 loss 0.00382526614703238\n",
      "Test: epoch 87 batch 0 loss 0.012435095384716988\n",
      "epoch 87 finished - avarage train loss 0.007585278358952752  avarage test loss 0.01356594916433096\n",
      "Training: epoch 88 batch 0 loss 0.013365213759243488\n",
      "Training: epoch 88 batch 10 loss 0.0048905122093856335\n",
      "Training: epoch 88 batch 20 loss 0.009195485152304173\n",
      "Test: epoch 88 batch 0 loss 0.012196781113743782\n",
      "epoch 88 finished - avarage train loss 0.0077522689845926805  avarage test loss 0.01316904742270708\n",
      "Training: epoch 89 batch 0 loss 0.00828652922064066\n",
      "Training: epoch 89 batch 10 loss 0.0025194063782691956\n",
      "Training: epoch 89 batch 20 loss 0.008560151793062687\n",
      "Test: epoch 89 batch 0 loss 0.019307930022478104\n",
      "epoch 89 finished - avarage train loss 0.00751467342167322  avarage test loss 0.02021094155497849\n",
      "Training: epoch 90 batch 0 loss 0.015047075226902962\n",
      "Training: epoch 90 batch 10 loss 0.01177375391125679\n",
      "Training: epoch 90 batch 20 loss 0.01141359657049179\n",
      "Test: epoch 90 batch 0 loss 0.013878121972084045\n",
      "epoch 90 finished - avarage train loss 0.010341589467535758  avarage test loss 0.015287145040929317\n",
      "Training: epoch 91 batch 0 loss 0.005913818720728159\n",
      "Training: epoch 91 batch 10 loss 0.004401843529194593\n",
      "Training: epoch 91 batch 20 loss 0.0021332113537937403\n",
      "Test: epoch 91 batch 0 loss 0.012523164972662926\n",
      "epoch 91 finished - avarage train loss 0.007355723160736519  avarage test loss 0.013306431821547449\n",
      "Training: epoch 92 batch 0 loss 0.004544050898402929\n",
      "Training: epoch 92 batch 10 loss 0.005498808808624744\n",
      "Training: epoch 92 batch 20 loss 0.0018888975027948618\n",
      "Test: epoch 92 batch 0 loss 0.014885279349982738\n",
      "epoch 92 finished - avarage train loss 0.005917544801847945  avarage test loss 0.01578710344620049\n",
      "Training: epoch 93 batch 0 loss 0.008941052481532097\n",
      "Training: epoch 93 batch 10 loss 0.011663380078971386\n",
      "Training: epoch 93 batch 20 loss 0.010050601325929165\n",
      "Test: epoch 93 batch 0 loss 0.014094267040491104\n",
      "epoch 93 finished - avarage train loss 0.009337573183764672  avarage test loss 0.015008149668574333\n",
      "Training: epoch 94 batch 0 loss 0.0043288045562803745\n",
      "Training: epoch 94 batch 10 loss 0.0032105615828186274\n",
      "Training: epoch 94 batch 20 loss 0.006038513500243425\n",
      "Test: epoch 94 batch 0 loss 0.01092648133635521\n",
      "epoch 94 finished - avarage train loss 0.008204210172246757  avarage test loss 0.01481244363822043\n",
      "Training: epoch 95 batch 0 loss 0.0075691016390919685\n",
      "Training: epoch 95 batch 10 loss 0.011311357840895653\n",
      "Training: epoch 95 batch 20 loss 0.005112018436193466\n",
      "Test: epoch 95 batch 0 loss 0.013758815824985504\n",
      "epoch 95 finished - avarage train loss 0.00741783346466977  avarage test loss 0.015404598321765661\n",
      "Training: epoch 96 batch 0 loss 0.008649168536067009\n",
      "Training: epoch 96 batch 10 loss 0.007694229483604431\n",
      "Training: epoch 96 batch 20 loss 0.006331631913781166\n",
      "Test: epoch 96 batch 0 loss 0.01583411544561386\n",
      "epoch 96 finished - avarage train loss 0.01036153146033657  avarage test loss 0.016494658309966326\n",
      "Training: epoch 97 batch 0 loss 0.007868548855185509\n",
      "Training: epoch 97 batch 10 loss 0.005694328807294369\n",
      "Training: epoch 97 batch 20 loss 0.0029352866113185883\n",
      "Test: epoch 97 batch 0 loss 0.013078724965453148\n",
      "epoch 97 finished - avarage train loss 0.00864035361607013  avarage test loss 0.013411320396699011\n",
      "Training: epoch 98 batch 0 loss 0.0026139700785279274\n",
      "Training: epoch 98 batch 10 loss 0.004407815169543028\n",
      "Training: epoch 98 batch 20 loss 0.0037633655592799187\n",
      "Test: epoch 98 batch 0 loss 0.012012460269033909\n",
      "epoch 98 finished - avarage train loss 0.006491791127763432  avarage test loss 0.01262410159688443\n",
      "Training: epoch 99 batch 0 loss 0.0022722019348293543\n",
      "Training: epoch 99 batch 10 loss 0.004463990218937397\n",
      "Training: epoch 99 batch 20 loss 0.0055975704453885555\n",
      "Test: epoch 99 batch 0 loss 0.016952114179730415\n",
      "epoch 99 finished - avarage train loss 0.0071748349221487494  avarage test loss 0.02005494711920619\n",
      "Training: epoch 100 batch 0 loss 0.014145673252642155\n",
      "Training: epoch 100 batch 10 loss 0.009790470823645592\n",
      "Training: epoch 100 batch 20 loss 0.004694050177931786\n",
      "Test: epoch 100 batch 0 loss 0.009840182960033417\n",
      "epoch 100 finished - avarage train loss 0.008426139452334109  avarage test loss 0.01237533357925713\n",
      "Training: epoch 101 batch 0 loss 0.005776557605713606\n",
      "Training: epoch 101 batch 10 loss 0.00804976373910904\n",
      "Training: epoch 101 batch 20 loss 0.005832359194755554\n",
      "Test: epoch 101 batch 0 loss 0.01284292433410883\n",
      "epoch 101 finished - avarage train loss 0.0076235214103784026  avarage test loss 0.013846987392753363\n",
      "Training: epoch 102 batch 0 loss 0.004463516175746918\n",
      "Training: epoch 102 batch 10 loss 0.006247736979275942\n",
      "Training: epoch 102 batch 20 loss 0.009514420293271542\n",
      "Test: epoch 102 batch 0 loss 0.011863994412124157\n",
      "epoch 102 finished - avarage train loss 0.007055600751833669  avarage test loss 0.013181106536649168\n",
      "Training: epoch 103 batch 0 loss 0.008032308891415596\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training: epoch 103 batch 10 loss 0.004282379522919655\n",
      "Training: epoch 103 batch 20 loss 0.006830684840679169\n",
      "Test: epoch 103 batch 0 loss 0.009094777517020702\n",
      "epoch 103 finished - avarage train loss 0.00865915095722624  avarage test loss 0.013060199504252523\n",
      "Training: epoch 104 batch 0 loss 0.004853500984609127\n",
      "Training: epoch 104 batch 10 loss 0.00485824653878808\n",
      "Training: epoch 104 batch 20 loss 0.00636728759855032\n",
      "Test: epoch 104 batch 0 loss 0.0121180210262537\n",
      "epoch 104 finished - avarage train loss 0.00691536100228028  avarage test loss 0.013744679861702025\n",
      "Training: epoch 105 batch 0 loss 0.0066367327235639095\n",
      "Training: epoch 105 batch 10 loss 0.009629408828914165\n",
      "Training: epoch 105 batch 20 loss 0.003192979609593749\n",
      "Test: epoch 105 batch 0 loss 0.010619410313665867\n",
      "epoch 105 finished - avarage train loss 0.006588454783797778  avarage test loss 0.012521797441877425\n",
      "Training: epoch 106 batch 0 loss 0.0070748841390013695\n",
      "Training: epoch 106 batch 10 loss 0.009374262765049934\n",
      "Training: epoch 106 batch 20 loss 0.00791027583181858\n",
      "Test: epoch 106 batch 0 loss 0.013107594102621078\n",
      "epoch 106 finished - avarage train loss 0.005996384637847799  avarage test loss 0.01452806661836803\n",
      "Training: epoch 107 batch 0 loss 0.010501133278012276\n",
      "Training: epoch 107 batch 10 loss 0.00586464861407876\n",
      "Training: epoch 107 batch 20 loss 0.004056653007864952\n",
      "Test: epoch 107 batch 0 loss 0.009922966361045837\n",
      "epoch 107 finished - avarage train loss 0.007794789423973396  avarage test loss 0.014116092468611896\n",
      "Training: epoch 108 batch 0 loss 0.0073773302137851715\n",
      "Training: epoch 108 batch 10 loss 0.004907005466520786\n",
      "Training: epoch 108 batch 20 loss 0.007730086799710989\n",
      "Test: epoch 108 batch 0 loss 0.014792975038290024\n",
      "epoch 108 finished - avarage train loss 0.007269526173456989  avarage test loss 0.015629133442416787\n",
      "Training: epoch 109 batch 0 loss 0.008226322010159492\n",
      "Training: epoch 109 batch 10 loss 0.012538105249404907\n",
      "Training: epoch 109 batch 20 loss 0.00572114298120141\n",
      "Test: epoch 109 batch 0 loss 0.012842888943850994\n",
      "epoch 109 finished - avarage train loss 0.007141637539587401  avarage test loss 0.01395645085722208\n",
      "Training: epoch 110 batch 0 loss 0.0034844637848436832\n",
      "Training: epoch 110 batch 10 loss 0.003975105471909046\n",
      "Training: epoch 110 batch 20 loss 0.006816468667238951\n",
      "Test: epoch 110 batch 0 loss 0.010944564826786518\n",
      "epoch 110 finished - avarage train loss 0.006366665643671977  avarage test loss 0.012794342241249979\n",
      "Training: epoch 111 batch 0 loss 0.008910894393920898\n",
      "Training: epoch 111 batch 10 loss 0.0026811540592461824\n",
      "Training: epoch 111 batch 20 loss 0.010267646051943302\n",
      "Test: epoch 111 batch 0 loss 0.010490432381629944\n",
      "epoch 111 finished - avarage train loss 0.007053948906732017  avarage test loss 0.012531559681519866\n",
      "Training: epoch 112 batch 0 loss 0.0071610999293625355\n",
      "Training: epoch 112 batch 10 loss 0.009931101463735104\n",
      "Training: epoch 112 batch 20 loss 0.0036165514029562473\n",
      "Test: epoch 112 batch 0 loss 0.010948792099952698\n",
      "epoch 112 finished - avarage train loss 0.007300109586453643  avarage test loss 0.01287677837535739\n",
      "Training: epoch 113 batch 0 loss 0.0164046473801136\n",
      "Training: epoch 113 batch 10 loss 0.003788728266954422\n",
      "Training: epoch 113 batch 20 loss 0.00718346331268549\n",
      "Test: epoch 113 batch 0 loss 0.009858304634690285\n",
      "epoch 113 finished - avarage train loss 0.0070514182624375  avarage test loss 0.012477518175728619\n",
      "Training: epoch 114 batch 0 loss 0.0029989248141646385\n",
      "Training: epoch 114 batch 10 loss 0.002799547277390957\n",
      "Training: epoch 114 batch 20 loss 0.005592729896306992\n",
      "Test: epoch 114 batch 0 loss 0.009254241362214088\n",
      "epoch 114 finished - avarage train loss 0.00804178106421926  avarage test loss 0.011920295073650777\n",
      "Training: epoch 115 batch 0 loss 0.002526752883568406\n",
      "Training: epoch 115 batch 10 loss 0.0037670102901756763\n",
      "Training: epoch 115 batch 20 loss 0.003098519518971443\n",
      "Test: epoch 115 batch 0 loss 0.013809286057949066\n",
      "epoch 115 finished - avarage train loss 0.007243200711070977  avarage test loss 0.016109059331938624\n",
      "Training: epoch 116 batch 0 loss 0.004109627101570368\n",
      "Training: epoch 116 batch 10 loss 0.002437953371554613\n",
      "Training: epoch 116 batch 20 loss 0.009869087487459183\n",
      "Test: epoch 116 batch 0 loss 0.009430534206330776\n",
      "epoch 116 finished - avarage train loss 0.00620344995745811  avarage test loss 0.012631945894099772\n",
      "Training: epoch 117 batch 0 loss 0.009787585586309433\n",
      "Training: epoch 117 batch 10 loss 0.004309596959501505\n",
      "Training: epoch 117 batch 20 loss 0.004580020438879728\n",
      "Test: epoch 117 batch 0 loss 0.009691021405160427\n",
      "epoch 117 finished - avarage train loss 0.008145918058038786  avarage test loss 0.013099064817652106\n",
      "Training: epoch 118 batch 0 loss 0.003362524090334773\n",
      "Training: epoch 118 batch 10 loss 0.006619451567530632\n",
      "Training: epoch 118 batch 20 loss 0.007914711721241474\n",
      "Test: epoch 118 batch 0 loss 0.012903428636491299\n",
      "epoch 118 finished - avarage train loss 0.0074812831349092826  avarage test loss 0.01547484879847616\n",
      "Training: epoch 119 batch 0 loss 0.0047498648054897785\n",
      "Training: epoch 119 batch 10 loss 0.004828345961868763\n",
      "Training: epoch 119 batch 20 loss 0.004792698193341494\n",
      "Test: epoch 119 batch 0 loss 0.01124655082821846\n",
      "epoch 119 finished - avarage train loss 0.005751413389526564  avarage test loss 0.012521895579993725\n",
      "Training: epoch 120 batch 0 loss 0.00329846260137856\n",
      "Training: epoch 120 batch 10 loss 0.005581074394285679\n",
      "Training: epoch 120 batch 20 loss 0.00545121356844902\n",
      "Test: epoch 120 batch 0 loss 0.013453399762511253\n",
      "epoch 120 finished - avarage train loss 0.007771201479923108  avarage test loss 0.014478856581263244\n",
      "Training: epoch 121 batch 0 loss 0.005383673124015331\n",
      "Training: epoch 121 batch 10 loss 0.006093516480177641\n",
      "Training: epoch 121 batch 20 loss 0.0070708938874304295\n",
      "Test: epoch 121 batch 0 loss 0.010297128930687904\n",
      "epoch 121 finished - avarage train loss 0.006973693404218246  avarage test loss 0.013251491531264037\n",
      "Training: epoch 122 batch 0 loss 0.00790176261216402\n",
      "Training: epoch 122 batch 10 loss 0.006818275898694992\n",
      "Training: epoch 122 batch 20 loss 0.006979204714298248\n",
      "Test: epoch 122 batch 0 loss 0.013303880579769611\n",
      "epoch 122 finished - avarage train loss 0.007007884126606172  avarage test loss 0.014532171771861613\n",
      "Training: epoch 123 batch 0 loss 0.002831157762557268\n",
      "Training: epoch 123 batch 10 loss 0.009953278116881847\n",
      "Training: epoch 123 batch 20 loss 0.0036205158103257418\n",
      "Test: epoch 123 batch 0 loss 0.011231081560254097\n",
      "epoch 123 finished - avarage train loss 0.008000592958053639  avarage test loss 0.012414320837706327\n",
      "Training: epoch 124 batch 0 loss 0.005847938824445009\n",
      "Training: epoch 124 batch 10 loss 0.002142559038475156\n",
      "Training: epoch 124 batch 20 loss 0.004046475049108267\n",
      "Test: epoch 124 batch 0 loss 0.01319048460572958\n",
      "epoch 124 finished - avarage train loss 0.005568122354753572  avarage test loss 0.014829276595264673\n",
      "Training: epoch 125 batch 0 loss 0.008355988189578056\n",
      "Training: epoch 125 batch 10 loss 0.00412796251475811\n",
      "Training: epoch 125 batch 20 loss 0.003003646619617939\n",
      "Test: epoch 125 batch 0 loss 0.012886314652860165\n",
      "epoch 125 finished - avarage train loss 0.005975697326056403  avarage test loss 0.014478108379989862\n",
      "Training: epoch 126 batch 0 loss 0.006918259430676699\n",
      "Training: epoch 126 batch 10 loss 0.00621554721146822\n",
      "Training: epoch 126 batch 20 loss 0.007176202721893787\n",
      "Test: epoch 126 batch 0 loss 0.013567809946835041\n",
      "epoch 126 finished - avarage train loss 0.007943590187692437  avarage test loss 0.015231133555062115\n",
      "Training: epoch 127 batch 0 loss 0.009003677405416965\n",
      "Training: epoch 127 batch 10 loss 0.008675428107380867\n",
      "Training: epoch 127 batch 20 loss 0.002315293066203594\n",
      "Test: epoch 127 batch 0 loss 0.009714972227811813\n",
      "epoch 127 finished - avarage train loss 0.007878199938267213  avarage test loss 0.012157475051935762\n",
      "Training: epoch 128 batch 0 loss 0.0037291529588401318\n",
      "Training: epoch 128 batch 10 loss 0.004060833714902401\n",
      "Training: epoch 128 batch 20 loss 0.004560617730021477\n",
      "Test: epoch 128 batch 0 loss 0.010254692286252975\n",
      "epoch 128 finished - avarage train loss 0.007169869595913795  avarage test loss 0.012476020958274603\n",
      "Training: epoch 129 batch 0 loss 0.007605399005115032\n",
      "Training: epoch 129 batch 10 loss 0.0036496217362582684\n",
      "Training: epoch 129 batch 20 loss 0.0039991531521081924\n",
      "Test: epoch 129 batch 0 loss 0.014482027851045132\n",
      "epoch 129 finished - avarage train loss 0.007869143986367974  avarage test loss 0.015460705384612083\n",
      "Training: epoch 130 batch 0 loss 0.005997530184686184\n",
      "Training: epoch 130 batch 10 loss 0.002698314841836691\n",
      "Training: epoch 130 batch 20 loss 0.0035057219211012125\n",
      "Test: epoch 130 batch 0 loss 0.01126579288393259\n",
      "epoch 130 finished - avarage train loss 0.006531943579140152  avarage test loss 0.012673149700276554\n",
      "Training: epoch 131 batch 0 loss 0.00496940640732646\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training: epoch 131 batch 10 loss 0.003033812390640378\n",
      "Training: epoch 131 batch 20 loss 0.002357098273932934\n",
      "Test: epoch 131 batch 0 loss 0.012037646025419235\n",
      "epoch 131 finished - avarage train loss 0.005890217497302541  avarage test loss 0.012872889405116439\n",
      "Training: epoch 132 batch 0 loss 0.004905457142740488\n",
      "Training: epoch 132 batch 10 loss 0.018871119245886803\n",
      "Training: epoch 132 batch 20 loss 0.005848424509167671\n",
      "Test: epoch 132 batch 0 loss 0.010785951279103756\n",
      "epoch 132 finished - avarage train loss 0.00689309387435687  avarage test loss 0.012849287362769246\n",
      "Training: epoch 133 batch 0 loss 0.006984318140894175\n",
      "Training: epoch 133 batch 10 loss 0.006122882012277842\n",
      "Training: epoch 133 batch 20 loss 0.010034052655100822\n",
      "Test: epoch 133 batch 0 loss 0.01129161473363638\n",
      "epoch 133 finished - avarage train loss 0.006835024241871875  avarage test loss 0.01253126235678792\n",
      "Training: epoch 134 batch 0 loss 0.008075034245848656\n",
      "Training: epoch 134 batch 10 loss 0.004218636080622673\n",
      "Training: epoch 134 batch 20 loss 0.006789986044168472\n",
      "Test: epoch 134 batch 0 loss 0.012127927504479885\n",
      "epoch 134 finished - avarage train loss 0.007255748885780059  avarage test loss 0.013293003779835999\n",
      "Training: epoch 135 batch 0 loss 0.004919486120343208\n",
      "Training: epoch 135 batch 10 loss 0.006460359785705805\n",
      "Training: epoch 135 batch 20 loss 0.002462361939251423\n",
      "Test: epoch 135 batch 0 loss 0.011333759874105453\n",
      "epoch 135 finished - avarage train loss 0.006327031099976137  avarage test loss 0.012639682041481137\n",
      "Training: epoch 136 batch 0 loss 0.010968479327857494\n",
      "Training: epoch 136 batch 10 loss 0.004451404325664043\n",
      "Training: epoch 136 batch 20 loss 0.00706806406378746\n",
      "Test: epoch 136 batch 0 loss 0.013147695921361446\n",
      "epoch 136 finished - avarage train loss 0.006426640208168277  avarage test loss 0.0145327482605353\n",
      "Training: epoch 137 batch 0 loss 0.002630847040563822\n",
      "Training: epoch 137 batch 10 loss 0.004698400851339102\n",
      "Training: epoch 137 batch 20 loss 0.004727657418698072\n",
      "Test: epoch 137 batch 0 loss 0.01499367505311966\n",
      "epoch 137 finished - avarage train loss 0.00718485107565106  avarage test loss 0.01818833383731544\n",
      "Training: epoch 138 batch 0 loss 0.003833633614704013\n",
      "Training: epoch 138 batch 10 loss 0.006327543407678604\n",
      "Training: epoch 138 batch 20 loss 0.015468320809304714\n",
      "Test: epoch 138 batch 0 loss 0.012277921661734581\n",
      "epoch 138 finished - avarage train loss 0.010085747553164074  avarage test loss 0.013942108256742358\n",
      "Training: epoch 139 batch 0 loss 0.006242395378649235\n",
      "Training: epoch 139 batch 10 loss 0.009106570854783058\n",
      "Training: epoch 139 batch 20 loss 0.01029275543987751\n",
      "Test: epoch 139 batch 0 loss 0.012915253639221191\n",
      "epoch 139 finished - avarage train loss 0.0076495897875906065  avarage test loss 0.015411074506118894\n",
      "Training: epoch 140 batch 0 loss 0.007474901620298624\n",
      "Training: epoch 140 batch 10 loss 0.0056114401668310165\n",
      "Training: epoch 140 batch 20 loss 0.0019496033200994134\n",
      "Test: epoch 140 batch 0 loss 0.014421176165342331\n",
      "epoch 140 finished - avarage train loss 0.007266201877741721  avarage test loss 0.016103655798360705\n",
      "Training: epoch 141 batch 0 loss 0.005823771934956312\n",
      "Training: epoch 141 batch 10 loss 0.009554554708302021\n",
      "Training: epoch 141 batch 20 loss 0.007741935085505247\n",
      "Test: epoch 141 batch 0 loss 0.011021167039871216\n",
      "epoch 141 finished - avarage train loss 0.00749625435002662  avarage test loss 0.012882908573374152\n",
      "Training: epoch 142 batch 0 loss 0.00573157612234354\n",
      "Training: epoch 142 batch 10 loss 0.008204071782529354\n",
      "Training: epoch 142 batch 20 loss 0.010649431496858597\n",
      "Test: epoch 142 batch 0 loss 0.011265103705227375\n",
      "epoch 142 finished - avarage train loss 0.00833847610568949  avarage test loss 0.01398579915985465\n",
      "Training: epoch 143 batch 0 loss 0.007091076113283634\n",
      "Training: epoch 143 batch 10 loss 0.004138635471463203\n",
      "Training: epoch 143 batch 20 loss 0.0066689737141132355\n",
      "Test: epoch 143 batch 0 loss 0.012106877751648426\n",
      "epoch 143 finished - avarage train loss 0.006306816470520249  avarage test loss 0.013318824581801891\n",
      "Training: epoch 144 batch 0 loss 0.005921347066760063\n",
      "Training: epoch 144 batch 10 loss 0.003930822480469942\n",
      "Training: epoch 144 batch 20 loss 0.004864525981247425\n",
      "Test: epoch 144 batch 0 loss 0.01021679025143385\n",
      "epoch 144 finished - avarage train loss 0.006278926803698313  avarage test loss 0.012664858717471361\n",
      "Training: epoch 145 batch 0 loss 0.004478455521166325\n",
      "Training: epoch 145 batch 10 loss 0.010186376050114632\n",
      "Training: epoch 145 batch 20 loss 0.004150502849370241\n",
      "Test: epoch 145 batch 0 loss 0.018812822178006172\n",
      "epoch 145 finished - avarage train loss 0.007405573429925175  avarage test loss 0.01987199834547937\n",
      "Training: epoch 146 batch 0 loss 0.015344091691076756\n",
      "Training: epoch 146 batch 10 loss 0.002979365410283208\n",
      "Training: epoch 146 batch 20 loss 0.006030688993632793\n",
      "Test: epoch 146 batch 0 loss 0.012038749642670155\n",
      "epoch 146 finished - avarage train loss 0.010860202785838267  avarage test loss 0.013148822588846087\n",
      "Training: epoch 147 batch 0 loss 0.007933019660413265\n",
      "Training: epoch 147 batch 10 loss 0.00889021810144186\n",
      "Training: epoch 147 batch 20 loss 0.0081030847504735\n",
      "Test: epoch 147 batch 0 loss 0.01245939638465643\n",
      "epoch 147 finished - avarage train loss 0.006629439656911739  avarage test loss 0.01341874129138887\n",
      "Training: epoch 148 batch 0 loss 0.004387711174786091\n",
      "Training: epoch 148 batch 10 loss 0.00498079601675272\n",
      "Training: epoch 148 batch 20 loss 0.004511569626629353\n",
      "Test: epoch 148 batch 0 loss 0.011456126347184181\n",
      "epoch 148 finished - avarage train loss 0.007251175176673408  avarage test loss 0.012950309668667614\n",
      "Training: epoch 149 batch 0 loss 0.00276330579072237\n",
      "Training: epoch 149 batch 10 loss 0.002786325290799141\n",
      "Training: epoch 149 batch 20 loss 0.003631760599091649\n",
      "Test: epoch 149 batch 0 loss 0.011351861990988255\n",
      "epoch 149 finished - avarage train loss 0.0059945027520558955  avarage test loss 0.01310819387435913\n",
      "Training: epoch 150 batch 0 loss 0.004712214227765799\n",
      "Training: epoch 150 batch 10 loss 0.007859419099986553\n",
      "Training: epoch 150 batch 20 loss 0.005961548071354628\n",
      "Test: epoch 150 batch 0 loss 0.011338027194142342\n",
      "epoch 150 finished - avarage train loss 0.00917158541590746  avarage test loss 0.013159487978555262\n",
      "Training: epoch 151 batch 0 loss 0.0069497125223279\n",
      "Training: epoch 151 batch 10 loss 0.013296114280819893\n",
      "Training: epoch 151 batch 20 loss 0.001994348829612136\n",
      "Test: epoch 151 batch 0 loss 0.012433289550244808\n",
      "epoch 151 finished - avarage train loss 0.007377689447382401  avarage test loss 0.013423922471702099\n",
      "Training: epoch 152 batch 0 loss 0.0027679032646119595\n",
      "Training: epoch 152 batch 10 loss 0.002204813063144684\n",
      "Training: epoch 152 batch 20 loss 0.002556944964453578\n",
      "Test: epoch 152 batch 0 loss 0.013396811671555042\n",
      "epoch 152 finished - avarage train loss 0.00577067407972083  avarage test loss 0.014178723678924143\n",
      "Training: epoch 153 batch 0 loss 0.0050577218644320965\n",
      "Training: epoch 153 batch 10 loss 0.004249182529747486\n",
      "Training: epoch 153 batch 20 loss 0.00718922633677721\n",
      "Test: epoch 153 batch 0 loss 0.01217829342931509\n",
      "epoch 153 finished - avarage train loss 0.0066351353849188  avarage test loss 0.013231280725449324\n",
      "Training: epoch 154 batch 0 loss 0.011709148064255714\n",
      "Training: epoch 154 batch 10 loss 0.003084525465965271\n",
      "Training: epoch 154 batch 20 loss 0.0031132337171584368\n",
      "Test: epoch 154 batch 0 loss 0.01184358261525631\n",
      "epoch 154 finished - avarage train loss 0.006480298245903747  avarage test loss 0.012779244221746922\n",
      "Training: epoch 155 batch 0 loss 0.006722424179315567\n",
      "Training: epoch 155 batch 10 loss 0.018117059022188187\n",
      "Training: epoch 155 batch 20 loss 0.00588316610082984\n",
      "Test: epoch 155 batch 0 loss 0.011212690733373165\n",
      "epoch 155 finished - avarage train loss 0.00713179052149042  avarage test loss 0.012747551780194044\n",
      "Training: epoch 156 batch 0 loss 0.009311530739068985\n",
      "Training: epoch 156 batch 10 loss 0.007255955599248409\n",
      "Training: epoch 156 batch 20 loss 0.005550839938223362\n",
      "Test: epoch 156 batch 0 loss 0.015272394753992558\n",
      "epoch 156 finished - avarage train loss 0.006109227731438546  avarage test loss 0.0168151818215847\n",
      "Training: epoch 157 batch 0 loss 0.007582631427794695\n",
      "Training: epoch 157 batch 10 loss 0.003678527195006609\n",
      "Training: epoch 157 batch 20 loss 0.002674346324056387\n",
      "Test: epoch 157 batch 0 loss 0.013894625008106232\n",
      "epoch 157 finished - avarage train loss 0.0077065149932329  avarage test loss 0.014413286000490189\n",
      "Training: epoch 158 batch 0 loss 0.007282969541847706\n",
      "Training: epoch 158 batch 10 loss 0.0051879738457500935\n",
      "Training: epoch 158 batch 20 loss 0.007241385988891125\n",
      "Test: epoch 158 batch 0 loss 0.014800074510276318\n",
      "epoch 158 finished - avarage train loss 0.007445872094931787  avarage test loss 0.016004396369680762\n",
      "Training: epoch 159 batch 0 loss 0.004706650972366333\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training: epoch 159 batch 10 loss 0.006530340760946274\n",
      "Training: epoch 159 batch 20 loss 0.011295410804450512\n",
      "Test: epoch 159 batch 0 loss 0.011352799832820892\n",
      "epoch 159 finished - avarage train loss 0.008114609345472578  avarage test loss 0.012367225019261241\n",
      "Training: epoch 160 batch 0 loss 0.0032687310595065355\n",
      "Training: epoch 160 batch 10 loss 0.005265601444989443\n",
      "Training: epoch 160 batch 20 loss 0.0041104573756456375\n",
      "Test: epoch 160 batch 0 loss 0.010791655629873276\n",
      "epoch 160 finished - avarage train loss 0.006428187343709427  avarage test loss 0.012860835413448513\n",
      "Training: epoch 161 batch 0 loss 0.004835959058254957\n",
      "Training: epoch 161 batch 10 loss 0.003337792120873928\n",
      "Training: epoch 161 batch 20 loss 0.00469678919762373\n",
      "Test: epoch 161 batch 0 loss 0.013973207212984562\n",
      "epoch 161 finished - avarage train loss 0.006754204494364816  avarage test loss 0.01526283542625606\n",
      "Training: epoch 162 batch 0 loss 0.006197625771164894\n",
      "Training: epoch 162 batch 10 loss 0.006099789869040251\n",
      "Training: epoch 162 batch 20 loss 0.007101160474121571\n",
      "Test: epoch 162 batch 0 loss 0.0120858671143651\n",
      "epoch 162 finished - avarage train loss 0.008415787713602185  avarage test loss 0.013546322821639478\n",
      "Training: epoch 163 batch 0 loss 0.00546102924272418\n",
      "Training: epoch 163 batch 10 loss 0.004584955982863903\n",
      "Training: epoch 163 batch 20 loss 0.005306052044034004\n",
      "Test: epoch 163 batch 0 loss 0.014041337184607983\n",
      "epoch 163 finished - avarage train loss 0.007397544827183773  avarage test loss 0.01490386319346726\n",
      "Training: epoch 164 batch 0 loss 0.0022050361149013042\n",
      "Training: epoch 164 batch 10 loss 0.016075624153017998\n",
      "Training: epoch 164 batch 20 loss 0.010412953794002533\n",
      "Test: epoch 164 batch 0 loss 0.011921276338398457\n",
      "epoch 164 finished - avarage train loss 0.00916604007241027  avarage test loss 0.012805181788280606\n",
      "Training: epoch 165 batch 0 loss 0.003631874918937683\n",
      "Training: epoch 165 batch 10 loss 0.008250596933066845\n",
      "Training: epoch 165 batch 20 loss 0.0023229815997183323\n",
      "Test: epoch 165 batch 0 loss 0.013607081025838852\n",
      "epoch 165 finished - avarage train loss 0.007375467271576154  avarage test loss 0.014339304529130459\n",
      "Training: epoch 166 batch 0 loss 0.003075449727475643\n",
      "Training: epoch 166 batch 10 loss 0.004581722430884838\n",
      "Training: epoch 166 batch 20 loss 0.0029827342368662357\n",
      "Test: epoch 166 batch 0 loss 0.01476228330284357\n",
      "epoch 166 finished - avarage train loss 0.006914598835182601  avarage test loss 0.015183912590146065\n",
      "Training: epoch 167 batch 0 loss 0.010223714634776115\n",
      "Training: epoch 167 batch 10 loss 0.004195446614176035\n",
      "Training: epoch 167 batch 20 loss 0.007937881164252758\n",
      "Test: epoch 167 batch 0 loss 0.014540407806634903\n",
      "epoch 167 finished - avarage train loss 0.007187075712236351  avarage test loss 0.016226754523813725\n",
      "Training: epoch 168 batch 0 loss 0.00909420382231474\n",
      "Training: epoch 168 batch 10 loss 0.010183556005358696\n",
      "Training: epoch 168 batch 20 loss 0.0057219224981963634\n",
      "Test: epoch 168 batch 0 loss 0.0111938351765275\n",
      "epoch 168 finished - avarage train loss 0.008016343605865178  avarage test loss 0.012443830608390272\n",
      "Training: epoch 169 batch 0 loss 0.0066815027967095375\n",
      "Training: epoch 169 batch 10 loss 0.0040523335337638855\n",
      "Training: epoch 169 batch 20 loss 0.004610074684023857\n",
      "Test: epoch 169 batch 0 loss 0.013137227855622768\n",
      "epoch 169 finished - avarage train loss 0.006770230347997156  avarage test loss 0.013752028113231063\n",
      "Training: epoch 170 batch 0 loss 0.009584231302142143\n",
      "Training: epoch 170 batch 10 loss 0.00655579287558794\n",
      "Training: epoch 170 batch 20 loss 0.010659455321729183\n",
      "Test: epoch 170 batch 0 loss 0.012765194289386272\n",
      "epoch 170 finished - avarage train loss 0.008747946262231162  avarage test loss 0.01312014483846724\n",
      "Training: epoch 171 batch 0 loss 0.004498484544456005\n",
      "Training: epoch 171 batch 10 loss 0.002637450350448489\n",
      "Training: epoch 171 batch 20 loss 0.011065332219004631\n",
      "Test: epoch 171 batch 0 loss 0.011057010851800442\n",
      "epoch 171 finished - avarage train loss 0.007387220654797195  avarage test loss 0.014750823611393571\n",
      "Training: epoch 172 batch 0 loss 0.00411040335893631\n",
      "Training: epoch 172 batch 10 loss 0.0061317263171076775\n",
      "Training: epoch 172 batch 20 loss 0.0021089836955070496\n",
      "Test: epoch 172 batch 0 loss 0.013744471594691277\n",
      "epoch 172 finished - avarage train loss 0.007331057523923187  avarage test loss 0.014374205493368208\n",
      "Training: epoch 173 batch 0 loss 0.01020191702991724\n",
      "Training: epoch 173 batch 10 loss 0.008898968808352947\n",
      "Training: epoch 173 batch 20 loss 0.0040191709995269775\n",
      "Test: epoch 173 batch 0 loss 0.013044025748968124\n",
      "epoch 173 finished - avarage train loss 0.007218937329337771  avarage test loss 0.01372771873138845\n",
      "Training: epoch 174 batch 0 loss 0.006321339402347803\n",
      "Training: epoch 174 batch 10 loss 0.004905484151095152\n",
      "Training: epoch 174 batch 20 loss 0.002213407075032592\n",
      "Test: epoch 174 batch 0 loss 0.011373830027878284\n",
      "epoch 174 finished - avarage train loss 0.008218029449725973  avarage test loss 0.013039215467870235\n",
      "Training: epoch 175 batch 0 loss 0.004914908669888973\n",
      "Training: epoch 175 batch 10 loss 0.009822897613048553\n",
      "Training: epoch 175 batch 20 loss 0.0031745124142616987\n",
      "Test: epoch 175 batch 0 loss 0.011183034628629684\n",
      "epoch 175 finished - avarage train loss 0.007678684469408773  avarage test loss 0.012402188731357455\n",
      "Training: epoch 176 batch 0 loss 0.007863274775445461\n",
      "Training: epoch 176 batch 10 loss 0.007207019254565239\n",
      "Training: epoch 176 batch 20 loss 0.008817240595817566\n",
      "Test: epoch 176 batch 0 loss 0.011287045665085316\n",
      "epoch 176 finished - avarage train loss 0.0064616692091884285  avarage test loss 0.013114026165567338\n",
      "Training: epoch 177 batch 0 loss 0.008665986359119415\n",
      "Training: epoch 177 batch 10 loss 0.0035718553699553013\n",
      "Training: epoch 177 batch 20 loss 0.0036403127014636993\n",
      "Test: epoch 177 batch 0 loss 0.017352864146232605\n",
      "epoch 177 finished - avarage train loss 0.0069634970919839265  avarage test loss 0.021387949585914612\n",
      "Training: epoch 178 batch 0 loss 0.01244531199336052\n",
      "Training: epoch 178 batch 10 loss 0.019600635394454002\n",
      "Training: epoch 178 batch 20 loss 0.007030342239886522\n",
      "Test: epoch 178 batch 0 loss 0.011192628182470798\n",
      "epoch 178 finished - avarage train loss 0.012383170666751164  avarage test loss 0.01467613154090941\n",
      "Training: epoch 179 batch 0 loss 0.007531533949077129\n",
      "Training: epoch 179 batch 10 loss 0.016792625188827515\n",
      "Training: epoch 179 batch 20 loss 0.0036009959876537323\n",
      "Test: epoch 179 batch 0 loss 0.010985072702169418\n",
      "epoch 179 finished - avarage train loss 0.009372671512116132  avarage test loss 0.013709975290112197\n",
      "Training: epoch 180 batch 0 loss 0.008628477342426777\n",
      "Training: epoch 180 batch 10 loss 0.004095248878002167\n",
      "Training: epoch 180 batch 20 loss 0.0030452455393970013\n",
      "Test: epoch 180 batch 0 loss 0.013069731183350086\n",
      "epoch 180 finished - avarage train loss 0.008514230504856798  avarage test loss 0.014275152469053864\n",
      "Training: epoch 181 batch 0 loss 0.005958632566034794\n",
      "Training: epoch 181 batch 10 loss 0.004314848221838474\n",
      "Training: epoch 181 batch 20 loss 0.004371847491711378\n",
      "Test: epoch 181 batch 0 loss 0.01547886710613966\n",
      "epoch 181 finished - avarage train loss 0.006673439566431375  avarage test loss 0.018744715955108404\n",
      "Training: epoch 182 batch 0 loss 0.00772792799398303\n",
      "Training: epoch 182 batch 10 loss 0.02249857597053051\n",
      "Training: epoch 182 batch 20 loss 0.005261098500341177\n",
      "Test: epoch 182 batch 0 loss 0.010892017744481564\n",
      "epoch 182 finished - avarage train loss 0.01029877546885661  avarage test loss 0.01679074135608971\n",
      "Training: epoch 183 batch 0 loss 0.00706997886300087\n",
      "Training: epoch 183 batch 10 loss 0.005310118198394775\n",
      "Training: epoch 183 batch 20 loss 0.004800270311534405\n",
      "Test: epoch 183 batch 0 loss 0.010899006389081478\n",
      "epoch 183 finished - avarage train loss 0.007959895181180588  avarage test loss 0.014042404480278492\n",
      "Training: epoch 184 batch 0 loss 0.01047362107783556\n",
      "Training: epoch 184 batch 10 loss 0.0045704017393291\n",
      "Training: epoch 184 batch 20 loss 0.003266532439738512\n",
      "Test: epoch 184 batch 0 loss 0.013313363306224346\n",
      "epoch 184 finished - avarage train loss 0.009491588775839272  avarage test loss 0.015112419030629098\n",
      "Training: epoch 185 batch 0 loss 0.0037151463329792023\n",
      "Training: epoch 185 batch 10 loss 0.01078601460903883\n",
      "Training: epoch 185 batch 20 loss 0.004283819813281298\n",
      "Test: epoch 185 batch 0 loss 0.01266010943800211\n",
      "epoch 185 finished - avarage train loss 0.008067543795011166  avarage test loss 0.01399355847388506\n",
      "Training: epoch 186 batch 0 loss 0.006607711315155029\n",
      "Training: epoch 186 batch 10 loss 0.00778122479096055\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training: epoch 186 batch 20 loss 0.003158442210406065\n",
      "Test: epoch 186 batch 0 loss 0.015329834073781967\n",
      "epoch 186 finished - avarage train loss 0.007105290978055062  avarage test loss 0.01783390319906175\n",
      "Training: epoch 187 batch 0 loss 0.00527802761644125\n",
      "Training: epoch 187 batch 10 loss 0.006907087285071611\n",
      "Training: epoch 187 batch 20 loss 0.004648618400096893\n",
      "Test: epoch 187 batch 0 loss 0.012489656917750835\n",
      "epoch 187 finished - avarage train loss 0.007242718081246933  avarage test loss 0.013212886755354702\n",
      "Training: epoch 188 batch 0 loss 0.004878674168139696\n",
      "Training: epoch 188 batch 10 loss 0.0038163396529853344\n",
      "Training: epoch 188 batch 20 loss 0.0017996665555983782\n",
      "Test: epoch 188 batch 0 loss 0.012728889472782612\n",
      "epoch 188 finished - avarage train loss 0.0075522135172424645  avarage test loss 0.01430448133032769\n",
      "Training: epoch 189 batch 0 loss 0.005086349789053202\n",
      "Training: epoch 189 batch 10 loss 0.0069807893596589565\n",
      "Training: epoch 189 batch 20 loss 0.0032055778428912163\n",
      "Test: epoch 189 batch 0 loss 0.012681367807090282\n",
      "epoch 189 finished - avarage train loss 0.006414293239278526  avarage test loss 0.013697492191568017\n",
      "Training: epoch 190 batch 0 loss 0.006347211543470621\n",
      "Training: epoch 190 batch 10 loss 0.004488199483603239\n",
      "Training: epoch 190 batch 20 loss 0.006662923842668533\n",
      "Test: epoch 190 batch 0 loss 0.013391568325459957\n",
      "epoch 190 finished - avarage train loss 0.008052479242905974  avarage test loss 0.01520561520010233\n",
      "Training: epoch 191 batch 0 loss 0.0066321492195129395\n",
      "Training: epoch 191 batch 10 loss 0.0027979856822639704\n",
      "Training: epoch 191 batch 20 loss 0.003340315306559205\n",
      "Test: epoch 191 batch 0 loss 0.010415956377983093\n",
      "epoch 191 finished - avarage train loss 0.007855510909174537  avarage test loss 0.012777824536897242\n",
      "Training: epoch 192 batch 0 loss 0.004629321396350861\n",
      "Training: epoch 192 batch 10 loss 0.006123787723481655\n",
      "Training: epoch 192 batch 20 loss 0.006249734666198492\n",
      "Test: epoch 192 batch 0 loss 0.014448603615164757\n",
      "epoch 192 finished - avarage train loss 0.006262052093696748  avarage test loss 0.016376623418182135\n",
      "Training: epoch 193 batch 0 loss 0.007022150792181492\n",
      "Training: epoch 193 batch 10 loss 0.006818120367825031\n",
      "Training: epoch 193 batch 20 loss 0.004117639269679785\n",
      "Test: epoch 193 batch 0 loss 0.013086152262985706\n",
      "epoch 193 finished - avarage train loss 0.007831913547526145  avarage test loss 0.014210540568456054\n",
      "Training: epoch 194 batch 0 loss 0.007589623332023621\n",
      "Training: epoch 194 batch 10 loss 0.003646572818979621\n",
      "Training: epoch 194 batch 20 loss 0.002268881071358919\n",
      "Test: epoch 194 batch 0 loss 0.01108900923281908\n",
      "epoch 194 finished - avarage train loss 0.006352245574817061  avarage test loss 0.01330497208982706\n",
      "Training: epoch 195 batch 0 loss 0.00786360539495945\n",
      "Training: epoch 195 batch 10 loss 0.004072442185133696\n",
      "Training: epoch 195 batch 20 loss 0.007971439510583878\n",
      "Test: epoch 195 batch 0 loss 0.013294396921992302\n",
      "epoch 195 finished - avarage train loss 0.0073478206478316205  avarage test loss 0.014521097764372826\n",
      "Training: epoch 196 batch 0 loss 0.005191469099372625\n",
      "Training: epoch 196 batch 10 loss 0.0021635275334119797\n",
      "Training: epoch 196 batch 20 loss 0.00815627258270979\n",
      "Test: epoch 196 batch 0 loss 0.01106406468898058\n",
      "epoch 196 finished - avarage train loss 0.0059697610358225885  avarage test loss 0.01298221992328763\n",
      "Training: epoch 197 batch 0 loss 0.0035491078160703182\n",
      "Training: epoch 197 batch 10 loss 0.0036677627358585596\n",
      "Training: epoch 197 batch 20 loss 0.0028125662356615067\n",
      "Test: epoch 197 batch 0 loss 0.011059817858040333\n",
      "epoch 197 finished - avarage train loss 0.006274649748931928  avarage test loss 0.012337645515799522\n",
      "Training: epoch 198 batch 0 loss 0.0049301027320325375\n",
      "Training: epoch 198 batch 10 loss 0.004023821093142033\n",
      "Training: epoch 198 batch 20 loss 0.003836361225694418\n",
      "Test: epoch 198 batch 0 loss 0.011091526597738266\n",
      "epoch 198 finished - avarage train loss 0.006928562748663384  avarage test loss 0.012224979815073311\n",
      "Training: epoch 199 batch 0 loss 0.0022827577777206898\n",
      "Training: epoch 199 batch 10 loss 0.0035385293886065483\n",
      "Training: epoch 199 batch 20 loss 0.005422375164926052\n",
      "Test: epoch 199 batch 0 loss 0.01192713063210249\n",
      "epoch 199 finished - avarage train loss 0.006399860095778673  avarage test loss 0.013237919309176505\n",
      "Training: epoch 0 batch 0 loss 0.6132044196128845\n",
      "Training: epoch 0 batch 10 loss 0.476012259721756\n",
      "Training: epoch 0 batch 20 loss 0.5815576910972595\n",
      "Test: epoch 0 batch 0 loss 0.3939953148365021\n",
      "epoch 0 finished - avarage train loss 0.525508541485359  avarage test loss 0.44297707453370094\n",
      "Training: epoch 1 batch 0 loss 0.8641027212142944\n",
      "Training: epoch 1 batch 10 loss 0.5484193563461304\n",
      "Training: epoch 1 batch 20 loss 0.429554283618927\n",
      "Test: epoch 1 batch 0 loss 0.39064568281173706\n",
      "epoch 1 finished - avarage train loss 0.5047916513064812  avarage test loss 0.4330955632030964\n",
      "Training: epoch 2 batch 0 loss 0.4119456112384796\n",
      "Training: epoch 2 batch 10 loss 0.7351114153862\n",
      "Training: epoch 2 batch 20 loss 0.42949584126472473\n",
      "Test: epoch 2 batch 0 loss 0.3938234746456146\n",
      "epoch 2 finished - avarage train loss 0.5106772991089985  avarage test loss 0.4377176910638809\n",
      "Training: epoch 3 batch 0 loss 0.44248244166374207\n",
      "Training: epoch 3 batch 10 loss 0.4709799885749817\n",
      "Training: epoch 3 batch 20 loss 0.6382970809936523\n",
      "Test: epoch 3 batch 0 loss 0.3986748456954956\n",
      "epoch 3 finished - avarage train loss 0.49657417628271827  avarage test loss 0.4400351047515869\n",
      "Training: epoch 4 batch 0 loss 0.7002326846122742\n",
      "Training: epoch 4 batch 10 loss 0.5670582056045532\n",
      "Training: epoch 4 batch 20 loss 0.6474523544311523\n",
      "Test: epoch 4 batch 0 loss 0.3911120891571045\n",
      "epoch 4 finished - avarage train loss 0.5058026046588503  avarage test loss 0.4334375411272049\n",
      "Training: epoch 5 batch 0 loss 0.4752887189388275\n",
      "Training: epoch 5 batch 10 loss 0.7235924005508423\n",
      "Training: epoch 5 batch 20 loss 0.44415268301963806\n",
      "Test: epoch 5 batch 0 loss 0.4005599319934845\n",
      "epoch 5 finished - avarage train loss 0.5040829120011165  avarage test loss 0.44169917702674866\n",
      "Training: epoch 6 batch 0 loss 0.29610300064086914\n",
      "Training: epoch 6 batch 10 loss 0.5152206420898438\n",
      "Training: epoch 6 batch 20 loss 0.44377633929252625\n",
      "Test: epoch 6 batch 0 loss 0.39513927698135376\n",
      "epoch 6 finished - avarage train loss 0.514181656056437  avarage test loss 0.43820745870471\n",
      "Training: epoch 7 batch 0 loss 0.6381806135177612\n",
      "Training: epoch 7 batch 10 loss 0.35344335436820984\n",
      "Training: epoch 7 batch 20 loss 0.8579195737838745\n",
      "Test: epoch 7 batch 0 loss 0.40013396739959717\n",
      "epoch 7 finished - avarage train loss 0.510543803716528  avarage test loss 0.4406915754079819\n",
      "Training: epoch 8 batch 0 loss 0.5408387184143066\n",
      "Training: epoch 8 batch 10 loss 0.4203833341598511\n",
      "Training: epoch 8 batch 20 loss 0.5305808782577515\n",
      "Test: epoch 8 batch 0 loss 0.3977971076965332\n",
      "epoch 8 finished - avarage train loss 0.5158575228576002  avarage test loss 0.44086122512817383\n",
      "Training: epoch 9 batch 0 loss 0.5784242749214172\n",
      "Training: epoch 9 batch 10 loss 0.3367522060871124\n",
      "Training: epoch 9 batch 20 loss 0.266834020614624\n",
      "Test: epoch 9 batch 0 loss 0.3918532133102417\n",
      "epoch 9 finished - avarage train loss 0.5102910723151832  avarage test loss 0.43706679344177246\n",
      "Training: epoch 10 batch 0 loss 0.42585793137550354\n",
      "Training: epoch 10 batch 10 loss 0.7117319107055664\n",
      "Training: epoch 10 batch 20 loss 0.5661912560462952\n",
      "Test: epoch 10 batch 0 loss 0.39682871103286743\n",
      "epoch 10 finished - avarage train loss 0.5067380276219599  avarage test loss 0.4367513135075569\n",
      "Training: epoch 11 batch 0 loss 0.43522942066192627\n",
      "Training: epoch 11 batch 10 loss 0.34947115182876587\n",
      "Training: epoch 11 batch 20 loss 0.5151396989822388\n",
      "Test: epoch 11 batch 0 loss 0.3984232544898987\n",
      "epoch 11 finished - avarage train loss 0.5061480629033056  avarage test loss 0.4388665556907654\n",
      "Training: epoch 12 batch 0 loss 0.48998621106147766\n",
      "Training: epoch 12 batch 10 loss 0.34108787775039673\n",
      "Training: epoch 12 batch 20 loss 0.44852739572525024\n",
      "Test: epoch 12 batch 0 loss 0.2045409381389618\n",
      "epoch 12 finished - avarage train loss 0.4844005688511092  avarage test loss 0.23951781913638115\n",
      "Training: epoch 13 batch 0 loss 0.3107629120349884\n",
      "Training: epoch 13 batch 10 loss 0.22325190901756287\n",
      "Training: epoch 13 batch 20 loss 0.16137246787548065\n",
      "Test: epoch 13 batch 0 loss 0.08674854785203934\n",
      "epoch 13 finished - avarage train loss 0.2689672790724656  avarage test loss 0.13516532629728317\n",
      "Training: epoch 14 batch 0 loss 0.1796204149723053\n",
      "Training: epoch 14 batch 10 loss 0.06950320303440094\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training: epoch 14 batch 20 loss 0.13831724226474762\n",
      "Test: epoch 14 batch 0 loss 0.02669038623571396\n",
      "epoch 14 finished - avarage train loss 0.1400836855173111  avarage test loss 0.024491409305483103\n",
      "Training: epoch 15 batch 0 loss 0.03778601065278053\n",
      "Training: epoch 15 batch 10 loss 0.012935640290379524\n",
      "Training: epoch 15 batch 20 loss 0.013377026654779911\n",
      "Test: epoch 15 batch 0 loss 0.01783262938261032\n",
      "epoch 15 finished - avarage train loss 0.022207809226780104  avarage test loss 0.021029542200267315\n",
      "Training: epoch 16 batch 0 loss 0.014879165217280388\n",
      "Training: epoch 16 batch 10 loss 0.016336897388100624\n",
      "Training: epoch 16 batch 20 loss 0.0159076526761055\n",
      "Test: epoch 16 batch 0 loss 0.012603798881173134\n",
      "epoch 16 finished - avarage train loss 0.015573117127320889  avarage test loss 0.016148336231708527\n",
      "Training: epoch 17 batch 0 loss 0.009110936895012856\n",
      "Training: epoch 17 batch 10 loss 0.008280121721327305\n",
      "Training: epoch 17 batch 20 loss 0.0053109535947442055\n",
      "Test: epoch 17 batch 0 loss 0.012490184977650642\n",
      "epoch 17 finished - avarage train loss 0.012224678287346816  avarage test loss 0.01580085977911949\n",
      "Training: epoch 18 batch 0 loss 0.006064611487090588\n",
      "Training: epoch 18 batch 10 loss 0.01355187501758337\n",
      "Training: epoch 18 batch 20 loss 0.007253546267747879\n",
      "Test: epoch 18 batch 0 loss 0.011998592875897884\n",
      "epoch 18 finished - avarage train loss 0.011707374023209358  avarage test loss 0.017324423184618354\n",
      "Training: epoch 19 batch 0 loss 0.014019928872585297\n",
      "Training: epoch 19 batch 10 loss 0.014867942780256271\n",
      "Training: epoch 19 batch 20 loss 0.008014443330466747\n",
      "Test: epoch 19 batch 0 loss 0.011661841534078121\n",
      "epoch 19 finished - avarage train loss 0.009743699868177545  avarage test loss 0.01598909101448953\n",
      "Training: epoch 20 batch 0 loss 0.015049025416374207\n",
      "Training: epoch 20 batch 10 loss 0.00807239394634962\n",
      "Training: epoch 20 batch 20 loss 0.00750355701893568\n",
      "Test: epoch 20 batch 0 loss 0.010009435005486012\n",
      "epoch 20 finished - avarage train loss 0.00935713060457131  avarage test loss 0.014483245555311441\n",
      "Training: epoch 21 batch 0 loss 0.005259054712951183\n",
      "Training: epoch 21 batch 10 loss 0.009313106536865234\n",
      "Training: epoch 21 batch 20 loss 0.01067992765456438\n",
      "Test: epoch 21 batch 0 loss 0.01300168875604868\n",
      "epoch 21 finished - avarage train loss 0.010181888865692348  avarage test loss 0.017276888247579336\n",
      "Training: epoch 22 batch 0 loss 0.014903255738317966\n",
      "Training: epoch 22 batch 10 loss 0.010323403403162956\n",
      "Training: epoch 22 batch 20 loss 0.0038351803086698055\n",
      "Test: epoch 22 batch 0 loss 0.010740234516561031\n",
      "epoch 22 finished - avarage train loss 0.01094090122857998  avarage test loss 0.01468384894542396\n",
      "Training: epoch 23 batch 0 loss 0.008394503965973854\n",
      "Training: epoch 23 batch 10 loss 0.008765149861574173\n",
      "Training: epoch 23 batch 20 loss 0.006926046684384346\n",
      "Test: epoch 23 batch 0 loss 0.013093598186969757\n",
      "epoch 23 finished - avarage train loss 0.010675371977789649  avarage test loss 0.017114725895226002\n",
      "Training: epoch 24 batch 0 loss 0.014686590991914272\n",
      "Training: epoch 24 batch 10 loss 0.0036544357426464558\n",
      "Training: epoch 24 batch 20 loss 0.0063961585983633995\n",
      "Test: epoch 24 batch 0 loss 0.013871973380446434\n",
      "epoch 24 finished - avarage train loss 0.009579736538680977  avarage test loss 0.017812195932492614\n",
      "Training: epoch 25 batch 0 loss 0.01630542427301407\n",
      "Training: epoch 25 batch 10 loss 0.012653798796236515\n",
      "Training: epoch 25 batch 20 loss 0.011130787432193756\n",
      "Test: epoch 25 batch 0 loss 0.013926452025771141\n",
      "epoch 25 finished - avarage train loss 0.010312752990887082  avarage test loss 0.01813271176069975\n",
      "Training: epoch 26 batch 0 loss 0.005972373764961958\n",
      "Training: epoch 26 batch 10 loss 0.011251814663410187\n",
      "Training: epoch 26 batch 20 loss 0.010603431612253189\n",
      "Test: epoch 26 batch 0 loss 0.011349083855748177\n",
      "epoch 26 finished - avarage train loss 0.009414297191361928  avarage test loss 0.015752287581562996\n",
      "Training: epoch 27 batch 0 loss 0.004661215003579855\n",
      "Training: epoch 27 batch 10 loss 0.0063430871814489365\n",
      "Training: epoch 27 batch 20 loss 0.0041338293813169\n",
      "Test: epoch 27 batch 0 loss 0.010097270831465721\n",
      "epoch 27 finished - avarage train loss 0.009393615423348444  avarage test loss 0.014277954585850239\n",
      "Training: epoch 28 batch 0 loss 0.012608559802174568\n",
      "Training: epoch 28 batch 10 loss 0.00808700267225504\n",
      "Training: epoch 28 batch 20 loss 0.008217732422053814\n",
      "Test: epoch 28 batch 0 loss 0.01200723834335804\n",
      "epoch 28 finished - avarage train loss 0.010021326846667919  avarage test loss 0.015768428798764944\n",
      "Training: epoch 29 batch 0 loss 0.009333799593150616\n",
      "Training: epoch 29 batch 10 loss 0.006342084147036076\n",
      "Training: epoch 29 batch 20 loss 0.0036556567065417767\n",
      "Test: epoch 29 batch 0 loss 0.012145976535975933\n",
      "epoch 29 finished - avarage train loss 0.008869815465255544  avarage test loss 0.016781529411673546\n",
      "Training: epoch 30 batch 0 loss 0.006147192325443029\n",
      "Training: epoch 30 batch 10 loss 0.010755508206784725\n",
      "Training: epoch 30 batch 20 loss 0.006554555147886276\n",
      "Test: epoch 30 batch 0 loss 0.013624216429889202\n",
      "epoch 30 finished - avarage train loss 0.00993871648699559  avarage test loss 0.01729001197963953\n",
      "Training: epoch 31 batch 0 loss 0.00802325177937746\n",
      "Training: epoch 31 batch 10 loss 0.00693033030256629\n",
      "Training: epoch 31 batch 20 loss 0.011640604585409164\n",
      "Test: epoch 31 batch 0 loss 0.011492217890918255\n",
      "epoch 31 finished - avarage train loss 0.009505858114952671  avarage test loss 0.015344601357355714\n",
      "Training: epoch 32 batch 0 loss 0.01032550074160099\n",
      "Training: epoch 32 batch 10 loss 0.009000522084534168\n",
      "Training: epoch 32 batch 20 loss 0.01079561747610569\n",
      "Test: epoch 32 batch 0 loss 0.013484206050634384\n",
      "epoch 32 finished - avarage train loss 0.009335072373907113  avarage test loss 0.01735825976356864\n",
      "Training: epoch 33 batch 0 loss 0.014295675791800022\n",
      "Training: epoch 33 batch 10 loss 0.005069241393357515\n",
      "Training: epoch 33 batch 20 loss 0.014166045933961868\n",
      "Test: epoch 33 batch 0 loss 0.018173089250922203\n",
      "epoch 33 finished - avarage train loss 0.010285528216125637  avarage test loss 0.022696872940286994\n",
      "Training: epoch 34 batch 0 loss 0.012584816664457321\n",
      "Training: epoch 34 batch 10 loss 0.009482131339609623\n",
      "Training: epoch 34 batch 20 loss 0.008352547883987427\n",
      "Test: epoch 34 batch 0 loss 0.008491440676152706\n",
      "epoch 34 finished - avarage train loss 0.009570995201196137  avarage test loss 0.012951466720551252\n",
      "Training: epoch 35 batch 0 loss 0.005717177875339985\n",
      "Training: epoch 35 batch 10 loss 0.006476671434938908\n",
      "Training: epoch 35 batch 20 loss 0.004975179210305214\n",
      "Test: epoch 35 batch 0 loss 0.01418417040258646\n",
      "epoch 35 finished - avarage train loss 0.008846888142027732  avarage test loss 0.01832801871933043\n",
      "Training: epoch 36 batch 0 loss 0.0074873873963952065\n",
      "Training: epoch 36 batch 10 loss 0.010394640266895294\n",
      "Training: epoch 36 batch 20 loss 0.013681907206773758\n",
      "Test: epoch 36 batch 0 loss 0.016215723007917404\n",
      "epoch 36 finished - avarage train loss 0.011563910460420724  avarage test loss 0.01940321340225637\n",
      "Training: epoch 37 batch 0 loss 0.007151808589696884\n",
      "Training: epoch 37 batch 10 loss 0.023314401507377625\n",
      "Training: epoch 37 batch 20 loss 0.006857916712760925\n",
      "Test: epoch 37 batch 0 loss 0.011654924601316452\n",
      "epoch 37 finished - avarage train loss 0.013879923968864926  avarage test loss 0.014429239323362708\n",
      "Training: epoch 38 batch 0 loss 0.004499734845012426\n",
      "Training: epoch 38 batch 10 loss 0.009697015397250652\n",
      "Training: epoch 38 batch 20 loss 0.004514995031058788\n",
      "Test: epoch 38 batch 0 loss 0.013555480167269707\n",
      "epoch 38 finished - avarage train loss 0.008173447351196203  avarage test loss 0.014661855646409094\n",
      "Training: epoch 39 batch 0 loss 0.00642623333260417\n",
      "Training: epoch 39 batch 10 loss 0.007984149269759655\n",
      "Training: epoch 39 batch 20 loss 0.011005126871168613\n",
      "Test: epoch 39 batch 0 loss 0.018676066771149635\n",
      "epoch 39 finished - avarage train loss 0.009462832987051585  avarage test loss 0.020476388046517968\n",
      "Training: epoch 40 batch 0 loss 0.007394836284220219\n",
      "Training: epoch 40 batch 10 loss 0.005865741055458784\n",
      "Training: epoch 40 batch 20 loss 0.008391782641410828\n",
      "Test: epoch 40 batch 0 loss 0.014327323995530605\n",
      "epoch 40 finished - avarage train loss 0.009363709467238393  avarage test loss 0.015528206480666995\n",
      "Training: epoch 41 batch 0 loss 0.0030433302745223045\n",
      "Training: epoch 41 batch 10 loss 0.007286220323294401\n",
      "Training: epoch 41 batch 20 loss 0.007427064701914787\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test: epoch 41 batch 0 loss 0.014534282498061657\n",
      "epoch 41 finished - avarage train loss 0.007703128094174738  avarage test loss 0.01724559091962874\n",
      "Training: epoch 42 batch 0 loss 0.006694676820188761\n",
      "Training: epoch 42 batch 10 loss 0.004512390121817589\n",
      "Training: epoch 42 batch 20 loss 0.0017725812504068017\n",
      "Test: epoch 42 batch 0 loss 0.00988929532468319\n",
      "epoch 42 finished - avarage train loss 0.008484522462822497  avarage test loss 0.013211866142228246\n",
      "Training: epoch 43 batch 0 loss 0.006372529547661543\n",
      "Training: epoch 43 batch 10 loss 0.008481737226247787\n",
      "Training: epoch 43 batch 20 loss 0.00948682427406311\n",
      "Test: epoch 43 batch 0 loss 0.015922484919428825\n",
      "epoch 43 finished - avarage train loss 0.008647836873243595  avarage test loss 0.0187193073797971\n",
      "Training: epoch 44 batch 0 loss 0.011763487011194229\n",
      "Training: epoch 44 batch 10 loss 0.004497248213738203\n",
      "Training: epoch 44 batch 20 loss 0.003708215896040201\n",
      "Test: epoch 44 batch 0 loss 0.016325905919075012\n",
      "epoch 44 finished - avarage train loss 0.009484206904368154  avarage test loss 0.018980980617925525\n",
      "Training: epoch 45 batch 0 loss 0.0060804663226008415\n",
      "Training: epoch 45 batch 10 loss 0.013492701575160027\n",
      "Training: epoch 45 batch 20 loss 0.028612755239009857\n",
      "Test: epoch 45 batch 0 loss 0.02374911680817604\n",
      "epoch 45 finished - avarage train loss 0.01972735525462134  avarage test loss 0.029569104313850403\n",
      "Training: epoch 46 batch 0 loss 0.020075272768735886\n",
      "Training: epoch 46 batch 10 loss 0.013492841273546219\n",
      "Training: epoch 46 batch 20 loss 0.009012406691908836\n",
      "Test: epoch 46 batch 0 loss 0.010161367245018482\n",
      "epoch 46 finished - avarage train loss 0.012118131387978792  avarage test loss 0.014534109504893422\n",
      "Training: epoch 47 batch 0 loss 0.010380683466792107\n",
      "Training: epoch 47 batch 10 loss 0.00736577995121479\n",
      "Training: epoch 47 batch 20 loss 0.016214312985539436\n",
      "Test: epoch 47 batch 0 loss 0.009179295040667057\n",
      "epoch 47 finished - avarage train loss 0.009918842336227154  avarage test loss 0.014082981739193201\n",
      "Training: epoch 48 batch 0 loss 0.008992165327072144\n",
      "Training: epoch 48 batch 10 loss 0.005397961474955082\n",
      "Training: epoch 48 batch 20 loss 0.009717059321701527\n",
      "Test: epoch 48 batch 0 loss 0.011671729385852814\n",
      "epoch 48 finished - avarage train loss 0.007477851399894932  avarage test loss 0.014984756708145142\n",
      "Training: epoch 49 batch 0 loss 0.008022637106478214\n",
      "Training: epoch 49 batch 10 loss 0.0025503479409962893\n",
      "Training: epoch 49 batch 20 loss 0.008895203471183777\n",
      "Test: epoch 49 batch 0 loss 0.013544203713536263\n",
      "epoch 49 finished - avarage train loss 0.006854266514359364  avarage test loss 0.01718795928172767\n",
      "Training: epoch 50 batch 0 loss 0.004664023872464895\n",
      "Training: epoch 50 batch 10 loss 0.009052172303199768\n",
      "Training: epoch 50 batch 20 loss 0.005576360039412975\n",
      "Test: epoch 50 batch 0 loss 0.012858813628554344\n",
      "epoch 50 finished - avarage train loss 0.006839815037453483  avarage test loss 0.014120276551693678\n",
      "Training: epoch 51 batch 0 loss 0.009249878115952015\n",
      "Training: epoch 51 batch 10 loss 0.004628360271453857\n",
      "Training: epoch 51 batch 20 loss 0.003665523137897253\n",
      "Test: epoch 51 batch 0 loss 0.010719743557274342\n",
      "epoch 51 finished - avarage train loss 0.008621784143856373  avarage test loss 0.014024245785549283\n",
      "Training: epoch 52 batch 0 loss 0.008172592148184776\n",
      "Training: epoch 52 batch 10 loss 0.004653014242649078\n",
      "Training: epoch 52 batch 20 loss 0.009903039783239365\n",
      "Test: epoch 52 batch 0 loss 0.011834445409476757\n",
      "epoch 52 finished - avarage train loss 0.008227065960266467  avarage test loss 0.013775849132798612\n",
      "Training: epoch 53 batch 0 loss 0.007753115613013506\n",
      "Training: epoch 53 batch 10 loss 0.004133324604481459\n",
      "Training: epoch 53 batch 20 loss 0.00405651330947876\n",
      "Test: epoch 53 batch 0 loss 0.012405076064169407\n",
      "epoch 53 finished - avarage train loss 0.007300164096924509  avarage test loss 0.014590788283385336\n",
      "Training: epoch 54 batch 0 loss 0.0039803278632462025\n",
      "Training: epoch 54 batch 10 loss 0.006527112331241369\n",
      "Training: epoch 54 batch 20 loss 0.01135755144059658\n",
      "Test: epoch 54 batch 0 loss 0.013317076489329338\n",
      "epoch 54 finished - avarage train loss 0.008020354579363403  avarage test loss 0.014694976853206754\n",
      "Training: epoch 55 batch 0 loss 0.005746614187955856\n",
      "Training: epoch 55 batch 10 loss 0.014154102653265\n",
      "Training: epoch 55 batch 20 loss 0.006140203680843115\n",
      "Test: epoch 55 batch 0 loss 0.020940907299518585\n",
      "epoch 55 finished - avarage train loss 0.0082572967873822  avarage test loss 0.02571367286145687\n",
      "Training: epoch 56 batch 0 loss 0.003634353633970022\n",
      "Training: epoch 56 batch 10 loss 0.007349138148128986\n",
      "Training: epoch 56 batch 20 loss 0.00427618995308876\n",
      "Test: epoch 56 batch 0 loss 0.012233608402311802\n",
      "epoch 56 finished - avarage train loss 0.0076870448839176314  avarage test loss 0.013668695813976228\n",
      "Training: epoch 57 batch 0 loss 0.008312467485666275\n",
      "Training: epoch 57 batch 10 loss 0.003163270652294159\n",
      "Training: epoch 57 batch 20 loss 0.0015049997018650174\n",
      "Test: epoch 57 batch 0 loss 0.011716259643435478\n",
      "epoch 57 finished - avarage train loss 0.0073875792220170645  avarage test loss 0.013450501253828406\n",
      "Training: epoch 58 batch 0 loss 0.004828762263059616\n",
      "Training: epoch 58 batch 10 loss 0.005615506321191788\n",
      "Training: epoch 58 batch 20 loss 0.0030601259786635637\n",
      "Test: epoch 58 batch 0 loss 0.014417320489883423\n",
      "epoch 58 finished - avarage train loss 0.007708147609734844  avarage test loss 0.0202120637986809\n",
      "Training: epoch 59 batch 0 loss 0.013347504660487175\n",
      "Training: epoch 59 batch 10 loss 0.008258763700723648\n",
      "Training: epoch 59 batch 20 loss 0.014230305328965187\n",
      "Test: epoch 59 batch 0 loss 0.013711174950003624\n",
      "epoch 59 finished - avarage train loss 0.013211447472587741  avarage test loss 0.015748750418424606\n",
      "Training: epoch 60 batch 0 loss 0.012807624414563179\n",
      "Training: epoch 60 batch 10 loss 0.006575895473361015\n",
      "Training: epoch 60 batch 20 loss 0.007137016858905554\n",
      "Test: epoch 60 batch 0 loss 0.014264360070228577\n",
      "epoch 60 finished - avarage train loss 0.009357306801168055  avarage test loss 0.015761016868054867\n",
      "Training: epoch 61 batch 0 loss 0.007643238641321659\n",
      "Training: epoch 61 batch 10 loss 0.006862595211714506\n",
      "Training: epoch 61 batch 20 loss 0.005289286375045776\n",
      "Test: epoch 61 batch 0 loss 0.01058762427419424\n",
      "epoch 61 finished - avarage train loss 0.007552510328141266  avarage test loss 0.013521181768737733\n",
      "Training: epoch 62 batch 0 loss 0.006262455601245165\n",
      "Training: epoch 62 batch 10 loss 0.004905234090983868\n",
      "Training: epoch 62 batch 20 loss 0.004452487453818321\n",
      "Test: epoch 62 batch 0 loss 0.010402093641459942\n",
      "epoch 62 finished - avarage train loss 0.008969345113969055  avarage test loss 0.013164712116122246\n",
      "Training: epoch 63 batch 0 loss 0.011359297670423985\n",
      "Training: epoch 63 batch 10 loss 0.006835825741291046\n",
      "Training: epoch 63 batch 20 loss 0.005511684343218803\n",
      "Test: epoch 63 batch 0 loss 0.010175121016800404\n",
      "epoch 63 finished - avarage train loss 0.008015161749489349  avarage test loss 0.012519279960542917\n",
      "Training: epoch 64 batch 0 loss 0.002395238261669874\n",
      "Training: epoch 64 batch 10 loss 0.0068464335054159164\n",
      "Training: epoch 64 batch 20 loss 0.010929593816399574\n",
      "Test: epoch 64 batch 0 loss 0.009602733887732029\n",
      "epoch 64 finished - avarage train loss 0.008532141343337196  avarage test loss 0.012937605497427285\n",
      "Training: epoch 65 batch 0 loss 0.005440396722406149\n",
      "Training: epoch 65 batch 10 loss 0.006289540324360132\n",
      "Training: epoch 65 batch 20 loss 0.005758128594607115\n",
      "Test: epoch 65 batch 0 loss 0.014947429299354553\n",
      "epoch 65 finished - avarage train loss 0.008066291888726169  avarage test loss 0.017340490594506264\n",
      "Training: epoch 66 batch 0 loss 0.00483830738812685\n",
      "Training: epoch 66 batch 10 loss 0.012265981175005436\n",
      "Training: epoch 66 batch 20 loss 0.006869792006909847\n",
      "Test: epoch 66 batch 0 loss 0.009281025268137455\n",
      "epoch 66 finished - avarage train loss 0.007189086172729731  avarage test loss 0.013038035482168198\n",
      "Training: epoch 67 batch 0 loss 0.005483448971062899\n",
      "Training: epoch 67 batch 10 loss 0.010776927694678307\n",
      "Training: epoch 67 batch 20 loss 0.003731393488124013\n",
      "Test: epoch 67 batch 0 loss 0.012928628362715244\n",
      "epoch 67 finished - avarage train loss 0.007908325192743334  avarage test loss 0.01379989879205823\n",
      "Training: epoch 68 batch 0 loss 0.008658068254590034\n",
      "Training: epoch 68 batch 10 loss 0.006670517381280661\n",
      "Training: epoch 68 batch 20 loss 0.005720663350075483\n",
      "Test: epoch 68 batch 0 loss 0.01189681887626648\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 68 finished - avarage train loss 0.006793066088495584  avarage test loss 0.012984594446606934\n",
      "Training: epoch 69 batch 0 loss 0.005484761670231819\n",
      "Training: epoch 69 batch 10 loss 0.0059279086999595165\n",
      "Training: epoch 69 batch 20 loss 0.005444218870252371\n",
      "Test: epoch 69 batch 0 loss 0.014049180783331394\n",
      "epoch 69 finished - avarage train loss 0.0066422550810565206  avarage test loss 0.015044496627524495\n",
      "Training: epoch 70 batch 0 loss 0.0028444852214306593\n",
      "Training: epoch 70 batch 10 loss 0.0058039030991494656\n",
      "Training: epoch 70 batch 20 loss 0.005020833574235439\n",
      "Test: epoch 70 batch 0 loss 0.01061673928052187\n",
      "epoch 70 finished - avarage train loss 0.006930803285975908  avarage test loss 0.01246532832738012\n",
      "Training: epoch 71 batch 0 loss 0.0011068833991885185\n",
      "Training: epoch 71 batch 10 loss 0.00414475379511714\n",
      "Training: epoch 71 batch 20 loss 0.006917235907167196\n",
      "Test: epoch 71 batch 0 loss 0.010234733112156391\n",
      "epoch 71 finished - avarage train loss 0.006963466952458538  avarage test loss 0.012736986624076962\n",
      "Training: epoch 72 batch 0 loss 0.007964837364852428\n",
      "Training: epoch 72 batch 10 loss 0.0030412173364311457\n",
      "Training: epoch 72 batch 20 loss 0.008269316516816616\n",
      "Test: epoch 72 batch 0 loss 0.012614132836461067\n",
      "epoch 72 finished - avarage train loss 0.007366629919551056  avarage test loss 0.014240471180528402\n",
      "Training: epoch 73 batch 0 loss 0.005437194835394621\n",
      "Training: epoch 73 batch 10 loss 0.01214330829679966\n",
      "Training: epoch 73 batch 20 loss 0.004473812412470579\n",
      "Test: epoch 73 batch 0 loss 0.00743981683626771\n",
      "epoch 73 finished - avarage train loss 0.00858375531268017  avarage test loss 0.012859513517469168\n",
      "Training: epoch 74 batch 0 loss 0.010858012363314629\n",
      "Training: epoch 74 batch 10 loss 0.016140686348080635\n",
      "Training: epoch 74 batch 20 loss 0.017650600522756577\n",
      "Test: epoch 74 batch 0 loss 0.006586112082004547\n",
      "epoch 74 finished - avarage train loss 0.011011227869011205  avarage test loss 0.01429751527030021\n",
      "Training: epoch 75 batch 0 loss 0.009174308739602566\n",
      "Training: epoch 75 batch 10 loss 0.008125001564621925\n",
      "Training: epoch 75 batch 20 loss 0.006425424944609404\n",
      "Test: epoch 75 batch 0 loss 0.011206149123609066\n",
      "epoch 75 finished - avarage train loss 0.008336731457504732  avarage test loss 0.0131878248648718\n",
      "Training: epoch 76 batch 0 loss 0.004729525186121464\n",
      "Training: epoch 76 batch 10 loss 0.007882943376898766\n",
      "Training: epoch 76 batch 20 loss 0.010802941396832466\n",
      "Test: epoch 76 batch 0 loss 0.01281902939081192\n",
      "epoch 76 finished - avarage train loss 0.008174215347088617  avarage test loss 0.014212264795787632\n",
      "Training: epoch 77 batch 0 loss 0.00600765785202384\n",
      "Training: epoch 77 batch 10 loss 0.004773261025547981\n",
      "Training: epoch 77 batch 20 loss 0.009315943345427513\n",
      "Test: epoch 77 batch 0 loss 0.016830703243613243\n",
      "epoch 77 finished - avarage train loss 0.01026757364964177  avarage test loss 0.029153309762477875\n",
      "Training: epoch 78 batch 0 loss 0.016120657324790955\n",
      "Training: epoch 78 batch 10 loss 0.011801568791270256\n",
      "Training: epoch 78 batch 20 loss 0.017485331743955612\n",
      "Test: epoch 78 batch 0 loss 0.01695825904607773\n",
      "epoch 78 finished - avarage train loss 0.015712408605834532  avarage test loss 0.01956298411823809\n",
      "Training: epoch 79 batch 0 loss 0.008570095524191856\n",
      "Training: epoch 79 batch 10 loss 0.008473849855363369\n",
      "Training: epoch 79 batch 20 loss 0.007625671103596687\n",
      "Test: epoch 79 batch 0 loss 0.012167380191385746\n",
      "epoch 79 finished - avarage train loss 0.01016212061272356  avarage test loss 0.014437742298468947\n",
      "Training: epoch 80 batch 0 loss 0.010393122211098671\n",
      "Training: epoch 80 batch 10 loss 0.0047374325804412365\n",
      "Training: epoch 80 batch 20 loss 0.0018384609138593078\n",
      "Test: epoch 80 batch 0 loss 0.010407502762973309\n",
      "epoch 80 finished - avarage train loss 0.009338798005422899  avarage test loss 0.015496432082727551\n",
      "Training: epoch 81 batch 0 loss 0.007916574366390705\n",
      "Training: epoch 81 batch 10 loss 0.006561191752552986\n",
      "Training: epoch 81 batch 20 loss 0.011087941005825996\n",
      "Test: epoch 81 batch 0 loss 0.013551870360970497\n",
      "epoch 81 finished - avarage train loss 0.007552825364059415  avarage test loss 0.014839909388683736\n",
      "Training: epoch 82 batch 0 loss 0.004938032012432814\n",
      "Training: epoch 82 batch 10 loss 0.005785799585282803\n",
      "Training: epoch 82 batch 20 loss 0.004949991125613451\n",
      "Test: epoch 82 batch 0 loss 0.014317692257463932\n",
      "epoch 82 finished - avarage train loss 0.0068781196444841295  avarage test loss 0.01692729303613305\n",
      "Training: epoch 83 batch 0 loss 0.006594382226467133\n",
      "Training: epoch 83 batch 10 loss 0.0033039574045687914\n",
      "Training: epoch 83 batch 20 loss 0.003909126855432987\n",
      "Test: epoch 83 batch 0 loss 0.011363262310624123\n",
      "epoch 83 finished - avarage train loss 0.005784728058517493  avarage test loss 0.013306172681041062\n",
      "Training: epoch 84 batch 0 loss 0.003206379944458604\n",
      "Training: epoch 84 batch 10 loss 0.0034685467835515738\n",
      "Training: epoch 84 batch 20 loss 0.010709935799241066\n",
      "Test: epoch 84 batch 0 loss 0.013300027698278427\n",
      "epoch 84 finished - avarage train loss 0.007977783270501372  avarage test loss 0.014887952944263816\n",
      "Training: epoch 85 batch 0 loss 0.002728382358327508\n",
      "Training: epoch 85 batch 10 loss 0.003666928270831704\n",
      "Training: epoch 85 batch 20 loss 0.004905965644866228\n",
      "Test: epoch 85 batch 0 loss 0.012335562147200108\n",
      "epoch 85 finished - avarage train loss 0.0076175009478525865  avarage test loss 0.013643104350194335\n",
      "Training: epoch 86 batch 0 loss 0.01258530281484127\n",
      "Training: epoch 86 batch 10 loss 0.006441598292440176\n",
      "Training: epoch 86 batch 20 loss 0.005680580157786608\n",
      "Test: epoch 86 batch 0 loss 0.01192314364016056\n",
      "epoch 86 finished - avarage train loss 0.008614415262729443  avarage test loss 0.013518304214812815\n",
      "Training: epoch 87 batch 0 loss 0.005997364409267902\n",
      "Training: epoch 87 batch 10 loss 0.0049583506770431995\n",
      "Training: epoch 87 batch 20 loss 0.01411465834826231\n",
      "Test: epoch 87 batch 0 loss 0.018336450681090355\n",
      "epoch 87 finished - avarage train loss 0.008616505054243166  avarage test loss 0.021523070987313986\n",
      "Training: epoch 88 batch 0 loss 0.0076608541421592236\n",
      "Training: epoch 88 batch 10 loss 0.005298756528645754\n",
      "Training: epoch 88 batch 20 loss 0.006789652165025473\n",
      "Test: epoch 88 batch 0 loss 0.011518768034875393\n",
      "epoch 88 finished - avarage train loss 0.007540591285917266  avarage test loss 0.013788127689622343\n",
      "Training: epoch 89 batch 0 loss 0.010046033188700676\n",
      "Training: epoch 89 batch 10 loss 0.0067881029099226\n",
      "Training: epoch 89 batch 20 loss 0.006218095775693655\n",
      "Test: epoch 89 batch 0 loss 0.01229897327721119\n",
      "epoch 89 finished - avarage train loss 0.007883030294986635  avarage test loss 0.013631080510094762\n",
      "Training: epoch 90 batch 0 loss 0.006051458418369293\n",
      "Training: epoch 90 batch 10 loss 0.003973845392465591\n",
      "Training: epoch 90 batch 20 loss 0.002226507756859064\n",
      "Test: epoch 90 batch 0 loss 0.01223768014460802\n",
      "epoch 90 finished - avarage train loss 0.007482479511085769  avarage test loss 0.01386182103306055\n",
      "Training: epoch 91 batch 0 loss 0.0030997609719634056\n",
      "Training: epoch 91 batch 10 loss 0.005014428403228521\n",
      "Training: epoch 91 batch 20 loss 0.00714280316606164\n",
      "Test: epoch 91 batch 0 loss 0.011745110154151917\n",
      "epoch 91 finished - avarage train loss 0.00710474773599156  avarage test loss 0.012723213294520974\n",
      "Training: epoch 92 batch 0 loss 0.010302012786269188\n",
      "Training: epoch 92 batch 10 loss 0.0038731126114726067\n",
      "Training: epoch 92 batch 20 loss 0.008784759789705276\n",
      "Test: epoch 92 batch 0 loss 0.017533978447318077\n",
      "epoch 92 finished - avarage train loss 0.009251409210264683  avarage test loss 0.017539183143526316\n",
      "Training: epoch 93 batch 0 loss 0.012542098760604858\n",
      "Training: epoch 93 batch 10 loss 0.010372322052717209\n",
      "Training: epoch 93 batch 20 loss 0.00612968485802412\n",
      "Test: epoch 93 batch 0 loss 0.013165956363081932\n",
      "epoch 93 finished - avarage train loss 0.007927474003799003  avarage test loss 0.013898463803343475\n",
      "Training: epoch 94 batch 0 loss 0.005225746426731348\n",
      "Training: epoch 94 batch 10 loss 0.00326746073551476\n",
      "Training: epoch 94 batch 20 loss 0.007194401230663061\n",
      "Test: epoch 94 batch 0 loss 0.01434808224439621\n",
      "epoch 94 finished - avarage train loss 0.007524572075986913  avarage test loss 0.015421662013977766\n",
      "Training: epoch 95 batch 0 loss 0.008663389831781387\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training: epoch 95 batch 10 loss 0.004536386579275131\n",
      "Training: epoch 95 batch 20 loss 0.004807967692613602\n",
      "Test: epoch 95 batch 0 loss 0.013395086862146854\n",
      "epoch 95 finished - avarage train loss 0.008421408173082203  avarage test loss 0.014288307167589664\n",
      "Training: epoch 96 batch 0 loss 0.007101146969944239\n",
      "Training: epoch 96 batch 10 loss 0.0029566616285592318\n",
      "Training: epoch 96 batch 20 loss 0.002565865172073245\n",
      "Test: epoch 96 batch 0 loss 0.01069210097193718\n",
      "epoch 96 finished - avarage train loss 0.005441101179618774  avarage test loss 0.012608347227796912\n",
      "Training: epoch 97 batch 0 loss 0.002148207277059555\n",
      "Training: epoch 97 batch 10 loss 0.0036634854041039944\n",
      "Training: epoch 97 batch 20 loss 0.01203925907611847\n",
      "Test: epoch 97 batch 0 loss 0.00970450695604086\n",
      "epoch 97 finished - avarage train loss 0.006513543932795011  avarage test loss 0.01294576225336641\n",
      "Training: epoch 98 batch 0 loss 0.00732358219102025\n",
      "Training: epoch 98 batch 10 loss 0.004408740904182196\n",
      "Training: epoch 98 batch 20 loss 0.004742137156426907\n",
      "Test: epoch 98 batch 0 loss 0.01086585596203804\n",
      "epoch 98 finished - avarage train loss 0.005470654204616259  avarage test loss 0.012299593538045883\n",
      "Training: epoch 99 batch 0 loss 0.004312554839998484\n",
      "Training: epoch 99 batch 10 loss 0.0068009779788553715\n",
      "Training: epoch 99 batch 20 loss 0.006874288897961378\n",
      "Test: epoch 99 batch 0 loss 0.012223021127283573\n",
      "epoch 99 finished - avarage train loss 0.007418178503626379  avarage test loss 0.013261215994134545\n",
      "Training: epoch 100 batch 0 loss 0.0058128722012043\n",
      "Training: epoch 100 batch 10 loss 0.007447803393006325\n",
      "Training: epoch 100 batch 20 loss 0.003321420168504119\n",
      "Test: epoch 100 batch 0 loss 0.010287562385201454\n",
      "epoch 100 finished - avarage train loss 0.007135940916386658  avarage test loss 0.012517509399913251\n",
      "Training: epoch 101 batch 0 loss 0.005828855559229851\n",
      "Training: epoch 101 batch 10 loss 0.003837481839582324\n",
      "Training: epoch 101 batch 20 loss 0.007610389031469822\n",
      "Test: epoch 101 batch 0 loss 0.009684445336461067\n",
      "epoch 101 finished - avarage train loss 0.006039543213836592  avarage test loss 0.011778700805734843\n",
      "Training: epoch 102 batch 0 loss 0.008458003401756287\n",
      "Training: epoch 102 batch 10 loss 0.00555796455591917\n",
      "Training: epoch 102 batch 20 loss 0.0075029125437140465\n",
      "Test: epoch 102 batch 0 loss 0.011088536120951176\n",
      "epoch 102 finished - avarage train loss 0.006976178325391535  avarage test loss 0.012594281695783138\n",
      "Training: epoch 103 batch 0 loss 0.007678595837205648\n",
      "Training: epoch 103 batch 10 loss 0.004051918163895607\n",
      "Training: epoch 103 batch 20 loss 0.003691643476486206\n",
      "Test: epoch 103 batch 0 loss 0.01125985849648714\n",
      "epoch 103 finished - avarage train loss 0.0064214867146300345  avarage test loss 0.012542040087282658\n",
      "Training: epoch 104 batch 0 loss 0.004456354305148125\n",
      "Training: epoch 104 batch 10 loss 0.006992398761212826\n",
      "Training: epoch 104 batch 20 loss 0.003639915492385626\n",
      "Test: epoch 104 batch 0 loss 0.01137926708906889\n",
      "epoch 104 finished - avarage train loss 0.006907305487527929  avarage test loss 0.012626492301933467\n",
      "Training: epoch 105 batch 0 loss 0.0042424676939845085\n",
      "Training: epoch 105 batch 10 loss 0.004965436179190874\n",
      "Training: epoch 105 batch 20 loss 0.004709466360509396\n",
      "Test: epoch 105 batch 0 loss 0.011881543323397636\n",
      "epoch 105 finished - avarage train loss 0.008256668040271977  avarage test loss 0.013132785214111209\n",
      "Training: epoch 106 batch 0 loss 0.0043144491501152515\n",
      "Training: epoch 106 batch 10 loss 0.008000662550330162\n",
      "Training: epoch 106 batch 20 loss 0.0037244760897010565\n",
      "Test: epoch 106 batch 0 loss 0.013347269035875797\n",
      "epoch 106 finished - avarage train loss 0.007726671085853515  avarage test loss 0.015773725463077426\n",
      "Training: epoch 107 batch 0 loss 0.008410299196839333\n",
      "Training: epoch 107 batch 10 loss 0.005674516316503286\n",
      "Training: epoch 107 batch 20 loss 0.01139845885336399\n",
      "Test: epoch 107 batch 0 loss 0.0186945591121912\n",
      "epoch 107 finished - avarage train loss 0.007168592941337104  avarage test loss 0.021677955985069275\n",
      "Training: epoch 108 batch 0 loss 0.015313941985368729\n",
      "Training: epoch 108 batch 10 loss 0.007289254572242498\n",
      "Training: epoch 108 batch 20 loss 0.005496867001056671\n",
      "Test: epoch 108 batch 0 loss 0.0129729975014925\n",
      "epoch 108 finished - avarage train loss 0.008983737869380877  avarage test loss 0.014614953193813562\n",
      "Training: epoch 109 batch 0 loss 0.004453666508197784\n",
      "Training: epoch 109 batch 10 loss 0.006268149707466364\n",
      "Training: epoch 109 batch 20 loss 0.009329220280051231\n",
      "Test: epoch 109 batch 0 loss 0.011256122030317783\n",
      "epoch 109 finished - avarage train loss 0.00912074247728391  avarage test loss 0.01293309684842825\n",
      "Training: epoch 110 batch 0 loss 0.0029309482779353857\n",
      "Training: epoch 110 batch 10 loss 0.006374692544341087\n",
      "Training: epoch 110 batch 20 loss 0.004930752329528332\n",
      "Test: epoch 110 batch 0 loss 0.010307339951395988\n",
      "epoch 110 finished - avarage train loss 0.007120949039556857  avarage test loss 0.014214481925591826\n",
      "Training: epoch 111 batch 0 loss 0.00546523742377758\n",
      "Training: epoch 111 batch 10 loss 0.007614503614604473\n",
      "Training: epoch 111 batch 20 loss 0.00532850157469511\n",
      "Test: epoch 111 batch 0 loss 0.010241299867630005\n",
      "epoch 111 finished - avarage train loss 0.009634183173806503  avarage test loss 0.012194247799925506\n",
      "Training: epoch 112 batch 0 loss 0.005588815081864595\n",
      "Training: epoch 112 batch 10 loss 0.004381299484521151\n",
      "Training: epoch 112 batch 20 loss 0.00790697056800127\n",
      "Test: epoch 112 batch 0 loss 0.010011917911469936\n",
      "epoch 112 finished - avarage train loss 0.006968973425281202  avarage test loss 0.011849458911456168\n",
      "Training: epoch 113 batch 0 loss 0.005784973967820406\n",
      "Training: epoch 113 batch 10 loss 0.004225464537739754\n",
      "Training: epoch 113 batch 20 loss 0.005217177327722311\n",
      "Test: epoch 113 batch 0 loss 0.01202937588095665\n",
      "epoch 113 finished - avarage train loss 0.006850841483261822  avarage test loss 0.014016313012689352\n",
      "Training: epoch 114 batch 0 loss 0.002108583925291896\n",
      "Training: epoch 114 batch 10 loss 0.010610493831336498\n",
      "Training: epoch 114 batch 20 loss 0.0062362016178667545\n",
      "Test: epoch 114 batch 0 loss 0.010633600875735283\n",
      "epoch 114 finished - avarage train loss 0.00829730014284623  avarage test loss 0.012383539462462068\n",
      "Training: epoch 115 batch 0 loss 0.005312442779541016\n",
      "Training: epoch 115 batch 10 loss 0.0041412473656237125\n",
      "Training: epoch 115 batch 20 loss 0.007519169244915247\n",
      "Test: epoch 115 batch 0 loss 0.01087650004774332\n",
      "epoch 115 finished - avarage train loss 0.00692350040029349  avarage test loss 0.012687003589235246\n",
      "Training: epoch 116 batch 0 loss 0.0034057400189340115\n",
      "Training: epoch 116 batch 10 loss 0.004057381302118301\n",
      "Training: epoch 116 batch 20 loss 0.00849396362900734\n",
      "Test: epoch 116 batch 0 loss 0.010568005032837391\n",
      "epoch 116 finished - avarage train loss 0.009166574346479672  avarage test loss 0.01319081976544112\n",
      "Training: epoch 117 batch 0 loss 0.006701684091240168\n",
      "Training: epoch 117 batch 10 loss 0.004156207200139761\n",
      "Training: epoch 117 batch 20 loss 0.004454578272998333\n",
      "Test: epoch 117 batch 0 loss 0.01019192673265934\n",
      "epoch 117 finished - avarage train loss 0.006833689977768166  avarage test loss 0.012097683385945857\n",
      "Training: epoch 118 batch 0 loss 0.002888619201257825\n",
      "Training: epoch 118 batch 10 loss 0.008299466222524643\n",
      "Training: epoch 118 batch 20 loss 0.005556771066039801\n",
      "Test: epoch 118 batch 0 loss 0.011033078655600548\n",
      "epoch 118 finished - avarage train loss 0.00796416802492378  avarage test loss 0.013074170215986669\n",
      "Training: epoch 119 batch 0 loss 0.006982812657952309\n",
      "Training: epoch 119 batch 10 loss 0.0017379324417561293\n",
      "Training: epoch 119 batch 20 loss 0.004916677251458168\n",
      "Test: epoch 119 batch 0 loss 0.009775764308869839\n",
      "epoch 119 finished - avarage train loss 0.007804610445324717  avarage test loss 0.012426778092049062\n",
      "Training: epoch 120 batch 0 loss 0.006461144424974918\n",
      "Training: epoch 120 batch 10 loss 0.010798404924571514\n",
      "Training: epoch 120 batch 20 loss 0.00614933529868722\n",
      "Test: epoch 120 batch 0 loss 0.010054033249616623\n",
      "epoch 120 finished - avarage train loss 0.006728963661489302  avarage test loss 0.012198480311781168\n",
      "Training: epoch 121 batch 0 loss 0.0038961032405495644\n",
      "Training: epoch 121 batch 10 loss 0.005104443524032831\n",
      "Training: epoch 121 batch 20 loss 0.011916955932974815\n",
      "Test: epoch 121 batch 0 loss 0.010995394550263882\n",
      "epoch 121 finished - avarage train loss 0.007101040694798375  avarage test loss 0.012944460613653064\n",
      "Training: epoch 122 batch 0 loss 0.0126796280965209\n",
      "Training: epoch 122 batch 10 loss 0.009665352292358875\n",
      "Training: epoch 122 batch 20 loss 0.0067136334255337715\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test: epoch 122 batch 0 loss 0.011489047668874264\n",
      "epoch 122 finished - avarage train loss 0.00886458834504773  avarage test loss 0.013805011752992868\n",
      "Training: epoch 123 batch 0 loss 0.004001484252512455\n",
      "Training: epoch 123 batch 10 loss 0.0024607249069958925\n",
      "Training: epoch 123 batch 20 loss 0.004867838695645332\n",
      "Test: epoch 123 batch 0 loss 0.01428300328552723\n",
      "epoch 123 finished - avarage train loss 0.007677215886109605  avarage test loss 0.01559205399826169\n",
      "Training: epoch 124 batch 0 loss 0.006670681294053793\n",
      "Training: epoch 124 batch 10 loss 0.0076859137043356895\n",
      "Training: epoch 124 batch 20 loss 0.003721583867445588\n",
      "Test: epoch 124 batch 0 loss 0.011672059074044228\n",
      "epoch 124 finished - avarage train loss 0.0069419788437542216  avarage test loss 0.013676935574039817\n",
      "Training: epoch 125 batch 0 loss 0.01432753074914217\n",
      "Training: epoch 125 batch 10 loss 0.0028030434623360634\n",
      "Training: epoch 125 batch 20 loss 0.00741665530949831\n",
      "Test: epoch 125 batch 0 loss 0.010718556120991707\n",
      "epoch 125 finished - avarage train loss 0.007410650245909547  avarage test loss 0.01275666302535683\n",
      "Training: epoch 126 batch 0 loss 0.004671019501984119\n",
      "Training: epoch 126 batch 10 loss 0.0029106303118169308\n",
      "Training: epoch 126 batch 20 loss 0.0066123586148023605\n",
      "Test: epoch 126 batch 0 loss 0.011677082628011703\n",
      "epoch 126 finished - avarage train loss 0.007355355139789653  avarage test loss 0.014277446549385786\n",
      "Training: epoch 127 batch 0 loss 0.012101035565137863\n",
      "Training: epoch 127 batch 10 loss 0.005276297684758902\n",
      "Training: epoch 127 batch 20 loss 0.0058020432479679585\n",
      "Test: epoch 127 batch 0 loss 0.012470983900129795\n",
      "epoch 127 finished - avarage train loss 0.007557602976609407  avarage test loss 0.015302431536838412\n",
      "Training: epoch 128 batch 0 loss 0.002313940552994609\n",
      "Training: epoch 128 batch 10 loss 0.0054694125428795815\n",
      "Training: epoch 128 batch 20 loss 0.016351280733942986\n",
      "Test: epoch 128 batch 0 loss 0.01597750373184681\n",
      "epoch 128 finished - avarage train loss 0.007908790300054282  avarage test loss 0.01934446324594319\n",
      "Training: epoch 129 batch 0 loss 0.012882431037724018\n",
      "Training: epoch 129 batch 10 loss 0.004990905523300171\n",
      "Training: epoch 129 batch 20 loss 0.002843034453690052\n",
      "Test: epoch 129 batch 0 loss 0.007650779094547033\n",
      "epoch 129 finished - avarage train loss 0.009192584772947533  avarage test loss 0.013110175495967269\n",
      "Training: epoch 130 batch 0 loss 0.012147330678999424\n",
      "Training: epoch 130 batch 10 loss 0.006874801591038704\n",
      "Training: epoch 130 batch 20 loss 0.0041243815794587135\n",
      "Test: epoch 130 batch 0 loss 0.007468660362064838\n",
      "epoch 130 finished - avarage train loss 0.00922612936621339  avarage test loss 0.01768846216145903\n",
      "Training: epoch 131 batch 0 loss 0.00782734900712967\n",
      "Training: epoch 131 batch 10 loss 0.00516671035438776\n",
      "Training: epoch 131 batch 20 loss 0.0074083441868424416\n",
      "Test: epoch 131 batch 0 loss 0.007841041311621666\n",
      "epoch 131 finished - avarage train loss 0.010339810288157957  avarage test loss 0.013045229716226459\n",
      "Training: epoch 132 batch 0 loss 0.0030858342070132494\n",
      "Training: epoch 132 batch 10 loss 0.006406767293810844\n",
      "Training: epoch 132 batch 20 loss 0.008728058077394962\n",
      "Test: epoch 132 batch 0 loss 0.013435340486466885\n",
      "epoch 132 finished - avarage train loss 0.007719813741679336  avarage test loss 0.015955632319673896\n",
      "Training: epoch 133 batch 0 loss 0.005506536923348904\n",
      "Training: epoch 133 batch 10 loss 0.0035360055044293404\n",
      "Training: epoch 133 batch 20 loss 0.00696748960763216\n",
      "Test: epoch 133 batch 0 loss 0.012801273725926876\n",
      "epoch 133 finished - avarage train loss 0.007221938801351292  avarage test loss 0.015849288552999496\n",
      "Training: epoch 134 batch 0 loss 0.006878494750708342\n",
      "Training: epoch 134 batch 10 loss 0.005687965080142021\n",
      "Training: epoch 134 batch 20 loss 0.003939841408282518\n",
      "Test: epoch 134 batch 0 loss 0.008892465382814407\n",
      "epoch 134 finished - avarage train loss 0.008608052651558456  avarage test loss 0.011704185919370502\n",
      "Training: epoch 135 batch 0 loss 0.0028710251208394766\n",
      "Training: epoch 135 batch 10 loss 0.00894024595618248\n",
      "Training: epoch 135 batch 20 loss 0.005910444539040327\n",
      "Test: epoch 135 batch 0 loss 0.01013332512229681\n",
      "epoch 135 finished - avarage train loss 0.00603919363066811  avarage test loss 0.012154099880717695\n",
      "Training: epoch 136 batch 0 loss 0.004524379037320614\n",
      "Training: epoch 136 batch 10 loss 0.006199014373123646\n",
      "Training: epoch 136 batch 20 loss 0.011062187142670155\n",
      "Test: epoch 136 batch 0 loss 0.015818007290363312\n",
      "epoch 136 finished - avarage train loss 0.008234935483477753  avarage test loss 0.019640278769657016\n",
      "Training: epoch 137 batch 0 loss 0.005637065041810274\n",
      "Training: epoch 137 batch 10 loss 0.0038931877352297306\n",
      "Training: epoch 137 batch 20 loss 0.003551191184669733\n",
      "Test: epoch 137 batch 0 loss 0.012906532734632492\n",
      "epoch 137 finished - avarage train loss 0.006771801256349888  avarage test loss 0.01585560431703925\n",
      "Training: epoch 138 batch 0 loss 0.006748265121132135\n",
      "Training: epoch 138 batch 10 loss 0.002077542245388031\n",
      "Training: epoch 138 batch 20 loss 0.006556998006999493\n",
      "Test: epoch 138 batch 0 loss 0.011210229247808456\n",
      "epoch 138 finished - avarage train loss 0.0066171513212009755  avarage test loss 0.0132078219903633\n",
      "Training: epoch 139 batch 0 loss 0.008380826562643051\n",
      "Training: epoch 139 batch 10 loss 0.003970102407038212\n",
      "Training: epoch 139 batch 20 loss 0.008372776210308075\n",
      "Test: epoch 139 batch 0 loss 0.015630917623639107\n",
      "epoch 139 finished - avarage train loss 0.00890001187357923  avarage test loss 0.017484543612226844\n",
      "Training: epoch 140 batch 0 loss 0.012955548241734505\n",
      "Training: epoch 140 batch 10 loss 0.009170706383883953\n",
      "Training: epoch 140 batch 20 loss 0.011753411032259464\n",
      "Test: epoch 140 batch 0 loss 0.008295223116874695\n",
      "epoch 140 finished - avarage train loss 0.008740529361791137  avarage test loss 0.013498060405254364\n",
      "Training: epoch 141 batch 0 loss 0.007823653519153595\n",
      "Training: epoch 141 batch 10 loss 0.001920767710544169\n",
      "Training: epoch 141 batch 20 loss 0.005025073420256376\n",
      "Test: epoch 141 batch 0 loss 0.013143104501068592\n",
      "epoch 141 finished - avarage train loss 0.007635246356146346  avarage test loss 0.01595435501076281\n",
      "Training: epoch 142 batch 0 loss 0.0032835318706929684\n",
      "Training: epoch 142 batch 10 loss 0.004213443025946617\n",
      "Training: epoch 142 batch 20 loss 0.01612413115799427\n",
      "Test: epoch 142 batch 0 loss 0.010982804000377655\n",
      "epoch 142 finished - avarage train loss 0.006587521373776013  avarage test loss 0.012746496475301683\n",
      "Training: epoch 143 batch 0 loss 0.0027936245314776897\n",
      "Training: epoch 143 batch 10 loss 0.004922422114759684\n",
      "Training: epoch 143 batch 20 loss 0.005694704595953226\n",
      "Test: epoch 143 batch 0 loss 0.012875504791736603\n",
      "epoch 143 finished - avarage train loss 0.0073909335769712925  avarage test loss 0.015065697487443686\n",
      "Training: epoch 144 batch 0 loss 0.007306219544261694\n",
      "Training: epoch 144 batch 10 loss 0.006229999475181103\n",
      "Training: epoch 144 batch 20 loss 0.015602356754243374\n",
      "Test: epoch 144 batch 0 loss 0.009082067757844925\n",
      "epoch 144 finished - avarage train loss 0.009482060093432665  avarage test loss 0.012584569165483117\n",
      "Training: epoch 145 batch 0 loss 0.004377780947834253\n",
      "Training: epoch 145 batch 10 loss 0.005545241292566061\n",
      "Training: epoch 145 batch 20 loss 0.011019140481948853\n",
      "Test: epoch 145 batch 0 loss 0.012980364263057709\n",
      "epoch 145 finished - avarage train loss 0.006867166283411969  avarage test loss 0.015332625014707446\n",
      "Training: epoch 146 batch 0 loss 0.005820312537252903\n",
      "Training: epoch 146 batch 10 loss 0.011516217142343521\n",
      "Training: epoch 146 batch 20 loss 0.008085640147328377\n",
      "Test: epoch 146 batch 0 loss 0.01133937668055296\n",
      "epoch 146 finished - avarage train loss 0.00887415873625412  avarage test loss 0.013573456089943647\n",
      "Training: epoch 147 batch 0 loss 0.003828005399554968\n",
      "Training: epoch 147 batch 10 loss 0.004885335918515921\n",
      "Training: epoch 147 batch 20 loss 0.002533908933401108\n",
      "Test: epoch 147 batch 0 loss 0.011101928539574146\n",
      "epoch 147 finished - avarage train loss 0.006502916618924716  avarage test loss 0.012780541204847395\n",
      "Training: epoch 148 batch 0 loss 0.013129607774317265\n",
      "Training: epoch 148 batch 10 loss 0.003376220352947712\n",
      "Training: epoch 148 batch 20 loss 0.005410768557339907\n",
      "Test: epoch 148 batch 0 loss 0.009539161808788776\n",
      "epoch 148 finished - avarage train loss 0.007809067578536683  avarage test loss 0.012041805894114077\n",
      "Training: epoch 149 batch 0 loss 0.00615291204303503\n",
      "Training: epoch 149 batch 10 loss 0.0035999834071844816\n",
      "Training: epoch 149 batch 20 loss 0.005146796349436045\n",
      "Test: epoch 149 batch 0 loss 0.010992906056344509\n",
      "epoch 149 finished - avarage train loss 0.008025440258969521  avarage test loss 0.012505092425271869\n",
      "Training: epoch 150 batch 0 loss 0.009569971822202206\n",
      "Training: epoch 150 batch 10 loss 0.008185010403394699\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training: epoch 150 batch 20 loss 0.0043093301355838776\n",
      "Test: epoch 150 batch 0 loss 0.011360186152160168\n",
      "epoch 150 finished - avarage train loss 0.0068490920861348  avarage test loss 0.012855453183874488\n",
      "Training: epoch 151 batch 0 loss 0.0069992500357329845\n",
      "Training: epoch 151 batch 10 loss 0.007742985151708126\n",
      "Training: epoch 151 batch 20 loss 0.0043570492416620255\n",
      "Test: epoch 151 batch 0 loss 0.01459483802318573\n",
      "epoch 151 finished - avarage train loss 0.006503830896690488  avarage test loss 0.016779300291091204\n",
      "Training: epoch 152 batch 0 loss 0.0068007782101631165\n",
      "Training: epoch 152 batch 10 loss 0.005165844224393368\n",
      "Training: epoch 152 batch 20 loss 0.008317284286022186\n",
      "Test: epoch 152 batch 0 loss 0.009355406276881695\n",
      "epoch 152 finished - avarage train loss 0.006922997453988626  avarage test loss 0.012664456968195736\n",
      "Training: epoch 153 batch 0 loss 0.007097961846739054\n",
      "Training: epoch 153 batch 10 loss 0.0073996903374791145\n",
      "Training: epoch 153 batch 20 loss 0.002295175800099969\n",
      "Test: epoch 153 batch 0 loss 0.011026296764612198\n",
      "epoch 153 finished - avarage train loss 0.0064209950284968165  avarage test loss 0.012840501847676933\n",
      "Training: epoch 154 batch 0 loss 0.0043984344229102135\n",
      "Training: epoch 154 batch 10 loss 0.001715123769827187\n",
      "Training: epoch 154 batch 20 loss 0.00911865197122097\n",
      "Test: epoch 154 batch 0 loss 0.011768677271902561\n",
      "epoch 154 finished - avarage train loss 0.007509415487534013  avarage test loss 0.013363135512918234\n",
      "Training: epoch 155 batch 0 loss 0.006198846735060215\n",
      "Training: epoch 155 batch 10 loss 0.003545413725078106\n",
      "Training: epoch 155 batch 20 loss 0.003957211971282959\n",
      "Test: epoch 155 batch 0 loss 0.012724153697490692\n",
      "epoch 155 finished - avarage train loss 0.0072153554426441936  avarage test loss 0.015325072105042636\n",
      "Training: epoch 156 batch 0 loss 0.008278194814920425\n",
      "Training: epoch 156 batch 10 loss 0.002797489520162344\n",
      "Training: epoch 156 batch 20 loss 0.004461127333343029\n",
      "Test: epoch 156 batch 0 loss 0.014184275642037392\n",
      "epoch 156 finished - avarage train loss 0.008352736268064072  avarage test loss 0.017005640547722578\n",
      "Training: epoch 157 batch 0 loss 0.004569372162222862\n",
      "Training: epoch 157 batch 10 loss 0.008315933868288994\n",
      "Training: epoch 157 batch 20 loss 0.007960130460560322\n",
      "Test: epoch 157 batch 0 loss 0.011117260903120041\n",
      "epoch 157 finished - avarage train loss 0.007234087181759292  avarage test loss 0.012895978055894375\n",
      "Training: epoch 158 batch 0 loss 0.0036663387436419725\n",
      "Training: epoch 158 batch 10 loss 0.0028217032086104155\n",
      "Training: epoch 158 batch 20 loss 0.006768177263438702\n",
      "Test: epoch 158 batch 0 loss 0.010704901069402695\n",
      "epoch 158 finished - avarage train loss 0.006179442693447244  avarage test loss 0.012329011224210262\n",
      "Training: epoch 159 batch 0 loss 0.0034139968920499086\n",
      "Training: epoch 159 batch 10 loss 0.0054489863105118275\n",
      "Training: epoch 159 batch 20 loss 0.0064332978799939156\n",
      "Test: epoch 159 batch 0 loss 0.011856205761432648\n",
      "epoch 159 finished - avarage train loss 0.007420461536545692  avarage test loss 0.013349942746572196\n",
      "Training: epoch 160 batch 0 loss 0.009032707661390305\n",
      "Training: epoch 160 batch 10 loss 0.006408969406038523\n",
      "Training: epoch 160 batch 20 loss 0.003270478220656514\n",
      "Test: epoch 160 batch 0 loss 0.010302992537617683\n",
      "epoch 160 finished - avarage train loss 0.006616662673909089  avarage test loss 0.012205468025058508\n",
      "Training: epoch 161 batch 0 loss 0.0069383298978209496\n",
      "Training: epoch 161 batch 10 loss 0.0042378222569823265\n",
      "Training: epoch 161 batch 20 loss 0.0030941006261855364\n",
      "Test: epoch 161 batch 0 loss 0.013688001781702042\n",
      "epoch 161 finished - avarage train loss 0.00879025509870001  avarage test loss 0.01591929281130433\n",
      "Training: epoch 162 batch 0 loss 0.005798443220555782\n",
      "Training: epoch 162 batch 10 loss 0.004897283390164375\n",
      "Training: epoch 162 batch 20 loss 0.006947652902454138\n",
      "Test: epoch 162 batch 0 loss 0.014334525913000107\n",
      "epoch 162 finished - avarage train loss 0.006420241098786737  avarage test loss 0.016902980161830783\n",
      "Training: epoch 163 batch 0 loss 0.007620671764016151\n",
      "Training: epoch 163 batch 10 loss 0.007803771644830704\n",
      "Training: epoch 163 batch 20 loss 0.003989923279732466\n",
      "Test: epoch 163 batch 0 loss 0.010918401181697845\n",
      "epoch 163 finished - avarage train loss 0.011502155729023546  avarage test loss 0.013421822106465697\n",
      "Training: epoch 164 batch 0 loss 0.00833629909902811\n",
      "Training: epoch 164 batch 10 loss 0.016405394300818443\n",
      "Training: epoch 164 batch 20 loss 0.00869100820273161\n",
      "Test: epoch 164 batch 0 loss 0.010606184601783752\n",
      "epoch 164 finished - avarage train loss 0.011582696922765723  avarage test loss 0.012958162114955485\n",
      "Training: epoch 165 batch 0 loss 0.006095480173826218\n",
      "Training: epoch 165 batch 10 loss 0.0023265238851308823\n",
      "Training: epoch 165 batch 20 loss 0.003544424194842577\n",
      "Test: epoch 165 batch 0 loss 0.011962874792516232\n",
      "epoch 165 finished - avarage train loss 0.008512773406531277  avarage test loss 0.013373206136748195\n",
      "Training: epoch 166 batch 0 loss 0.003423274029046297\n",
      "Training: epoch 166 batch 10 loss 0.0038637197576463223\n",
      "Training: epoch 166 batch 20 loss 0.0038164786528795958\n",
      "Test: epoch 166 batch 0 loss 0.012244641780853271\n",
      "epoch 166 finished - avarage train loss 0.00692937464116077  avarage test loss 0.013912622700445354\n",
      "Training: epoch 167 batch 0 loss 0.002263830043375492\n",
      "Training: epoch 167 batch 10 loss 0.005622635595500469\n",
      "Training: epoch 167 batch 20 loss 0.007609429303556681\n",
      "Test: epoch 167 batch 0 loss 0.010540658608078957\n",
      "epoch 167 finished - avarage train loss 0.005751385176875468  avarage test loss 0.012700383667834103\n",
      "Training: epoch 168 batch 0 loss 0.004438529722392559\n",
      "Training: epoch 168 batch 10 loss 0.01338666956871748\n",
      "Training: epoch 168 batch 20 loss 0.00909394770860672\n",
      "Test: epoch 168 batch 0 loss 0.011432847939431667\n",
      "epoch 168 finished - avarage train loss 0.00797148669491811  avarage test loss 0.012934945756569505\n",
      "Training: epoch 169 batch 0 loss 0.005004818085581064\n",
      "Training: epoch 169 batch 10 loss 0.0019760597497224808\n",
      "Training: epoch 169 batch 20 loss 0.003417038358747959\n",
      "Test: epoch 169 batch 0 loss 0.010215488262474537\n",
      "epoch 169 finished - avarage train loss 0.007365014018683598  avarage test loss 0.012071258621290326\n",
      "Training: epoch 170 batch 0 loss 0.008403883315622807\n",
      "Training: epoch 170 batch 10 loss 0.007354943081736565\n",
      "Training: epoch 170 batch 20 loss 0.003521773498505354\n",
      "Test: epoch 170 batch 0 loss 0.011494562961161137\n",
      "epoch 170 finished - avarage train loss 0.007459889744119397  avarage test loss 0.012888998608104885\n",
      "Training: epoch 171 batch 0 loss 0.0035082469694316387\n",
      "Training: epoch 171 batch 10 loss 0.005878825206309557\n",
      "Training: epoch 171 batch 20 loss 0.006811121478676796\n",
      "Test: epoch 171 batch 0 loss 0.01014522835612297\n",
      "epoch 171 finished - avarage train loss 0.007664539987735194  avarage test loss 0.012199017335660756\n",
      "Training: epoch 172 batch 0 loss 0.01290172804147005\n",
      "Training: epoch 172 batch 10 loss 0.005407177843153477\n",
      "Training: epoch 172 batch 20 loss 0.00549741555005312\n",
      "Test: epoch 172 batch 0 loss 0.011018316261470318\n",
      "epoch 172 finished - avarage train loss 0.007474013202791584  avarage test loss 0.012518229545094073\n",
      "Training: epoch 173 batch 0 loss 0.006874546874314547\n",
      "Training: epoch 173 batch 10 loss 0.00501133780926466\n",
      "Training: epoch 173 batch 20 loss 0.004903996828943491\n",
      "Test: epoch 173 batch 0 loss 0.012148912064731121\n",
      "epoch 173 finished - avarage train loss 0.007694432391496054  avarage test loss 0.013479988439939916\n",
      "Training: epoch 174 batch 0 loss 0.00445261225104332\n",
      "Training: epoch 174 batch 10 loss 0.007190622389316559\n",
      "Training: epoch 174 batch 20 loss 0.006068998947739601\n",
      "Test: epoch 174 batch 0 loss 0.014119947329163551\n",
      "epoch 174 finished - avarage train loss 0.007840028032660484  avarage test loss 0.01653747307136655\n",
      "Training: epoch 175 batch 0 loss 0.004341260064393282\n",
      "Training: epoch 175 batch 10 loss 0.00850913766771555\n",
      "Training: epoch 175 batch 20 loss 0.0031374902464449406\n",
      "Test: epoch 175 batch 0 loss 0.014709546230733395\n",
      "epoch 175 finished - avarage train loss 0.005776314169232701  avarage test loss 0.016864130506291986\n",
      "Training: epoch 176 batch 0 loss 0.007836735807359219\n",
      "Training: epoch 176 batch 10 loss 0.007601331453770399\n",
      "Training: epoch 176 batch 20 loss 0.011043288744986057\n",
      "Test: epoch 176 batch 0 loss 0.014826168306171894\n",
      "epoch 176 finished - avarage train loss 0.01373296545754219  avarage test loss 0.01904580951668322\n",
      "Training: epoch 177 batch 0 loss 0.009586876258254051\n",
      "Training: epoch 177 batch 10 loss 0.020547647029161453\n",
      "Training: epoch 177 batch 20 loss 0.011389397084712982\n",
      "Test: epoch 177 batch 0 loss 0.006637443322688341\n",
      "epoch 177 finished - avarage train loss 0.012030790736577633  avarage test loss 0.012597467168234289\n",
      "Training: epoch 178 batch 0 loss 0.005515281576663256\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training: epoch 178 batch 10 loss 0.00492512620985508\n",
      "Training: epoch 178 batch 20 loss 0.01149461418390274\n",
      "Test: epoch 178 batch 0 loss 0.01041374821215868\n",
      "epoch 178 finished - avarage train loss 0.008386480613964898  avarage test loss 0.013026396743953228\n",
      "Training: epoch 179 batch 0 loss 0.005380385555326939\n",
      "Training: epoch 179 batch 10 loss 0.006161563564091921\n",
      "Training: epoch 179 batch 20 loss 0.005128983873873949\n",
      "Test: epoch 179 batch 0 loss 0.010213319212198257\n",
      "epoch 179 finished - avarage train loss 0.006804249177140923  avarage test loss 0.012442679260857403\n",
      "Training: epoch 180 batch 0 loss 0.004843221977353096\n",
      "Training: epoch 180 batch 10 loss 0.0062241749837994576\n",
      "Training: epoch 180 batch 20 loss 0.003052476327866316\n",
      "Test: epoch 180 batch 0 loss 0.008908119052648544\n",
      "epoch 180 finished - avarage train loss 0.005481676407286833  avarage test loss 0.01225397374946624\n",
      "Training: epoch 181 batch 0 loss 0.007331050466746092\n",
      "Training: epoch 181 batch 10 loss 0.007752409670501947\n",
      "Training: epoch 181 batch 20 loss 0.004317512270063162\n",
      "Test: epoch 181 batch 0 loss 0.010237668640911579\n",
      "epoch 181 finished - avarage train loss 0.006826298849271803  avarage test loss 0.012245324905961752\n",
      "Training: epoch 182 batch 0 loss 0.008711358532309532\n",
      "Training: epoch 182 batch 10 loss 0.0022269103210419416\n",
      "Training: epoch 182 batch 20 loss 0.002132704248651862\n",
      "Test: epoch 182 batch 0 loss 0.010592998005449772\n",
      "epoch 182 finished - avarage train loss 0.006832675085049765  avarage test loss 0.012494362657889724\n",
      "Training: epoch 183 batch 0 loss 0.008782568387687206\n",
      "Training: epoch 183 batch 10 loss 0.0028814009856432676\n",
      "Training: epoch 183 batch 20 loss 0.012369553558528423\n",
      "Test: epoch 183 batch 0 loss 0.010952544398605824\n",
      "epoch 183 finished - avarage train loss 0.006939610349945724  avarage test loss 0.012529113213531673\n",
      "Training: epoch 184 batch 0 loss 0.006872258614748716\n",
      "Training: epoch 184 batch 10 loss 0.0019037025049328804\n",
      "Training: epoch 184 batch 20 loss 0.013174195773899555\n",
      "Test: epoch 184 batch 0 loss 0.011862149462103844\n",
      "epoch 184 finished - avarage train loss 0.008065862456689877  avarage test loss 0.014048437122255564\n",
      "Training: epoch 185 batch 0 loss 0.003801988437771797\n",
      "Training: epoch 185 batch 10 loss 0.009592712856829166\n",
      "Training: epoch 185 batch 20 loss 0.004734453279525042\n",
      "Test: epoch 185 batch 0 loss 0.011342805810272694\n",
      "epoch 185 finished - avarage train loss 0.007404513893938014  avarage test loss 0.012974696233868599\n",
      "Training: epoch 186 batch 0 loss 0.007807206362485886\n",
      "Training: epoch 186 batch 10 loss 0.00506749888882041\n",
      "Training: epoch 186 batch 20 loss 0.003183477558195591\n",
      "Test: epoch 186 batch 0 loss 0.010424276813864708\n",
      "epoch 186 finished - avarage train loss 0.0065736520898945884  avarage test loss 0.012091042590327561\n",
      "Training: epoch 187 batch 0 loss 0.00481460290029645\n",
      "Training: epoch 187 batch 10 loss 0.005325626116245985\n",
      "Training: epoch 187 batch 20 loss 0.00347533100284636\n",
      "Test: epoch 187 batch 0 loss 0.009086407721042633\n",
      "epoch 187 finished - avarage train loss 0.007360232648728737  avarage test loss 0.011436917819082737\n",
      "Training: epoch 188 batch 0 loss 0.004182754550129175\n",
      "Training: epoch 188 batch 10 loss 0.0218253992497921\n",
      "Training: epoch 188 batch 20 loss 0.009277847595512867\n",
      "Test: epoch 188 batch 0 loss 0.009389553219079971\n",
      "epoch 188 finished - avarage train loss 0.007129811159945254  avarage test loss 0.012343945098109543\n",
      "Training: epoch 189 batch 0 loss 0.012668147683143616\n",
      "Training: epoch 189 batch 10 loss 0.005174883175641298\n",
      "Training: epoch 189 batch 20 loss 0.006652218755334616\n",
      "Test: epoch 189 batch 0 loss 0.009886009618639946\n",
      "epoch 189 finished - avarage train loss 0.007844743768459764  avarage test loss 0.012867790646851063\n",
      "Training: epoch 190 batch 0 loss 0.00569118233397603\n",
      "Training: epoch 190 batch 10 loss 0.004336144775152206\n",
      "Training: epoch 190 batch 20 loss 0.0039818244986236095\n",
      "Test: epoch 190 batch 0 loss 0.011174994520843029\n",
      "epoch 190 finished - avarage train loss 0.007961897468515512  avarage test loss 0.013379533309489489\n",
      "Training: epoch 191 batch 0 loss 0.005586700513958931\n",
      "Training: epoch 191 batch 10 loss 0.005463044159114361\n",
      "Training: epoch 191 batch 20 loss 0.007491231430321932\n",
      "Test: epoch 191 batch 0 loss 0.009008405730128288\n",
      "epoch 191 finished - avarage train loss 0.007817637295751223  avarage test loss 0.011285196291282773\n",
      "Training: epoch 192 batch 0 loss 0.008578536100685596\n",
      "Training: epoch 192 batch 10 loss 0.002938830293715\n",
      "Training: epoch 192 batch 20 loss 0.0009436754626221955\n",
      "Test: epoch 192 batch 0 loss 0.010179190896451473\n",
      "epoch 192 finished - avarage train loss 0.0069178184353874935  avarage test loss 0.01223235938232392\n",
      "Training: epoch 193 batch 0 loss 0.004246722906827927\n",
      "Training: epoch 193 batch 10 loss 0.00958667229861021\n",
      "Training: epoch 193 batch 20 loss 0.007048487663269043\n",
      "Test: epoch 193 batch 0 loss 0.00905763078480959\n",
      "epoch 193 finished - avarage train loss 0.007551245118395008  avarage test loss 0.011400799616239965\n",
      "Training: epoch 194 batch 0 loss 0.008628596551716328\n",
      "Training: epoch 194 batch 10 loss 0.0026068708393722773\n",
      "Training: epoch 194 batch 20 loss 0.0023942640982568264\n",
      "Test: epoch 194 batch 0 loss 0.010420962236821651\n",
      "epoch 194 finished - avarage train loss 0.0066859019631198766  avarage test loss 0.015648858854547143\n",
      "Training: epoch 195 batch 0 loss 0.007484850939363241\n",
      "Training: epoch 195 batch 10 loss 0.013096863403916359\n",
      "Training: epoch 195 batch 20 loss 0.012089895084500313\n",
      "Test: epoch 195 batch 0 loss 0.011494339443743229\n",
      "epoch 195 finished - avarage train loss 0.011130244904679471  avarage test loss 0.015386261511594057\n",
      "Training: epoch 196 batch 0 loss 0.0076936352998018265\n",
      "Training: epoch 196 batch 10 loss 0.006836072076112032\n",
      "Training: epoch 196 batch 20 loss 0.006149051245301962\n",
      "Test: epoch 196 batch 0 loss 0.010039126500487328\n",
      "epoch 196 finished - avarage train loss 0.009923771802528665  avarage test loss 0.012802222394384444\n",
      "Training: epoch 197 batch 0 loss 0.0030080596916377544\n",
      "Training: epoch 197 batch 10 loss 0.004866216331720352\n",
      "Training: epoch 197 batch 20 loss 0.00303448922932148\n",
      "Test: epoch 197 batch 0 loss 0.00751836970448494\n",
      "epoch 197 finished - avarage train loss 0.00822649632002516  avarage test loss 0.013025450287386775\n",
      "Training: epoch 198 batch 0 loss 0.004732299596071243\n",
      "Training: epoch 198 batch 10 loss 0.003344080876559019\n",
      "Training: epoch 198 batch 20 loss 0.019104335457086563\n",
      "Test: epoch 198 batch 0 loss 0.014994872733950615\n",
      "epoch 198 finished - avarage train loss 0.008587661391959107  avarage test loss 0.018541466910392046\n",
      "Training: epoch 199 batch 0 loss 0.005735937971621752\n",
      "Training: epoch 199 batch 10 loss 0.004500401671975851\n",
      "Training: epoch 199 batch 20 loss 0.006080209743231535\n",
      "Test: epoch 199 batch 0 loss 0.010615519247949123\n",
      "epoch 199 finished - avarage train loss 0.006987597656853754  avarage test loss 0.012570372549816966\n",
      "Training: epoch 0 batch 0 loss 0.8589246869087219\n",
      "Training: epoch 0 batch 10 loss 0.6294524073600769\n",
      "Training: epoch 0 batch 20 loss 0.6216280460357666\n",
      "Test: epoch 0 batch 0 loss 0.41104403138160706\n",
      "epoch 0 finished - avarage train loss 0.5280228514095833  avarage test loss 0.45801500976085663\n",
      "Training: epoch 1 batch 0 loss 0.7338869571685791\n",
      "Training: epoch 1 batch 10 loss 0.31228527426719666\n",
      "Training: epoch 1 batch 20 loss 0.6556468605995178\n",
      "Test: epoch 1 batch 0 loss 0.39967986941337585\n",
      "epoch 1 finished - avarage train loss 0.4965767888673421  avarage test loss 0.44422829151153564\n",
      "Training: epoch 2 batch 0 loss 0.32621145248413086\n",
      "Training: epoch 2 batch 10 loss 0.6514390110969543\n",
      "Training: epoch 2 batch 20 loss 0.6097330451011658\n",
      "Test: epoch 2 batch 0 loss 0.4125577211380005\n",
      "epoch 2 finished - avarage train loss 0.5042266496296587  avarage test loss 0.45870763063430786\n",
      "Training: epoch 3 batch 0 loss 0.5374334454536438\n",
      "Training: epoch 3 batch 10 loss 0.3136303126811981\n",
      "Training: epoch 3 batch 20 loss 0.6039658188819885\n",
      "Test: epoch 3 batch 0 loss 0.3957541882991791\n",
      "epoch 3 finished - avarage train loss 0.5163688536348015  avarage test loss 0.44616781175136566\n",
      "Training: epoch 4 batch 0 loss 0.5482345819473267\n",
      "Training: epoch 4 batch 10 loss 0.3608167767524719\n",
      "Training: epoch 4 batch 20 loss 0.617115318775177\n",
      "Test: epoch 4 batch 0 loss 0.40078824758529663\n",
      "epoch 4 finished - avarage train loss 0.5204585626207548  avarage test loss 0.4466169588267803\n",
      "Training: epoch 5 batch 0 loss 0.6914735436439514\n",
      "Training: epoch 5 batch 10 loss 0.6287977695465088\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training: epoch 5 batch 20 loss 0.36822575330734253\n",
      "Test: epoch 5 batch 0 loss 0.4007602334022522\n",
      "epoch 5 finished - avarage train loss 0.5070443739151133  avarage test loss 0.4420986771583557\n",
      "Training: epoch 6 batch 0 loss 0.3940519094467163\n",
      "Training: epoch 6 batch 10 loss 0.36967164278030396\n",
      "Training: epoch 6 batch 20 loss 0.5645398497581482\n",
      "Test: epoch 6 batch 0 loss 0.40514352917671204\n",
      "epoch 6 finished - avarage train loss 0.5446133058646629  avarage test loss 0.4533931612968445\n",
      "Training: epoch 7 batch 0 loss 0.7329332828521729\n",
      "Training: epoch 7 batch 10 loss 0.382337749004364\n",
      "Training: epoch 7 batch 20 loss 0.5799247026443481\n",
      "Test: epoch 7 batch 0 loss 0.3965897560119629\n",
      "epoch 7 finished - avarage train loss 0.5425932612912409  avarage test loss 0.442091416567564\n",
      "Training: epoch 8 batch 0 loss 0.479826420545578\n",
      "Training: epoch 8 batch 10 loss 0.5865402817726135\n",
      "Training: epoch 8 batch 20 loss 0.8700971007347107\n",
      "Test: epoch 8 batch 0 loss 0.4083607494831085\n",
      "epoch 8 finished - avarage train loss 0.5052182407214724  avarage test loss 0.4548516944050789\n",
      "Training: epoch 9 batch 0 loss 0.6372160911560059\n",
      "Training: epoch 9 batch 10 loss 0.6364893913269043\n",
      "Training: epoch 9 batch 20 loss 0.19238030910491943\n",
      "Test: epoch 9 batch 0 loss 0.40664079785346985\n",
      "epoch 9 finished - avarage train loss 0.5052794844939791  avarage test loss 0.4524546265602112\n",
      "Training: epoch 10 batch 0 loss 0.7157831788063049\n",
      "Training: epoch 10 batch 10 loss 0.32985061407089233\n",
      "Training: epoch 10 batch 20 loss 0.4103419780731201\n",
      "Test: epoch 10 batch 0 loss 0.40063440799713135\n",
      "epoch 10 finished - avarage train loss 0.5023704078690759  avarage test loss 0.4460409916937351\n",
      "Training: epoch 11 batch 0 loss 0.3006531596183777\n",
      "Training: epoch 11 batch 10 loss 0.41889938712120056\n",
      "Training: epoch 11 batch 20 loss 0.6220933794975281\n",
      "Test: epoch 11 batch 0 loss 0.40483587980270386\n",
      "epoch 11 finished - avarage train loss 0.5412140827754448  avarage test loss 0.4457133933901787\n",
      "Training: epoch 12 batch 0 loss 0.6387192010879517\n",
      "Training: epoch 12 batch 10 loss 0.3473701477050781\n",
      "Training: epoch 12 batch 20 loss 0.5392877459526062\n",
      "Test: epoch 12 batch 0 loss 0.39802783727645874\n",
      "epoch 12 finished - avarage train loss 0.5338469877325255  avarage test loss 0.4454626142978668\n",
      "Training: epoch 13 batch 0 loss 0.5969848036766052\n",
      "Training: epoch 13 batch 10 loss 0.5607668161392212\n",
      "Training: epoch 13 batch 20 loss 0.5130653381347656\n",
      "Test: epoch 13 batch 0 loss 0.39291226863861084\n",
      "epoch 13 finished - avarage train loss 0.5056069863253626  avarage test loss 0.4366234466433525\n",
      "Training: epoch 14 batch 0 loss 0.6495656967163086\n",
      "Training: epoch 14 batch 10 loss 0.4371497631072998\n",
      "Training: epoch 14 batch 20 loss 0.7346610426902771\n",
      "Test: epoch 14 batch 0 loss 0.41092705726623535\n",
      "epoch 14 finished - avarage train loss 0.5294192387112255  avarage test loss 0.4622962549328804\n",
      "Training: epoch 15 batch 0 loss 0.4466971755027771\n",
      "Training: epoch 15 batch 10 loss 0.38547515869140625\n",
      "Training: epoch 15 batch 20 loss 0.4132941961288452\n",
      "Test: epoch 15 batch 0 loss 0.40348532795906067\n",
      "epoch 15 finished - avarage train loss 0.5158283093879963  avarage test loss 0.45250559598207474\n",
      "Training: epoch 16 batch 0 loss 0.93910813331604\n",
      "Training: epoch 16 batch 10 loss 0.6970277428627014\n",
      "Training: epoch 16 batch 20 loss 0.4370346963405609\n",
      "Test: epoch 16 batch 0 loss 0.4022524356842041\n",
      "epoch 16 finished - avarage train loss 0.5056690409265715  avarage test loss 0.44909510761499405\n",
      "Training: epoch 17 batch 0 loss 0.42235812544822693\n",
      "Training: epoch 17 batch 10 loss 0.3911135792732239\n",
      "Training: epoch 17 batch 20 loss 0.622950553894043\n",
      "Test: epoch 17 batch 0 loss 0.4061843752861023\n",
      "epoch 17 finished - avarage train loss 0.5029882428974941  avarage test loss 0.45268354564905167\n",
      "Training: epoch 18 batch 0 loss 0.6899634599685669\n",
      "Training: epoch 18 batch 10 loss 0.4984750747680664\n",
      "Training: epoch 18 batch 20 loss 0.5696864128112793\n",
      "Test: epoch 18 batch 0 loss 0.39777132868766785\n",
      "epoch 18 finished - avarage train loss 0.5099772430699447  avarage test loss 0.44618044793605804\n",
      "Training: epoch 19 batch 0 loss 0.6781418919563293\n",
      "Training: epoch 19 batch 10 loss 0.47158005833625793\n",
      "Training: epoch 19 batch 20 loss 0.48630788922309875\n",
      "Test: epoch 19 batch 0 loss 0.40227168798446655\n",
      "epoch 19 finished - avarage train loss 0.5319328477670406  avarage test loss 0.44822145253419876\n",
      "Training: epoch 20 batch 0 loss 0.8079119324684143\n",
      "Training: epoch 20 batch 10 loss 0.29592934250831604\n",
      "Training: epoch 20 batch 20 loss 0.39229220151901245\n",
      "Test: epoch 20 batch 0 loss 0.4076034128665924\n",
      "epoch 20 finished - avarage train loss 0.49776563870495766  avarage test loss 0.45547591894865036\n",
      "Training: epoch 21 batch 0 loss 0.4386298656463623\n",
      "Training: epoch 21 batch 10 loss 0.52521812915802\n",
      "Training: epoch 21 batch 20 loss 0.3846414387226105\n",
      "Test: epoch 21 batch 0 loss 0.39992213249206543\n",
      "epoch 21 finished - avarage train loss 0.4949199043471238  avarage test loss 0.4459049664437771\n",
      "Training: epoch 22 batch 0 loss 0.6063666939735413\n",
      "Training: epoch 22 batch 10 loss 0.6211969256401062\n",
      "Training: epoch 22 batch 20 loss 0.3367077112197876\n",
      "Test: epoch 22 batch 0 loss 0.4023759067058563\n",
      "epoch 22 finished - avarage train loss 0.5345573065609768  avarage test loss 0.4474566802382469\n",
      "Training: epoch 23 batch 0 loss 0.3896405100822449\n",
      "Training: epoch 23 batch 10 loss 0.7158646583557129\n",
      "Training: epoch 23 batch 20 loss 0.7718026638031006\n",
      "Test: epoch 23 batch 0 loss 0.40794506669044495\n",
      "epoch 23 finished - avarage train loss 0.5428074973410574  avarage test loss 0.45369746536016464\n",
      "Training: epoch 24 batch 0 loss 0.4323098659515381\n",
      "Training: epoch 24 batch 10 loss 0.6226523518562317\n",
      "Training: epoch 24 batch 20 loss 0.38404175639152527\n",
      "Test: epoch 24 batch 0 loss 0.39731693267822266\n",
      "epoch 24 finished - avarage train loss 0.5171229387151783  avarage test loss 0.44535915553569794\n",
      "Training: epoch 25 batch 0 loss 0.4749472439289093\n",
      "Training: epoch 25 batch 10 loss 0.38022974133491516\n",
      "Training: epoch 25 batch 20 loss 0.7821558117866516\n",
      "Test: epoch 25 batch 0 loss 0.4004139304161072\n",
      "epoch 25 finished - avarage train loss 0.5202201553459825  avarage test loss 0.4467689022421837\n",
      "Training: epoch 26 batch 0 loss 0.5148864388465881\n",
      "Training: epoch 26 batch 10 loss 0.4469672441482544\n",
      "Training: epoch 26 batch 20 loss 0.6253221035003662\n",
      "Test: epoch 26 batch 0 loss 0.3995460867881775\n",
      "epoch 26 finished - avarage train loss 0.5002716039789135  avarage test loss 0.4456041008234024\n",
      "Training: epoch 27 batch 0 loss 0.3600449562072754\n",
      "Training: epoch 27 batch 10 loss 0.5004806518554688\n",
      "Training: epoch 27 batch 20 loss 0.43830472230911255\n",
      "Test: epoch 27 batch 0 loss 0.3999316096305847\n",
      "epoch 27 finished - avarage train loss 0.5035556739774244  avarage test loss 0.44688600301742554\n",
      "Training: epoch 28 batch 0 loss 0.5128921270370483\n",
      "Training: epoch 28 batch 10 loss 0.5777856111526489\n",
      "Training: epoch 28 batch 20 loss 0.5240361094474792\n",
      "Test: epoch 28 batch 0 loss 0.3988633453845978\n",
      "epoch 28 finished - avarage train loss 0.4942727444757675  avarage test loss 0.4444121643900871\n",
      "Training: epoch 29 batch 0 loss 0.49907219409942627\n",
      "Training: epoch 29 batch 10 loss 0.7170920372009277\n",
      "Training: epoch 29 batch 20 loss 0.46333780884742737\n",
      "Test: epoch 29 batch 0 loss 0.39940065145492554\n",
      "epoch 29 finished - avarage train loss 0.4990352916306463  avarage test loss 0.4454130604863167\n",
      "Training: epoch 30 batch 0 loss 0.7922947406768799\n",
      "Training: epoch 30 batch 10 loss 0.4994965195655823\n",
      "Training: epoch 30 batch 20 loss 0.3286949694156647\n",
      "Test: epoch 30 batch 0 loss 0.3991001546382904\n",
      "epoch 30 finished - avarage train loss 0.5058919155392153  avarage test loss 0.44330254942178726\n",
      "Training: epoch 31 batch 0 loss 0.34697332978248596\n",
      "Training: epoch 31 batch 10 loss 0.6526812314987183\n",
      "Training: epoch 31 batch 20 loss 0.47755861282348633\n",
      "Test: epoch 31 batch 0 loss 0.4003866910934448\n",
      "epoch 31 finished - avarage train loss 0.5420307044325203  avarage test loss 0.44681358337402344\n",
      "Training: epoch 32 batch 0 loss 0.2647527754306793\n",
      "Training: epoch 32 batch 10 loss 0.45107218623161316\n",
      "Training: epoch 32 batch 20 loss 0.7089508771896362\n",
      "Test: epoch 32 batch 0 loss 0.39963498711586\n",
      "epoch 32 finished - avarage train loss 0.5049026043250643  avarage test loss 0.445478904992342\n",
      "Training: epoch 33 batch 0 loss 0.419005811214447\n",
      "Training: epoch 33 batch 10 loss 0.4232478141784668\n",
      "Training: epoch 33 batch 20 loss 0.3923133313655853\n",
      "Test: epoch 33 batch 0 loss 0.397674560546875\n",
      "epoch 33 finished - avarage train loss 0.5046126338942297  avarage test loss 0.44425736740231514\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training: epoch 34 batch 0 loss 0.5736154913902283\n",
      "Training: epoch 34 batch 10 loss 0.49509209394454956\n",
      "Training: epoch 34 batch 20 loss 0.44307467341423035\n",
      "Test: epoch 34 batch 0 loss 0.3993878662586212\n",
      "epoch 34 finished - avarage train loss 0.509180084898554  avarage test loss 0.4457735978066921\n",
      "Training: epoch 35 batch 0 loss 0.4698176980018616\n",
      "Training: epoch 35 batch 10 loss 0.7718856334686279\n",
      "Training: epoch 35 batch 20 loss 0.5543032884597778\n",
      "Test: epoch 35 batch 0 loss 0.3978199064731598\n",
      "epoch 35 finished - avarage train loss 0.5050550894490604  avarage test loss 0.4477068707346916\n",
      "Training: epoch 36 batch 0 loss 0.5514761805534363\n",
      "Training: epoch 36 batch 10 loss 0.64273601770401\n",
      "Training: epoch 36 batch 20 loss 0.36719122529029846\n",
      "Test: epoch 36 batch 0 loss 0.39362260699272156\n",
      "epoch 36 finished - avarage train loss 0.5116089202206711  avarage test loss 0.44360271096229553\n",
      "Training: epoch 37 batch 0 loss 0.4227592647075653\n",
      "Training: epoch 37 batch 10 loss 0.6477601528167725\n",
      "Training: epoch 37 batch 20 loss 0.7668759226799011\n",
      "Test: epoch 37 batch 0 loss 0.39982691407203674\n",
      "epoch 37 finished - avarage train loss 0.5096762426968279  avarage test loss 0.44895078241825104\n",
      "Training: epoch 38 batch 0 loss 0.5924130082130432\n",
      "Training: epoch 38 batch 10 loss 0.8028791546821594\n",
      "Training: epoch 38 batch 20 loss 0.40032532811164856\n",
      "Test: epoch 38 batch 0 loss 0.391539990901947\n",
      "epoch 38 finished - avarage train loss 0.5021510853849608  avarage test loss 0.4451787546277046\n",
      "Training: epoch 39 batch 0 loss 0.22229430079460144\n",
      "Training: epoch 39 batch 10 loss 0.37521713972091675\n",
      "Training: epoch 39 batch 20 loss 0.4201575815677643\n",
      "Test: epoch 39 batch 0 loss 0.39958223700523376\n",
      "epoch 39 finished - avarage train loss 0.505577834515736  avarage test loss 0.4493454620242119\n",
      "Training: epoch 40 batch 0 loss 0.3163318932056427\n",
      "Training: epoch 40 batch 10 loss 0.4527603089809418\n",
      "Training: epoch 40 batch 20 loss 1.0352224111557007\n",
      "Test: epoch 40 batch 0 loss 0.39932379126548767\n",
      "epoch 40 finished - avarage train loss 0.49918200435309573  avarage test loss 0.446839302778244\n",
      "Training: epoch 41 batch 0 loss 0.5473789572715759\n",
      "Training: epoch 41 batch 10 loss 0.4832119047641754\n",
      "Training: epoch 41 batch 20 loss 0.3864695727825165\n",
      "Test: epoch 41 batch 0 loss 0.39140772819519043\n",
      "epoch 41 finished - avarage train loss 0.5109426081180573  avarage test loss 0.4381854012608528\n",
      "Training: epoch 42 batch 0 loss 0.5944403409957886\n",
      "Training: epoch 42 batch 10 loss 0.3337463140487671\n",
      "Training: epoch 42 batch 20 loss 0.44823363423347473\n",
      "Test: epoch 42 batch 0 loss 0.39880460500717163\n",
      "epoch 42 finished - avarage train loss 0.5475292154427233  avarage test loss 0.4471879303455353\n",
      "Training: epoch 43 batch 0 loss 0.5317180156707764\n",
      "Training: epoch 43 batch 10 loss 0.43996259570121765\n",
      "Training: epoch 43 batch 20 loss 0.3741372227668762\n",
      "Test: epoch 43 batch 0 loss 0.3965749740600586\n",
      "epoch 43 finished - avarage train loss 0.5063773008256123  avarage test loss 0.44925206899642944\n",
      "Training: epoch 44 batch 0 loss 0.379461407661438\n",
      "Training: epoch 44 batch 10 loss 0.43105992674827576\n",
      "Training: epoch 44 batch 20 loss 0.514745831489563\n",
      "Test: epoch 44 batch 0 loss 0.3946065306663513\n",
      "epoch 44 finished - avarage train loss 0.5008015714842697  avarage test loss 0.4472084566950798\n",
      "Training: epoch 45 batch 0 loss 0.46956688165664673\n",
      "Training: epoch 45 batch 10 loss 0.45637020468711853\n",
      "Training: epoch 45 batch 20 loss 0.2854071855545044\n",
      "Test: epoch 45 batch 0 loss 0.3999096751213074\n",
      "epoch 45 finished - avarage train loss 0.5251970722757536  avarage test loss 0.44913150370121\n",
      "Training: epoch 46 batch 0 loss 0.4951721429824829\n",
      "Training: epoch 46 batch 10 loss 0.5141622424125671\n",
      "Training: epoch 46 batch 20 loss 0.7432789206504822\n",
      "Test: epoch 46 batch 0 loss 0.4000377655029297\n",
      "epoch 46 finished - avarage train loss 0.5013893287757347  avarage test loss 0.4500977694988251\n",
      "Training: epoch 47 batch 0 loss 0.7189195156097412\n",
      "Training: epoch 47 batch 10 loss 0.2895527780056\n",
      "Training: epoch 47 batch 20 loss 0.4397161304950714\n",
      "Test: epoch 47 batch 0 loss 0.3991779088973999\n",
      "epoch 47 finished - avarage train loss 0.5219052896417421  avarage test loss 0.44370611384510994\n",
      "Training: epoch 48 batch 0 loss 0.44455620646476746\n",
      "Training: epoch 48 batch 10 loss 0.34302255511283875\n",
      "Training: epoch 48 batch 20 loss 0.5835055112838745\n",
      "Test: epoch 48 batch 0 loss 0.39853349328041077\n",
      "epoch 48 finished - avarage train loss 0.49986563725718136  avarage test loss 0.44693388789892197\n",
      "Training: epoch 49 batch 0 loss 0.5720235109329224\n",
      "Training: epoch 49 batch 10 loss 0.6214556694030762\n",
      "Training: epoch 49 batch 20 loss 0.3747955858707428\n",
      "Test: epoch 49 batch 0 loss 0.3924522399902344\n",
      "epoch 49 finished - avarage train loss 0.510824086337254  avarage test loss 0.4456244930624962\n",
      "Training: epoch 50 batch 0 loss 0.2850058674812317\n",
      "Training: epoch 50 batch 10 loss 0.4909384250640869\n",
      "Training: epoch 50 batch 20 loss 0.456998735666275\n",
      "Test: epoch 50 batch 0 loss 0.39374566078186035\n",
      "epoch 50 finished - avarage train loss 0.49790426266604454  avarage test loss 0.44831323623657227\n",
      "Training: epoch 51 batch 0 loss 0.724966824054718\n",
      "Training: epoch 51 batch 10 loss 0.5838973522186279\n",
      "Training: epoch 51 batch 20 loss 0.5144730806350708\n",
      "Test: epoch 51 batch 0 loss 0.3961854577064514\n",
      "epoch 51 finished - avarage train loss 0.5035435207958879  avarage test loss 0.4428494870662689\n",
      "Training: epoch 52 batch 0 loss 0.4559788703918457\n",
      "Training: epoch 52 batch 10 loss 0.44924384355545044\n",
      "Training: epoch 52 batch 20 loss 0.5627045631408691\n",
      "Test: epoch 52 batch 0 loss 0.3960722088813782\n",
      "epoch 52 finished - avarage train loss 0.5123528350016167  avarage test loss 0.4484683647751808\n",
      "Training: epoch 53 batch 0 loss 0.2755299508571625\n",
      "Training: epoch 53 batch 10 loss 0.747288703918457\n",
      "Training: epoch 53 batch 20 loss 0.28939127922058105\n",
      "Test: epoch 53 batch 0 loss 0.3922797739505768\n",
      "epoch 53 finished - avarage train loss 0.5035327035805275  avarage test loss 0.44773413240909576\n",
      "Training: epoch 54 batch 0 loss 0.501480758190155\n",
      "Training: epoch 54 batch 10 loss 0.6145180463790894\n",
      "Training: epoch 54 batch 20 loss 0.4152749478816986\n",
      "Test: epoch 54 batch 0 loss 0.39278826117515564\n",
      "epoch 54 finished - avarage train loss 0.5021685557118778  avarage test loss 0.4448264166712761\n",
      "Training: epoch 55 batch 0 loss 0.5061033368110657\n",
      "Training: epoch 55 batch 10 loss 0.5924506187438965\n",
      "Training: epoch 55 batch 20 loss 0.883073627948761\n",
      "Test: epoch 55 batch 0 loss 0.39715370535850525\n",
      "epoch 55 finished - avarage train loss 0.5142278311581447  avarage test loss 0.44955864548683167\n",
      "Training: epoch 56 batch 0 loss 0.4353867173194885\n",
      "Training: epoch 56 batch 10 loss 0.5789315104484558\n",
      "Training: epoch 56 batch 20 loss 0.38266050815582275\n",
      "Test: epoch 56 batch 0 loss 0.39765074849128723\n",
      "epoch 56 finished - avarage train loss 0.5049728571340956  avarage test loss 0.4482264146208763\n",
      "Training: epoch 57 batch 0 loss 0.7923948764801025\n",
      "Training: epoch 57 batch 10 loss 0.4004919230937958\n",
      "Training: epoch 57 batch 20 loss 0.5700689554214478\n",
      "Test: epoch 57 batch 0 loss 0.3973068296909332\n",
      "epoch 57 finished - avarage train loss 0.5244599539658119  avarage test loss 0.4477028101682663\n",
      "Training: epoch 58 batch 0 loss 0.49347320199012756\n",
      "Training: epoch 58 batch 10 loss 0.7014888525009155\n",
      "Training: epoch 58 batch 20 loss 0.49033698439598083\n",
      "Test: epoch 58 batch 0 loss 0.39623546600341797\n",
      "epoch 58 finished - avarage train loss 0.507320839783241  avarage test loss 0.4454992190003395\n",
      "Training: epoch 59 batch 0 loss 0.709846019744873\n",
      "Training: epoch 59 batch 10 loss 0.5423958897590637\n",
      "Training: epoch 59 batch 20 loss 0.2733480632305145\n",
      "Test: epoch 59 batch 0 loss 0.39557960629463196\n",
      "epoch 59 finished - avarage train loss 0.5017525152913456  avarage test loss 0.44515229761600494\n",
      "Training: epoch 60 batch 0 loss 0.27254757285118103\n",
      "Training: epoch 60 batch 10 loss 0.33214473724365234\n",
      "Training: epoch 60 batch 20 loss 0.5873497128486633\n",
      "Test: epoch 60 batch 0 loss 0.39927399158477783\n",
      "epoch 60 finished - avarage train loss 0.513954353743586  avarage test loss 0.4490717500448227\n",
      "Training: epoch 61 batch 0 loss 0.686909019947052\n",
      "Training: epoch 61 batch 10 loss 0.3151127099990845\n",
      "Training: epoch 61 batch 20 loss 0.30273428559303284\n",
      "Test: epoch 61 batch 0 loss 0.40020522475242615\n",
      "epoch 61 finished - avarage train loss 0.510088594823048  avarage test loss 0.45012545585632324\n",
      "Training: epoch 62 batch 0 loss 0.5425205230712891\n",
      "Training: epoch 62 batch 10 loss 0.7153984308242798\n",
      "Training: epoch 62 batch 20 loss 0.5221019387245178\n",
      "Test: epoch 62 batch 0 loss 0.3963417410850525\n",
      "epoch 62 finished - avarage train loss 0.5087677158158401  avarage test loss 0.448952779173851\n",
      "Training: epoch 63 batch 0 loss 0.6989274024963379\n",
      "Training: epoch 63 batch 10 loss 0.4808449447154999\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training: epoch 63 batch 20 loss 0.5899022817611694\n",
      "Test: epoch 63 batch 0 loss 0.400950163602829\n",
      "epoch 63 finished - avarage train loss 0.5336034410986407  avarage test loss 0.4508851170539856\n",
      "Training: epoch 64 batch 0 loss 0.7667462229728699\n",
      "Training: epoch 64 batch 10 loss 0.4109950363636017\n",
      "Training: epoch 64 batch 20 loss 0.5305867791175842\n",
      "Test: epoch 64 batch 0 loss 0.3998175859451294\n",
      "epoch 64 finished - avarage train loss 0.507266981848355  avarage test loss 0.4504651874303818\n",
      "Training: epoch 65 batch 0 loss 0.338954895734787\n",
      "Training: epoch 65 batch 10 loss 0.7580592632293701\n",
      "Training: epoch 65 batch 20 loss 0.4639340043067932\n",
      "Test: epoch 65 batch 0 loss 0.39705890417099\n",
      "epoch 65 finished - avarage train loss 0.5392810116554129  avarage test loss 0.4474394991993904\n",
      "Training: epoch 66 batch 0 loss 0.3397773504257202\n",
      "Training: epoch 66 batch 10 loss 0.6447072625160217\n",
      "Training: epoch 66 batch 20 loss 0.36721810698509216\n",
      "Test: epoch 66 batch 0 loss 0.40231332182884216\n",
      "epoch 66 finished - avarage train loss 0.5132232668070957  avarage test loss 0.45285336673259735\n",
      "Training: epoch 67 batch 0 loss 0.6986615061759949\n",
      "Training: epoch 67 batch 10 loss 0.5062314867973328\n",
      "Training: epoch 67 batch 20 loss 0.6915764212608337\n",
      "Test: epoch 67 batch 0 loss 0.398867130279541\n",
      "epoch 67 finished - avarage train loss 0.5061371018146646  avarage test loss 0.4482210651040077\n",
      "Training: epoch 68 batch 0 loss 0.39988061785697937\n",
      "Training: epoch 68 batch 10 loss 0.5996605157852173\n",
      "Training: epoch 68 batch 20 loss 0.7553572654724121\n",
      "Test: epoch 68 batch 0 loss 0.3975248634815216\n",
      "epoch 68 finished - avarage train loss 0.5027196705341339  avarage test loss 0.44751760363578796\n",
      "Training: epoch 69 batch 0 loss 0.45380356907844543\n",
      "Training: epoch 69 batch 10 loss 0.43942153453826904\n",
      "Training: epoch 69 batch 20 loss 0.5689613819122314\n",
      "Test: epoch 69 batch 0 loss 0.39917701482772827\n",
      "epoch 69 finished - avarage train loss 0.5028570294380188  avarage test loss 0.4508530870079994\n",
      "Training: epoch 70 batch 0 loss 0.4376600682735443\n",
      "Training: epoch 70 batch 10 loss 0.3606731593608856\n",
      "Training: epoch 70 batch 20 loss 0.5233481526374817\n",
      "Test: epoch 70 batch 0 loss 0.39672890305519104\n",
      "epoch 70 finished - avarage train loss 0.49892918816928206  avarage test loss 0.4502708837389946\n",
      "Training: epoch 71 batch 0 loss 0.6658130288124084\n",
      "Training: epoch 71 batch 10 loss 0.3815523684024811\n",
      "Training: epoch 71 batch 20 loss 0.4545944035053253\n",
      "Test: epoch 71 batch 0 loss 0.3975840210914612\n",
      "epoch 71 finished - avarage train loss 0.5127514364390537  avarage test loss 0.4473852068185806\n",
      "Training: epoch 72 batch 0 loss 0.7097461223602295\n",
      "Training: epoch 72 batch 10 loss 0.5010620951652527\n",
      "Training: epoch 72 batch 20 loss 0.3606850504875183\n",
      "Test: epoch 72 batch 0 loss 0.396429181098938\n",
      "epoch 72 finished - avarage train loss 0.5134534435025577  avarage test loss 0.4485550671815872\n",
      "Training: epoch 73 batch 0 loss 0.44141653180122375\n",
      "Training: epoch 73 batch 10 loss 0.49866145849227905\n",
      "Training: epoch 73 batch 20 loss 0.5934892296791077\n",
      "Test: epoch 73 batch 0 loss 0.399435818195343\n",
      "epoch 73 finished - avarage train loss 0.5044437461885912  avarage test loss 0.44887325167655945\n",
      "Training: epoch 74 batch 0 loss 0.4143485128879547\n",
      "Training: epoch 74 batch 10 loss 0.42128700017929077\n",
      "Training: epoch 74 batch 20 loss 0.6171104907989502\n",
      "Test: epoch 74 batch 0 loss 0.3958583474159241\n",
      "epoch 74 finished - avarage train loss 0.5096886774589275  avarage test loss 0.44741666316986084\n",
      "Training: epoch 75 batch 0 loss 0.5657989978790283\n",
      "Training: epoch 75 batch 10 loss 0.538938581943512\n",
      "Training: epoch 75 batch 20 loss 0.43121981620788574\n",
      "Test: epoch 75 batch 0 loss 0.39424315094947815\n",
      "epoch 75 finished - avarage train loss 0.5422686276764705  avarage test loss 0.4489915519952774\n",
      "Training: epoch 76 batch 0 loss 0.5419129729270935\n",
      "Training: epoch 76 batch 10 loss 0.4469642639160156\n",
      "Training: epoch 76 batch 20 loss 0.5005557537078857\n",
      "Test: epoch 76 batch 0 loss 0.4029256999492645\n",
      "epoch 76 finished - avarage train loss 0.5034739447051081  avarage test loss 0.45287342369556427\n",
      "Training: epoch 77 batch 0 loss 0.30502548813819885\n",
      "Training: epoch 77 batch 10 loss 0.507664680480957\n",
      "Training: epoch 77 batch 20 loss 0.4881995916366577\n",
      "Test: epoch 77 batch 0 loss 0.3954398036003113\n",
      "epoch 77 finished - avarage train loss 0.5155569982939753  avarage test loss 0.44834674894809723\n",
      "Training: epoch 78 batch 0 loss 0.48494380712509155\n",
      "Training: epoch 78 batch 10 loss 0.4477459788322449\n",
      "Training: epoch 78 batch 20 loss 0.4985048174858093\n",
      "Test: epoch 78 batch 0 loss 0.3996495306491852\n",
      "epoch 78 finished - avarage train loss 0.5028014234427748  avarage test loss 0.44961460679769516\n",
      "Training: epoch 79 batch 0 loss 0.3732273280620575\n",
      "Training: epoch 79 batch 10 loss 0.33318063616752625\n",
      "Training: epoch 79 batch 20 loss 0.9571780562400818\n",
      "Test: epoch 79 batch 0 loss 0.39928048849105835\n",
      "epoch 79 finished - avarage train loss 0.5218100989687031  avarage test loss 0.448564276099205\n",
      "Training: epoch 80 batch 0 loss 0.47397908568382263\n",
      "Training: epoch 80 batch 10 loss 0.4843415915966034\n",
      "Training: epoch 80 batch 20 loss 0.6668498516082764\n",
      "Test: epoch 80 batch 0 loss 0.3957567811012268\n",
      "epoch 80 finished - avarage train loss 0.49679550631292935  avarage test loss 0.4479164853692055\n",
      "Training: epoch 81 batch 0 loss 0.5208508372306824\n",
      "Training: epoch 81 batch 10 loss 0.5343166589736938\n",
      "Training: epoch 81 batch 20 loss 0.42393627762794495\n",
      "Test: epoch 81 batch 0 loss 0.3968654274940491\n",
      "epoch 81 finished - avarage train loss 0.504905751553075  avarage test loss 0.44875532388687134\n",
      "Training: epoch 82 batch 0 loss 0.38597428798675537\n",
      "Training: epoch 82 batch 10 loss 0.4766372740268707\n",
      "Training: epoch 82 batch 20 loss 0.48641180992126465\n",
      "Test: epoch 82 batch 0 loss 0.40010249614715576\n",
      "epoch 82 finished - avarage train loss 0.5010295762070294  avarage test loss 0.44944679737091064\n",
      "Training: epoch 83 batch 0 loss 0.2808521091938019\n",
      "Training: epoch 83 batch 10 loss 0.45509105920791626\n",
      "Training: epoch 83 batch 20 loss 0.7105389833450317\n",
      "Test: epoch 83 batch 0 loss 0.40079882740974426\n",
      "epoch 83 finished - avarage train loss 0.5122130780384458  avarage test loss 0.4510468989610672\n",
      "Training: epoch 84 batch 0 loss 0.5223838090896606\n",
      "Training: epoch 84 batch 10 loss 0.5198293328285217\n",
      "Training: epoch 84 batch 20 loss 0.4060137867927551\n",
      "Test: epoch 84 batch 0 loss 0.3972492218017578\n",
      "epoch 84 finished - avarage train loss 0.5000536626782911  avarage test loss 0.44737663865089417\n",
      "Training: epoch 85 batch 0 loss 0.37036368250846863\n",
      "Training: epoch 85 batch 10 loss 0.49769410490989685\n",
      "Training: epoch 85 batch 20 loss 0.507391095161438\n",
      "Test: epoch 85 batch 0 loss 0.3971481919288635\n",
      "epoch 85 finished - avarage train loss 0.4954758070666215  avarage test loss 0.4497140869498253\n",
      "Training: epoch 86 batch 0 loss 0.5867327451705933\n",
      "Training: epoch 86 batch 10 loss 0.445671945810318\n",
      "Training: epoch 86 batch 20 loss 0.5977002382278442\n",
      "Test: epoch 86 batch 0 loss 0.3953944444656372\n",
      "epoch 86 finished - avarage train loss 0.5161979794502258  avarage test loss 0.44794658571481705\n",
      "Training: epoch 87 batch 0 loss 0.3374803364276886\n",
      "Training: epoch 87 batch 10 loss 0.6316657662391663\n",
      "Training: epoch 87 batch 20 loss 0.6136978268623352\n",
      "Test: epoch 87 batch 0 loss 0.4068681299686432\n",
      "epoch 87 finished - avarage train loss 0.5509936902029761  avarage test loss 0.4535748064517975\n",
      "Training: epoch 88 batch 0 loss 0.3531574308872223\n",
      "Training: epoch 88 batch 10 loss 0.8553243279457092\n",
      "Training: epoch 88 batch 20 loss 0.618218719959259\n",
      "Test: epoch 88 batch 0 loss 0.40942975878715515\n",
      "epoch 88 finished - avarage train loss 0.5199137484205181  avarage test loss 0.4540431275963783\n",
      "Training: epoch 89 batch 0 loss 0.3927295506000519\n",
      "Training: epoch 89 batch 10 loss 0.6873739957809448\n",
      "Training: epoch 89 batch 20 loss 0.6208498477935791\n",
      "Test: epoch 89 batch 0 loss 0.39289116859436035\n",
      "epoch 89 finished - avarage train loss 0.4969186695485279  avarage test loss 0.4468894973397255\n",
      "Training: epoch 90 batch 0 loss 0.49016010761260986\n",
      "Training: epoch 90 batch 10 loss 0.5415351986885071\n",
      "Training: epoch 90 batch 20 loss 0.6590480208396912\n",
      "Test: epoch 90 batch 0 loss 0.39452213048934937\n",
      "epoch 90 finished - avarage train loss 0.5014235736994908  avarage test loss 0.4469531327486038\n",
      "Training: epoch 91 batch 0 loss 0.4644719064235687\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training: epoch 91 batch 10 loss 0.6057445406913757\n",
      "Training: epoch 91 batch 20 loss 1.038316011428833\n",
      "Test: epoch 91 batch 0 loss 0.3982800543308258\n",
      "epoch 91 finished - avarage train loss 0.534874926353323  avarage test loss 0.44918093085289\n",
      "Training: epoch 92 batch 0 loss 0.5494359731674194\n",
      "Training: epoch 92 batch 10 loss 0.42357754707336426\n",
      "Training: epoch 92 batch 20 loss 0.5121806263923645\n",
      "Test: epoch 92 batch 0 loss 0.40639016032218933\n",
      "epoch 92 finished - avarage train loss 0.523958288390061  avarage test loss 0.45635030418634415\n",
      "Training: epoch 93 batch 0 loss 0.5659483671188354\n",
      "Training: epoch 93 batch 10 loss 0.6324973702430725\n",
      "Training: epoch 93 batch 20 loss 0.5338762998580933\n",
      "Test: epoch 93 batch 0 loss 0.4044606387615204\n",
      "epoch 93 finished - avarage train loss 0.5143651787577004  avarage test loss 0.4483758769929409\n",
      "Training: epoch 94 batch 0 loss 0.3715131878852844\n",
      "Training: epoch 94 batch 10 loss 0.3497161269187927\n",
      "Training: epoch 94 batch 20 loss 0.4085228145122528\n",
      "Test: epoch 94 batch 0 loss 0.39263808727264404\n",
      "epoch 94 finished - avarage train loss 0.5033160725544239  avarage test loss 0.4451684355735779\n",
      "Training: epoch 95 batch 0 loss 0.49645283818244934\n",
      "Training: epoch 95 batch 10 loss 0.41910219192504883\n",
      "Training: epoch 95 batch 20 loss 0.38702914118766785\n",
      "Test: epoch 95 batch 0 loss 0.39792516827583313\n",
      "epoch 95 finished - avarage train loss 0.5285257769042048  avarage test loss 0.4490774944424629\n",
      "Training: epoch 96 batch 0 loss 0.5413707494735718\n",
      "Training: epoch 96 batch 10 loss 0.48349905014038086\n",
      "Training: epoch 96 batch 20 loss 0.6020517945289612\n",
      "Test: epoch 96 batch 0 loss 0.3982163369655609\n",
      "epoch 96 finished - avarage train loss 0.5030022397123534  avarage test loss 0.44755442440509796\n",
      "Training: epoch 97 batch 0 loss 0.3644435703754425\n",
      "Training: epoch 97 batch 10 loss 0.40219026803970337\n",
      "Training: epoch 97 batch 20 loss 0.4301423132419586\n",
      "Test: epoch 97 batch 0 loss 0.3943532109260559\n",
      "epoch 97 finished - avarage train loss 0.5036632953018978  avarage test loss 0.4441292881965637\n",
      "Training: epoch 98 batch 0 loss 0.6275063753128052\n",
      "Training: epoch 98 batch 10 loss 0.3697085976600647\n",
      "Training: epoch 98 batch 20 loss 0.6545170545578003\n",
      "Test: epoch 98 batch 0 loss 0.38579583168029785\n",
      "epoch 98 finished - avarage train loss 0.5015140473842621  avarage test loss 0.4398520588874817\n",
      "Training: epoch 99 batch 0 loss 0.9874337315559387\n",
      "Training: epoch 99 batch 10 loss 0.46066832542419434\n",
      "Training: epoch 99 batch 20 loss 0.3132280111312866\n",
      "Test: epoch 99 batch 0 loss 0.1325751096010208\n",
      "epoch 99 finished - avarage train loss 0.41253194469830085  avarage test loss 0.17078393325209618\n",
      "Training: epoch 100 batch 0 loss 0.1721869707107544\n",
      "Training: epoch 100 batch 10 loss 0.33624520897865295\n",
      "Training: epoch 100 batch 20 loss 0.11659947782754898\n",
      "Test: epoch 100 batch 0 loss 0.10291536897420883\n",
      "epoch 100 finished - avarage train loss 0.16101309297413663  avarage test loss 0.10742985270917416\n",
      "Training: epoch 101 batch 0 loss 0.11896879971027374\n",
      "Training: epoch 101 batch 10 loss 0.10000567138195038\n",
      "Training: epoch 101 batch 20 loss 0.03244060277938843\n",
      "Test: epoch 101 batch 0 loss 0.04122116416692734\n",
      "epoch 101 finished - avarage train loss 0.056681046326612604  avarage test loss 0.048972141928970814\n",
      "Training: epoch 102 batch 0 loss 0.030059820041060448\n",
      "Training: epoch 102 batch 10 loss 0.01942262053489685\n",
      "Training: epoch 102 batch 20 loss 0.03244703263044357\n",
      "Test: epoch 102 batch 0 loss 0.032047417014837265\n",
      "epoch 102 finished - avarage train loss 0.027475930300766026  avarage test loss 0.03879036009311676\n",
      "Training: epoch 103 batch 0 loss 0.02433924749493599\n",
      "Training: epoch 103 batch 10 loss 0.03397354111075401\n",
      "Training: epoch 103 batch 20 loss 0.016308413818478584\n",
      "Test: epoch 103 batch 0 loss 0.02828485146164894\n",
      "epoch 103 finished - avarage train loss 0.025222716082272858  avarage test loss 0.03600616613402963\n",
      "Training: epoch 104 batch 0 loss 0.02145528607070446\n",
      "Training: epoch 104 batch 10 loss 0.020852629095315933\n",
      "Training: epoch 104 batch 20 loss 0.02630290575325489\n",
      "Test: epoch 104 batch 0 loss 0.027876028791069984\n",
      "epoch 104 finished - avarage train loss 0.023165282921801353  avarage test loss 0.03576122643426061\n",
      "Training: epoch 105 batch 0 loss 0.02036946639418602\n",
      "Training: epoch 105 batch 10 loss 0.01788225956261158\n",
      "Training: epoch 105 batch 20 loss 0.014419031329452991\n",
      "Test: epoch 105 batch 0 loss 0.011936728842556477\n",
      "epoch 105 finished - avarage train loss 0.021700612911633377  avarage test loss 0.014235959853976965\n",
      "Training: epoch 106 batch 0 loss 0.0072597935795784\n",
      "Training: epoch 106 batch 10 loss 0.011763082817196846\n",
      "Training: epoch 106 batch 20 loss 0.004907386377453804\n",
      "Test: epoch 106 batch 0 loss 0.0161347147077322\n",
      "epoch 106 finished - avarage train loss 0.012837876983243844  avarage test loss 0.01758459850680083\n",
      "Training: epoch 107 batch 0 loss 0.008652429096400738\n",
      "Training: epoch 107 batch 10 loss 0.009121434763073921\n",
      "Training: epoch 107 batch 20 loss 0.00912789348512888\n",
      "Test: epoch 107 batch 0 loss 0.015974801033735275\n",
      "epoch 107 finished - avarage train loss 0.009354916666152662  avarage test loss 0.01638578320853412\n",
      "Training: epoch 108 batch 0 loss 0.004663371946662664\n",
      "Training: epoch 108 batch 10 loss 0.006797816604375839\n",
      "Training: epoch 108 batch 20 loss 0.022125886753201485\n",
      "Test: epoch 108 batch 0 loss 0.01679633930325508\n",
      "epoch 108 finished - avarage train loss 0.012477244812481362  avarage test loss 0.0174266395624727\n",
      "Training: epoch 109 batch 0 loss 0.00538316136226058\n",
      "Training: epoch 109 batch 10 loss 0.013556724414229393\n",
      "Training: epoch 109 batch 20 loss 0.014670097269117832\n",
      "Test: epoch 109 batch 0 loss 0.014621905982494354\n",
      "epoch 109 finished - avarage train loss 0.012773432399563748  avarage test loss 0.015526870964094996\n",
      "Training: epoch 110 batch 0 loss 0.00704098679125309\n",
      "Training: epoch 110 batch 10 loss 0.004140687175095081\n",
      "Training: epoch 110 batch 20 loss 0.007726581767201424\n",
      "Test: epoch 110 batch 0 loss 0.014773709699511528\n",
      "epoch 110 finished - avarage train loss 0.008276186385673696  avarage test loss 0.01773609104566276\n",
      "Training: epoch 111 batch 0 loss 0.013233592733740807\n",
      "Training: epoch 111 batch 10 loss 0.007045784033834934\n",
      "Training: epoch 111 batch 20 loss 0.008594581857323647\n",
      "Test: epoch 111 batch 0 loss 0.017520900815725327\n",
      "epoch 111 finished - avarage train loss 0.009081830270588398  avarage test loss 0.018909984035417438\n",
      "Training: epoch 112 batch 0 loss 0.01025219727307558\n",
      "Training: epoch 112 batch 10 loss 0.009211515076458454\n",
      "Training: epoch 112 batch 20 loss 0.006873247213661671\n",
      "Test: epoch 112 batch 0 loss 0.013374857604503632\n",
      "epoch 112 finished - avarage train loss 0.009031196156966275  avarage test loss 0.014075477025471628\n",
      "Training: epoch 113 batch 0 loss 0.0074236211366951466\n",
      "Training: epoch 113 batch 10 loss 0.00904714036732912\n",
      "Training: epoch 113 batch 20 loss 0.006127547938376665\n",
      "Test: epoch 113 batch 0 loss 0.014730769209563732\n",
      "epoch 113 finished - avarage train loss 0.007840809741474945  avarage test loss 0.015711918706074357\n",
      "Training: epoch 114 batch 0 loss 0.004173855762928724\n",
      "Training: epoch 114 batch 10 loss 0.008937496691942215\n",
      "Training: epoch 114 batch 20 loss 0.0052820672281086445\n",
      "Test: epoch 114 batch 0 loss 0.015416711568832397\n",
      "epoch 114 finished - avarage train loss 0.006944178426959391  avarage test loss 0.01562552247196436\n",
      "Training: epoch 115 batch 0 loss 0.005566480569541454\n",
      "Training: epoch 115 batch 10 loss 0.00524227786809206\n",
      "Training: epoch 115 batch 20 loss 0.004229696001857519\n",
      "Test: epoch 115 batch 0 loss 0.014256814494729042\n",
      "epoch 115 finished - avarage train loss 0.0074849538906509505  avarage test loss 0.015752590727061033\n",
      "Training: epoch 116 batch 0 loss 0.008355158381164074\n",
      "Training: epoch 116 batch 10 loss 0.0040242476388812065\n",
      "Training: epoch 116 batch 20 loss 0.0069627040065824986\n",
      "Test: epoch 116 batch 0 loss 0.01228294800966978\n",
      "epoch 116 finished - avarage train loss 0.008730443202537196  avarage test loss 0.014792930101975799\n",
      "Training: epoch 117 batch 0 loss 0.0038468907587230206\n",
      "Training: epoch 117 batch 10 loss 0.011039670556783676\n",
      "Training: epoch 117 batch 20 loss 0.004504490178078413\n",
      "Test: epoch 117 batch 0 loss 0.014636998064815998\n",
      "epoch 117 finished - avarage train loss 0.01307155506621147  avarage test loss 0.017098558833822608\n",
      "Training: epoch 118 batch 0 loss 0.00888022594153881\n",
      "Training: epoch 118 batch 10 loss 0.013169605284929276\n",
      "Training: epoch 118 batch 20 loss 0.005194253288209438\n",
      "Test: epoch 118 batch 0 loss 0.011747618205845356\n",
      "epoch 118 finished - avarage train loss 0.01041749387112414  avarage test loss 0.013424039934761822\n",
      "Training: epoch 119 batch 0 loss 0.007251358591020107\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training: epoch 119 batch 10 loss 0.006256473250687122\n",
      "Training: epoch 119 batch 20 loss 0.01165829785168171\n",
      "Test: epoch 119 batch 0 loss 0.01285011600703001\n",
      "epoch 119 finished - avarage train loss 0.01091978450080958  avarage test loss 0.013905271422117949\n",
      "Training: epoch 120 batch 0 loss 0.006086792331188917\n",
      "Training: epoch 120 batch 10 loss 0.0043687159195542336\n",
      "Training: epoch 120 batch 20 loss 0.005551412235945463\n",
      "Test: epoch 120 batch 0 loss 0.011881190352141857\n",
      "epoch 120 finished - avarage train loss 0.007502396382262994  avarage test loss 0.013260055566206574\n",
      "Training: epoch 121 batch 0 loss 0.0036316318437457085\n",
      "Training: epoch 121 batch 10 loss 0.0049842242151498795\n",
      "Training: epoch 121 batch 20 loss 0.007057896815240383\n",
      "Test: epoch 121 batch 0 loss 0.010538292117416859\n",
      "epoch 121 finished - avarage train loss 0.007711930288772644  avarage test loss 0.013424572069197893\n",
      "Training: epoch 122 batch 0 loss 0.009674626402556896\n",
      "Training: epoch 122 batch 10 loss 0.00864182785153389\n",
      "Training: epoch 122 batch 20 loss 0.006106286309659481\n",
      "Test: epoch 122 batch 0 loss 0.013486713171005249\n",
      "epoch 122 finished - avarage train loss 0.008690180746709993  avarage test loss 0.015284985536709428\n",
      "Training: epoch 123 batch 0 loss 0.005076169967651367\n",
      "Training: epoch 123 batch 10 loss 0.005191459320485592\n",
      "Training: epoch 123 batch 20 loss 0.004289706237614155\n",
      "Test: epoch 123 batch 0 loss 0.012314608320593834\n",
      "epoch 123 finished - avarage train loss 0.007372520691394035  avarage test loss 0.013987090555019677\n",
      "Training: epoch 124 batch 0 loss 0.006094237323850393\n",
      "Training: epoch 124 batch 10 loss 0.007578114978969097\n",
      "Training: epoch 124 batch 20 loss 0.009780610911548138\n",
      "Test: epoch 124 batch 0 loss 0.010913084261119366\n",
      "epoch 124 finished - avarage train loss 0.006638188123831461  avarage test loss 0.0125472272047773\n",
      "Training: epoch 125 batch 0 loss 0.005684779025614262\n",
      "Training: epoch 125 batch 10 loss 0.0036546129267662764\n",
      "Training: epoch 125 batch 20 loss 0.002779655857011676\n",
      "Test: epoch 125 batch 0 loss 0.011429817415773869\n",
      "epoch 125 finished - avarage train loss 0.006396285844324478  avarage test loss 0.01267830329015851\n",
      "Training: epoch 126 batch 0 loss 0.0023684920743107796\n",
      "Training: epoch 126 batch 10 loss 0.006987549364566803\n",
      "Training: epoch 126 batch 20 loss 0.004734588321298361\n",
      "Test: epoch 126 batch 0 loss 0.011533048003911972\n",
      "epoch 126 finished - avarage train loss 0.006549551649468726  avarage test loss 0.012783841928467155\n",
      "Training: epoch 127 batch 0 loss 0.00452518230304122\n",
      "Training: epoch 127 batch 10 loss 0.009118217043578625\n",
      "Training: epoch 127 batch 20 loss 0.006396752782166004\n",
      "Test: epoch 127 batch 0 loss 0.013342455960810184\n",
      "epoch 127 finished - avarage train loss 0.007830709724783384  avarage test loss 0.013865559478290379\n",
      "Training: epoch 128 batch 0 loss 0.0050242384895682335\n",
      "Training: epoch 128 batch 10 loss 0.004506386816501617\n",
      "Training: epoch 128 batch 20 loss 0.00738905044272542\n",
      "Test: epoch 128 batch 0 loss 0.013512167148292065\n",
      "epoch 128 finished - avarage train loss 0.007247168803587556  avarage test loss 0.014681642292998731\n",
      "Training: epoch 129 batch 0 loss 0.006977866403758526\n",
      "Training: epoch 129 batch 10 loss 0.0062751914374530315\n",
      "Training: epoch 129 batch 20 loss 0.00471936771646142\n",
      "Test: epoch 129 batch 0 loss 0.014661070890724659\n",
      "epoch 129 finished - avarage train loss 0.0068447886773481455  avarage test loss 0.016091306693851948\n",
      "Training: epoch 130 batch 0 loss 0.006849991157650948\n",
      "Training: epoch 130 batch 10 loss 0.006031033582985401\n",
      "Training: epoch 130 batch 20 loss 0.007560931146144867\n",
      "Test: epoch 130 batch 0 loss 0.014444789849221706\n",
      "epoch 130 finished - avarage train loss 0.007903343765065074  avarage test loss 0.015604006825014949\n",
      "Training: epoch 131 batch 0 loss 0.005465954542160034\n",
      "Training: epoch 131 batch 10 loss 0.0029040814843028784\n",
      "Training: epoch 131 batch 20 loss 0.0029226033948361874\n",
      "Test: epoch 131 batch 0 loss 0.012964415363967419\n",
      "epoch 131 finished - avarage train loss 0.006872590848019925  avarage test loss 0.013930602930486202\n",
      "Training: epoch 132 batch 0 loss 0.005329219624400139\n",
      "Training: epoch 132 batch 10 loss 0.0036453527864068747\n",
      "Training: epoch 132 batch 20 loss 0.003276598174124956\n",
      "Test: epoch 132 batch 0 loss 0.009319190867245197\n",
      "epoch 132 finished - avarage train loss 0.008789814884196324  avarage test loss 0.014317700522951782\n",
      "Training: epoch 133 batch 0 loss 0.006256296299397945\n",
      "Training: epoch 133 batch 10 loss 0.006213902961462736\n",
      "Training: epoch 133 batch 20 loss 0.009305768646299839\n",
      "Test: epoch 133 batch 0 loss 0.011701386421918869\n",
      "epoch 133 finished - avarage train loss 0.009077275183918918  avarage test loss 0.013308477587997913\n",
      "Training: epoch 134 batch 0 loss 0.007535742130130529\n",
      "Training: epoch 134 batch 10 loss 0.00424126535654068\n",
      "Training: epoch 134 batch 20 loss 0.0033408738672733307\n",
      "Test: epoch 134 batch 0 loss 0.012287594377994537\n",
      "epoch 134 finished - avarage train loss 0.007301401611867136  avarage test loss 0.013847758527845144\n",
      "Training: epoch 135 batch 0 loss 0.005930385086685419\n",
      "Training: epoch 135 batch 10 loss 0.0024141077883541584\n",
      "Training: epoch 135 batch 20 loss 0.011457966640591621\n",
      "Test: epoch 135 batch 0 loss 0.014703615568578243\n",
      "epoch 135 finished - avarage train loss 0.008410187157127878  avarage test loss 0.015869149938225746\n",
      "Training: epoch 136 batch 0 loss 0.008002828806638718\n",
      "Training: epoch 136 batch 10 loss 0.005400644149631262\n",
      "Training: epoch 136 batch 20 loss 0.009585057385265827\n",
      "Test: epoch 136 batch 0 loss 0.01389218308031559\n",
      "epoch 136 finished - avarage train loss 0.008723155884393331  avarage test loss 0.01532279362436384\n",
      "Training: epoch 137 batch 0 loss 0.005297721363604069\n",
      "Training: epoch 137 batch 10 loss 0.006014951504766941\n",
      "Training: epoch 137 batch 20 loss 0.0047640991397202015\n",
      "Test: epoch 137 batch 0 loss 0.010130355134606361\n",
      "epoch 137 finished - avarage train loss 0.00898073362347124  avarage test loss 0.013284449349157512\n",
      "Training: epoch 138 batch 0 loss 0.0059827170334756374\n",
      "Training: epoch 138 batch 10 loss 0.005820033140480518\n",
      "Training: epoch 138 batch 20 loss 0.004911447409540415\n",
      "Test: epoch 138 batch 0 loss 0.011739175766706467\n",
      "epoch 138 finished - avarage train loss 0.008432107407535458  avarage test loss 0.012707113870419562\n",
      "Training: epoch 139 batch 0 loss 0.006008336320519447\n",
      "Training: epoch 139 batch 10 loss 0.006060154642909765\n",
      "Training: epoch 139 batch 20 loss 0.007837441749870777\n",
      "Test: epoch 139 batch 0 loss 0.010893354192376137\n",
      "epoch 139 finished - avarage train loss 0.0070848622327220855  avarage test loss 0.012640154454857111\n",
      "Training: epoch 140 batch 0 loss 0.004174475558102131\n",
      "Training: epoch 140 batch 10 loss 0.003891073865815997\n",
      "Training: epoch 140 batch 20 loss 0.007214309647679329\n",
      "Test: epoch 140 batch 0 loss 0.013932260684669018\n",
      "epoch 140 finished - avarage train loss 0.007199111996732396  avarage test loss 0.01587949343957007\n",
      "Training: epoch 141 batch 0 loss 0.005150377284735441\n",
      "Training: epoch 141 batch 10 loss 0.006510972045361996\n",
      "Training: epoch 141 batch 20 loss 0.008868384175002575\n",
      "Test: epoch 141 batch 0 loss 0.017665553838014603\n",
      "epoch 141 finished - avarage train loss 0.007468439807216155  avarage test loss 0.01995348115451634\n",
      "Training: epoch 142 batch 0 loss 0.009532975032925606\n",
      "Training: epoch 142 batch 10 loss 0.004566752351820469\n",
      "Training: epoch 142 batch 20 loss 0.005149252712726593\n",
      "Test: epoch 142 batch 0 loss 0.01054123230278492\n",
      "epoch 142 finished - avarage train loss 0.010984419925331041  avarage test loss 0.01247798337135464\n",
      "Training: epoch 143 batch 0 loss 0.004387786611914635\n",
      "Training: epoch 143 batch 10 loss 0.006160626653581858\n",
      "Training: epoch 143 batch 20 loss 0.008710703812539577\n",
      "Test: epoch 143 batch 0 loss 0.01321421004831791\n",
      "epoch 143 finished - avarage train loss 0.006587364566351833  avarage test loss 0.0149930021725595\n",
      "Training: epoch 144 batch 0 loss 0.006840141024440527\n",
      "Training: epoch 144 batch 10 loss 0.0036317710764706135\n",
      "Training: epoch 144 batch 20 loss 0.010749590583145618\n",
      "Test: epoch 144 batch 0 loss 0.012863784097135067\n",
      "epoch 144 finished - avarage train loss 0.0066987681764595465  avarage test loss 0.014317323919385672\n",
      "Training: epoch 145 batch 0 loss 0.006010851822793484\n",
      "Training: epoch 145 batch 10 loss 0.0059132762253284454\n",
      "Training: epoch 145 batch 20 loss 0.008169792592525482\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test: epoch 145 batch 0 loss 0.014198877848684788\n",
      "epoch 145 finished - avarage train loss 0.00801772822561706  avarage test loss 0.016093770740553737\n",
      "Training: epoch 146 batch 0 loss 0.0024142831098288298\n",
      "Training: epoch 146 batch 10 loss 0.00604880740866065\n",
      "Training: epoch 146 batch 20 loss 0.004006220027804375\n",
      "Test: epoch 146 batch 0 loss 0.01271870918571949\n",
      "epoch 146 finished - avarage train loss 0.008651368107229215  avarage test loss 0.013614705647341907\n",
      "Training: epoch 147 batch 0 loss 0.005738226696848869\n",
      "Training: epoch 147 batch 10 loss 0.004605778492987156\n",
      "Training: epoch 147 batch 20 loss 0.003514629090204835\n",
      "Test: epoch 147 batch 0 loss 0.012249244377017021\n",
      "epoch 147 finished - avarage train loss 0.008742183559285155  avarage test loss 0.013043023995123804\n",
      "Training: epoch 148 batch 0 loss 0.009684442542493343\n",
      "Training: epoch 148 batch 10 loss 0.00264581385999918\n",
      "Training: epoch 148 batch 20 loss 0.004619434010237455\n",
      "Test: epoch 148 batch 0 loss 0.011963291093707085\n",
      "epoch 148 finished - avarage train loss 0.008118699898879075  avarage test loss 0.012912340112961829\n",
      "Training: epoch 149 batch 0 loss 0.004046288318932056\n",
      "Training: epoch 149 batch 10 loss 0.01291285827755928\n",
      "Training: epoch 149 batch 20 loss 0.0017416918417438865\n",
      "Test: epoch 149 batch 0 loss 0.012709313072264194\n",
      "epoch 149 finished - avarage train loss 0.007979545850660009  avarage test loss 0.013612924376502633\n",
      "Training: epoch 150 batch 0 loss 0.004635263234376907\n",
      "Training: epoch 150 batch 10 loss 0.011585722677409649\n",
      "Training: epoch 150 batch 20 loss 0.005495552904903889\n",
      "Test: epoch 150 batch 0 loss 0.012756094336509705\n",
      "epoch 150 finished - avarage train loss 0.007642034087972394  avarage test loss 0.013669796753674746\n",
      "Training: epoch 151 batch 0 loss 0.007048772647976875\n",
      "Training: epoch 151 batch 10 loss 0.0013942942023277283\n",
      "Training: epoch 151 batch 20 loss 0.004493800923228264\n",
      "Test: epoch 151 batch 0 loss 0.010562528856098652\n",
      "epoch 151 finished - avarage train loss 0.007914670116814047  avarage test loss 0.012328780023381114\n",
      "Training: epoch 152 batch 0 loss 0.007382301613688469\n",
      "Training: epoch 152 batch 10 loss 0.005587635096162558\n",
      "Training: epoch 152 batch 20 loss 0.009395538829267025\n",
      "Test: epoch 152 batch 0 loss 0.011641484685242176\n",
      "epoch 152 finished - avarage train loss 0.007835765489666113  avarage test loss 0.012635126360692084\n",
      "Training: epoch 153 batch 0 loss 0.004814808256924152\n",
      "Training: epoch 153 batch 10 loss 0.0047856164164841175\n",
      "Training: epoch 153 batch 20 loss 0.004117359407246113\n",
      "Test: epoch 153 batch 0 loss 0.01242061797529459\n",
      "epoch 153 finished - avarage train loss 0.007842190017731026  avarage test loss 0.013254532823339105\n",
      "Training: epoch 154 batch 0 loss 0.0072602275758981705\n",
      "Training: epoch 154 batch 10 loss 0.009951407089829445\n",
      "Training: epoch 154 batch 20 loss 0.004031582269817591\n",
      "Test: epoch 154 batch 0 loss 0.012922423891723156\n",
      "epoch 154 finished - avarage train loss 0.008166599769851771  avarage test loss 0.014093133737333119\n",
      "Training: epoch 155 batch 0 loss 0.012315811589360237\n",
      "Training: epoch 155 batch 10 loss 0.0033776587806642056\n",
      "Training: epoch 155 batch 20 loss 0.010435623116791248\n",
      "Test: epoch 155 batch 0 loss 0.016795391216874123\n",
      "epoch 155 finished - avarage train loss 0.008194032462378001  avarage test loss 0.018059014109894633\n",
      "Training: epoch 156 batch 0 loss 0.007507129106670618\n",
      "Training: epoch 156 batch 10 loss 0.009073331020772457\n",
      "Training: epoch 156 batch 20 loss 0.004414333030581474\n",
      "Test: epoch 156 batch 0 loss 0.013852739706635475\n",
      "epoch 156 finished - avarage train loss 0.010791726193615589  avarage test loss 0.015576687990687788\n",
      "Training: epoch 157 batch 0 loss 0.0038038103375583887\n",
      "Training: epoch 157 batch 10 loss 0.0037043937481939793\n",
      "Training: epoch 157 batch 20 loss 0.003939824644476175\n",
      "Test: epoch 157 batch 0 loss 0.01423198077827692\n",
      "epoch 157 finished - avarage train loss 0.0061212378425587865  avarage test loss 0.015626309206709266\n",
      "Training: epoch 158 batch 0 loss 0.006344310007989407\n",
      "Training: epoch 158 batch 10 loss 0.015772098675370216\n",
      "Training: epoch 158 batch 20 loss 0.004449890926480293\n",
      "Test: epoch 158 batch 0 loss 0.009984523057937622\n",
      "epoch 158 finished - avarage train loss 0.006671396461476026  avarage test loss 0.012803238583728671\n",
      "Training: epoch 159 batch 0 loss 0.016131529584527016\n",
      "Training: epoch 159 batch 10 loss 0.0028090241830796003\n",
      "Training: epoch 159 batch 20 loss 0.006525654811412096\n",
      "Test: epoch 159 batch 0 loss 0.011996766552329063\n",
      "epoch 159 finished - avarage train loss 0.008526394191872457  avarage test loss 0.013061804696917534\n",
      "Training: epoch 160 batch 0 loss 0.003397780703380704\n",
      "Training: epoch 160 batch 10 loss 0.0046212999150156975\n",
      "Training: epoch 160 batch 20 loss 0.009203379042446613\n",
      "Test: epoch 160 batch 0 loss 0.01068510115146637\n",
      "epoch 160 finished - avarage train loss 0.007180643894163699  avarage test loss 0.012202803627587855\n",
      "Training: epoch 161 batch 0 loss 0.0026997642125934362\n",
      "Training: epoch 161 batch 10 loss 0.00822329893708229\n",
      "Training: epoch 161 batch 20 loss 0.004156908951699734\n",
      "Test: epoch 161 batch 0 loss 0.010394885204732418\n",
      "epoch 161 finished - avarage train loss 0.0076838893302041906  avarage test loss 0.013538176892325282\n",
      "Training: epoch 162 batch 0 loss 0.006934081669896841\n",
      "Training: epoch 162 batch 10 loss 0.006235464476048946\n",
      "Training: epoch 162 batch 20 loss 0.0046386029571294785\n",
      "Test: epoch 162 batch 0 loss 0.010720830410718918\n",
      "epoch 162 finished - avarage train loss 0.0075820109983585  avarage test loss 0.012857362045906484\n",
      "Training: epoch 163 batch 0 loss 0.007624175865203142\n",
      "Training: epoch 163 batch 10 loss 0.004027226939797401\n",
      "Training: epoch 163 batch 20 loss 0.008187361061573029\n",
      "Test: epoch 163 batch 0 loss 0.011759834364056587\n",
      "epoch 163 finished - avarage train loss 0.0066879148241774785  avarage test loss 0.013407720136456192\n",
      "Training: epoch 164 batch 0 loss 0.0061266315169632435\n",
      "Training: epoch 164 batch 10 loss 0.005337024573236704\n",
      "Training: epoch 164 batch 20 loss 0.004704892635345459\n",
      "Test: epoch 164 batch 0 loss 0.011869916692376137\n",
      "epoch 164 finished - avarage train loss 0.00865641025151929  avarage test loss 0.013522014254704118\n",
      "Training: epoch 165 batch 0 loss 0.004458471667021513\n",
      "Training: epoch 165 batch 10 loss 0.009986981749534607\n",
      "Training: epoch 165 batch 20 loss 0.0030567622743546963\n",
      "Test: epoch 165 batch 0 loss 0.01259706262499094\n",
      "epoch 165 finished - avarage train loss 0.0067376005463302135  avarage test loss 0.014143241220153868\n",
      "Training: epoch 166 batch 0 loss 0.006765213329344988\n",
      "Training: epoch 166 batch 10 loss 0.00804482027888298\n",
      "Training: epoch 166 batch 20 loss 0.004186599515378475\n",
      "Test: epoch 166 batch 0 loss 0.011027967557311058\n",
      "epoch 166 finished - avarage train loss 0.007423993577409921  avarage test loss 0.012591673410497606\n",
      "Training: epoch 167 batch 0 loss 0.004594688303768635\n",
      "Training: epoch 167 batch 10 loss 0.003758272621780634\n",
      "Training: epoch 167 batch 20 loss 0.005495050456374884\n",
      "Test: epoch 167 batch 0 loss 0.011450965888798237\n",
      "epoch 167 finished - avarage train loss 0.00804465976624011  avarage test loss 0.012715927674435079\n",
      "Training: epoch 168 batch 0 loss 0.007759516127407551\n",
      "Training: epoch 168 batch 10 loss 0.007154651451855898\n",
      "Training: epoch 168 batch 20 loss 0.009220498614013195\n",
      "Test: epoch 168 batch 0 loss 0.01250145211815834\n",
      "epoch 168 finished - avarage train loss 0.007309653282422444  avarage test loss 0.013645767234265804\n",
      "Training: epoch 169 batch 0 loss 0.0058087920770049095\n",
      "Training: epoch 169 batch 10 loss 0.0065031107515096664\n",
      "Training: epoch 169 batch 20 loss 0.0026104559656232595\n",
      "Test: epoch 169 batch 0 loss 0.010259997099637985\n",
      "epoch 169 finished - avarage train loss 0.008027040142694423  avarage test loss 0.014199310564436018\n",
      "Training: epoch 170 batch 0 loss 0.005720245651900768\n",
      "Training: epoch 170 batch 10 loss 0.007234394084662199\n",
      "Training: epoch 170 batch 20 loss 0.004211448132991791\n",
      "Test: epoch 170 batch 0 loss 0.01214133482426405\n",
      "epoch 170 finished - avarage train loss 0.006707225500702344  avarage test loss 0.012916070991195738\n",
      "Training: epoch 171 batch 0 loss 0.005828472785651684\n",
      "Training: epoch 171 batch 10 loss 0.006230339873582125\n",
      "Training: epoch 171 batch 20 loss 0.01041797548532486\n",
      "Test: epoch 171 batch 0 loss 0.011690974235534668\n",
      "epoch 171 finished - avarage train loss 0.008038172642860946  avarage test loss 0.012681250344030559\n",
      "Training: epoch 172 batch 0 loss 0.0024912431836128235\n",
      "Training: epoch 172 batch 10 loss 0.007982008159160614\n",
      "Training: epoch 172 batch 20 loss 0.006221645511686802\n",
      "Test: epoch 172 batch 0 loss 0.012366860173642635\n",
      "epoch 172 finished - avarage train loss 0.007552508296894616  avarage test loss 0.013150657177902758\n",
      "Training: epoch 173 batch 0 loss 0.0033728540875017643\n",
      "Training: epoch 173 batch 10 loss 0.006222501862794161\n",
      "Training: epoch 173 batch 20 loss 0.006624226924031973\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test: epoch 173 batch 0 loss 0.011342991143465042\n",
      "epoch 173 finished - avarage train loss 0.006868297467008233  avarage test loss 0.012413266347721219\n",
      "Training: epoch 174 batch 0 loss 0.002427157014608383\n",
      "Training: epoch 174 batch 10 loss 0.004952105227857828\n",
      "Training: epoch 174 batch 20 loss 0.01123550534248352\n",
      "Test: epoch 174 batch 0 loss 0.013062299229204655\n",
      "epoch 174 finished - avarage train loss 0.007977354540971333  avarage test loss 0.013828422641381621\n",
      "Training: epoch 175 batch 0 loss 0.006806288845837116\n",
      "Training: epoch 175 batch 10 loss 0.007553191855549812\n",
      "Training: epoch 175 batch 20 loss 0.012478857301175594\n",
      "Test: epoch 175 batch 0 loss 0.0117549579590559\n",
      "epoch 175 finished - avarage train loss 0.007516206394688323  avarage test loss 0.01266110420692712\n",
      "Training: epoch 176 batch 0 loss 0.004093823488801718\n",
      "Training: epoch 176 batch 10 loss 0.0077722943387925625\n",
      "Training: epoch 176 batch 20 loss 0.015994831919670105\n",
      "Test: epoch 176 batch 0 loss 0.014574944972991943\n",
      "epoch 176 finished - avarage train loss 0.007773870008130525  avarage test loss 0.015465503325685859\n",
      "Training: epoch 177 batch 0 loss 0.009906205348670483\n",
      "Training: epoch 177 batch 10 loss 0.004191568121314049\n",
      "Training: epoch 177 batch 20 loss 0.005701073445379734\n",
      "Test: epoch 177 batch 0 loss 0.014433985576033592\n",
      "epoch 177 finished - avarage train loss 0.009987887492852992  avarage test loss 0.015498447697609663\n",
      "Training: epoch 178 batch 0 loss 0.006087020039558411\n",
      "Training: epoch 178 batch 10 loss 0.007263584993779659\n",
      "Training: epoch 178 batch 20 loss 0.005349573213607073\n",
      "Test: epoch 178 batch 0 loss 0.01190103404223919\n",
      "epoch 178 finished - avarage train loss 0.007226990535855293  avarage test loss 0.013077983283437788\n",
      "Training: epoch 179 batch 0 loss 0.0026420531794428825\n",
      "Training: epoch 179 batch 10 loss 0.009350087493658066\n",
      "Training: epoch 179 batch 20 loss 0.00795707106590271\n",
      "Test: epoch 179 batch 0 loss 0.01321032177656889\n",
      "epoch 179 finished - avarage train loss 0.007328426983239579  avarage test loss 0.013968328014016151\n",
      "Training: epoch 180 batch 0 loss 0.00792311504483223\n",
      "Training: epoch 180 batch 10 loss 0.005257728975266218\n",
      "Training: epoch 180 batch 20 loss 0.009098811075091362\n",
      "Test: epoch 180 batch 0 loss 0.014491063542664051\n",
      "epoch 180 finished - avarage train loss 0.007216301040145858  avarage test loss 0.015012651216238737\n",
      "Training: epoch 181 batch 0 loss 0.007534871343523264\n",
      "Training: epoch 181 batch 10 loss 0.006248220335692167\n",
      "Training: epoch 181 batch 20 loss 0.0026422275695949793\n",
      "Test: epoch 181 batch 0 loss 0.010794737376272678\n",
      "epoch 181 finished - avarage train loss 0.011952666332945228  avarage test loss 0.013138437643647194\n",
      "Training: epoch 182 batch 0 loss 0.0017545252339914441\n",
      "Training: epoch 182 batch 10 loss 0.007093334570527077\n",
      "Training: epoch 182 batch 20 loss 0.009703188203275204\n",
      "Test: epoch 182 batch 0 loss 0.013585062697529793\n",
      "epoch 182 finished - avarage train loss 0.009734321108633846  avarage test loss 0.01483207440469414\n",
      "Training: epoch 183 batch 0 loss 0.0014884833944961429\n",
      "Training: epoch 183 batch 10 loss 0.005593310575932264\n",
      "Training: epoch 183 batch 20 loss 0.0034186511766165495\n",
      "Test: epoch 183 batch 0 loss 0.013046052306890488\n",
      "epoch 183 finished - avarage train loss 0.008051440364616955  avarage test loss 0.013707558391615748\n",
      "Training: epoch 184 batch 0 loss 0.004054011777043343\n",
      "Training: epoch 184 batch 10 loss 0.006714640185236931\n",
      "Training: epoch 184 batch 20 loss 0.007141096983104944\n",
      "Test: epoch 184 batch 0 loss 0.012647499330341816\n",
      "epoch 184 finished - avarage train loss 0.00710037316934302  avarage test loss 0.013600794482044876\n",
      "Training: epoch 185 batch 0 loss 0.004050197079777718\n",
      "Training: epoch 185 batch 10 loss 0.00884244404733181\n",
      "Training: epoch 185 batch 20 loss 0.0048673516139388084\n",
      "Test: epoch 185 batch 0 loss 0.012346915900707245\n",
      "epoch 185 finished - avarage train loss 0.007505879849837771  avarage test loss 0.013248407281935215\n",
      "Training: epoch 186 batch 0 loss 0.008810199797153473\n",
      "Training: epoch 186 batch 10 loss 0.004583217203617096\n",
      "Training: epoch 186 batch 20 loss 0.006724211387336254\n",
      "Test: epoch 186 batch 0 loss 0.011018765158951283\n",
      "epoch 186 finished - avarage train loss 0.007844120537027204  avarage test loss 0.012754089431837201\n",
      "Training: epoch 187 batch 0 loss 0.0022371155209839344\n",
      "Training: epoch 187 batch 10 loss 0.0074659488163888454\n",
      "Training: epoch 187 batch 20 loss 0.005893196910619736\n",
      "Test: epoch 187 batch 0 loss 0.014805421233177185\n",
      "epoch 187 finished - avarage train loss 0.008681249877052575  avarage test loss 0.016497483011335135\n",
      "Training: epoch 188 batch 0 loss 0.006007427349686623\n",
      "Training: epoch 188 batch 10 loss 0.005737262777984142\n",
      "Training: epoch 188 batch 20 loss 0.008689145557582378\n",
      "Test: epoch 188 batch 0 loss 0.012431927025318146\n",
      "epoch 188 finished - avarage train loss 0.007971719180330121  avarage test loss 0.013235028134658933\n",
      "Training: epoch 189 batch 0 loss 0.005427420139312744\n",
      "Training: epoch 189 batch 10 loss 0.003577818861231208\n",
      "Training: epoch 189 batch 20 loss 0.0035771315451711416\n",
      "Test: epoch 189 batch 0 loss 0.009695417247712612\n",
      "epoch 189 finished - avarage train loss 0.007091534340047631  avarage test loss 0.013888415065594018\n",
      "Training: epoch 190 batch 0 loss 0.009528394788503647\n",
      "Training: epoch 190 batch 10 loss 0.00830093678086996\n",
      "Training: epoch 190 batch 20 loss 0.014937127940356731\n",
      "Test: epoch 190 batch 0 loss 0.01315885316580534\n",
      "epoch 190 finished - avarage train loss 0.0072439662943562045  avarage test loss 0.01372039527632296\n",
      "Training: epoch 191 batch 0 loss 0.003869143780320883\n",
      "Training: epoch 191 batch 10 loss 0.005390447564423084\n",
      "Training: epoch 191 batch 20 loss 0.005792759358882904\n",
      "Test: epoch 191 batch 0 loss 0.014888362027704716\n",
      "epoch 191 finished - avarage train loss 0.008014940287404019  avarage test loss 0.015727304155007005\n",
      "Training: epoch 192 batch 0 loss 0.005185944959521294\n",
      "Training: epoch 192 batch 10 loss 0.009558064863085747\n",
      "Training: epoch 192 batch 20 loss 0.007674810942262411\n",
      "Test: epoch 192 batch 0 loss 0.012648724019527435\n",
      "epoch 192 finished - avarage train loss 0.008709965305852479  avarage test loss 0.013827960472553968\n",
      "Training: epoch 193 batch 0 loss 0.006219018250703812\n",
      "Training: epoch 193 batch 10 loss 0.002294593956321478\n",
      "Training: epoch 193 batch 20 loss 0.006010537967085838\n",
      "Test: epoch 193 batch 0 loss 0.013349377550184727\n",
      "epoch 193 finished - avarage train loss 0.009511612163021647  avarage test loss 0.014434294425882399\n",
      "Training: epoch 194 batch 0 loss 0.0028795814141631126\n",
      "Training: epoch 194 batch 10 loss 0.006518584676086903\n",
      "Training: epoch 194 batch 20 loss 0.006778600160032511\n",
      "Test: epoch 194 batch 0 loss 0.010961928404867649\n",
      "epoch 194 finished - avarage train loss 0.007409309430433245  avarage test loss 0.01272681844420731\n",
      "Training: epoch 195 batch 0 loss 0.0062506492249667645\n",
      "Training: epoch 195 batch 10 loss 0.007918372750282288\n",
      "Training: epoch 195 batch 20 loss 0.0033734748139977455\n",
      "Test: epoch 195 batch 0 loss 0.012543788179755211\n",
      "epoch 195 finished - avarage train loss 0.006967648903935634  avarage test loss 0.013969590188935399\n",
      "Training: epoch 196 batch 0 loss 0.00357812293805182\n",
      "Training: epoch 196 batch 10 loss 0.0057049342431128025\n",
      "Training: epoch 196 batch 20 loss 0.003371625207364559\n",
      "Test: epoch 196 batch 0 loss 0.01193118467926979\n",
      "epoch 196 finished - avarage train loss 0.007046609209721972  avarage test loss 0.013109620311297476\n",
      "Training: epoch 197 batch 0 loss 0.0037937534507364035\n",
      "Training: epoch 197 batch 10 loss 0.0052697015926241875\n",
      "Training: epoch 197 batch 20 loss 0.001018961425870657\n",
      "Test: epoch 197 batch 0 loss 0.012524657882750034\n",
      "epoch 197 finished - avarage train loss 0.005793417129537155  avarage test loss 0.013426096527837217\n",
      "Training: epoch 198 batch 0 loss 0.006217143032699823\n",
      "Training: epoch 198 batch 10 loss 0.007556150667369366\n",
      "Training: epoch 198 batch 20 loss 0.008486099541187286\n",
      "Test: epoch 198 batch 0 loss 0.014445928856730461\n",
      "epoch 198 finished - avarage train loss 0.0071990894523031755  avarage test loss 0.0157121445517987\n",
      "Training: epoch 199 batch 0 loss 0.010379397310316563\n",
      "Training: epoch 199 batch 10 loss 0.0038792272098362446\n",
      "Training: epoch 199 batch 20 loss 0.006052780896425247\n",
      "Test: epoch 199 batch 0 loss 0.014650222845375538\n",
      "epoch 199 finished - avarage train loss 0.007282114973099068  avarage test loss 0.01658123219385743\n",
      "Training: epoch 0 batch 0 loss 0.628009557723999\n",
      "Training: epoch 0 batch 10 loss 0.7082407474517822\n",
      "Training: epoch 0 batch 20 loss 0.5973394513130188\n",
      "Test: epoch 0 batch 0 loss 0.412086546421051\n",
      "epoch 0 finished - avarage train loss 0.5201282244304131  avarage test loss 0.45494142174720764\n",
      "Training: epoch 1 batch 0 loss 0.52216637134552\n",
      "Training: epoch 1 batch 10 loss 0.7118954062461853\n",
      "Training: epoch 1 batch 20 loss 0.42564237117767334\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test: epoch 1 batch 0 loss 0.39749518036842346\n",
      "epoch 1 finished - avarage train loss 0.5046036438695316  avarage test loss 0.43937139958143234\n",
      "Training: epoch 2 batch 0 loss 0.6586347818374634\n",
      "Training: epoch 2 batch 10 loss 0.5122227072715759\n",
      "Training: epoch 2 batch 20 loss 0.30988821387290955\n",
      "Test: epoch 2 batch 0 loss 0.3899762034416199\n",
      "epoch 2 finished - avarage train loss 0.5012653079526178  avarage test loss 0.43435683846473694\n",
      "Training: epoch 3 batch 0 loss 0.41023188829421997\n",
      "Training: epoch 3 batch 10 loss 0.5418822169303894\n",
      "Training: epoch 3 batch 20 loss 0.4924706518650055\n",
      "Test: epoch 3 batch 0 loss 0.3953438699245453\n",
      "epoch 3 finished - avarage train loss 0.5048400800803612  avarage test loss 0.4362166114151478\n",
      "Training: epoch 4 batch 0 loss 0.4292355179786682\n",
      "Training: epoch 4 batch 10 loss 0.619055986404419\n",
      "Training: epoch 4 batch 20 loss 0.6752825379371643\n",
      "Test: epoch 4 batch 0 loss 0.3972167670726776\n",
      "epoch 4 finished - avarage train loss 0.5034287140287202  avarage test loss 0.4375546984374523\n",
      "Training: epoch 5 batch 0 loss 0.52542644739151\n",
      "Training: epoch 5 batch 10 loss 0.6975713968276978\n",
      "Training: epoch 5 batch 20 loss 0.6811929941177368\n",
      "Test: epoch 5 batch 0 loss 0.4027361571788788\n",
      "epoch 5 finished - avarage train loss 0.5388499164375765  avarage test loss 0.4459435045719147\n",
      "Training: epoch 6 batch 0 loss 0.3717687726020813\n",
      "Training: epoch 6 batch 10 loss 0.8196634650230408\n",
      "Training: epoch 6 batch 20 loss 0.5014959573745728\n",
      "Test: epoch 6 batch 0 loss 0.38699349761009216\n",
      "epoch 6 finished - avarage train loss 0.5122826489908942  avarage test loss 0.42798955738544464\n",
      "Training: epoch 7 batch 0 loss 0.3371889889240265\n",
      "Training: epoch 7 batch 10 loss 0.5535749197006226\n",
      "Training: epoch 7 batch 20 loss 0.13625071942806244\n",
      "Test: epoch 7 batch 0 loss 0.057007595896720886\n",
      "epoch 7 finished - avarage train loss 0.23780051464664526  avarage test loss 0.07941477745771408\n",
      "Training: epoch 8 batch 0 loss 0.057465922087430954\n",
      "Training: epoch 8 batch 10 loss 0.05449921265244484\n",
      "Training: epoch 8 batch 20 loss 0.02194780856370926\n",
      "Test: epoch 8 batch 0 loss 0.02477419003844261\n",
      "epoch 8 finished - avarage train loss 0.04457441309531187  avarage test loss 0.029274212196469307\n",
      "Training: epoch 9 batch 0 loss 0.019352128729224205\n",
      "Training: epoch 9 batch 10 loss 0.012033525854349136\n",
      "Training: epoch 9 batch 20 loss 0.007746948394924402\n",
      "Test: epoch 9 batch 0 loss 0.019298138096928596\n",
      "epoch 9 finished - avarage train loss 0.015692986756691646  avarage test loss 0.020598180824890733\n",
      "Training: epoch 10 batch 0 loss 0.011871815659105778\n",
      "Training: epoch 10 batch 10 loss 0.008145573548972607\n",
      "Training: epoch 10 batch 20 loss 0.006192532833665609\n",
      "Test: epoch 10 batch 0 loss 0.01324386615306139\n",
      "epoch 10 finished - avarage train loss 0.008441389409889435  avarage test loss 0.014540812466293573\n",
      "Training: epoch 11 batch 0 loss 0.004186069592833519\n",
      "Training: epoch 11 batch 10 loss 0.006274685729295015\n",
      "Training: epoch 11 batch 20 loss 0.0026357807219028473\n",
      "Test: epoch 11 batch 0 loss 0.017550352960824966\n",
      "epoch 11 finished - avarage train loss 0.008715910981569824  avarage test loss 0.01814139960333705\n",
      "Training: epoch 12 batch 0 loss 0.009664536453783512\n",
      "Training: epoch 12 batch 10 loss 0.005031211767345667\n",
      "Training: epoch 12 batch 20 loss 0.007941116578876972\n",
      "Test: epoch 12 batch 0 loss 0.010849631391465664\n",
      "epoch 12 finished - avarage train loss 0.007651927712341321  avarage test loss 0.014504886232316494\n",
      "Training: epoch 13 batch 0 loss 0.007102315779775381\n",
      "Training: epoch 13 batch 10 loss 0.005090272519737482\n",
      "Training: epoch 13 batch 20 loss 0.006375398952513933\n",
      "Test: epoch 13 batch 0 loss 0.011252045631408691\n",
      "epoch 13 finished - avarage train loss 0.00875713564616082  avarage test loss 0.01384683814831078\n",
      "Training: epoch 14 batch 0 loss 0.002292899414896965\n",
      "Training: epoch 14 batch 10 loss 0.012139475904405117\n",
      "Training: epoch 14 batch 20 loss 0.005148995667695999\n",
      "Test: epoch 14 batch 0 loss 0.011518831364810467\n",
      "epoch 14 finished - avarage train loss 0.007915665732760882  avarage test loss 0.01342649559956044\n",
      "Training: epoch 15 batch 0 loss 0.007984195835888386\n",
      "Training: epoch 15 batch 10 loss 0.007379143964499235\n",
      "Training: epoch 15 batch 20 loss 0.003049417631700635\n",
      "Test: epoch 15 batch 0 loss 0.011221381835639477\n",
      "epoch 15 finished - avarage train loss 0.008571035891032681  avarage test loss 0.013033067458309233\n",
      "Training: epoch 16 batch 0 loss 0.0073649766854941845\n",
      "Training: epoch 16 batch 10 loss 0.006101639475673437\n",
      "Training: epoch 16 batch 20 loss 0.0046865083277225494\n",
      "Test: epoch 16 batch 0 loss 0.014245986938476562\n",
      "epoch 16 finished - avarage train loss 0.007420216004588994  avarage test loss 0.01529059186577797\n",
      "Training: epoch 17 batch 0 loss 0.007350442931056023\n",
      "Training: epoch 17 batch 10 loss 0.008048223331570625\n",
      "Training: epoch 17 batch 20 loss 0.010391965508460999\n",
      "Test: epoch 17 batch 0 loss 0.01078084111213684\n",
      "epoch 17 finished - avarage train loss 0.006527981536205986  avarage test loss 0.013624507933855057\n",
      "Training: epoch 18 batch 0 loss 0.005715081933885813\n",
      "Training: epoch 18 batch 10 loss 0.017351897433400154\n",
      "Training: epoch 18 batch 20 loss 0.004787222482264042\n",
      "Test: epoch 18 batch 0 loss 0.014136054553091526\n",
      "epoch 18 finished - avarage train loss 0.007126643346494128  avarage test loss 0.016601228620857\n",
      "Training: epoch 19 batch 0 loss 0.006612477358430624\n",
      "Training: epoch 19 batch 10 loss 0.007740285247564316\n",
      "Training: epoch 19 batch 20 loss 0.009695792570710182\n",
      "Test: epoch 19 batch 0 loss 0.009415446780622005\n",
      "epoch 19 finished - avarage train loss 0.006801391060560428  avarage test loss 0.012364889495074749\n",
      "Training: epoch 20 batch 0 loss 0.006126794498413801\n",
      "Training: epoch 20 batch 10 loss 0.002185701159760356\n",
      "Training: epoch 20 batch 20 loss 0.0037114170845597982\n",
      "Test: epoch 20 batch 0 loss 0.010614831000566483\n",
      "epoch 20 finished - avarage train loss 0.006178266513321934  avarage test loss 0.012475070543587208\n",
      "Training: epoch 21 batch 0 loss 0.0076479376293718815\n",
      "Training: epoch 21 batch 10 loss 0.0023611453361809254\n",
      "Training: epoch 21 batch 20 loss 0.0037756762467324734\n",
      "Test: epoch 21 batch 0 loss 0.009263396263122559\n",
      "epoch 21 finished - avarage train loss 0.006233192586885958  avarage test loss 0.013258639839477837\n",
      "Training: epoch 22 batch 0 loss 0.007503465283662081\n",
      "Training: epoch 22 batch 10 loss 0.003913797903805971\n",
      "Training: epoch 22 batch 20 loss 0.0040978239849209785\n",
      "Test: epoch 22 batch 0 loss 0.011440764181315899\n",
      "epoch 22 finished - avarage train loss 0.009602340179142254  avarage test loss 0.013907865271903574\n",
      "Training: epoch 23 batch 0 loss 0.008793753571808338\n",
      "Training: epoch 23 batch 10 loss 0.0015641986392438412\n",
      "Training: epoch 23 batch 20 loss 0.009024206548929214\n",
      "Test: epoch 23 batch 0 loss 0.013162873685359955\n",
      "epoch 23 finished - avarage train loss 0.0064826826966399775  avarage test loss 0.015422458644025028\n",
      "Training: epoch 24 batch 0 loss 0.002846747636795044\n",
      "Training: epoch 24 batch 10 loss 0.004867292009294033\n",
      "Training: epoch 24 batch 20 loss 0.004437600262463093\n",
      "Test: epoch 24 batch 0 loss 0.012733731418848038\n",
      "epoch 24 finished - avarage train loss 0.0069404199542798875  avarage test loss 0.013954733964055777\n",
      "Training: epoch 25 batch 0 loss 0.0037933546118438244\n",
      "Training: epoch 25 batch 10 loss 0.003809907240793109\n",
      "Training: epoch 25 batch 20 loss 0.006007149815559387\n",
      "Test: epoch 25 batch 0 loss 0.014559472911059856\n",
      "epoch 25 finished - avarage train loss 0.005828609401039009  avarage test loss 0.016623051604256034\n",
      "Training: epoch 26 batch 0 loss 0.007923080585896969\n",
      "Training: epoch 26 batch 10 loss 0.004627972841262817\n",
      "Training: epoch 26 batch 20 loss 0.004337731283158064\n",
      "Test: epoch 26 batch 0 loss 0.015147892758250237\n",
      "epoch 26 finished - avarage train loss 0.008258462939732548  avarage test loss 0.01764381304383278\n",
      "Training: epoch 27 batch 0 loss 0.006835129577666521\n",
      "Training: epoch 27 batch 10 loss 0.0046258652582764626\n",
      "Training: epoch 27 batch 20 loss 0.005462959874421358\n",
      "Test: epoch 27 batch 0 loss 0.014532996341586113\n",
      "epoch 27 finished - avarage train loss 0.008923302011179  avarage test loss 0.01676253415644169\n",
      "Training: epoch 28 batch 0 loss 0.01473994366824627\n",
      "Training: epoch 28 batch 10 loss 0.0071919700130820274\n",
      "Training: epoch 28 batch 20 loss 0.0031795913819223642\n",
      "Test: epoch 28 batch 0 loss 0.013156198896467686\n",
      "epoch 28 finished - avarage train loss 0.00774221834405486  avarage test loss 0.016091609839349985\n",
      "Training: epoch 29 batch 0 loss 0.006636480800807476\n",
      "Training: epoch 29 batch 10 loss 0.0035511143505573273\n",
      "Training: epoch 29 batch 20 loss 0.004511551465839148\n",
      "Test: epoch 29 batch 0 loss 0.014285294339060783\n",
      "epoch 29 finished - avarage train loss 0.00813689470644398  avarage test loss 0.015268190763890743\n",
      "Training: epoch 30 batch 0 loss 0.009500641375780106\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training: epoch 30 batch 10 loss 0.008188036270439625\n",
      "Training: epoch 30 batch 20 loss 0.0035325177013874054\n",
      "Test: epoch 30 batch 0 loss 0.01309256162494421\n",
      "epoch 30 finished - avarage train loss 0.007585439248941839  avarage test loss 0.014078045380301774\n",
      "Training: epoch 31 batch 0 loss 0.003869788022711873\n",
      "Training: epoch 31 batch 10 loss 0.001503158244304359\n",
      "Training: epoch 31 batch 20 loss 0.013882484287023544\n",
      "Test: epoch 31 batch 0 loss 0.01129620149731636\n",
      "epoch 31 finished - avarage train loss 0.007628640083692454  avarage test loss 0.013037884375080466\n",
      "Training: epoch 32 batch 0 loss 0.0052205477841198444\n",
      "Training: epoch 32 batch 10 loss 0.004172976594418287\n",
      "Training: epoch 32 batch 20 loss 0.003666758071631193\n",
      "Test: epoch 32 batch 0 loss 0.010428112000226974\n",
      "epoch 32 finished - avarage train loss 0.007538692889221269  avarage test loss 0.012378789368085563\n",
      "Training: epoch 33 batch 0 loss 0.01303137093782425\n",
      "Training: epoch 33 batch 10 loss 0.0022621590178459883\n",
      "Training: epoch 33 batch 20 loss 0.004400861915200949\n",
      "Test: epoch 33 batch 0 loss 0.009571373462677002\n",
      "epoch 33 finished - avarage train loss 0.00720235951855008  avarage test loss 0.012482890277169645\n",
      "Training: epoch 34 batch 0 loss 0.008721936494112015\n",
      "Training: epoch 34 batch 10 loss 0.008858894929289818\n",
      "Training: epoch 34 batch 20 loss 0.0044425493106245995\n",
      "Test: epoch 34 batch 0 loss 0.013737392611801624\n",
      "epoch 34 finished - avarage train loss 0.007098957892218283  avarage test loss 0.015049459994770586\n",
      "Training: epoch 35 batch 0 loss 0.004075478296726942\n",
      "Training: epoch 35 batch 10 loss 0.0019509693374857306\n",
      "Training: epoch 35 batch 20 loss 0.009766887873411179\n",
      "Test: epoch 35 batch 0 loss 0.010938395746052265\n",
      "epoch 35 finished - avarage train loss 0.008195807398618039  avarage test loss 0.01266189746093005\n",
      "Training: epoch 36 batch 0 loss 0.008078351616859436\n",
      "Training: epoch 36 batch 10 loss 0.007126824930310249\n",
      "Training: epoch 36 batch 20 loss 0.008309231139719486\n",
      "Test: epoch 36 batch 0 loss 0.014301958493888378\n",
      "epoch 36 finished - avarage train loss 0.007174782912600143  avarage test loss 0.015369456494227052\n",
      "Training: epoch 37 batch 0 loss 0.006635118275880814\n",
      "Training: epoch 37 batch 10 loss 0.003957581706345081\n",
      "Training: epoch 37 batch 20 loss 0.007594970520585775\n",
      "Test: epoch 37 batch 0 loss 0.0146426847204566\n",
      "epoch 37 finished - avarage train loss 0.006153654620244071  avarage test loss 0.015665446408092976\n",
      "Training: epoch 38 batch 0 loss 0.0035961386747658253\n",
      "Training: epoch 38 batch 10 loss 0.004413075279444456\n",
      "Training: epoch 38 batch 20 loss 0.003609808860346675\n",
      "Test: epoch 38 batch 0 loss 0.01299561932682991\n",
      "epoch 38 finished - avarage train loss 0.007375884295344867  avarage test loss 0.01367470296099782\n",
      "Training: epoch 39 batch 0 loss 0.005942337214946747\n",
      "Training: epoch 39 batch 10 loss 0.004531072452664375\n",
      "Training: epoch 39 batch 20 loss 0.008936763741075993\n",
      "Test: epoch 39 batch 0 loss 0.014337596483528614\n",
      "epoch 39 finished - avarage train loss 0.007740096325568598  avarage test loss 0.016731403302401304\n",
      "Training: epoch 40 batch 0 loss 0.0034897099249064922\n",
      "Training: epoch 40 batch 10 loss 0.0036690125707536936\n",
      "Training: epoch 40 batch 20 loss 0.005346135701984167\n",
      "Test: epoch 40 batch 0 loss 0.012316436506807804\n",
      "epoch 40 finished - avarage train loss 0.006996942017677015  avarage test loss 0.013383991783484817\n",
      "Training: epoch 41 batch 0 loss 0.002172631910070777\n",
      "Training: epoch 41 batch 10 loss 0.007163939066231251\n",
      "Training: epoch 41 batch 20 loss 0.0051155597902834415\n",
      "Test: epoch 41 batch 0 loss 0.01380701083689928\n",
      "epoch 41 finished - avarage train loss 0.005939425290401639  avarage test loss 0.015162348747253418\n",
      "Training: epoch 42 batch 0 loss 0.006316511891782284\n",
      "Training: epoch 42 batch 10 loss 0.0037042293697595596\n",
      "Training: epoch 42 batch 20 loss 0.006238377187401056\n",
      "Test: epoch 42 batch 0 loss 0.011315014213323593\n",
      "epoch 42 finished - avarage train loss 0.007786484186312762  avarage test loss 0.013178919325582683\n",
      "Training: epoch 43 batch 0 loss 0.010596530511975288\n",
      "Training: epoch 43 batch 10 loss 0.005550493951886892\n",
      "Training: epoch 43 batch 20 loss 0.004453679546713829\n",
      "Test: epoch 43 batch 0 loss 0.011986278928816319\n",
      "epoch 43 finished - avarage train loss 0.007677840903915208  avarage test loss 0.013553386786952615\n",
      "Training: epoch 44 batch 0 loss 0.005769023671746254\n",
      "Training: epoch 44 batch 10 loss 0.004120384808629751\n",
      "Training: epoch 44 batch 20 loss 0.003204273758456111\n",
      "Test: epoch 44 batch 0 loss 0.014675426296889782\n",
      "epoch 44 finished - avarage train loss 0.0064525103903022305  avarage test loss 0.01739860908128321\n",
      "Training: epoch 45 batch 0 loss 0.00727542769163847\n",
      "Training: epoch 45 batch 10 loss 0.004401530139148235\n",
      "Training: epoch 45 batch 20 loss 0.004276667255908251\n",
      "Test: epoch 45 batch 0 loss 0.010975259356200695\n",
      "epoch 45 finished - avarage train loss 0.006746946071306693  avarage test loss 0.013736594934016466\n",
      "Training: epoch 46 batch 0 loss 0.010226592421531677\n",
      "Training: epoch 46 batch 10 loss 0.01889639161527157\n",
      "Training: epoch 46 batch 20 loss 0.007652090862393379\n",
      "Test: epoch 46 batch 0 loss 0.016575070098042488\n",
      "epoch 46 finished - avarage train loss 0.008867072407156229  avarage test loss 0.018447599140927196\n",
      "Training: epoch 47 batch 0 loss 0.013302408158779144\n",
      "Training: epoch 47 batch 10 loss 0.002628978341817856\n",
      "Training: epoch 47 batch 20 loss 0.015574083663523197\n",
      "Test: epoch 47 batch 0 loss 0.011606460437178612\n",
      "epoch 47 finished - avarage train loss 0.010648580565085185  avarage test loss 0.012884930707514286\n",
      "Training: epoch 48 batch 0 loss 0.0034224155824631453\n",
      "Training: epoch 48 batch 10 loss 0.011078355833888054\n",
      "Training: epoch 48 batch 20 loss 0.011552311480045319\n",
      "Test: epoch 48 batch 0 loss 0.019734680652618408\n",
      "epoch 48 finished - avarage train loss 0.012735655041539977  avarage test loss 0.02167794294655323\n",
      "Training: epoch 49 batch 0 loss 0.007890504784882069\n",
      "Training: epoch 49 batch 10 loss 0.0266799945384264\n",
      "Training: epoch 49 batch 20 loss 0.022147897630929947\n",
      "Test: epoch 49 batch 0 loss 0.022711358964443207\n",
      "epoch 49 finished - avarage train loss 0.021199916360964036  avarage test loss 0.03033058950677514\n",
      "Training: epoch 50 batch 0 loss 0.024249808862805367\n",
      "Training: epoch 50 batch 10 loss 0.003879366908222437\n",
      "Training: epoch 50 batch 20 loss 0.0039041745476424694\n",
      "Test: epoch 50 batch 0 loss 0.010422233492136002\n",
      "epoch 50 finished - avarage train loss 0.011010328548607128  avarage test loss 0.012351346667855978\n",
      "Training: epoch 51 batch 0 loss 0.00890952255576849\n",
      "Training: epoch 51 batch 10 loss 0.010272521525621414\n",
      "Training: epoch 51 batch 20 loss 0.0052285827696323395\n",
      "Test: epoch 51 batch 0 loss 0.010290820151567459\n",
      "epoch 51 finished - avarage train loss 0.009097115329370416  avarage test loss 0.012041756184771657\n",
      "Training: epoch 52 batch 0 loss 0.004010505974292755\n",
      "Training: epoch 52 batch 10 loss 0.004490087274461985\n",
      "Training: epoch 52 batch 20 loss 0.003695621620863676\n",
      "Test: epoch 52 batch 0 loss 0.009817716665565968\n",
      "epoch 52 finished - avarage train loss 0.007016925903936398  avarage test loss 0.012102038250304759\n",
      "Training: epoch 53 batch 0 loss 0.002930693794041872\n",
      "Training: epoch 53 batch 10 loss 0.0039966213516891\n",
      "Training: epoch 53 batch 20 loss 0.005179876461625099\n",
      "Test: epoch 53 batch 0 loss 0.008885888382792473\n",
      "epoch 53 finished - avarage train loss 0.008151188741662893  avarage test loss 0.012320718495175242\n",
      "Training: epoch 54 batch 0 loss 0.009119484573602676\n",
      "Training: epoch 54 batch 10 loss 0.01873057708144188\n",
      "Training: epoch 54 batch 20 loss 0.00262762070633471\n",
      "Test: epoch 54 batch 0 loss 0.011173456907272339\n",
      "epoch 54 finished - avarage train loss 0.009358638544277898  avarage test loss 0.012775061302818358\n",
      "Training: epoch 55 batch 0 loss 0.009620863944292068\n",
      "Training: epoch 55 batch 10 loss 0.007449962664395571\n",
      "Training: epoch 55 batch 20 loss 0.003715032711625099\n",
      "Test: epoch 55 batch 0 loss 0.01191458199173212\n",
      "epoch 55 finished - avarage train loss 0.007083425563278383  avarage test loss 0.013651093235239387\n",
      "Training: epoch 56 batch 0 loss 0.004991183523088694\n",
      "Training: epoch 56 batch 10 loss 0.0040365480817854404\n",
      "Training: epoch 56 batch 20 loss 0.006583158392459154\n",
      "Test: epoch 56 batch 0 loss 0.010786054655909538\n",
      "epoch 56 finished - avarage train loss 0.0072202868518772825  avarage test loss 0.012235921574756503\n",
      "Training: epoch 57 batch 0 loss 0.009556413628160954\n",
      "Training: epoch 57 batch 10 loss 0.002091636648401618\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training: epoch 57 batch 20 loss 0.013179105706512928\n",
      "Test: epoch 57 batch 0 loss 0.013408245518803596\n",
      "epoch 57 finished - avarage train loss 0.008122018000496358  avarage test loss 0.013765050447545946\n",
      "Training: epoch 58 batch 0 loss 0.004826701246201992\n",
      "Training: epoch 58 batch 10 loss 0.004853371996432543\n",
      "Training: epoch 58 batch 20 loss 0.004025728907436132\n",
      "Test: epoch 58 batch 0 loss 0.011416628956794739\n",
      "epoch 58 finished - avarage train loss 0.0066302261507973585  avarage test loss 0.012622137204743922\n",
      "Training: epoch 59 batch 0 loss 0.006224352866411209\n",
      "Training: epoch 59 batch 10 loss 0.00500084925442934\n",
      "Training: epoch 59 batch 20 loss 0.002047427697107196\n",
      "Test: epoch 59 batch 0 loss 0.013441297225654125\n",
      "epoch 59 finished - avarage train loss 0.006739819470536092  avarage test loss 0.014439572812989354\n",
      "Training: epoch 60 batch 0 loss 0.0039168233051896095\n",
      "Training: epoch 60 batch 10 loss 0.002070004353299737\n",
      "Training: epoch 60 batch 20 loss 0.003953872248530388\n",
      "Test: epoch 60 batch 0 loss 0.01467816811054945\n",
      "epoch 60 finished - avarage train loss 0.007911038392319762  avarage test loss 0.01701045292429626\n",
      "Training: epoch 61 batch 0 loss 0.004501461982727051\n",
      "Training: epoch 61 batch 10 loss 0.0033334563486278057\n",
      "Training: epoch 61 batch 20 loss 0.0066853235475718975\n",
      "Test: epoch 61 batch 0 loss 0.011483235284686089\n",
      "epoch 61 finished - avarage train loss 0.006401988581336778  avarage test loss 0.012317947228439152\n",
      "Training: epoch 62 batch 0 loss 0.0036379885859787464\n",
      "Training: epoch 62 batch 10 loss 0.002690718974918127\n",
      "Training: epoch 62 batch 20 loss 0.0022689870093017817\n",
      "Test: epoch 62 batch 0 loss 0.0100393146276474\n",
      "epoch 62 finished - avarage train loss 0.006862126925060975  avarage test loss 0.012161231832578778\n",
      "Training: epoch 63 batch 0 loss 0.00801258534193039\n",
      "Training: epoch 63 batch 10 loss 0.006918807048350573\n",
      "Training: epoch 63 batch 20 loss 0.0030425891745835543\n",
      "Test: epoch 63 batch 0 loss 0.009822789579629898\n",
      "epoch 63 finished - avarage train loss 0.008084064823222058  avarage test loss 0.012038394226692617\n",
      "Training: epoch 64 batch 0 loss 0.0054211923852562904\n",
      "Training: epoch 64 batch 10 loss 0.005765384528785944\n",
      "Training: epoch 64 batch 20 loss 0.0024157054722309113\n",
      "Test: epoch 64 batch 0 loss 0.010906371288001537\n",
      "epoch 64 finished - avarage train loss 0.007437242435869472  avarage test loss 0.013430832303129137\n",
      "Training: epoch 65 batch 0 loss 0.0027693877927958965\n",
      "Training: epoch 65 batch 10 loss 0.007237651851028204\n",
      "Training: epoch 65 batch 20 loss 0.005840419791638851\n",
      "Test: epoch 65 batch 0 loss 0.012388703413307667\n",
      "epoch 65 finished - avarage train loss 0.006969729695340683  avarage test loss 0.013418632792308927\n",
      "Training: epoch 66 batch 0 loss 0.005474038887768984\n",
      "Training: epoch 66 batch 10 loss 0.0036446922458708286\n",
      "Training: epoch 66 batch 20 loss 0.012477506883442402\n",
      "Test: epoch 66 batch 0 loss 0.012306459248065948\n",
      "epoch 66 finished - avarage train loss 0.006587777921030748  avarage test loss 0.013555469689890742\n",
      "Training: epoch 67 batch 0 loss 0.0022167598363012075\n",
      "Training: epoch 67 batch 10 loss 0.005232989322394133\n",
      "Training: epoch 67 batch 20 loss 0.006376735866069794\n",
      "Test: epoch 67 batch 0 loss 0.014308390207588673\n",
      "epoch 67 finished - avarage train loss 0.006305553051161355  avarage test loss 0.015142577234655619\n",
      "Training: epoch 68 batch 0 loss 0.010046384297311306\n",
      "Training: epoch 68 batch 10 loss 0.0032028546556830406\n",
      "Training: epoch 68 batch 20 loss 0.006433498114347458\n",
      "Test: epoch 68 batch 0 loss 0.011858525685966015\n",
      "epoch 68 finished - avarage train loss 0.006999071261941873  avarage test loss 0.01314091612584889\n",
      "Training: epoch 69 batch 0 loss 0.011508118361234665\n",
      "Training: epoch 69 batch 10 loss 0.007439119275659323\n",
      "Training: epoch 69 batch 20 loss 0.005477835424244404\n",
      "Test: epoch 69 batch 0 loss 0.01636825129389763\n",
      "epoch 69 finished - avarage train loss 0.009040011546638763  avarage test loss 0.01944886497221887\n",
      "Training: epoch 70 batch 0 loss 0.009800705127418041\n",
      "Training: epoch 70 batch 10 loss 0.00419920589774847\n",
      "Training: epoch 70 batch 20 loss 0.009739640168845654\n",
      "Test: epoch 70 batch 0 loss 0.014451531693339348\n",
      "epoch 70 finished - avarage train loss 0.010632779898828474  avarage test loss 0.015544281341135502\n",
      "Training: epoch 71 batch 0 loss 0.004227799363434315\n",
      "Training: epoch 71 batch 10 loss 0.007535574957728386\n",
      "Training: epoch 71 batch 20 loss 0.005723151843994856\n",
      "Test: epoch 71 batch 0 loss 0.010180000215768814\n",
      "epoch 71 finished - avarage train loss 0.008257994083045372  avarage test loss 0.013097019982524216\n",
      "Training: epoch 72 batch 0 loss 0.010286671109497547\n",
      "Training: epoch 72 batch 10 loss 0.010366670787334442\n",
      "Training: epoch 72 batch 20 loss 0.010107642970979214\n",
      "Test: epoch 72 batch 0 loss 0.010498648509383202\n",
      "epoch 72 finished - avarage train loss 0.00826373536557216  avarage test loss 0.014699503779411316\n",
      "Training: epoch 73 batch 0 loss 0.007475912105292082\n",
      "Training: epoch 73 batch 10 loss 0.012318377383053303\n",
      "Training: epoch 73 batch 20 loss 0.004498646594583988\n",
      "Test: epoch 73 batch 0 loss 0.011641442775726318\n",
      "epoch 73 finished - avarage train loss 0.008048458803637788  avarage test loss 0.01549724314827472\n",
      "Training: epoch 74 batch 0 loss 0.0055045755580067635\n",
      "Training: epoch 74 batch 10 loss 0.006984783336520195\n",
      "Training: epoch 74 batch 20 loss 0.009921311400830746\n",
      "Test: epoch 74 batch 0 loss 0.01162046566605568\n",
      "epoch 74 finished - avarage train loss 0.006128680089424397  avarage test loss 0.013683606404811144\n",
      "Training: epoch 75 batch 0 loss 0.004990328568965197\n",
      "Training: epoch 75 batch 10 loss 0.008057652041316032\n",
      "Training: epoch 75 batch 20 loss 0.009526632726192474\n",
      "Test: epoch 75 batch 0 loss 0.010432789102196693\n",
      "epoch 75 finished - avarage train loss 0.007014086486065182  avarage test loss 0.011923019774258137\n",
      "Training: epoch 76 batch 0 loss 0.003635864704847336\n",
      "Training: epoch 76 batch 10 loss 0.008156652562320232\n",
      "Training: epoch 76 batch 20 loss 0.01083674281835556\n",
      "Test: epoch 76 batch 0 loss 0.010315932333469391\n",
      "epoch 76 finished - avarage train loss 0.00721540163544103  avarage test loss 0.012185989646241069\n",
      "Training: epoch 77 batch 0 loss 0.008198576048016548\n",
      "Training: epoch 77 batch 10 loss 0.004850742872804403\n",
      "Training: epoch 77 batch 20 loss 0.005476327612996101\n",
      "Test: epoch 77 batch 0 loss 0.010768502950668335\n",
      "epoch 77 finished - avarage train loss 0.006575771013339018  avarage test loss 0.012105608475394547\n",
      "Training: epoch 78 batch 0 loss 0.002451973967254162\n",
      "Training: epoch 78 batch 10 loss 0.0028782603330910206\n",
      "Training: epoch 78 batch 20 loss 0.0046701980754733086\n",
      "Test: epoch 78 batch 0 loss 0.010954247787594795\n",
      "epoch 78 finished - avarage train loss 0.005871802785209027  avarage test loss 0.011829155148006976\n",
      "Training: epoch 79 batch 0 loss 0.0030922889709472656\n",
      "Training: epoch 79 batch 10 loss 0.007759438827633858\n",
      "Training: epoch 79 batch 20 loss 0.0019389177905395627\n",
      "Test: epoch 79 batch 0 loss 0.011743007227778435\n",
      "epoch 79 finished - avarage train loss 0.007122887744054455  avarage test loss 0.012369996518827975\n",
      "Training: epoch 80 batch 0 loss 0.007173630408942699\n",
      "Training: epoch 80 batch 10 loss 0.004452468827366829\n",
      "Training: epoch 80 batch 20 loss 0.0028697357047349215\n",
      "Test: epoch 80 batch 0 loss 0.012474292889237404\n",
      "epoch 80 finished - avarage train loss 0.006549722948593312  avarage test loss 0.014174607116729021\n",
      "Training: epoch 81 batch 0 loss 0.0042912960052490234\n",
      "Training: epoch 81 batch 10 loss 0.006453288719058037\n",
      "Training: epoch 81 batch 20 loss 0.005580871365964413\n",
      "Test: epoch 81 batch 0 loss 0.010875958018004894\n",
      "epoch 81 finished - avarage train loss 0.007142926469959062  avarage test loss 0.012602612259797752\n",
      "Training: epoch 82 batch 0 loss 0.011423127725720406\n",
      "Training: epoch 82 batch 10 loss 0.005077804438769817\n",
      "Training: epoch 82 batch 20 loss 0.005369558930397034\n",
      "Test: epoch 82 batch 0 loss 0.010527214966714382\n",
      "epoch 82 finished - avarage train loss 0.007258955092201459  avarage test loss 0.012256390531547368\n",
      "Training: epoch 83 batch 0 loss 0.003240998135879636\n",
      "Training: epoch 83 batch 10 loss 0.009977534413337708\n",
      "Training: epoch 83 batch 20 loss 0.0104984687641263\n",
      "Test: epoch 83 batch 0 loss 0.010765597224235535\n",
      "epoch 83 finished - avarage train loss 0.00861818245451512  avarage test loss 0.012356982217170298\n",
      "Training: epoch 84 batch 0 loss 0.003931439947336912\n",
      "Training: epoch 84 batch 10 loss 0.011523760855197906\n",
      "Training: epoch 84 batch 20 loss 0.0030646701343357563\n",
      "Test: epoch 84 batch 0 loss 0.012266943231225014\n",
      "epoch 84 finished - avarage train loss 0.006444357937715691  avarage test loss 0.013000820064917207\n",
      "Training: epoch 85 batch 0 loss 0.004957057535648346\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training: epoch 85 batch 10 loss 0.010693462565541267\n",
      "Training: epoch 85 batch 20 loss 0.00639997748658061\n",
      "Test: epoch 85 batch 0 loss 0.011700300499796867\n",
      "epoch 85 finished - avarage train loss 0.008407126044340688  avarage test loss 0.012967399728950113\n",
      "Training: epoch 86 batch 0 loss 0.006875106133520603\n",
      "Training: epoch 86 batch 10 loss 0.011531418189406395\n",
      "Training: epoch 86 batch 20 loss 0.003332430263981223\n",
      "Test: epoch 86 batch 0 loss 0.012794870883226395\n",
      "epoch 86 finished - avarage train loss 0.007115763355174969  avarage test loss 0.012975058751180768\n",
      "Training: epoch 87 batch 0 loss 0.0046071987599134445\n",
      "Training: epoch 87 batch 10 loss 0.0037026044446974993\n",
      "Training: epoch 87 batch 20 loss 0.005390153266489506\n",
      "Test: epoch 87 batch 0 loss 0.011521991342306137\n",
      "epoch 87 finished - avarage train loss 0.005887707514720487  avarage test loss 0.012948284391313791\n",
      "Training: epoch 88 batch 0 loss 0.007360121235251427\n",
      "Training: epoch 88 batch 10 loss 0.003672681050375104\n",
      "Training: epoch 88 batch 20 loss 0.008855871856212616\n",
      "Test: epoch 88 batch 0 loss 0.01464097574353218\n",
      "epoch 88 finished - avarage train loss 0.006561212458422986  avarage test loss 0.015290435636416078\n",
      "Training: epoch 89 batch 0 loss 0.007781848311424255\n",
      "Training: epoch 89 batch 10 loss 0.008936198428273201\n",
      "Training: epoch 89 batch 20 loss 0.0023687127977609634\n",
      "Test: epoch 89 batch 0 loss 0.011772430501878262\n",
      "epoch 89 finished - avarage train loss 0.00806885023986728  avarage test loss 0.01283443404827267\n",
      "Training: epoch 90 batch 0 loss 0.002929608803242445\n",
      "Training: epoch 90 batch 10 loss 0.008517523296177387\n",
      "Training: epoch 90 batch 20 loss 0.008272064849734306\n",
      "Test: epoch 90 batch 0 loss 0.012736905366182327\n",
      "epoch 90 finished - avarage train loss 0.006828431072163171  avarage test loss 0.013824748690240085\n",
      "Training: epoch 91 batch 0 loss 0.0131727559491992\n",
      "Training: epoch 91 batch 10 loss 0.0033724214881658554\n",
      "Training: epoch 91 batch 20 loss 0.009049007669091225\n",
      "Test: epoch 91 batch 0 loss 0.013258932158350945\n",
      "epoch 91 finished - avarage train loss 0.009651667045043975  avarage test loss 0.013796197483316064\n",
      "Training: epoch 92 batch 0 loss 0.019101500511169434\n",
      "Training: epoch 92 batch 10 loss 0.0030958617571741343\n",
      "Training: epoch 92 batch 20 loss 0.0051512098871171474\n",
      "Test: epoch 92 batch 0 loss 0.014024554751813412\n",
      "epoch 92 finished - avarage train loss 0.009605089595926732  avarage test loss 0.015122707467526197\n",
      "Training: epoch 93 batch 0 loss 0.0029033601749688387\n",
      "Training: epoch 93 batch 10 loss 0.0077983541414141655\n",
      "Training: epoch 93 batch 20 loss 0.004962562117725611\n",
      "Test: epoch 93 batch 0 loss 0.012152839452028275\n",
      "epoch 93 finished - avarage train loss 0.0065288708127778155  avarage test loss 0.012929274118505418\n",
      "Training: epoch 94 batch 0 loss 0.008025667630136013\n",
      "Training: epoch 94 batch 10 loss 0.008159749209880829\n",
      "Training: epoch 94 batch 20 loss 0.00461537903174758\n",
      "Test: epoch 94 batch 0 loss 0.013005632907152176\n",
      "epoch 94 finished - avarage train loss 0.007656525820493698  avarage test loss 0.016480362508445978\n",
      "Training: epoch 95 batch 0 loss 0.02939431555569172\n",
      "Training: epoch 95 batch 10 loss 0.008101138286292553\n",
      "Training: epoch 95 batch 20 loss 0.008277395740151405\n",
      "Test: epoch 95 batch 0 loss 0.013565325178205967\n",
      "epoch 95 finished - avarage train loss 0.01118999555835436  avarage test loss 0.014761073747649789\n",
      "Training: epoch 96 batch 0 loss 0.006583912298083305\n",
      "Training: epoch 96 batch 10 loss 0.0034496174193918705\n",
      "Training: epoch 96 batch 20 loss 0.007612170185893774\n",
      "Test: epoch 96 batch 0 loss 0.01380104385316372\n",
      "epoch 96 finished - avarage train loss 0.006364960311751427  avarage test loss 0.014934080420061946\n",
      "Training: epoch 97 batch 0 loss 0.002931387862190604\n",
      "Training: epoch 97 batch 10 loss 0.0032393510919064283\n",
      "Training: epoch 97 batch 20 loss 0.005543198902159929\n",
      "Test: epoch 97 batch 0 loss 0.013226349838078022\n",
      "epoch 97 finished - avarage train loss 0.00817270855547796  avarage test loss 0.013903822167776525\n",
      "Training: epoch 98 batch 0 loss 0.0030481060966849327\n",
      "Training: epoch 98 batch 10 loss 0.005303399171680212\n",
      "Training: epoch 98 batch 20 loss 0.0037601925432682037\n",
      "Test: epoch 98 batch 0 loss 0.010868211276829243\n",
      "epoch 98 finished - avarage train loss 0.006817854716089265  avarage test loss 0.012486266321502626\n",
      "Training: epoch 99 batch 0 loss 0.006548223085701466\n",
      "Training: epoch 99 batch 10 loss 0.003856149036437273\n",
      "Training: epoch 99 batch 20 loss 0.0037634496111422777\n",
      "Test: epoch 99 batch 0 loss 0.018414871767163277\n",
      "epoch 99 finished - avarage train loss 0.007137498898624346  avarage test loss 0.019459953997284174\n",
      "Training: epoch 100 batch 0 loss 0.011050403118133545\n",
      "Training: epoch 100 batch 10 loss 0.010671250522136688\n",
      "Training: epoch 100 batch 20 loss 0.007266262546181679\n",
      "Test: epoch 100 batch 0 loss 0.017903964966535568\n",
      "epoch 100 finished - avarage train loss 0.007793406747153093  avarage test loss 0.020664942683652043\n",
      "Training: epoch 101 batch 0 loss 0.0075862472876906395\n",
      "Training: epoch 101 batch 10 loss 0.009836601093411446\n",
      "Training: epoch 101 batch 20 loss 0.007612253073602915\n",
      "Test: epoch 101 batch 0 loss 0.010856169275939465\n",
      "epoch 101 finished - avarage train loss 0.008969589891233322  avarage test loss 0.012409907416440547\n",
      "Training: epoch 102 batch 0 loss 0.007075141184031963\n",
      "Training: epoch 102 batch 10 loss 0.003721553133800626\n",
      "Training: epoch 102 batch 20 loss 0.006465064361691475\n",
      "Test: epoch 102 batch 0 loss 0.011135941371321678\n",
      "epoch 102 finished - avarage train loss 0.005899256689409758  avarage test loss 0.012566497549414635\n",
      "Training: epoch 103 batch 0 loss 0.003936344292014837\n",
      "Training: epoch 103 batch 10 loss 0.008175436407327652\n",
      "Training: epoch 103 batch 20 loss 0.005674581974744797\n",
      "Test: epoch 103 batch 0 loss 0.011911069974303246\n",
      "epoch 103 finished - avarage train loss 0.006826279592571844  avarage test loss 0.01273426041007042\n",
      "Training: epoch 104 batch 0 loss 0.004294842015951872\n",
      "Training: epoch 104 batch 10 loss 0.00476584630087018\n",
      "Training: epoch 104 batch 20 loss 0.004859325475990772\n",
      "Test: epoch 104 batch 0 loss 0.011115619912743568\n",
      "epoch 104 finished - avarage train loss 0.007559294511307159  avarage test loss 0.012978061044123024\n",
      "Training: epoch 105 batch 0 loss 0.003041430376470089\n",
      "Training: epoch 105 batch 10 loss 0.009461459703743458\n",
      "Training: epoch 105 batch 20 loss 0.0077198948711156845\n",
      "Test: epoch 105 batch 0 loss 0.01363394595682621\n",
      "epoch 105 finished - avarage train loss 0.006453949171279011  avarage test loss 0.013924434315413237\n",
      "Training: epoch 106 batch 0 loss 0.0028530065901577473\n",
      "Training: epoch 106 batch 10 loss 0.01137502770870924\n",
      "Training: epoch 106 batch 20 loss 0.002654820214956999\n",
      "Test: epoch 106 batch 0 loss 0.011868637055158615\n",
      "epoch 106 finished - avarage train loss 0.007437084905867432  avarage test loss 0.012531570391729474\n",
      "Training: epoch 107 batch 0 loss 0.003438170999288559\n",
      "Training: epoch 107 batch 10 loss 0.004670531023293734\n",
      "Training: epoch 107 batch 20 loss 0.00437319977208972\n",
      "Test: epoch 107 batch 0 loss 0.013839631341397762\n",
      "epoch 107 finished - avarage train loss 0.006583762387263364  avarage test loss 0.014338205801323056\n",
      "Training: epoch 108 batch 0 loss 0.0083243940025568\n",
      "Training: epoch 108 batch 10 loss 0.005462393164634705\n",
      "Training: epoch 108 batch 20 loss 0.005855001043528318\n",
      "Test: epoch 108 batch 0 loss 0.01213730126619339\n",
      "epoch 108 finished - avarage train loss 0.00749645886364682  avarage test loss 0.01291378156747669\n",
      "Training: epoch 109 batch 0 loss 0.00572355929762125\n",
      "Training: epoch 109 batch 10 loss 0.0033780562225729227\n",
      "Training: epoch 109 batch 20 loss 0.003237444208934903\n",
      "Test: epoch 109 batch 0 loss 0.013705684803426266\n",
      "epoch 109 finished - avarage train loss 0.005960194430121317  avarage test loss 0.015382871381007135\n",
      "Training: epoch 110 batch 0 loss 0.005925164557993412\n",
      "Training: epoch 110 batch 10 loss 0.010841747745871544\n",
      "Training: epoch 110 batch 20 loss 0.0048653921112418175\n",
      "Test: epoch 110 batch 0 loss 0.013300972059369087\n",
      "epoch 110 finished - avarage train loss 0.006732962547865664  avarage test loss 0.014241554425098002\n",
      "Training: epoch 111 batch 0 loss 0.002165318001061678\n",
      "Training: epoch 111 batch 10 loss 0.007202272303402424\n",
      "Training: epoch 111 batch 20 loss 0.0057016098871827126\n",
      "Test: epoch 111 batch 0 loss 0.012891007587313652\n",
      "epoch 111 finished - avarage train loss 0.007095360935762011  avarage test loss 0.012926509021781385\n",
      "Training: epoch 112 batch 0 loss 0.004141142126172781\n",
      "Training: epoch 112 batch 10 loss 0.005244940519332886\n",
      "Training: epoch 112 batch 20 loss 0.0031119694467633963\n",
      "Test: epoch 112 batch 0 loss 0.011484031565487385\n",
      "epoch 112 finished - avarage train loss 0.006220284345607948  avarage test loss 0.012752917828038335\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training: epoch 113 batch 0 loss 0.0041039916686713696\n",
      "Training: epoch 113 batch 10 loss 0.006411363370716572\n",
      "Training: epoch 113 batch 20 loss 0.005824856925755739\n",
      "Test: epoch 113 batch 0 loss 0.01238759234547615\n",
      "epoch 113 finished - avarage train loss 0.007127979103925413  avarage test loss 0.012712347088381648\n",
      "Training: epoch 114 batch 0 loss 0.0028280201368033886\n",
      "Training: epoch 114 batch 10 loss 0.006939617916941643\n",
      "Training: epoch 114 batch 20 loss 0.005117807071655989\n",
      "Test: epoch 114 batch 0 loss 0.018480738624930382\n",
      "epoch 114 finished - avarage train loss 0.007562965973569402  avarage test loss 0.020509739872068167\n",
      "Training: epoch 115 batch 0 loss 0.0067306989803910255\n",
      "Training: epoch 115 batch 10 loss 0.00830756314098835\n",
      "Training: epoch 115 batch 20 loss 0.0032643515150994062\n",
      "Test: epoch 115 batch 0 loss 0.01323883980512619\n",
      "epoch 115 finished - avarage train loss 0.007554991989685544  avarage test loss 0.014428276335820556\n",
      "Training: epoch 116 batch 0 loss 0.0020256054122000933\n",
      "Training: epoch 116 batch 10 loss 0.00466958386823535\n",
      "Training: epoch 116 batch 20 loss 0.009083077311515808\n",
      "Test: epoch 116 batch 0 loss 0.011697736568748951\n",
      "epoch 116 finished - avarage train loss 0.005524604364523086  avarage test loss 0.013442079885862768\n",
      "Training: epoch 117 batch 0 loss 0.003442521672695875\n",
      "Training: epoch 117 batch 10 loss 0.006032806821167469\n",
      "Training: epoch 117 batch 20 loss 0.003487290581688285\n",
      "Test: epoch 117 batch 0 loss 0.010095151141285896\n",
      "epoch 117 finished - avarage train loss 0.007015895724681945  avarage test loss 0.013976192567497492\n",
      "Training: epoch 118 batch 0 loss 0.011094833724200726\n",
      "Training: epoch 118 batch 10 loss 0.0068581597879529\n",
      "Training: epoch 118 batch 20 loss 0.008871183730661869\n",
      "Test: epoch 118 batch 0 loss 0.017811276018619537\n",
      "epoch 118 finished - avarage train loss 0.00954497204531498  avarage test loss 0.022045976715162396\n",
      "Training: epoch 119 batch 0 loss 0.011727781035006046\n",
      "Training: epoch 119 batch 10 loss 0.006376929115504026\n",
      "Training: epoch 119 batch 20 loss 0.005788515321910381\n",
      "Test: epoch 119 batch 0 loss 0.012182649224996567\n",
      "epoch 119 finished - avarage train loss 0.008868028979812717  avarage test loss 0.013319780817255378\n",
      "Training: epoch 120 batch 0 loss 0.004390302579849958\n",
      "Training: epoch 120 batch 10 loss 0.011008385568857193\n",
      "Training: epoch 120 batch 20 loss 0.004030326846987009\n",
      "Test: epoch 120 batch 0 loss 0.012516283430159092\n",
      "epoch 120 finished - avarage train loss 0.0070635862407627805  avarage test loss 0.014427307294681668\n",
      "Training: epoch 121 batch 0 loss 0.008558112196624279\n",
      "Training: epoch 121 batch 10 loss 0.0024504957254976034\n",
      "Training: epoch 121 batch 20 loss 0.004725010599941015\n",
      "Test: epoch 121 batch 0 loss 0.011275081895291805\n",
      "epoch 121 finished - avarage train loss 0.007581508862946568  avarage test loss 0.01233372138813138\n",
      "Training: epoch 122 batch 0 loss 0.004223847296088934\n",
      "Training: epoch 122 batch 10 loss 0.004158441908657551\n",
      "Training: epoch 122 batch 20 loss 0.004185808822512627\n",
      "Test: epoch 122 batch 0 loss 0.012664051726460457\n",
      "epoch 122 finished - avarage train loss 0.006009448456160468  avarage test loss 0.01386344211641699\n",
      "Training: epoch 123 batch 0 loss 0.0068086604587733746\n",
      "Training: epoch 123 batch 10 loss 0.007442300207912922\n",
      "Training: epoch 123 batch 20 loss 0.010219455696642399\n",
      "Test: epoch 123 batch 0 loss 0.016133660450577736\n",
      "epoch 123 finished - avarage train loss 0.008696635679245508  avarage test loss 0.017905479297041893\n",
      "Training: epoch 124 batch 0 loss 0.011483441106975079\n",
      "Training: epoch 124 batch 10 loss 0.00781272817403078\n",
      "Training: epoch 124 batch 20 loss 0.008562149479985237\n",
      "Test: epoch 124 batch 0 loss 0.014798908494412899\n",
      "epoch 124 finished - avarage train loss 0.011130003467330644  avarage test loss 0.017353161703795195\n",
      "Training: epoch 125 batch 0 loss 0.003981424495577812\n",
      "Training: epoch 125 batch 10 loss 0.020908283069729805\n",
      "Training: epoch 125 batch 20 loss 0.003273992333561182\n",
      "Test: epoch 125 batch 0 loss 0.011682100594043732\n",
      "epoch 125 finished - avarage train loss 0.008119490928947926  avarage test loss 0.012598774512298405\n",
      "Training: epoch 126 batch 0 loss 0.007555906195193529\n",
      "Training: epoch 126 batch 10 loss 0.0033764857798814774\n",
      "Training: epoch 126 batch 20 loss 0.005851853173226118\n",
      "Test: epoch 126 batch 0 loss 0.01365118008106947\n",
      "epoch 126 finished - avarage train loss 0.007662720689228897  avarage test loss 0.014183186227455735\n",
      "Training: epoch 127 batch 0 loss 0.00998435914516449\n",
      "Training: epoch 127 batch 10 loss 0.006395380944013596\n",
      "Training: epoch 127 batch 20 loss 0.009928381070494652\n",
      "Test: epoch 127 batch 0 loss 0.011310442350804806\n",
      "epoch 127 finished - avarage train loss 0.008407888789500656  avarage test loss 0.013751871709246188\n",
      "Training: epoch 128 batch 0 loss 0.0034897360019385815\n",
      "Training: epoch 128 batch 10 loss 0.00403591338545084\n",
      "Training: epoch 128 batch 20 loss 0.004979107528924942\n",
      "Test: epoch 128 batch 0 loss 0.013300242833793163\n",
      "epoch 128 finished - avarage train loss 0.006286798478585893  avarage test loss 0.015909387497231364\n",
      "Training: epoch 129 batch 0 loss 0.008312457241117954\n",
      "Training: epoch 129 batch 10 loss 0.0067544239573180676\n",
      "Training: epoch 129 batch 20 loss 0.005428997799754143\n",
      "Test: epoch 129 batch 0 loss 0.010370893403887749\n",
      "epoch 129 finished - avarage train loss 0.006950772921394171  avarage test loss 0.013723001931793988\n",
      "Training: epoch 130 batch 0 loss 0.00358979357406497\n",
      "Training: epoch 130 batch 10 loss 0.009743906557559967\n",
      "Training: epoch 130 batch 20 loss 0.004614683799445629\n",
      "Test: epoch 130 batch 0 loss 0.014603161253035069\n",
      "epoch 130 finished - avarage train loss 0.008039051421982205  avarage test loss 0.01441992912441492\n",
      "Training: epoch 131 batch 0 loss 0.006176875904202461\n",
      "Training: epoch 131 batch 10 loss 0.006718076765537262\n",
      "Training: epoch 131 batch 20 loss 0.0008373098680749536\n",
      "Test: epoch 131 batch 0 loss 0.0136262783780694\n",
      "epoch 131 finished - avarage train loss 0.007867400466207543  avarage test loss 0.015424763201735914\n",
      "Training: epoch 132 batch 0 loss 0.008173706009984016\n",
      "Training: epoch 132 batch 10 loss 0.003936042543500662\n",
      "Training: epoch 132 batch 20 loss 0.002490484621375799\n",
      "Test: epoch 132 batch 0 loss 0.012203153222799301\n",
      "epoch 132 finished - avarage train loss 0.006077741634036447  avarage test loss 0.013106747530400753\n",
      "Training: epoch 133 batch 0 loss 0.003310680389404297\n",
      "Training: epoch 133 batch 10 loss 0.01299006212502718\n",
      "Training: epoch 133 batch 20 loss 0.005337893031537533\n",
      "Test: epoch 133 batch 0 loss 0.01196476723998785\n",
      "epoch 133 finished - avarage train loss 0.007146679531333262  avarage test loss 0.012456738157197833\n",
      "Training: epoch 134 batch 0 loss 0.005963990930467844\n",
      "Training: epoch 134 batch 10 loss 0.005643654614686966\n",
      "Training: epoch 134 batch 20 loss 0.0039687990210950375\n",
      "Test: epoch 134 batch 0 loss 0.011031270027160645\n",
      "epoch 134 finished - avarage train loss 0.008187098203804987  avarage test loss 0.01213765551801771\n",
      "Training: epoch 135 batch 0 loss 0.005043890792876482\n",
      "Training: epoch 135 batch 10 loss 0.002663557417690754\n",
      "Training: epoch 135 batch 20 loss 0.00538070872426033\n",
      "Test: epoch 135 batch 0 loss 0.010647088289260864\n",
      "epoch 135 finished - avarage train loss 0.00724303385981455  avarage test loss 0.01385376078542322\n",
      "Training: epoch 136 batch 0 loss 0.005396530032157898\n",
      "Training: epoch 136 batch 10 loss 0.009531624615192413\n",
      "Training: epoch 136 batch 20 loss 0.006584271788597107\n",
      "Test: epoch 136 batch 0 loss 0.014882315881550312\n",
      "epoch 136 finished - avarage train loss 0.01067169117420141  avarage test loss 0.016401559114456177\n",
      "Training: epoch 137 batch 0 loss 0.008356832899153233\n",
      "Training: epoch 137 batch 10 loss 0.0033635052386671305\n",
      "Training: epoch 137 batch 20 loss 0.007035513408482075\n",
      "Test: epoch 137 batch 0 loss 0.013080289587378502\n",
      "epoch 137 finished - avarage train loss 0.007927531882285559  avarage test loss 0.014761689933948219\n",
      "Training: epoch 138 batch 0 loss 0.009502769447863102\n",
      "Training: epoch 138 batch 10 loss 0.0018711391603574157\n",
      "Training: epoch 138 batch 20 loss 0.00809677317738533\n",
      "Test: epoch 138 batch 0 loss 0.013727206736803055\n",
      "epoch 138 finished - avarage train loss 0.007813774976590327  avarage test loss 0.01625520922243595\n",
      "Training: epoch 139 batch 0 loss 0.0038788050878793\n",
      "Training: epoch 139 batch 10 loss 0.01713497005403042\n",
      "Training: epoch 139 batch 20 loss 0.004888307303190231\n",
      "Test: epoch 139 batch 0 loss 0.009205842390656471\n",
      "epoch 139 finished - avarage train loss 0.010066322227618817  avarage test loss 0.011504292255267501\n",
      "Training: epoch 140 batch 0 loss 0.0012620420893654227\n",
      "Training: epoch 140 batch 10 loss 0.00550243491306901\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training: epoch 140 batch 20 loss 0.00987164955586195\n",
      "Test: epoch 140 batch 0 loss 0.012072830460965633\n",
      "epoch 140 finished - avarage train loss 0.007387242890123664  avarage test loss 0.015736871398985386\n",
      "Training: epoch 141 batch 0 loss 0.009395578876137733\n",
      "Training: epoch 141 batch 10 loss 0.004468861967325211\n",
      "Training: epoch 141 batch 20 loss 0.0052026864141225815\n",
      "Test: epoch 141 batch 0 loss 0.010504404082894325\n",
      "epoch 141 finished - avarage train loss 0.00672138923104724  avarage test loss 0.01202121318783611\n",
      "Training: epoch 142 batch 0 loss 0.005109845660626888\n",
      "Training: epoch 142 batch 10 loss 0.003385756863281131\n",
      "Training: epoch 142 batch 20 loss 0.00788489542901516\n",
      "Test: epoch 142 batch 0 loss 0.015933707356452942\n",
      "epoch 142 finished - avarage train loss 0.007399080650367099  avarage test loss 0.017790414858609438\n",
      "Training: epoch 143 batch 0 loss 0.017061816528439522\n",
      "Training: epoch 143 batch 10 loss 0.011071712709963322\n",
      "Training: epoch 143 batch 20 loss 0.005948296748101711\n",
      "Test: epoch 143 batch 0 loss 0.020513325929641724\n",
      "epoch 143 finished - avarage train loss 0.01023101096908594  avarage test loss 0.021911163115873933\n",
      "Training: epoch 144 batch 0 loss 0.01311639603227377\n",
      "Training: epoch 144 batch 10 loss 0.011967994272708893\n",
      "Training: epoch 144 batch 20 loss 0.008434019051492214\n",
      "Test: epoch 144 batch 0 loss 0.013076256029307842\n",
      "epoch 144 finished - avarage train loss 0.010955540712216291  avarage test loss 0.014547268627211452\n",
      "Training: epoch 145 batch 0 loss 0.005789489485323429\n",
      "Training: epoch 145 batch 10 loss 0.003912295680493116\n",
      "Training: epoch 145 batch 20 loss 0.007780576590448618\n",
      "Test: epoch 145 batch 0 loss 0.010392588563263416\n",
      "epoch 145 finished - avarage train loss 0.007948101401842874  avarage test loss 0.012026415322907269\n",
      "Training: epoch 146 batch 0 loss 0.004398555029183626\n",
      "Training: epoch 146 batch 10 loss 0.014550415799021721\n",
      "Training: epoch 146 batch 20 loss 0.004583342466503382\n",
      "Test: epoch 146 batch 0 loss 0.010710892267525196\n",
      "epoch 146 finished - avarage train loss 0.006139966578961447  avarage test loss 0.012176871648989618\n",
      "Training: epoch 147 batch 0 loss 0.0036876415833830833\n",
      "Training: epoch 147 batch 10 loss 0.006351569667458534\n",
      "Training: epoch 147 batch 20 loss 0.004326918628066778\n",
      "Test: epoch 147 batch 0 loss 0.010095386765897274\n",
      "epoch 147 finished - avarage train loss 0.00691371041366124  avarage test loss 0.012790557113476098\n",
      "Training: epoch 148 batch 0 loss 0.006534426007419825\n",
      "Training: epoch 148 batch 10 loss 0.0033340242225676775\n",
      "Training: epoch 148 batch 20 loss 0.0047765797935426235\n",
      "Test: epoch 148 batch 0 loss 0.010840224102139473\n",
      "epoch 148 finished - avarage train loss 0.0068735908315484895  avarage test loss 0.012233452522195876\n",
      "Training: epoch 149 batch 0 loss 0.005949798040091991\n",
      "Training: epoch 149 batch 10 loss 0.003834928385913372\n",
      "Training: epoch 149 batch 20 loss 0.0037807198241353035\n",
      "Test: epoch 149 batch 0 loss 0.013370394706726074\n",
      "epoch 149 finished - avarage train loss 0.006319401729530816  avarage test loss 0.013849096489138901\n",
      "Training: epoch 150 batch 0 loss 0.005600962322205305\n",
      "Training: epoch 150 batch 10 loss 0.005223492626100779\n",
      "Training: epoch 150 batch 20 loss 0.0026473619509488344\n",
      "Test: epoch 150 batch 0 loss 0.013278172351419926\n",
      "epoch 150 finished - avarage train loss 0.006444330351298739  avarage test loss 0.01532062015030533\n",
      "Training: epoch 151 batch 0 loss 0.006031632889062166\n",
      "Training: epoch 151 batch 10 loss 0.007641983218491077\n",
      "Training: epoch 151 batch 20 loss 0.006277935579419136\n",
      "Test: epoch 151 batch 0 loss 0.009735656902194023\n",
      "epoch 151 finished - avarage train loss 0.008224512506597515  avarage test loss 0.01262705447152257\n",
      "Training: epoch 152 batch 0 loss 0.0029114834032952785\n",
      "Training: epoch 152 batch 10 loss 0.005004906561225653\n",
      "Training: epoch 152 batch 20 loss 0.007435129024088383\n",
      "Test: epoch 152 batch 0 loss 0.009806143119931221\n",
      "epoch 152 finished - avarage train loss 0.006166727795940021  avarage test loss 0.012405796675011516\n",
      "Training: epoch 153 batch 0 loss 0.0035895344335585833\n",
      "Training: epoch 153 batch 10 loss 0.004911948926746845\n",
      "Training: epoch 153 batch 20 loss 0.006204833276569843\n",
      "Test: epoch 153 batch 0 loss 0.010583363473415375\n",
      "epoch 153 finished - avarage train loss 0.007410514425357868  avarage test loss 0.014609732083044946\n",
      "Training: epoch 154 batch 0 loss 0.006713801063597202\n",
      "Training: epoch 154 batch 10 loss 0.008348864503204823\n",
      "Training: epoch 154 batch 20 loss 0.0043566590175032616\n",
      "Test: epoch 154 batch 0 loss 0.014399378560483456\n",
      "epoch 154 finished - avarage train loss 0.007612892699524246  avarage test loss 0.018151398515328765\n",
      "Training: epoch 155 batch 0 loss 0.007086297497153282\n",
      "Training: epoch 155 batch 10 loss 0.010127290152013302\n",
      "Training: epoch 155 batch 20 loss 0.002958640456199646\n",
      "Test: epoch 155 batch 0 loss 0.012746360152959824\n",
      "epoch 155 finished - avarage train loss 0.007881077865524024  avarage test loss 0.014687981456518173\n",
      "Training: epoch 156 batch 0 loss 0.003069996600970626\n",
      "Training: epoch 156 batch 10 loss 0.0022969061974436045\n",
      "Training: epoch 156 batch 20 loss 0.005167155992239714\n",
      "Test: epoch 156 batch 0 loss 0.013064712285995483\n",
      "epoch 156 finished - avarage train loss 0.0065489703596665946  avarage test loss 0.014872085070237517\n",
      "Training: epoch 157 batch 0 loss 0.009821813553571701\n",
      "Training: epoch 157 batch 10 loss 0.004334155935794115\n",
      "Training: epoch 157 batch 20 loss 0.005216922145336866\n",
      "Test: epoch 157 batch 0 loss 0.009375734254717827\n",
      "epoch 157 finished - avarage train loss 0.0052157173069707794  avarage test loss 0.011343216523528099\n",
      "Training: epoch 158 batch 0 loss 0.012605411931872368\n",
      "Training: epoch 158 batch 10 loss 0.0022131751757115126\n",
      "Training: epoch 158 batch 20 loss 0.003461122279986739\n",
      "Test: epoch 158 batch 0 loss 0.011770883575081825\n",
      "epoch 158 finished - avarage train loss 0.006818741865070729  avarage test loss 0.012890600017271936\n",
      "Training: epoch 159 batch 0 loss 0.0033560560550540686\n",
      "Training: epoch 159 batch 10 loss 0.0048094261437654495\n",
      "Training: epoch 159 batch 20 loss 0.005398667883127928\n",
      "Test: epoch 159 batch 0 loss 0.011908693239092827\n",
      "epoch 159 finished - avarage train loss 0.007139437261904622  avarage test loss 0.01363851549103856\n",
      "Training: epoch 160 batch 0 loss 0.0068503012880682945\n",
      "Training: epoch 160 batch 10 loss 0.013784618116915226\n",
      "Training: epoch 160 batch 20 loss 0.0064929453656077385\n",
      "Test: epoch 160 batch 0 loss 0.014947042800486088\n",
      "epoch 160 finished - avarage train loss 0.007475691574529327  avarage test loss 0.018280946649610996\n",
      "Training: epoch 161 batch 0 loss 0.004965521860867739\n",
      "Training: epoch 161 batch 10 loss 0.005469867959618568\n",
      "Training: epoch 161 batch 20 loss 0.0069775828160345554\n",
      "Test: epoch 161 batch 0 loss 0.011158648878335953\n",
      "epoch 161 finished - avarage train loss 0.007517856609975469  avarage test loss 0.012104405439458787\n",
      "Training: epoch 162 batch 0 loss 0.009432240389287472\n",
      "Training: epoch 162 batch 10 loss 0.002499837428331375\n",
      "Training: epoch 162 batch 20 loss 0.004169802647083998\n",
      "Test: epoch 162 batch 0 loss 0.012985640205442905\n",
      "epoch 162 finished - avarage train loss 0.007231189307339233  avarage test loss 0.01500454789493233\n",
      "Training: epoch 163 batch 0 loss 0.006753167137503624\n",
      "Training: epoch 163 batch 10 loss 0.005057804752141237\n",
      "Training: epoch 163 batch 20 loss 0.004531772807240486\n",
      "Test: epoch 163 batch 0 loss 0.01158433873206377\n",
      "epoch 163 finished - avarage train loss 0.007117140837463325  avarage test loss 0.013365293038077652\n",
      "Training: epoch 164 batch 0 loss 0.00495825195685029\n",
      "Training: epoch 164 batch 10 loss 0.004878469742834568\n",
      "Training: epoch 164 batch 20 loss 0.006845100782811642\n",
      "Test: epoch 164 batch 0 loss 0.01169385015964508\n",
      "epoch 164 finished - avarage train loss 0.0053784772753715515  avarage test loss 0.013907430227845907\n",
      "Training: epoch 165 batch 0 loss 0.005242864601314068\n",
      "Training: epoch 165 batch 10 loss 0.003611866384744644\n",
      "Training: epoch 165 batch 20 loss 0.0013781188754364848\n",
      "Test: epoch 165 batch 0 loss 0.010258959606289864\n",
      "epoch 165 finished - avarage train loss 0.007534686239147237  avarage test loss 0.011768587515689433\n",
      "Training: epoch 166 batch 0 loss 0.0034501084592193365\n",
      "Training: epoch 166 batch 10 loss 0.00451869610697031\n",
      "Training: epoch 166 batch 20 loss 0.005954677704721689\n",
      "Test: epoch 166 batch 0 loss 0.010469852946698666\n",
      "epoch 166 finished - avarage train loss 0.007237993421611087  avarage test loss 0.012148253503255546\n",
      "Training: epoch 167 batch 0 loss 0.0076217553578317165\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training: epoch 167 batch 10 loss 0.0045157079584896564\n",
      "Training: epoch 167 batch 20 loss 0.002675620373338461\n",
      "Test: epoch 167 batch 0 loss 0.010195367969572544\n",
      "epoch 167 finished - avarage train loss 0.007077659680989796  avarage test loss 0.011787737719714642\n",
      "Training: epoch 168 batch 0 loss 0.00628857035189867\n",
      "Training: epoch 168 batch 10 loss 0.004113056231290102\n",
      "Training: epoch 168 batch 20 loss 0.007639338728040457\n",
      "Test: epoch 168 batch 0 loss 0.00974243599921465\n",
      "epoch 168 finished - avarage train loss 0.006471772280778608  avarage test loss 0.011528457864187658\n",
      "Training: epoch 169 batch 0 loss 0.008960077539086342\n",
      "Training: epoch 169 batch 10 loss 0.003751874202862382\n",
      "Training: epoch 169 batch 20 loss 0.009626785293221474\n",
      "Test: epoch 169 batch 0 loss 0.01029476523399353\n",
      "epoch 169 finished - avarage train loss 0.005997399233506414  avarage test loss 0.011745259747840464\n",
      "Training: epoch 170 batch 0 loss 0.004674610681831837\n",
      "Training: epoch 170 batch 10 loss 0.0038667689077556133\n",
      "Training: epoch 170 batch 20 loss 0.003789641661569476\n",
      "Test: epoch 170 batch 0 loss 0.012159617617726326\n",
      "epoch 170 finished - avarage train loss 0.0058342039842030095  avarage test loss 0.014986336813308299\n",
      "Training: epoch 171 batch 0 loss 0.003501203376799822\n",
      "Training: epoch 171 batch 10 loss 0.0024336171336472034\n",
      "Training: epoch 171 batch 20 loss 0.003663374111056328\n",
      "Test: epoch 171 batch 0 loss 0.011096318252384663\n",
      "epoch 171 finished - avarage train loss 0.006300886346284172  avarage test loss 0.013033906230702996\n",
      "Training: epoch 172 batch 0 loss 0.008048954419791698\n",
      "Training: epoch 172 batch 10 loss 0.0025601405650377274\n",
      "Training: epoch 172 batch 20 loss 0.004728642292320728\n",
      "Test: epoch 172 batch 0 loss 0.010419356636703014\n",
      "epoch 172 finished - avarage train loss 0.007496284361093722  avarage test loss 0.011528248665854335\n",
      "Training: epoch 173 batch 0 loss 0.003918810747563839\n",
      "Training: epoch 173 batch 10 loss 0.005543107632547617\n",
      "Training: epoch 173 batch 20 loss 0.007474047131836414\n",
      "Test: epoch 173 batch 0 loss 0.011353034526109695\n",
      "epoch 173 finished - avarage train loss 0.008049061417515421  avarage test loss 0.013329186593182385\n",
      "Training: epoch 174 batch 0 loss 0.005717664025723934\n",
      "Training: epoch 174 batch 10 loss 0.0065939160995185375\n",
      "Training: epoch 174 batch 20 loss 0.00694475369527936\n",
      "Test: epoch 174 batch 0 loss 0.01554802805185318\n",
      "epoch 174 finished - avarage train loss 0.006747342557806907  avarage test loss 0.01994287734851241\n",
      "Training: epoch 175 batch 0 loss 0.009048514068126678\n",
      "Training: epoch 175 batch 10 loss 0.021562868729233742\n",
      "Training: epoch 175 batch 20 loss 0.011747063137590885\n",
      "Test: epoch 175 batch 0 loss 0.008483871817588806\n",
      "epoch 175 finished - avarage train loss 0.013704114625680035  avarage test loss 0.013059878372587264\n",
      "Training: epoch 176 batch 0 loss 0.008665574714541435\n",
      "Training: epoch 176 batch 10 loss 0.012819007970392704\n",
      "Training: epoch 176 batch 20 loss 0.005662822164595127\n",
      "Test: epoch 176 batch 0 loss 0.02250676043331623\n",
      "epoch 176 finished - avarage train loss 0.01232723112956717  avarage test loss 0.021490344777703285\n",
      "Training: epoch 177 batch 0 loss 0.011647717095911503\n",
      "Training: epoch 177 batch 10 loss 0.006558769382536411\n",
      "Training: epoch 177 batch 20 loss 0.006946138571947813\n",
      "Test: epoch 177 batch 0 loss 0.008036520332098007\n",
      "epoch 177 finished - avarage train loss 0.009302881345602459  avarage test loss 0.013087306404486299\n",
      "Training: epoch 178 batch 0 loss 0.009474514052271843\n",
      "Training: epoch 178 batch 10 loss 0.007002538535743952\n",
      "Training: epoch 178 batch 20 loss 0.0067992075346410275\n",
      "Test: epoch 178 batch 0 loss 0.007740647066384554\n",
      "epoch 178 finished - avarage train loss 0.008260856872296026  avarage test loss 0.014333731262013316\n",
      "Training: epoch 179 batch 0 loss 0.010385203175246716\n",
      "Training: epoch 179 batch 10 loss 0.0065390621311962605\n",
      "Training: epoch 179 batch 20 loss 0.005917259491980076\n",
      "Test: epoch 179 batch 0 loss 0.007248200010508299\n",
      "epoch 179 finished - avarage train loss 0.008747618902346184  avarage test loss 0.012382089742459357\n",
      "Training: epoch 180 batch 0 loss 0.003486533183604479\n",
      "Training: epoch 180 batch 10 loss 0.0052468921057879925\n",
      "Training: epoch 180 batch 20 loss 0.003640285227447748\n",
      "Test: epoch 180 batch 0 loss 0.01002262532711029\n",
      "epoch 180 finished - avarage train loss 0.006640747045006217  avarage test loss 0.01252882566768676\n",
      "Training: epoch 181 batch 0 loss 0.0074477107264101505\n",
      "Training: epoch 181 batch 10 loss 0.005798333324491978\n",
      "Training: epoch 181 batch 20 loss 0.005010111257433891\n",
      "Test: epoch 181 batch 0 loss 0.007879354991018772\n",
      "epoch 181 finished - avarage train loss 0.005254226656437948  avarage test loss 0.01151652482803911\n",
      "Training: epoch 182 batch 0 loss 0.0071351914666593075\n",
      "Training: epoch 182 batch 10 loss 0.0028143234085291624\n",
      "Training: epoch 182 batch 20 loss 0.0032273426186293364\n",
      "Test: epoch 182 batch 0 loss 0.01094343513250351\n",
      "epoch 182 finished - avarage train loss 0.005361271640767568  avarage test loss 0.01276724983472377\n",
      "Training: epoch 183 batch 0 loss 0.0028166172560304403\n",
      "Training: epoch 183 batch 10 loss 0.01053626835346222\n",
      "Training: epoch 183 batch 20 loss 0.012664739973843098\n",
      "Test: epoch 183 batch 0 loss 0.009550311602652073\n",
      "epoch 183 finished - avarage train loss 0.0075079590194569576  avarage test loss 0.011754547944292426\n",
      "Training: epoch 184 batch 0 loss 0.0038618012331426144\n",
      "Training: epoch 184 batch 10 loss 0.004676350392401218\n",
      "Training: epoch 184 batch 20 loss 0.0069703892804682255\n",
      "Test: epoch 184 batch 0 loss 0.008126351051032543\n",
      "epoch 184 finished - avarage train loss 0.006602157003663737  avarage test loss 0.011737655149772763\n",
      "Training: epoch 185 batch 0 loss 0.003154804464429617\n",
      "Training: epoch 185 batch 10 loss 0.004825893323868513\n",
      "Training: epoch 185 batch 20 loss 0.0027267951518297195\n",
      "Test: epoch 185 batch 0 loss 0.00563362892717123\n",
      "epoch 185 finished - avarage train loss 0.007005536471140282  avarage test loss 0.010931709315627813\n",
      "Training: epoch 186 batch 0 loss 0.006071620620787144\n",
      "Training: epoch 186 batch 10 loss 0.005301169119775295\n",
      "Training: epoch 186 batch 20 loss 0.003249499714002013\n",
      "Test: epoch 186 batch 0 loss 0.012203404679894447\n",
      "epoch 186 finished - avarage train loss 0.007653360889325368  avarage test loss 0.01679817633703351\n",
      "Training: epoch 187 batch 0 loss 0.007570790126919746\n",
      "Training: epoch 187 batch 10 loss 0.015157407149672508\n",
      "Training: epoch 187 batch 20 loss 0.005546103697270155\n",
      "Test: epoch 187 batch 0 loss 0.004728770349174738\n",
      "epoch 187 finished - avarage train loss 0.009428541553367314  avarage test loss 0.011301099439151585\n",
      "Training: epoch 188 batch 0 loss 0.004908844828605652\n",
      "Training: epoch 188 batch 10 loss 0.003043303731828928\n",
      "Training: epoch 188 batch 20 loss 0.007001521065831184\n",
      "Test: epoch 188 batch 0 loss 0.008975106291472912\n",
      "epoch 188 finished - avarage train loss 0.0087419605195715  avarage test loss 0.012514470960013568\n",
      "Training: epoch 189 batch 0 loss 0.012669190764427185\n",
      "Training: epoch 189 batch 10 loss 0.00505648460239172\n",
      "Training: epoch 189 batch 20 loss 0.009609036147594452\n",
      "Test: epoch 189 batch 0 loss 0.012886944226920605\n",
      "epoch 189 finished - avarage train loss 0.008010035059575376  avarage test loss 0.017274398123845458\n",
      "Training: epoch 190 batch 0 loss 0.011394035071134567\n",
      "Training: epoch 190 batch 10 loss 0.016590842977166176\n",
      "Training: epoch 190 batch 20 loss 0.003178216051310301\n",
      "Test: epoch 190 batch 0 loss 0.009662365540862083\n",
      "epoch 190 finished - avarage train loss 0.006795142917765369  avarage test loss 0.012189717614091933\n",
      "Training: epoch 191 batch 0 loss 0.003065634286031127\n",
      "Training: epoch 191 batch 10 loss 0.004827162716537714\n",
      "Training: epoch 191 batch 20 loss 0.0046071638353168964\n",
      "Test: epoch 191 batch 0 loss 0.009905786253511906\n",
      "epoch 191 finished - avarage train loss 0.0065388939632423995  avarage test loss 0.012481047306209803\n",
      "Training: epoch 192 batch 0 loss 0.004839451517909765\n",
      "Training: epoch 192 batch 10 loss 0.0037694163620471954\n",
      "Training: epoch 192 batch 20 loss 0.003679082030430436\n",
      "Test: epoch 192 batch 0 loss 0.013532480224967003\n",
      "epoch 192 finished - avarage train loss 0.006357157626992156  avarage test loss 0.01314848754554987\n",
      "Training: epoch 193 batch 0 loss 0.008841465227305889\n",
      "Training: epoch 193 batch 10 loss 0.0029449292924255133\n",
      "Training: epoch 193 batch 20 loss 0.005603466648608446\n",
      "Test: epoch 193 batch 0 loss 0.011894794180989265\n",
      "epoch 193 finished - avarage train loss 0.006735854420875167  avarage test loss 0.014133443823084235\n",
      "Training: epoch 194 batch 0 loss 0.005952626932412386\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training: epoch 194 batch 10 loss 0.00851221103221178\n",
      "Training: epoch 194 batch 20 loss 0.006459708791226149\n",
      "Test: epoch 194 batch 0 loss 0.010296860709786415\n",
      "epoch 194 finished - avarage train loss 0.007083999069311239  avarage test loss 0.012356493738479912\n",
      "Training: epoch 195 batch 0 loss 0.004453345201909542\n",
      "Training: epoch 195 batch 10 loss 0.01605493389070034\n",
      "Training: epoch 195 batch 20 loss 0.004677032120525837\n",
      "Test: epoch 195 batch 0 loss 0.010454602539539337\n",
      "epoch 195 finished - avarage train loss 0.0076404157463975  avarage test loss 0.012609909637831151\n",
      "Training: epoch 196 batch 0 loss 0.0018471934599801898\n",
      "Training: epoch 196 batch 10 loss 0.007743699476122856\n",
      "Training: epoch 196 batch 20 loss 0.003980130888521671\n",
      "Test: epoch 196 batch 0 loss 0.014120902866125107\n",
      "epoch 196 finished - avarage train loss 0.006568996617088801  avarage test loss 0.016196226584725082\n",
      "Training: epoch 197 batch 0 loss 0.00413249246776104\n",
      "Training: epoch 197 batch 10 loss 0.010738473385572433\n",
      "Training: epoch 197 batch 20 loss 0.004492149688303471\n",
      "Test: epoch 197 batch 0 loss 0.008989519439637661\n",
      "epoch 197 finished - avarage train loss 0.0073747377409118  avarage test loss 0.011260397790465504\n",
      "Training: epoch 198 batch 0 loss 0.002517062472179532\n",
      "Training: epoch 198 batch 10 loss 0.0073744566179811954\n",
      "Training: epoch 198 batch 20 loss 0.0035521185491234064\n",
      "Test: epoch 198 batch 0 loss 0.012231606058776379\n",
      "epoch 198 finished - avarage train loss 0.0062763810575265306  avarage test loss 0.014407295384444296\n",
      "Training: epoch 199 batch 0 loss 0.004819764755666256\n",
      "Training: epoch 199 batch 10 loss 0.006464486941695213\n",
      "Training: epoch 199 batch 20 loss 0.00641976110637188\n",
      "Test: epoch 199 batch 0 loss 0.010833492502570152\n",
      "epoch 199 finished - avarage train loss 0.006893716704357287  avarage test loss 0.012113755336031318\n",
      "Training: epoch 0 batch 0 loss 0.47070956230163574\n",
      "Training: epoch 0 batch 10 loss 0.5552318096160889\n",
      "Training: epoch 0 batch 20 loss 0.5283823609352112\n",
      "Test: epoch 0 batch 0 loss 0.39027923345565796\n",
      "epoch 0 finished - avarage train loss 0.5065059661865234  avarage test loss 0.4405631795525551\n",
      "Training: epoch 1 batch 0 loss 0.39403092861175537\n",
      "Training: epoch 1 batch 10 loss 0.37148699164390564\n",
      "Training: epoch 1 batch 20 loss 0.42750102281570435\n",
      "Test: epoch 1 batch 0 loss 0.40209320187568665\n",
      "epoch 1 finished - avarage train loss 0.5152079627431673  avarage test loss 0.4433434121310711\n",
      "Training: epoch 2 batch 0 loss 0.499289870262146\n",
      "Training: epoch 2 batch 10 loss 0.35650500655174255\n",
      "Training: epoch 2 batch 20 loss 0.40942180156707764\n",
      "Test: epoch 2 batch 0 loss 0.3960120975971222\n",
      "epoch 2 finished - avarage train loss 0.5086519707893503  avarage test loss 0.43838947266340256\n",
      "Training: epoch 3 batch 0 loss 0.5173134207725525\n",
      "Training: epoch 3 batch 10 loss 0.6336879730224609\n",
      "Training: epoch 3 batch 20 loss 0.3195212781429291\n",
      "Test: epoch 3 batch 0 loss 0.39571934938430786\n",
      "epoch 3 finished - avarage train loss 0.5041124080789501  avarage test loss 0.4393039494752884\n",
      "Training: epoch 4 batch 0 loss 0.8266555666923523\n",
      "Training: epoch 4 batch 10 loss 0.39191460609436035\n",
      "Training: epoch 4 batch 20 loss 0.390450656414032\n",
      "Test: epoch 4 batch 0 loss 0.395801305770874\n",
      "epoch 4 finished - avarage train loss 0.528652900251849  avarage test loss 0.43635763600468636\n",
      "Training: epoch 5 batch 0 loss 0.48734524846076965\n",
      "Training: epoch 5 batch 10 loss 0.44510215520858765\n",
      "Training: epoch 5 batch 20 loss 0.38020798563957214\n",
      "Test: epoch 5 batch 0 loss 0.4001598656177521\n",
      "epoch 5 finished - avarage train loss 0.5047643986241571  avarage test loss 0.44302837550640106\n",
      "Training: epoch 6 batch 0 loss 0.4267455041408539\n",
      "Training: epoch 6 batch 10 loss 0.21099744737148285\n",
      "Training: epoch 6 batch 20 loss 0.41522714495658875\n",
      "Test: epoch 6 batch 0 loss 0.39943167567253113\n",
      "epoch 6 finished - avarage train loss 0.5009489213598186  avarage test loss 0.4395972341299057\n",
      "Training: epoch 7 batch 0 loss 0.28272733092308044\n",
      "Training: epoch 7 batch 10 loss 0.6151823401451111\n",
      "Training: epoch 7 batch 20 loss 0.6176593899726868\n",
      "Test: epoch 7 batch 0 loss 0.40231823921203613\n",
      "epoch 7 finished - avarage train loss 0.5044044903640089  avarage test loss 0.4423345997929573\n",
      "Training: epoch 8 batch 0 loss 0.43864455819129944\n",
      "Training: epoch 8 batch 10 loss 0.28689733147621155\n",
      "Training: epoch 8 batch 20 loss 0.2748401165008545\n",
      "Test: epoch 8 batch 0 loss 0.3950463831424713\n",
      "epoch 8 finished - avarage train loss 0.5097453625037752  avarage test loss 0.43868445605039597\n",
      "Training: epoch 9 batch 0 loss 0.34267717599868774\n",
      "Training: epoch 9 batch 10 loss 0.8091216087341309\n",
      "Training: epoch 9 batch 20 loss 0.43207457661628723\n",
      "Test: epoch 9 batch 0 loss 0.39730650186538696\n",
      "epoch 9 finished - avarage train loss 0.5066577761337675  avarage test loss 0.4371165297925472\n",
      "Training: epoch 10 batch 0 loss 0.43737104535102844\n",
      "Training: epoch 10 batch 10 loss 0.6374502182006836\n",
      "Training: epoch 10 batch 20 loss 0.37270060181617737\n",
      "Test: epoch 10 batch 0 loss 0.4233168363571167\n",
      "epoch 10 finished - avarage train loss 0.513501663660181  avarage test loss 0.4671821668744087\n",
      "Training: epoch 11 batch 0 loss 0.6714760661125183\n",
      "Training: epoch 11 batch 10 loss 0.580828845500946\n",
      "Training: epoch 11 batch 20 loss 0.17478041350841522\n",
      "Test: epoch 11 batch 0 loss 0.08254191279411316\n",
      "epoch 11 finished - avarage train loss 0.34361401200294495  avarage test loss 0.09158662334084511\n",
      "Training: epoch 12 batch 0 loss 0.07471805065870285\n",
      "Training: epoch 12 batch 10 loss 0.04986976832151413\n",
      "Training: epoch 12 batch 20 loss 0.05335025489330292\n",
      "Test: epoch 12 batch 0 loss 0.043915972113609314\n",
      "epoch 12 finished - avarage train loss 0.04615245508993494  avarage test loss 0.04522109590470791\n",
      "Training: epoch 13 batch 0 loss 0.026584040373563766\n",
      "Training: epoch 13 batch 10 loss 0.023678487166762352\n",
      "Training: epoch 13 batch 20 loss 0.02600586600601673\n",
      "Test: epoch 13 batch 0 loss 0.036727264523506165\n",
      "epoch 13 finished - avarage train loss 0.03096206884445815  avarage test loss 0.04023475851863623\n",
      "Training: epoch 14 batch 0 loss 0.036820873618125916\n",
      "Training: epoch 14 batch 10 loss 0.014880640432238579\n",
      "Training: epoch 14 batch 20 loss 0.027011115103960037\n",
      "Test: epoch 14 batch 0 loss 0.0313604399561882\n",
      "epoch 14 finished - avarage train loss 0.02716205249829539  avarage test loss 0.03284050617367029\n",
      "Training: epoch 15 batch 0 loss 0.018914924934506416\n",
      "Training: epoch 15 batch 10 loss 0.012415418401360512\n",
      "Training: epoch 15 batch 20 loss 0.02027040719985962\n",
      "Test: epoch 15 batch 0 loss 0.024156669154763222\n",
      "epoch 15 finished - avarage train loss 0.020290950473783344  avarage test loss 0.02575619053095579\n",
      "Training: epoch 16 batch 0 loss 0.01545431837439537\n",
      "Training: epoch 16 batch 10 loss 0.013344812206923962\n",
      "Training: epoch 16 batch 20 loss 0.01810033805668354\n",
      "Test: epoch 16 batch 0 loss 0.018522288650274277\n",
      "epoch 16 finished - avarage train loss 0.014808411404875845  avarage test loss 0.022419496905058622\n",
      "Training: epoch 17 batch 0 loss 0.008546968922019005\n",
      "Training: epoch 17 batch 10 loss 0.014024942182004452\n",
      "Training: epoch 17 batch 20 loss 0.005834972485899925\n",
      "Test: epoch 17 batch 0 loss 0.017375197261571884\n",
      "epoch 17 finished - avarage train loss 0.01254685126758855  avarage test loss 0.018791850190609694\n",
      "Training: epoch 18 batch 0 loss 0.007112725637853146\n",
      "Training: epoch 18 batch 10 loss 0.007202517241239548\n",
      "Training: epoch 18 batch 20 loss 0.0013732281513512135\n",
      "Test: epoch 18 batch 0 loss 0.013856146484613419\n",
      "epoch 18 finished - avarage train loss 0.009214153256395767  avarage test loss 0.017909617628902197\n",
      "Training: epoch 19 batch 0 loss 0.006557897198945284\n",
      "Training: epoch 19 batch 10 loss 0.005251158494502306\n",
      "Training: epoch 19 batch 20 loss 0.007610667496919632\n",
      "Test: epoch 19 batch 0 loss 0.011346526443958282\n",
      "epoch 19 finished - avarage train loss 0.008145267673735988  avarage test loss 0.01377618859987706\n",
      "Training: epoch 20 batch 0 loss 0.01534238364547491\n",
      "Training: epoch 20 batch 10 loss 0.003849093336611986\n",
      "Training: epoch 20 batch 20 loss 0.006637140642851591\n",
      "Test: epoch 20 batch 0 loss 0.014168948866426945\n",
      "epoch 20 finished - avarage train loss 0.006813623396487072  avarage test loss 0.01589105767197907\n",
      "Training: epoch 21 batch 0 loss 0.003784249071031809\n",
      "Training: epoch 21 batch 10 loss 0.006037504877895117\n",
      "Training: epoch 21 batch 20 loss 0.009600428864359856\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test: epoch 21 batch 0 loss 0.01254506316035986\n",
      "epoch 21 finished - avarage train loss 0.007077203473429485  avarage test loss 0.015143071650527418\n",
      "Training: epoch 22 batch 0 loss 0.005966568365693092\n",
      "Training: epoch 22 batch 10 loss 0.004438579548150301\n",
      "Training: epoch 22 batch 20 loss 0.00684251869097352\n",
      "Test: epoch 22 batch 0 loss 0.011407757177948952\n",
      "epoch 22 finished - avarage train loss 0.008301621471563804  avarage test loss 0.015913979499600828\n",
      "Training: epoch 23 batch 0 loss 0.0036573228426277637\n",
      "Training: epoch 23 batch 10 loss 0.00436697481200099\n",
      "Training: epoch 23 batch 20 loss 0.002669764216989279\n",
      "Test: epoch 23 batch 0 loss 0.012504606507718563\n",
      "epoch 23 finished - avarage train loss 0.006880361891897588  avarage test loss 0.013819508138112724\n",
      "Training: epoch 24 batch 0 loss 0.00854495819658041\n",
      "Training: epoch 24 batch 10 loss 0.0070062968879938126\n",
      "Training: epoch 24 batch 20 loss 0.0034133223816752434\n",
      "Test: epoch 24 batch 0 loss 0.016218135133385658\n",
      "epoch 24 finished - avarage train loss 0.0074508918553655  avarage test loss 0.016862368094734848\n",
      "Training: epoch 25 batch 0 loss 0.004792171064764261\n",
      "Training: epoch 25 batch 10 loss 0.005150218028575182\n",
      "Training: epoch 25 batch 20 loss 0.00486454414203763\n",
      "Test: epoch 25 batch 0 loss 0.013866398483514786\n",
      "epoch 25 finished - avarage train loss 0.007820853273031014  avarage test loss 0.013787855859845877\n",
      "Training: epoch 26 batch 0 loss 0.009407605044543743\n",
      "Training: epoch 26 batch 10 loss 0.002367134904488921\n",
      "Training: epoch 26 batch 20 loss 0.007190662901848555\n",
      "Test: epoch 26 batch 0 loss 0.012847508303821087\n",
      "epoch 26 finished - avarage train loss 0.00729622174825134  avarage test loss 0.014209143235348165\n",
      "Training: epoch 27 batch 0 loss 0.003661670023575425\n",
      "Training: epoch 27 batch 10 loss 0.006979477591812611\n",
      "Training: epoch 27 batch 20 loss 0.012492715381085873\n",
      "Test: epoch 27 batch 0 loss 0.014730483293533325\n",
      "epoch 27 finished - avarage train loss 0.00753908716784469  avarage test loss 0.016260257631074637\n",
      "Training: epoch 28 batch 0 loss 0.005706328433007002\n",
      "Training: epoch 28 batch 10 loss 0.0031528936233371496\n",
      "Training: epoch 28 batch 20 loss 0.0027236160822212696\n",
      "Test: epoch 28 batch 0 loss 0.014958783984184265\n",
      "epoch 28 finished - avarage train loss 0.007047401909748542  avarage test loss 0.015414075460284948\n",
      "Training: epoch 29 batch 0 loss 0.007084817159920931\n",
      "Training: epoch 29 batch 10 loss 0.0026619730051606894\n",
      "Training: epoch 29 batch 20 loss 0.009576153941452503\n",
      "Test: epoch 29 batch 0 loss 0.012066436931490898\n",
      "epoch 29 finished - avarage train loss 0.0073473968398596705  avarage test loss 0.013616677490063012\n",
      "Training: epoch 30 batch 0 loss 0.00555229838937521\n",
      "Training: epoch 30 batch 10 loss 0.009030415676534176\n",
      "Training: epoch 30 batch 20 loss 0.005021282937377691\n",
      "Test: epoch 30 batch 0 loss 0.013204573653638363\n",
      "epoch 30 finished - avarage train loss 0.007889466312039515  avarage test loss 0.016125223482958972\n",
      "Training: epoch 31 batch 0 loss 0.005041462369263172\n",
      "Training: epoch 31 batch 10 loss 0.008421952836215496\n",
      "Training: epoch 31 batch 20 loss 0.009779964573681355\n",
      "Test: epoch 31 batch 0 loss 0.015465936623513699\n",
      "epoch 31 finished - avarage train loss 0.007297419975030011  avarage test loss 0.015757579589262605\n",
      "Training: epoch 32 batch 0 loss 0.008966962806880474\n",
      "Training: epoch 32 batch 10 loss 0.007055848836898804\n",
      "Training: epoch 32 batch 20 loss 0.005824049934744835\n",
      "Test: epoch 32 batch 0 loss 0.01576533354818821\n",
      "epoch 32 finished - avarage train loss 0.006225112343913522  avarage test loss 0.014788011903874576\n",
      "Training: epoch 33 batch 0 loss 0.00607153819873929\n",
      "Training: epoch 33 batch 10 loss 0.006076630670577288\n",
      "Training: epoch 33 batch 20 loss 0.0033333622850477695\n",
      "Test: epoch 33 batch 0 loss 0.012881373055279255\n",
      "epoch 33 finished - avarage train loss 0.006458235306440499  avarage test loss 0.01409799448447302\n",
      "Training: epoch 34 batch 0 loss 0.0038694210816174746\n",
      "Training: epoch 34 batch 10 loss 0.006811425555497408\n",
      "Training: epoch 34 batch 20 loss 0.012377197854220867\n",
      "Test: epoch 34 batch 0 loss 0.015082920901477337\n",
      "epoch 34 finished - avarage train loss 0.007752414335143463  avarage test loss 0.014838030096143484\n",
      "Training: epoch 35 batch 0 loss 0.012338015250861645\n",
      "Training: epoch 35 batch 10 loss 0.0024437736719846725\n",
      "Training: epoch 35 batch 20 loss 0.009021194651722908\n",
      "Test: epoch 35 batch 0 loss 0.015323411673307419\n",
      "epoch 35 finished - avarage train loss 0.007335613248869777  avarage test loss 0.014702674699947238\n",
      "Training: epoch 36 batch 0 loss 0.0034854018595069647\n",
      "Training: epoch 36 batch 10 loss 0.007711110170930624\n",
      "Training: epoch 36 batch 20 loss 0.005158266983926296\n",
      "Test: epoch 36 batch 0 loss 0.013351456262171268\n",
      "epoch 36 finished - avarage train loss 0.005898544910074821  avarage test loss 0.014069366385228932\n",
      "Training: epoch 37 batch 0 loss 0.0075939130038022995\n",
      "Training: epoch 37 batch 10 loss 0.0043419357389211655\n",
      "Training: epoch 37 batch 20 loss 0.005710034165531397\n",
      "Test: epoch 37 batch 0 loss 0.009035208262503147\n",
      "epoch 37 finished - avarage train loss 0.007134079262774823  avarage test loss 0.013073284993879497\n",
      "Training: epoch 38 batch 0 loss 0.004738228395581245\n",
      "Training: epoch 38 batch 10 loss 0.0043692393228411674\n",
      "Training: epoch 38 batch 20 loss 0.003638958791270852\n",
      "Test: epoch 38 batch 0 loss 0.011133204214274883\n",
      "epoch 38 finished - avarage train loss 0.006864257237135336  avarage test loss 0.012923886300995946\n",
      "Training: epoch 39 batch 0 loss 0.004080414306372404\n",
      "Training: epoch 39 batch 10 loss 0.006302742753177881\n",
      "Training: epoch 39 batch 20 loss 0.0033089877106249332\n",
      "Test: epoch 39 batch 0 loss 0.012929284013807774\n",
      "epoch 39 finished - avarage train loss 0.006009232754626408  avarage test loss 0.014787860913202167\n",
      "Training: epoch 40 batch 0 loss 0.010616779327392578\n",
      "Training: epoch 40 batch 10 loss 0.006428594700992107\n",
      "Training: epoch 40 batch 20 loss 0.013198446482419968\n",
      "Test: epoch 40 batch 0 loss 0.011919188313186169\n",
      "epoch 40 finished - avarage train loss 0.007074179657702816  avarage test loss 0.01527374261058867\n",
      "Training: epoch 41 batch 0 loss 0.007394667714834213\n",
      "Training: epoch 41 batch 10 loss 0.004738987423479557\n",
      "Training: epoch 41 batch 20 loss 0.006500571500509977\n",
      "Test: epoch 41 batch 0 loss 0.013512114062905312\n",
      "epoch 41 finished - avarage train loss 0.007369963440176998  avarage test loss 0.014167614746838808\n",
      "Training: epoch 42 batch 0 loss 0.010530898347496986\n",
      "Training: epoch 42 batch 10 loss 0.0035272815730422735\n",
      "Training: epoch 42 batch 20 loss 0.005264275707304478\n",
      "Test: epoch 42 batch 0 loss 0.013216620311141014\n",
      "epoch 42 finished - avarage train loss 0.007090774147996101  avarage test loss 0.014181779231876135\n",
      "Training: epoch 43 batch 0 loss 0.003629429033026099\n",
      "Training: epoch 43 batch 10 loss 0.005129700526595116\n",
      "Training: epoch 43 batch 20 loss 0.003395592328161001\n",
      "Test: epoch 43 batch 0 loss 0.011791464872658253\n",
      "epoch 43 finished - avarage train loss 0.005877282269748634  avarage test loss 0.013695214176550508\n",
      "Training: epoch 44 batch 0 loss 0.008952725678682327\n",
      "Training: epoch 44 batch 10 loss 0.005652491003274918\n",
      "Training: epoch 44 batch 20 loss 0.006526688579469919\n",
      "Test: epoch 44 batch 0 loss 0.013101471588015556\n",
      "epoch 44 finished - avarage train loss 0.007368471104137856  avarage test loss 0.014639613102190197\n",
      "Training: epoch 45 batch 0 loss 0.0034705691505223513\n",
      "Training: epoch 45 batch 10 loss 0.006391884293407202\n",
      "Training: epoch 45 batch 20 loss 0.005076993722468615\n",
      "Test: epoch 45 batch 0 loss 0.012316826730966568\n",
      "epoch 45 finished - avarage train loss 0.006368283294783584  avarage test loss 0.013732316787354648\n",
      "Training: epoch 46 batch 0 loss 0.004039361607283354\n",
      "Training: epoch 46 batch 10 loss 0.004884537775069475\n",
      "Training: epoch 46 batch 20 loss 0.003400945570319891\n",
      "Test: epoch 46 batch 0 loss 0.01335995178669691\n",
      "epoch 46 finished - avarage train loss 0.006999647349183415  avarage test loss 0.014331835322082043\n",
      "Training: epoch 47 batch 0 loss 0.004532722290605307\n",
      "Training: epoch 47 batch 10 loss 0.00397048844024539\n",
      "Training: epoch 47 batch 20 loss 0.00951655488461256\n",
      "Test: epoch 47 batch 0 loss 0.01472737267613411\n",
      "epoch 47 finished - avarage train loss 0.0072645151000148775  avarage test loss 0.014567539212293923\n",
      "Training: epoch 48 batch 0 loss 0.00371297518722713\n",
      "Training: epoch 48 batch 10 loss 0.005016772076487541\n",
      "Training: epoch 48 batch 20 loss 0.005440336186438799\n",
      "Test: epoch 48 batch 0 loss 0.012614632025361061\n",
      "epoch 48 finished - avarage train loss 0.007400128227690685  avarage test loss 0.014291227678768337\n",
      "Training: epoch 49 batch 0 loss 0.007203654386103153\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training: epoch 49 batch 10 loss 0.0022096624597907066\n",
      "Training: epoch 49 batch 20 loss 0.0060386802069842815\n",
      "Test: epoch 49 batch 0 loss 0.013191461563110352\n",
      "epoch 49 finished - avarage train loss 0.007580151428179494  avarage test loss 0.015199119225144386\n",
      "Training: epoch 50 batch 0 loss 0.0062471963465213776\n",
      "Training: epoch 50 batch 10 loss 0.003045235062018037\n",
      "Training: epoch 50 batch 20 loss 0.0030102701857686043\n",
      "Test: epoch 50 batch 0 loss 0.01338854432106018\n",
      "epoch 50 finished - avarage train loss 0.007106084569260992  avarage test loss 0.015155329834669828\n",
      "Training: epoch 51 batch 0 loss 0.003753379685804248\n",
      "Training: epoch 51 batch 10 loss 0.0047463770024478436\n",
      "Training: epoch 51 batch 20 loss 0.0050138793885707855\n",
      "Test: epoch 51 batch 0 loss 0.013385049067437649\n",
      "epoch 51 finished - avarage train loss 0.00682767321644672  avarage test loss 0.015389585052616894\n",
      "Training: epoch 52 batch 0 loss 0.004535299725830555\n",
      "Training: epoch 52 batch 10 loss 0.002932614181190729\n",
      "Training: epoch 52 batch 20 loss 0.00801607221364975\n",
      "Test: epoch 52 batch 0 loss 0.013052169233560562\n",
      "epoch 52 finished - avarage train loss 0.006083455330146284  avarage test loss 0.014004330732859671\n",
      "Training: epoch 53 batch 0 loss 0.0036422039847820997\n",
      "Training: epoch 53 batch 10 loss 0.006188512779772282\n",
      "Training: epoch 53 batch 20 loss 0.004905711393803358\n",
      "Test: epoch 53 batch 0 loss 0.013693790882825851\n",
      "epoch 53 finished - avarage train loss 0.00741516303367399  avarage test loss 0.01422206754796207\n",
      "Training: epoch 54 batch 0 loss 0.008865555748343468\n",
      "Training: epoch 54 batch 10 loss 0.004767539445310831\n",
      "Training: epoch 54 batch 20 loss 0.007990024983882904\n",
      "Test: epoch 54 batch 0 loss 0.019940491765737534\n",
      "epoch 54 finished - avarage train loss 0.008405900186987529  avarage test loss 0.018456335179507732\n",
      "Training: epoch 55 batch 0 loss 0.00981720257550478\n",
      "Training: epoch 55 batch 10 loss 0.005072858650237322\n",
      "Training: epoch 55 batch 20 loss 0.010289972648024559\n",
      "Test: epoch 55 batch 0 loss 0.021420791745185852\n",
      "epoch 55 finished - avarage train loss 0.00843734759837389  avarage test loss 0.018629357684403658\n",
      "Training: epoch 56 batch 0 loss 0.005794600583612919\n",
      "Training: epoch 56 batch 10 loss 0.011250840499997139\n",
      "Training: epoch 56 batch 20 loss 0.005265846848487854\n",
      "Test: epoch 56 batch 0 loss 0.02152407541871071\n",
      "epoch 56 finished - avarage train loss 0.010164319770410657  avarage test loss 0.018491049180738628\n",
      "Training: epoch 57 batch 0 loss 0.006468896754086018\n",
      "Training: epoch 57 batch 10 loss 0.010445692576467991\n",
      "Training: epoch 57 batch 20 loss 0.0033658097963780165\n",
      "Test: epoch 57 batch 0 loss 0.019189681857824326\n",
      "epoch 57 finished - avarage train loss 0.008238356640755102  avarage test loss 0.015937930438667536\n",
      "Training: epoch 58 batch 0 loss 0.009687647223472595\n",
      "Training: epoch 58 batch 10 loss 0.012220711447298527\n",
      "Training: epoch 58 batch 20 loss 0.015888281166553497\n",
      "Test: epoch 58 batch 0 loss 0.01994149200618267\n",
      "epoch 58 finished - avarage train loss 0.008978156477277136  avarage test loss 0.018160746432840824\n",
      "Training: epoch 59 batch 0 loss 0.027089614421129227\n",
      "Training: epoch 59 batch 10 loss 0.013876806013286114\n",
      "Training: epoch 59 batch 20 loss 0.002978962380439043\n",
      "Test: epoch 59 batch 0 loss 0.019491786137223244\n",
      "epoch 59 finished - avarage train loss 0.010246230779087236  avarage test loss 0.01686333364341408\n",
      "Training: epoch 60 batch 0 loss 0.006734606809914112\n",
      "Training: epoch 60 batch 10 loss 0.014187312684953213\n",
      "Training: epoch 60 batch 20 loss 0.0038189850747585297\n",
      "Test: epoch 60 batch 0 loss 0.01883195899426937\n",
      "epoch 60 finished - avarage train loss 0.0069855836236142905  avarage test loss 0.01866409881040454\n",
      "Training: epoch 61 batch 0 loss 0.006121598184108734\n",
      "Training: epoch 61 batch 10 loss 0.012673193588852882\n",
      "Training: epoch 61 batch 20 loss 0.0034762786235660315\n",
      "Test: epoch 61 batch 0 loss 0.019456027075648308\n",
      "epoch 61 finished - avarage train loss 0.007137965250374942  avarage test loss 0.01860172103624791\n",
      "Training: epoch 62 batch 0 loss 0.0076067810878157616\n",
      "Training: epoch 62 batch 10 loss 0.010779587551951408\n",
      "Training: epoch 62 batch 20 loss 0.005608207546174526\n",
      "Test: epoch 62 batch 0 loss 0.01999717205762863\n",
      "epoch 62 finished - avarage train loss 0.007706299520515162  avarage test loss 0.016549836844205856\n",
      "Training: epoch 63 batch 0 loss 0.006057295016944408\n",
      "Training: epoch 63 batch 10 loss 0.004131892696022987\n",
      "Training: epoch 63 batch 20 loss 0.0051502445712685585\n",
      "Test: epoch 63 batch 0 loss 0.01675466261804104\n",
      "epoch 63 finished - avarage train loss 0.006978946922603866  avarage test loss 0.01536427391692996\n",
      "Training: epoch 64 batch 0 loss 0.004351597744971514\n",
      "Training: epoch 64 batch 10 loss 0.005951220635324717\n",
      "Training: epoch 64 batch 20 loss 0.005537697114050388\n",
      "Test: epoch 64 batch 0 loss 0.01874859444797039\n",
      "epoch 64 finished - avarage train loss 0.008029299234200654  avarage test loss 0.023350390139967203\n",
      "Training: epoch 65 batch 0 loss 0.017720721662044525\n",
      "Training: epoch 65 batch 10 loss 0.012597547844052315\n",
      "Training: epoch 65 batch 20 loss 0.011526462621986866\n",
      "Test: epoch 65 batch 0 loss 0.011684133671224117\n",
      "epoch 65 finished - avarage train loss 0.011040779938600186  avarage test loss 0.016696493374183774\n",
      "Training: epoch 66 batch 0 loss 0.004538490902632475\n",
      "Training: epoch 66 batch 10 loss 0.003367979545146227\n",
      "Training: epoch 66 batch 20 loss 0.004498948343098164\n",
      "Test: epoch 66 batch 0 loss 0.010800324380397797\n",
      "epoch 66 finished - avarage train loss 0.006974443528351599  avarage test loss 0.013696108013391495\n",
      "Training: epoch 67 batch 0 loss 0.0072552296333014965\n",
      "Training: epoch 67 batch 10 loss 0.00654203025624156\n",
      "Training: epoch 67 batch 20 loss 0.005373828113079071\n",
      "Test: epoch 67 batch 0 loss 0.00972696952521801\n",
      "epoch 67 finished - avarage train loss 0.006839467838791938  avarage test loss 0.01331189856864512\n",
      "Training: epoch 68 batch 0 loss 0.006067999172955751\n",
      "Training: epoch 68 batch 10 loss 0.005933987442404032\n",
      "Training: epoch 68 batch 20 loss 0.002498383168131113\n",
      "Test: epoch 68 batch 0 loss 0.010021987371146679\n",
      "epoch 68 finished - avarage train loss 0.0066348715877995405  avarage test loss 0.012609448051080108\n",
      "Training: epoch 69 batch 0 loss 0.0061762151308357716\n",
      "Training: epoch 69 batch 10 loss 0.004625841975212097\n",
      "Training: epoch 69 batch 20 loss 0.006395744159817696\n",
      "Test: epoch 69 batch 0 loss 0.017130324617028236\n",
      "epoch 69 finished - avarage train loss 0.006565372852993937  avarage test loss 0.015536232967860997\n",
      "Training: epoch 70 batch 0 loss 0.006674660835415125\n",
      "Training: epoch 70 batch 10 loss 0.005909464787691832\n",
      "Training: epoch 70 batch 20 loss 0.009889363311231136\n",
      "Test: epoch 70 batch 0 loss 0.01889285258948803\n",
      "epoch 70 finished - avarage train loss 0.008925602674998086  avarage test loss 0.017871107906103134\n",
      "Training: epoch 71 batch 0 loss 0.011444554664194584\n",
      "Training: epoch 71 batch 10 loss 0.006110311020165682\n",
      "Training: epoch 71 batch 20 loss 0.0061501977033913136\n",
      "Test: epoch 71 batch 0 loss 0.017050793394446373\n",
      "epoch 71 finished - avarage train loss 0.009964886225824213  avarage test loss 0.016124597867019475\n",
      "Training: epoch 72 batch 0 loss 0.006337466184049845\n",
      "Training: epoch 72 batch 10 loss 0.005583505146205425\n",
      "Training: epoch 72 batch 20 loss 0.005637381225824356\n",
      "Test: epoch 72 batch 0 loss 0.016540249809622765\n",
      "epoch 72 finished - avarage train loss 0.008297117009502033  avarage test loss 0.015659522614441812\n",
      "Training: epoch 73 batch 0 loss 0.0037593827582895756\n",
      "Training: epoch 73 batch 10 loss 0.006962069775909185\n",
      "Training: epoch 73 batch 20 loss 0.005984017159789801\n",
      "Test: epoch 73 batch 0 loss 0.01622861623764038\n",
      "epoch 73 finished - avarage train loss 0.00745790347392703  avarage test loss 0.015983944293111563\n",
      "Training: epoch 74 batch 0 loss 0.004536117892712355\n",
      "Training: epoch 74 batch 10 loss 0.0052688242867589\n",
      "Training: epoch 74 batch 20 loss 0.004854812752455473\n",
      "Test: epoch 74 batch 0 loss 0.0181694645434618\n",
      "epoch 74 finished - avarage train loss 0.007417072092407736  avarage test loss 0.015419741393998265\n",
      "Training: epoch 75 batch 0 loss 0.0052163624204695225\n",
      "Training: epoch 75 batch 10 loss 0.012262200936675072\n",
      "Training: epoch 75 batch 20 loss 0.0028483369387686253\n",
      "Test: epoch 75 batch 0 loss 0.01714528538286686\n",
      "epoch 75 finished - avarage train loss 0.007380603322887729  avarage test loss 0.0174072285881266\n",
      "Training: epoch 76 batch 0 loss 0.005184840876609087\n",
      "Training: epoch 76 batch 10 loss 0.006970938760787249\n",
      "Training: epoch 76 batch 20 loss 0.015046191401779652\n",
      "Test: epoch 76 batch 0 loss 0.016349051147699356\n",
      "epoch 76 finished - avarage train loss 0.007867997287419336  avarage test loss 0.017529693664982915\n",
      "Training: epoch 77 batch 0 loss 0.0030107712373137474\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training: epoch 77 batch 10 loss 0.00527251185849309\n",
      "Training: epoch 77 batch 20 loss 0.003894091583788395\n",
      "Test: epoch 77 batch 0 loss 0.016940327361226082\n",
      "epoch 77 finished - avarage train loss 0.007298269413475846  avarage test loss 0.015195155865512788\n",
      "Training: epoch 78 batch 0 loss 0.006118676625192165\n",
      "Training: epoch 78 batch 10 loss 0.00439680740237236\n",
      "Training: epoch 78 batch 20 loss 0.012186665087938309\n",
      "Test: epoch 78 batch 0 loss 0.016820218414068222\n",
      "epoch 78 finished - avarage train loss 0.007909217439913031  avarage test loss 0.014855947345495224\n",
      "Training: epoch 79 batch 0 loss 0.007778539787977934\n",
      "Training: epoch 79 batch 10 loss 0.004119044169783592\n",
      "Training: epoch 79 batch 20 loss 0.014915434643626213\n",
      "Test: epoch 79 batch 0 loss 0.015300577506422997\n",
      "epoch 79 finished - avarage train loss 0.00736082137305418  avarage test loss 0.015557717764750123\n",
      "Training: epoch 80 batch 0 loss 0.003046757774427533\n",
      "Training: epoch 80 batch 10 loss 0.004761112853884697\n",
      "Training: epoch 80 batch 20 loss 0.0053652552887797356\n",
      "Test: epoch 80 batch 0 loss 0.01404502335935831\n",
      "epoch 80 finished - avarage train loss 0.00664860595435161  avarage test loss 0.01719957491150126\n",
      "Training: epoch 81 batch 0 loss 0.008434241637587547\n",
      "Training: epoch 81 batch 10 loss 0.0030015085358172655\n",
      "Training: epoch 81 batch 20 loss 0.005859571509063244\n",
      "Test: epoch 81 batch 0 loss 0.010209532454609871\n",
      "epoch 81 finished - avarage train loss 0.008190711462420636  avarage test loss 0.014880891656503081\n",
      "Training: epoch 82 batch 0 loss 0.005082013551145792\n",
      "Training: epoch 82 batch 10 loss 0.0060662985779345036\n",
      "Training: epoch 82 batch 20 loss 0.006905617192387581\n",
      "Test: epoch 82 batch 0 loss 0.009427754208445549\n",
      "epoch 82 finished - avarage train loss 0.007209536739914068  avarage test loss 0.013187811244279146\n",
      "Training: epoch 83 batch 0 loss 0.0041433824226260185\n",
      "Training: epoch 83 batch 10 loss 0.0032289756927639246\n",
      "Training: epoch 83 batch 20 loss 0.0019460259936749935\n",
      "Test: epoch 83 batch 0 loss 0.009505940601229668\n",
      "epoch 83 finished - avarage train loss 0.007020172462316936  avarage test loss 0.012278860667720437\n",
      "Training: epoch 84 batch 0 loss 0.0025187190622091293\n",
      "Training: epoch 84 batch 10 loss 0.005063037853688002\n",
      "Training: epoch 84 batch 20 loss 0.0030314833857119083\n",
      "Test: epoch 84 batch 0 loss 0.009439904242753983\n",
      "epoch 84 finished - avarage train loss 0.008578104793961192  avarage test loss 0.013159354799427092\n",
      "Training: epoch 85 batch 0 loss 0.004105277359485626\n",
      "Training: epoch 85 batch 10 loss 0.008241035044193268\n",
      "Training: epoch 85 batch 20 loss 0.005941988434642553\n",
      "Test: epoch 85 batch 0 loss 0.009263262152671814\n",
      "epoch 85 finished - avarage train loss 0.007084046137230149  avarage test loss 0.012500745709985495\n",
      "Training: epoch 86 batch 0 loss 0.007340423297137022\n",
      "Training: epoch 86 batch 10 loss 0.005240771919488907\n",
      "Training: epoch 86 batch 20 loss 0.006700487341731787\n",
      "Test: epoch 86 batch 0 loss 0.009377806447446346\n",
      "epoch 86 finished - avarage train loss 0.007330112115094631  avarage test loss 0.012512357207015157\n",
      "Training: epoch 87 batch 0 loss 0.004289714153856039\n",
      "Training: epoch 87 batch 10 loss 0.005629321094602346\n",
      "Training: epoch 87 batch 20 loss 0.008379652164876461\n",
      "Test: epoch 87 batch 0 loss 0.009922890923917294\n",
      "epoch 87 finished - avarage train loss 0.006111840589036201  avarage test loss 0.012823297525756061\n",
      "Training: epoch 88 batch 0 loss 0.0028907228261232376\n",
      "Training: epoch 88 batch 10 loss 0.003527779132127762\n",
      "Training: epoch 88 batch 20 loss 0.004019655752927065\n",
      "Test: epoch 88 batch 0 loss 0.011155468411743641\n",
      "epoch 88 finished - avarage train loss 0.007461358427776602  avarage test loss 0.014664273359812796\n",
      "Training: epoch 89 batch 0 loss 0.0054525528103113174\n",
      "Training: epoch 89 batch 10 loss 0.0017546794842928648\n",
      "Training: epoch 89 batch 20 loss 0.0031894645653665066\n",
      "Test: epoch 89 batch 0 loss 0.009885584935545921\n",
      "epoch 89 finished - avarage train loss 0.008655340025394127  avarage test loss 0.012361764907836914\n",
      "Training: epoch 90 batch 0 loss 0.004136383533477783\n",
      "Training: epoch 90 batch 10 loss 0.006047812756150961\n",
      "Training: epoch 90 batch 20 loss 0.01666751131415367\n",
      "Test: epoch 90 batch 0 loss 0.00987024325877428\n",
      "epoch 90 finished - avarage train loss 0.006869605983254211  avarage test loss 0.01281682844273746\n",
      "Training: epoch 91 batch 0 loss 0.00616881949827075\n",
      "Training: epoch 91 batch 10 loss 0.002820161171257496\n",
      "Training: epoch 91 batch 20 loss 0.0024840596597641706\n",
      "Test: epoch 91 batch 0 loss 0.009846572764217854\n",
      "epoch 91 finished - avarage train loss 0.005758677994639709  avarage test loss 0.01229690236505121\n",
      "Training: epoch 92 batch 0 loss 0.003502697916701436\n",
      "Training: epoch 92 batch 10 loss 0.008246038109064102\n",
      "Training: epoch 92 batch 20 loss 0.002391053829342127\n",
      "Test: epoch 92 batch 0 loss 0.009689710102975368\n",
      "epoch 92 finished - avarage train loss 0.007950350529803285  avarage test loss 0.012631343444809318\n",
      "Training: epoch 93 batch 0 loss 0.006214768625795841\n",
      "Training: epoch 93 batch 10 loss 0.0020916666835546494\n",
      "Training: epoch 93 batch 20 loss 0.005917869973927736\n",
      "Test: epoch 93 batch 0 loss 0.010001958347856998\n",
      "epoch 93 finished - avarage train loss 0.006380133961070457  avarage test loss 0.012861086870543659\n",
      "Training: epoch 94 batch 0 loss 0.008780904114246368\n",
      "Training: epoch 94 batch 10 loss 0.00342728802934289\n",
      "Training: epoch 94 batch 20 loss 0.003953208215534687\n",
      "Test: epoch 94 batch 0 loss 0.009756511077284813\n",
      "epoch 94 finished - avarage train loss 0.006360114455736917  avarage test loss 0.012492976384237409\n",
      "Training: epoch 95 batch 0 loss 0.005984203889966011\n",
      "Training: epoch 95 batch 10 loss 0.0064427731558680534\n",
      "Training: epoch 95 batch 20 loss 0.00655502500012517\n",
      "Test: epoch 95 batch 0 loss 0.009895499795675278\n",
      "epoch 95 finished - avarage train loss 0.007227646395692538  avarage test loss 0.01266694301739335\n",
      "Training: epoch 96 batch 0 loss 0.005197908729314804\n",
      "Training: epoch 96 batch 10 loss 0.008304186165332794\n",
      "Training: epoch 96 batch 20 loss 0.004623845685273409\n",
      "Test: epoch 96 batch 0 loss 0.01094469428062439\n",
      "epoch 96 finished - avarage train loss 0.006576220689598343  avarage test loss 0.012953469646163285\n",
      "Training: epoch 97 batch 0 loss 0.007382835261523724\n",
      "Training: epoch 97 batch 10 loss 0.00548126082867384\n",
      "Training: epoch 97 batch 20 loss 0.003979973960667849\n",
      "Test: epoch 97 batch 0 loss 0.00982594769448042\n",
      "epoch 97 finished - avarage train loss 0.006884398910313331  avarage test loss 0.012376092257909477\n",
      "Training: epoch 98 batch 0 loss 0.004656246397644281\n",
      "Training: epoch 98 batch 10 loss 0.003619615687057376\n",
      "Training: epoch 98 batch 20 loss 0.005821566097438335\n",
      "Test: epoch 98 batch 0 loss 0.010922795161604881\n",
      "epoch 98 finished - avarage train loss 0.0065622339747717666  avarage test loss 0.01344410190358758\n",
      "Training: epoch 99 batch 0 loss 0.005287059582769871\n",
      "Training: epoch 99 batch 10 loss 0.004978188779205084\n",
      "Training: epoch 99 batch 20 loss 0.003048329846933484\n",
      "Test: epoch 99 batch 0 loss 0.013641607947647572\n",
      "epoch 99 finished - avarage train loss 0.0073870830398438305  avarage test loss 0.014425661996938288\n",
      "Training: epoch 100 batch 0 loss 0.005580299999564886\n",
      "Training: epoch 100 batch 10 loss 0.00207530427724123\n",
      "Training: epoch 100 batch 20 loss 0.003340476891025901\n",
      "Test: epoch 100 batch 0 loss 0.01564415916800499\n",
      "epoch 100 finished - avarage train loss 0.005792742651277061  avarage test loss 0.015190804027952254\n",
      "Training: epoch 101 batch 0 loss 0.005999784450978041\n",
      "Training: epoch 101 batch 10 loss 0.005164252128452063\n",
      "Training: epoch 101 batch 20 loss 0.00854127574712038\n",
      "Test: epoch 101 batch 0 loss 0.009951164945960045\n",
      "epoch 101 finished - avarage train loss 0.00775090682898359  avarage test loss 0.012935902224853635\n",
      "Training: epoch 102 batch 0 loss 0.0035798654425889254\n",
      "Training: epoch 102 batch 10 loss 0.0034840344451367855\n",
      "Training: epoch 102 batch 20 loss 0.006863548886030912\n",
      "Test: epoch 102 batch 0 loss 0.011745073832571507\n",
      "epoch 102 finished - avarage train loss 0.009270622724153358  avarage test loss 0.014194059534929693\n",
      "Training: epoch 103 batch 0 loss 0.004379283171147108\n",
      "Training: epoch 103 batch 10 loss 0.002688081469386816\n",
      "Training: epoch 103 batch 20 loss 0.00920343492180109\n",
      "Test: epoch 103 batch 0 loss 0.014031129889190197\n",
      "epoch 103 finished - avarage train loss 0.008043799589484417  avarage test loss 0.015485115931369364\n",
      "Training: epoch 104 batch 0 loss 0.005738569889217615\n",
      "Training: epoch 104 batch 10 loss 0.005685756448656321\n",
      "Training: epoch 104 batch 20 loss 0.013072255067527294\n",
      "Test: epoch 104 batch 0 loss 0.012598228640854359\n",
      "epoch 104 finished - avarage train loss 0.006440357385395929  avarage test loss 0.014258757350035012\n",
      "Training: epoch 105 batch 0 loss 0.002643830608576536\n",
      "Training: epoch 105 batch 10 loss 0.005627995822578669\n",
      "Training: epoch 105 batch 20 loss 0.011898964643478394\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test: epoch 105 batch 0 loss 0.011127050966024399\n",
      "epoch 105 finished - avarage train loss 0.006593205323763962  avarage test loss 0.013717823196202517\n",
      "Training: epoch 106 batch 0 loss 0.0014910072786733508\n",
      "Training: epoch 106 batch 10 loss 0.00694790156558156\n",
      "Training: epoch 106 batch 20 loss 0.005575411021709442\n",
      "Test: epoch 106 batch 0 loss 0.009291003458201885\n",
      "epoch 106 finished - avarage train loss 0.008050469621405777  avarage test loss 0.012743469444103539\n",
      "Training: epoch 107 batch 0 loss 0.0036395431961864233\n",
      "Training: epoch 107 batch 10 loss 0.0036856173537671566\n",
      "Training: epoch 107 batch 20 loss 0.005059993825852871\n",
      "Test: epoch 107 batch 0 loss 0.010344380512833595\n",
      "epoch 107 finished - avarage train loss 0.007621750508531414  avarage test loss 0.012919246451929212\n",
      "Training: epoch 108 batch 0 loss 0.0065823509357869625\n",
      "Training: epoch 108 batch 10 loss 0.0058144167996943\n",
      "Training: epoch 108 batch 20 loss 0.0038267511408776045\n",
      "Test: epoch 108 batch 0 loss 0.009672367013990879\n",
      "epoch 108 finished - avarage train loss 0.007095171090472361  avarage test loss 0.012017177417874336\n",
      "Training: epoch 109 batch 0 loss 0.007462997920811176\n",
      "Training: epoch 109 batch 10 loss 0.00629916088655591\n",
      "Training: epoch 109 batch 20 loss 0.0075601572170853615\n",
      "Test: epoch 109 batch 0 loss 0.009531084448099136\n",
      "epoch 109 finished - avarage train loss 0.005508858943358064  avarage test loss 0.011734602390788496\n",
      "Training: epoch 110 batch 0 loss 0.006605574861168861\n",
      "Training: epoch 110 batch 10 loss 0.006167394109070301\n",
      "Training: epoch 110 batch 20 loss 0.00694679468870163\n",
      "Test: epoch 110 batch 0 loss 0.010777559131383896\n",
      "epoch 110 finished - avarage train loss 0.007779537507429205  avarage test loss 0.013303357176482677\n",
      "Training: epoch 111 batch 0 loss 0.007768731564283371\n",
      "Training: epoch 111 batch 10 loss 0.0038771037943661213\n",
      "Training: epoch 111 batch 20 loss 0.00470183789730072\n",
      "Test: epoch 111 batch 0 loss 0.009533804841339588\n",
      "epoch 111 finished - avarage train loss 0.007474092590012427  avarage test loss 0.011773582547903061\n",
      "Training: epoch 112 batch 0 loss 0.008434728719294071\n",
      "Training: epoch 112 batch 10 loss 0.005054329056292772\n",
      "Training: epoch 112 batch 20 loss 0.0038819226901978254\n",
      "Test: epoch 112 batch 0 loss 0.011763414368033409\n",
      "epoch 112 finished - avarage train loss 0.006970111714226419  avarage test loss 0.01509375893510878\n",
      "Training: epoch 113 batch 0 loss 0.004933450371026993\n",
      "Training: epoch 113 batch 10 loss 0.0043466296046972275\n",
      "Training: epoch 113 batch 20 loss 0.01545857172459364\n",
      "Test: epoch 113 batch 0 loss 0.010391867719590664\n",
      "epoch 113 finished - avarage train loss 0.008520721168867472  avarage test loss 0.012893808889202774\n",
      "Training: epoch 114 batch 0 loss 0.008748065680265427\n",
      "Training: epoch 114 batch 10 loss 0.0019559215288609266\n",
      "Training: epoch 114 batch 20 loss 0.002634878270328045\n",
      "Test: epoch 114 batch 0 loss 0.01052615512162447\n",
      "epoch 114 finished - avarage train loss 0.00693843070947533  avarage test loss 0.012342132162302732\n",
      "Training: epoch 115 batch 0 loss 0.0035631409846246243\n",
      "Training: epoch 115 batch 10 loss 0.0022000051103532314\n",
      "Training: epoch 115 batch 20 loss 0.003844520077109337\n",
      "Test: epoch 115 batch 0 loss 0.010469287633895874\n",
      "epoch 115 finished - avarage train loss 0.007656256852928421  avarage test loss 0.012236960348673165\n",
      "Training: epoch 116 batch 0 loss 0.0028170496225357056\n",
      "Training: epoch 116 batch 10 loss 0.006573444232344627\n",
      "Training: epoch 116 batch 20 loss 0.005352507345378399\n",
      "Test: epoch 116 batch 0 loss 0.010344414040446281\n",
      "epoch 116 finished - avarage train loss 0.007132052243205494  avarage test loss 0.012136645382270217\n",
      "Training: epoch 117 batch 0 loss 0.0075471773743629456\n",
      "Training: epoch 117 batch 10 loss 0.0057898531667888165\n",
      "Training: epoch 117 batch 20 loss 0.004181741736829281\n",
      "Test: epoch 117 batch 0 loss 0.010550581850111485\n",
      "epoch 117 finished - avarage train loss 0.007775103536851961  avarage test loss 0.012412445968948305\n",
      "Training: epoch 118 batch 0 loss 0.005103680770844221\n",
      "Training: epoch 118 batch 10 loss 0.004451887682080269\n",
      "Training: epoch 118 batch 20 loss 0.006408581044524908\n",
      "Test: epoch 118 batch 0 loss 0.010214602574706078\n",
      "epoch 118 finished - avarage train loss 0.00655352501680368  avarage test loss 0.012479349155910313\n",
      "Training: epoch 119 batch 0 loss 0.0019156303023919463\n",
      "Training: epoch 119 batch 10 loss 0.0048522064462304115\n",
      "Training: epoch 119 batch 20 loss 0.004551839083433151\n",
      "Test: epoch 119 batch 0 loss 0.011267214082181454\n",
      "epoch 119 finished - avarage train loss 0.005968956577848515  avarage test loss 0.012912725564092398\n",
      "Training: epoch 120 batch 0 loss 0.007183234207332134\n",
      "Training: epoch 120 batch 10 loss 0.003028029575943947\n",
      "Training: epoch 120 batch 20 loss 0.004120383411645889\n",
      "Test: epoch 120 batch 0 loss 0.011373157612979412\n",
      "epoch 120 finished - avarage train loss 0.007811038040331212  avarage test loss 0.013057176489382982\n",
      "Training: epoch 121 batch 0 loss 0.004287757910788059\n",
      "Training: epoch 121 batch 10 loss 0.007233186159282923\n",
      "Training: epoch 121 batch 20 loss 0.006880716886371374\n",
      "Test: epoch 121 batch 0 loss 0.012652130797505379\n",
      "epoch 121 finished - avarage train loss 0.007888164941285705  avarage test loss 0.015472166473045945\n",
      "Training: epoch 122 batch 0 loss 0.011114644818007946\n",
      "Training: epoch 122 batch 10 loss 0.007444888353347778\n",
      "Training: epoch 122 batch 20 loss 0.0052551995031535625\n",
      "Test: epoch 122 batch 0 loss 0.010470468550920486\n",
      "epoch 122 finished - avarage train loss 0.008089988540601114  avarage test loss 0.012328598066233099\n",
      "Training: epoch 123 batch 0 loss 0.005945373792201281\n",
      "Training: epoch 123 batch 10 loss 0.006294271908700466\n",
      "Training: epoch 123 batch 20 loss 0.0025901393964886665\n",
      "Test: epoch 123 batch 0 loss 0.011666209436953068\n",
      "epoch 123 finished - avarage train loss 0.005430258501417421  avarage test loss 0.014504265040159225\n",
      "Training: epoch 124 batch 0 loss 0.005151464603841305\n",
      "Training: epoch 124 batch 10 loss 0.010740209370851517\n",
      "Training: epoch 124 batch 20 loss 0.005881651770323515\n",
      "Test: epoch 124 batch 0 loss 0.008875628001987934\n",
      "epoch 124 finished - avarage train loss 0.0076126818753907395  avarage test loss 0.011847393354400992\n",
      "Training: epoch 125 batch 0 loss 0.0031572175212204456\n",
      "Training: epoch 125 batch 10 loss 0.005097841378301382\n",
      "Training: epoch 125 batch 20 loss 0.00349396001547575\n",
      "Test: epoch 125 batch 0 loss 0.010570103302598\n",
      "epoch 125 finished - avarage train loss 0.0069845966143726275  avarage test loss 0.01247847848571837\n",
      "Training: epoch 126 batch 0 loss 0.003797929035499692\n",
      "Training: epoch 126 batch 10 loss 0.00510811572894454\n",
      "Training: epoch 126 batch 20 loss 0.0033312244340777397\n",
      "Test: epoch 126 batch 0 loss 0.009913039393723011\n",
      "epoch 126 finished - avarage train loss 0.007288610787484153  avarage test loss 0.012555202702060342\n",
      "Training: epoch 127 batch 0 loss 0.005940020550042391\n",
      "Training: epoch 127 batch 10 loss 0.0028107785619795322\n",
      "Training: epoch 127 batch 20 loss 0.011994416825473309\n",
      "Test: epoch 127 batch 0 loss 0.010794933885335922\n",
      "epoch 127 finished - avarage train loss 0.006241611108697694  avarage test loss 0.012553078355267644\n",
      "Training: epoch 128 batch 0 loss 0.005507989786565304\n",
      "Training: epoch 128 batch 10 loss 0.004007156938314438\n",
      "Training: epoch 128 batch 20 loss 0.003081437200307846\n",
      "Test: epoch 128 batch 0 loss 0.010132362134754658\n",
      "epoch 128 finished - avarage train loss 0.006629858463039172  avarage test loss 0.011986088007688522\n",
      "Training: epoch 129 batch 0 loss 0.005914241075515747\n",
      "Training: epoch 129 batch 10 loss 0.008706876076757908\n",
      "Training: epoch 129 batch 20 loss 0.00807080790400505\n",
      "Test: epoch 129 batch 0 loss 0.010500176809728146\n",
      "epoch 129 finished - avarage train loss 0.007363673893670584  avarage test loss 0.012444936437532306\n",
      "Training: epoch 130 batch 0 loss 0.015931015834212303\n",
      "Training: epoch 130 batch 10 loss 0.004716442432254553\n",
      "Training: epoch 130 batch 20 loss 0.004286111798137426\n",
      "Test: epoch 130 batch 0 loss 0.009777819737792015\n",
      "epoch 130 finished - avarage train loss 0.006726943125049102  avarage test loss 0.012371354270726442\n",
      "Training: epoch 131 batch 0 loss 0.002964058890938759\n",
      "Training: epoch 131 batch 10 loss 0.005429822485893965\n",
      "Training: epoch 131 batch 20 loss 0.002054456854239106\n",
      "Test: epoch 131 batch 0 loss 0.00818853359669447\n",
      "epoch 131 finished - avarage train loss 0.007176246815200509  avarage test loss 0.011610560468398035\n",
      "Training: epoch 132 batch 0 loss 0.0031984352972358465\n",
      "Training: epoch 132 batch 10 loss 0.012024346739053726\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training: epoch 132 batch 20 loss 0.005648486781865358\n",
      "Test: epoch 132 batch 0 loss 0.010905362665653229\n",
      "epoch 132 finished - avarage train loss 0.008139527001386059  avarage test loss 0.013709738035686314\n",
      "Training: epoch 133 batch 0 loss 0.009342959150671959\n",
      "Training: epoch 133 batch 10 loss 0.007331383880227804\n",
      "Training: epoch 133 batch 20 loss 0.008730163797736168\n",
      "Test: epoch 133 batch 0 loss 0.010079520754516125\n",
      "epoch 133 finished - avarage train loss 0.0071562108953065915  avarage test loss 0.012172534363344312\n",
      "Training: epoch 134 batch 0 loss 0.0021604406647384167\n",
      "Training: epoch 134 batch 10 loss 0.0041737728752195835\n",
      "Training: epoch 134 batch 20 loss 0.004251058679074049\n",
      "Test: epoch 134 batch 0 loss 0.012697114609181881\n",
      "epoch 134 finished - avarage train loss 0.006396504424127011  avarage test loss 0.014871610328555107\n",
      "Training: epoch 135 batch 0 loss 0.007802736945450306\n",
      "Training: epoch 135 batch 10 loss 0.007817322388291359\n",
      "Training: epoch 135 batch 20 loss 0.008913403376936913\n",
      "Test: epoch 135 batch 0 loss 0.010361757129430771\n",
      "epoch 135 finished - avarage train loss 0.008811116515626681  avarage test loss 0.012102309032343328\n",
      "Training: epoch 136 batch 0 loss 0.0071298363618552685\n",
      "Training: epoch 136 batch 10 loss 0.005634843371808529\n",
      "Training: epoch 136 batch 20 loss 0.004997506272047758\n",
      "Test: epoch 136 batch 0 loss 0.010219999589025974\n",
      "epoch 136 finished - avarage train loss 0.007339853327721357  avarage test loss 0.012138747842982411\n",
      "Training: epoch 137 batch 0 loss 0.010132617317140102\n",
      "Training: epoch 137 batch 10 loss 0.0038227043114602566\n",
      "Training: epoch 137 batch 20 loss 0.006355614867061377\n",
      "Test: epoch 137 batch 0 loss 0.01098684873431921\n",
      "epoch 137 finished - avarage train loss 0.008483199097601504  avarage test loss 0.01306631660554558\n",
      "Training: epoch 138 batch 0 loss 0.002042694715783\n",
      "Training: epoch 138 batch 10 loss 0.0042828102596104145\n",
      "Training: epoch 138 batch 20 loss 0.004357950761914253\n",
      "Test: epoch 138 batch 0 loss 0.00948017742484808\n",
      "epoch 138 finished - avarage train loss 0.00593307558527409  avarage test loss 0.011769476579502225\n",
      "Training: epoch 139 batch 0 loss 0.005098299123346806\n",
      "Training: epoch 139 batch 10 loss 0.0040782662108540535\n",
      "Training: epoch 139 batch 20 loss 0.013516453094780445\n",
      "Test: epoch 139 batch 0 loss 0.011614350602030754\n",
      "epoch 139 finished - avarage train loss 0.0064817165217266  avarage test loss 0.014513059635646641\n",
      "Training: epoch 140 batch 0 loss 0.003563260193914175\n",
      "Training: epoch 140 batch 10 loss 0.009561290964484215\n",
      "Training: epoch 140 batch 20 loss 0.008639783598482609\n",
      "Test: epoch 140 batch 0 loss 0.009113849140703678\n",
      "epoch 140 finished - avarage train loss 0.007751869407064956  avarage test loss 0.011918655480258167\n",
      "Training: epoch 141 batch 0 loss 0.004450270906090736\n",
      "Training: epoch 141 batch 10 loss 0.00481877475976944\n",
      "Training: epoch 141 batch 20 loss 0.006069484632462263\n",
      "Test: epoch 141 batch 0 loss 0.011567062698304653\n",
      "epoch 141 finished - avarage train loss 0.007482649806629995  avarage test loss 0.014165600878186524\n",
      "Training: epoch 142 batch 0 loss 0.004824709612876177\n",
      "Training: epoch 142 batch 10 loss 0.02277802675962448\n",
      "Training: epoch 142 batch 20 loss 0.004229681566357613\n",
      "Test: epoch 142 batch 0 loss 0.009821921586990356\n",
      "epoch 142 finished - avarage train loss 0.007170307217165828  avarage test loss 0.011683694086968899\n",
      "Training: epoch 143 batch 0 loss 0.011749698780477047\n",
      "Training: epoch 143 batch 10 loss 0.0032207074109464884\n",
      "Training: epoch 143 batch 20 loss 0.006031041033565998\n",
      "Test: epoch 143 batch 0 loss 0.010365966707468033\n",
      "epoch 143 finished - avarage train loss 0.007374555731577606  avarage test loss 0.012498476426117122\n",
      "Training: epoch 144 batch 0 loss 0.006278734188526869\n",
      "Training: epoch 144 batch 10 loss 0.0069473134353756905\n",
      "Training: epoch 144 batch 20 loss 0.003639556933194399\n",
      "Test: epoch 144 batch 0 loss 0.01141888927668333\n",
      "epoch 144 finished - avarage train loss 0.007264545107067659  avarage test loss 0.013543320237658918\n",
      "Training: epoch 145 batch 0 loss 0.005909730214625597\n",
      "Training: epoch 145 batch 10 loss 0.0044548530131578445\n",
      "Training: epoch 145 batch 20 loss 0.005228481721132994\n",
      "Test: epoch 145 batch 0 loss 0.010079332627356052\n",
      "epoch 145 finished - avarage train loss 0.007564555861633913  avarage test loss 0.011968684033490717\n",
      "Training: epoch 146 batch 0 loss 0.009961935691535473\n",
      "Training: epoch 146 batch 10 loss 0.00317538739182055\n",
      "Training: epoch 146 batch 20 loss 0.014789155684411526\n",
      "Test: epoch 146 batch 0 loss 0.010643478482961655\n",
      "epoch 146 finished - avarage train loss 0.006884066990159195  avarage test loss 0.012396628502756357\n",
      "Training: epoch 147 batch 0 loss 0.0032452144660055637\n",
      "Training: epoch 147 batch 10 loss 0.002956734038889408\n",
      "Training: epoch 147 batch 20 loss 0.0027560011949390173\n",
      "Test: epoch 147 batch 0 loss 0.010562258772552013\n",
      "epoch 147 finished - avarage train loss 0.005951432832356157  avarage test loss 0.012377548147924244\n",
      "Training: epoch 148 batch 0 loss 0.005180394276976585\n",
      "Training: epoch 148 batch 10 loss 0.008246060460805893\n",
      "Training: epoch 148 batch 20 loss 0.010035889223217964\n",
      "Test: epoch 148 batch 0 loss 0.012821412645280361\n",
      "epoch 148 finished - avarage train loss 0.007595788711553504  avarage test loss 0.014849054627120495\n",
      "Training: epoch 149 batch 0 loss 0.00514349527657032\n",
      "Training: epoch 149 batch 10 loss 0.006540995091199875\n",
      "Training: epoch 149 batch 20 loss 0.004933943971991539\n",
      "Test: epoch 149 batch 0 loss 0.010371004231274128\n",
      "epoch 149 finished - avarage train loss 0.007331724936977543  avarage test loss 0.012352692661806941\n",
      "Training: epoch 150 batch 0 loss 0.012018347159028053\n",
      "Training: epoch 150 batch 10 loss 0.004874441307038069\n",
      "Training: epoch 150 batch 20 loss 0.007768281735479832\n",
      "Test: epoch 150 batch 0 loss 0.010946152731776237\n",
      "epoch 150 finished - avarage train loss 0.006819828975431878  avarage test loss 0.012696188874542713\n",
      "Training: epoch 151 batch 0 loss 0.00486920727416873\n",
      "Training: epoch 151 batch 10 loss 0.012277966365218163\n",
      "Training: epoch 151 batch 20 loss 0.0033713928423821926\n",
      "Test: epoch 151 batch 0 loss 0.011155454441905022\n",
      "epoch 151 finished - avarage train loss 0.007317193037155887  avarage test loss 0.012883275630883873\n",
      "Training: epoch 152 batch 0 loss 0.008611330762505531\n",
      "Training: epoch 152 batch 10 loss 0.003024057252332568\n",
      "Training: epoch 152 batch 20 loss 0.012238575145602226\n",
      "Test: epoch 152 batch 0 loss 0.012582739815115929\n",
      "epoch 152 finished - avarage train loss 0.007748795400277294  avarage test loss 0.014296341454610229\n",
      "Training: epoch 153 batch 0 loss 0.006241673603653908\n",
      "Training: epoch 153 batch 10 loss 0.008656615391373634\n",
      "Training: epoch 153 batch 20 loss 0.011864676140248775\n",
      "Test: epoch 153 batch 0 loss 0.01088065654039383\n",
      "epoch 153 finished - avarage train loss 0.008605120984580496  avarage test loss 0.01277632697019726\n",
      "Training: epoch 154 batch 0 loss 0.00993572361767292\n",
      "Training: epoch 154 batch 10 loss 0.003620243165642023\n",
      "Training: epoch 154 batch 20 loss 0.0024165944196283817\n",
      "Test: epoch 154 batch 0 loss 0.01088687777519226\n",
      "epoch 154 finished - avarage train loss 0.006883735993298991  avarage test loss 0.012732620816677809\n",
      "Training: epoch 155 batch 0 loss 0.005422207526862621\n",
      "Training: epoch 155 batch 10 loss 0.007316648028790951\n",
      "Training: epoch 155 batch 20 loss 0.0024953335523605347\n",
      "Test: epoch 155 batch 0 loss 0.010929570533335209\n",
      "epoch 155 finished - avarage train loss 0.00731603863071008  avarage test loss 0.012895741150714457\n",
      "Training: epoch 156 batch 0 loss 0.004604544956237078\n",
      "Training: epoch 156 batch 10 loss 0.008606532588601112\n",
      "Training: epoch 156 batch 20 loss 0.004434206057339907\n",
      "Test: epoch 156 batch 0 loss 0.010746496729552746\n",
      "epoch 156 finished - avarage train loss 0.006586376770303167  avarage test loss 0.012631442630663514\n",
      "Training: epoch 157 batch 0 loss 0.004273907747119665\n",
      "Training: epoch 157 batch 10 loss 0.002066666493192315\n",
      "Training: epoch 157 batch 20 loss 0.00372644723393023\n",
      "Test: epoch 157 batch 0 loss 0.010941879823803902\n",
      "epoch 157 finished - avarage train loss 0.006688908990954274  avarage test loss 0.012743499362841249\n",
      "Training: epoch 158 batch 0 loss 0.0017136624082922935\n",
      "Training: epoch 158 batch 10 loss 0.0016933033475652337\n",
      "Training: epoch 158 batch 20 loss 0.003761186497285962\n",
      "Test: epoch 158 batch 0 loss 0.010635134764015675\n",
      "epoch 158 finished - avarage train loss 0.006487317162501658  avarage test loss 0.01265369902830571\n",
      "Training: epoch 159 batch 0 loss 0.006086359266191721\n",
      "Training: epoch 159 batch 10 loss 0.002678843680769205\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training: epoch 159 batch 20 loss 0.0051180957816541195\n",
      "Test: epoch 159 batch 0 loss 0.011097081005573273\n",
      "epoch 159 finished - avarage train loss 0.006369048063161558  avarage test loss 0.012957698432728648\n",
      "Training: epoch 160 batch 0 loss 0.0030879811383783817\n",
      "Training: epoch 160 batch 10 loss 0.0032974565401673317\n",
      "Training: epoch 160 batch 20 loss 0.006681533996015787\n",
      "Test: epoch 160 batch 0 loss 0.01141597330570221\n",
      "epoch 160 finished - avarage train loss 0.006026119860852587  avarage test loss 0.013336582342162728\n",
      "Training: epoch 161 batch 0 loss 0.006067944224923849\n",
      "Training: epoch 161 batch 10 loss 0.004634435288608074\n",
      "Training: epoch 161 batch 20 loss 0.005824482534080744\n",
      "Test: epoch 161 batch 0 loss 0.009364090859889984\n",
      "epoch 161 finished - avarage train loss 0.008978365775968495  avarage test loss 0.012201662524603307\n",
      "Training: epoch 162 batch 0 loss 0.011939560063183308\n",
      "Training: epoch 162 batch 10 loss 0.007615466136485338\n",
      "Training: epoch 162 batch 20 loss 0.005263748113065958\n",
      "Test: epoch 162 batch 0 loss 0.010106392204761505\n",
      "epoch 162 finished - avarage train loss 0.007335976882163307  avarage test loss 0.012554254964925349\n",
      "Training: epoch 163 batch 0 loss 0.0029367380775511265\n",
      "Training: epoch 163 batch 10 loss 0.002529351506382227\n",
      "Training: epoch 163 batch 20 loss 0.0034706650767475367\n",
      "Test: epoch 163 batch 0 loss 0.0101466104388237\n",
      "epoch 163 finished - avarage train loss 0.006362808595315136  avarage test loss 0.01236206479370594\n",
      "Training: epoch 164 batch 0 loss 0.005772328935563564\n",
      "Training: epoch 164 batch 10 loss 0.0047323377802968025\n",
      "Training: epoch 164 batch 20 loss 0.006032883655279875\n",
      "Test: epoch 164 batch 0 loss 0.010701313614845276\n",
      "epoch 164 finished - avarage train loss 0.006886682599574199  avarage test loss 0.013006694498471916\n",
      "Training: epoch 165 batch 0 loss 0.006032808218151331\n",
      "Training: epoch 165 batch 10 loss 0.005543685518205166\n",
      "Training: epoch 165 batch 20 loss 0.005382073577493429\n",
      "Test: epoch 165 batch 0 loss 0.00902969017624855\n",
      "epoch 165 finished - avarage train loss 0.006924470766158453  avarage test loss 0.013764054980129004\n",
      "Training: epoch 166 batch 0 loss 0.011693008244037628\n",
      "Training: epoch 166 batch 10 loss 0.020473770797252655\n",
      "Training: epoch 166 batch 20 loss 0.007190657779574394\n",
      "Test: epoch 166 batch 0 loss 0.011595790274441242\n",
      "epoch 166 finished - avarage train loss 0.008671655688948673  avarage test loss 0.013945769751444459\n",
      "Training: epoch 167 batch 0 loss 0.01193747203797102\n",
      "Training: epoch 167 batch 10 loss 0.009905426762998104\n",
      "Training: epoch 167 batch 20 loss 0.004333637189120054\n",
      "Test: epoch 167 batch 0 loss 0.012391671538352966\n",
      "epoch 167 finished - avarage train loss 0.006351054927077273  avarage test loss 0.015119968331418931\n",
      "Training: epoch 168 batch 0 loss 0.008106417953968048\n",
      "Training: epoch 168 batch 10 loss 0.010554767213761806\n",
      "Training: epoch 168 batch 20 loss 0.0035685780458152294\n",
      "Test: epoch 168 batch 0 loss 0.01016208715736866\n",
      "epoch 168 finished - avarage train loss 0.007209496163152929  avarage test loss 0.012612061807885766\n",
      "Training: epoch 169 batch 0 loss 0.002366321859881282\n",
      "Training: epoch 169 batch 10 loss 0.006631307303905487\n",
      "Training: epoch 169 batch 20 loss 0.00841427594423294\n",
      "Test: epoch 169 batch 0 loss 0.010219147428870201\n",
      "epoch 169 finished - avarage train loss 0.006617585967840819  avarage test loss 0.01255770050920546\n",
      "Training: epoch 170 batch 0 loss 0.009070214815437794\n",
      "Training: epoch 170 batch 10 loss 0.009576989337801933\n",
      "Training: epoch 170 batch 20 loss 0.008310490287840366\n",
      "Test: epoch 170 batch 0 loss 0.016550840809941292\n",
      "epoch 170 finished - avarage train loss 0.007496342765456387  avarage test loss 0.014983277302235365\n",
      "Training: epoch 171 batch 0 loss 0.005971600767225027\n",
      "Training: epoch 171 batch 10 loss 0.0023696906864643097\n",
      "Training: epoch 171 batch 20 loss 0.006899120286107063\n",
      "Test: epoch 171 batch 0 loss 0.017063260078430176\n",
      "epoch 171 finished - avarage train loss 0.006555360276252031  avarage test loss 0.015256309183314443\n",
      "Training: epoch 172 batch 0 loss 0.004700677469372749\n",
      "Training: epoch 172 batch 10 loss 0.005020825657993555\n",
      "Training: epoch 172 batch 20 loss 0.006244915537536144\n",
      "Test: epoch 172 batch 0 loss 0.019681528210639954\n",
      "epoch 172 finished - avarage train loss 0.007245883080658728  avarage test loss 0.01593738584779203\n",
      "Training: epoch 173 batch 0 loss 0.005675147287547588\n",
      "Training: epoch 173 batch 10 loss 0.009103440679609776\n",
      "Training: epoch 173 batch 20 loss 0.003381692338734865\n",
      "Test: epoch 173 batch 0 loss 0.013598940335214138\n",
      "epoch 173 finished - avarage train loss 0.007543661864474416  avarage test loss 0.014611246064305305\n",
      "Training: epoch 174 batch 0 loss 0.01096677128225565\n",
      "Training: epoch 174 batch 10 loss 0.002334550255909562\n",
      "Training: epoch 174 batch 20 loss 0.00426662340760231\n",
      "Test: epoch 174 batch 0 loss 0.011255648918449879\n",
      "epoch 174 finished - avarage train loss 0.007025865428470846  avarage test loss 0.013770304853096604\n",
      "Training: epoch 175 batch 0 loss 0.0056165773421525955\n",
      "Training: epoch 175 batch 10 loss 0.0042352499440312386\n",
      "Training: epoch 175 batch 20 loss 0.0016909020487219095\n",
      "Test: epoch 175 batch 0 loss 0.010949278250336647\n",
      "epoch 175 finished - avarage train loss 0.007908021123951366  avarage test loss 0.013189381919801235\n",
      "Training: epoch 176 batch 0 loss 0.009263056330382824\n",
      "Training: epoch 176 batch 10 loss 0.010424640960991383\n",
      "Training: epoch 176 batch 20 loss 0.0026853398885577917\n",
      "Test: epoch 176 batch 0 loss 0.01050208043307066\n",
      "epoch 176 finished - avarage train loss 0.007481908995722388  avarage test loss 0.013094249065034091\n",
      "Training: epoch 177 batch 0 loss 0.005005400162190199\n",
      "Training: epoch 177 batch 10 loss 0.001859051059000194\n",
      "Training: epoch 177 batch 20 loss 0.008721484802663326\n",
      "Test: epoch 177 batch 0 loss 0.010382010601460934\n",
      "epoch 177 finished - avarage train loss 0.006635415259395437  avarage test loss 0.013575397664681077\n",
      "Training: epoch 178 batch 0 loss 0.005164118483662605\n",
      "Training: epoch 178 batch 10 loss 0.008906427770853043\n",
      "Training: epoch 178 batch 20 loss 0.006243624724447727\n",
      "Test: epoch 178 batch 0 loss 0.010848631151020527\n",
      "epoch 178 finished - avarage train loss 0.008766088185125384  avarage test loss 0.013182095135562122\n",
      "Training: epoch 179 batch 0 loss 0.0036125497426837683\n",
      "Training: epoch 179 batch 10 loss 0.008147941902279854\n",
      "Training: epoch 179 batch 20 loss 0.001525118132121861\n",
      "Test: epoch 179 batch 0 loss 0.010156202130019665\n",
      "epoch 179 finished - avarage train loss 0.006148666898109789  avarage test loss 0.012115996447391808\n",
      "Training: epoch 180 batch 0 loss 0.0021601906046271324\n",
      "Training: epoch 180 batch 10 loss 0.002485843375325203\n",
      "Training: epoch 180 batch 20 loss 0.005742312408983707\n",
      "Test: epoch 180 batch 0 loss 0.011138702742755413\n",
      "epoch 180 finished - avarage train loss 0.005735106139989762  avarage test loss 0.013497124658897519\n",
      "Training: epoch 181 batch 0 loss 0.0025319529231637716\n",
      "Training: epoch 181 batch 10 loss 0.0035409661941230297\n",
      "Training: epoch 181 batch 20 loss 0.004022317938506603\n",
      "Test: epoch 181 batch 0 loss 0.009825002402067184\n",
      "epoch 181 finished - avarage train loss 0.0067504296753684  avarage test loss 0.012079160893335938\n",
      "Training: epoch 182 batch 0 loss 0.0031784954480826855\n",
      "Training: epoch 182 batch 10 loss 0.00438492838293314\n",
      "Training: epoch 182 batch 20 loss 0.007147650700062513\n",
      "Test: epoch 182 batch 0 loss 0.011076191440224648\n",
      "epoch 182 finished - avarage train loss 0.005967973354111971  avarage test loss 0.01311517192516476\n",
      "Training: epoch 183 batch 0 loss 0.006626702379435301\n",
      "Training: epoch 183 batch 10 loss 0.008395334705710411\n",
      "Training: epoch 183 batch 20 loss 0.005826221313327551\n",
      "Test: epoch 183 batch 0 loss 0.009864497929811478\n",
      "epoch 183 finished - avarage train loss 0.007132066309387828  avarage test loss 0.011735670850612223\n",
      "Training: epoch 184 batch 0 loss 0.004700989928096533\n",
      "Training: epoch 184 batch 10 loss 0.004085213877260685\n",
      "Training: epoch 184 batch 20 loss 0.004498463124036789\n",
      "Test: epoch 184 batch 0 loss 0.009522584266960621\n",
      "epoch 184 finished - avarage train loss 0.006244162711764461  avarage test loss 0.011584556195884943\n",
      "Training: epoch 185 batch 0 loss 0.007396012078970671\n",
      "Training: epoch 185 batch 10 loss 0.01271687913686037\n",
      "Training: epoch 185 batch 20 loss 0.006313082296401262\n",
      "Test: epoch 185 batch 0 loss 0.00979742780327797\n",
      "epoch 185 finished - avarage train loss 0.006727985443611597  avarage test loss 0.01187539054080844\n",
      "Training: epoch 186 batch 0 loss 0.003303956938907504\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training: epoch 186 batch 10 loss 0.004853914491832256\n",
      "Training: epoch 186 batch 20 loss 0.0071880207397043705\n",
      "Test: epoch 186 batch 0 loss 0.009532159194350243\n",
      "epoch 186 finished - avarage train loss 0.006357182459584598  avarage test loss 0.011620424687862396\n",
      "Training: epoch 187 batch 0 loss 0.0037572693545371294\n",
      "Training: epoch 187 batch 10 loss 0.0025258888490498066\n",
      "Training: epoch 187 batch 20 loss 0.009634103626012802\n",
      "Test: epoch 187 batch 0 loss 0.008862627670168877\n",
      "epoch 187 finished - avarage train loss 0.005320548778399825  avarage test loss 0.011369480635039508\n",
      "Training: epoch 188 batch 0 loss 0.010072657838463783\n",
      "Training: epoch 188 batch 10 loss 0.003709888318553567\n",
      "Training: epoch 188 batch 20 loss 0.0036328344140201807\n",
      "Test: epoch 188 batch 0 loss 0.009597240015864372\n",
      "epoch 188 finished - avarage train loss 0.007270317813702698  avarage test loss 0.011641113902442157\n",
      "Training: epoch 189 batch 0 loss 0.005245773587375879\n",
      "Training: epoch 189 batch 10 loss 0.004151228815317154\n",
      "Training: epoch 189 batch 20 loss 0.003759917803108692\n",
      "Test: epoch 189 batch 0 loss 0.010737466625869274\n",
      "epoch 189 finished - avarage train loss 0.005075371968720494  avarage test loss 0.01302104361820966\n",
      "Training: epoch 190 batch 0 loss 0.00423470139503479\n",
      "Training: epoch 190 batch 10 loss 0.0029250141233205795\n",
      "Training: epoch 190 batch 20 loss 0.006092557683587074\n",
      "Test: epoch 190 batch 0 loss 0.009124082513153553\n",
      "epoch 190 finished - avarage train loss 0.006960639328663719  avarage test loss 0.011865874170325696\n",
      "Training: epoch 191 batch 0 loss 0.010211009532213211\n",
      "Training: epoch 191 batch 10 loss 0.009150896221399307\n",
      "Training: epoch 191 batch 20 loss 0.004247733391821384\n",
      "Test: epoch 191 batch 0 loss 0.010160768404603004\n",
      "epoch 191 finished - avarage train loss 0.007115416276943067  avarage test loss 0.012015773332677782\n",
      "Training: epoch 192 batch 0 loss 0.006911789998412132\n",
      "Training: epoch 192 batch 10 loss 0.002411862136796117\n",
      "Training: epoch 192 batch 20 loss 0.0047644660808146\n",
      "Test: epoch 192 batch 0 loss 0.010465884581208229\n",
      "epoch 192 finished - avarage train loss 0.006299782134913679  avarage test loss 0.012391474563628435\n",
      "Training: epoch 193 batch 0 loss 0.006064672023057938\n",
      "Training: epoch 193 batch 10 loss 0.010115526616573334\n",
      "Training: epoch 193 batch 20 loss 0.010046158917248249\n",
      "Test: epoch 193 batch 0 loss 0.010235768742859364\n",
      "epoch 193 finished - avarage train loss 0.0076854644535944375  avarage test loss 0.012460129102692008\n",
      "Training: epoch 194 batch 0 loss 0.0025122808292508125\n",
      "Training: epoch 194 batch 10 loss 0.005064669065177441\n",
      "Training: epoch 194 batch 20 loss 0.00614542793482542\n",
      "Test: epoch 194 batch 0 loss 0.010222557000815868\n",
      "epoch 194 finished - avarage train loss 0.0074662402079537  avarage test loss 0.012279063113965094\n",
      "Training: epoch 195 batch 0 loss 0.007667649537324905\n",
      "Training: epoch 195 batch 10 loss 0.005871109198778868\n",
      "Training: epoch 195 batch 20 loss 0.006079914979636669\n",
      "Test: epoch 195 batch 0 loss 0.010201746597886086\n",
      "epoch 195 finished - avarage train loss 0.005743239480391915  avarage test loss 0.012093157041817904\n",
      "Training: epoch 196 batch 0 loss 0.005798434838652611\n",
      "Training: epoch 196 batch 10 loss 0.010448344051837921\n",
      "Training: epoch 196 batch 20 loss 0.008794628083705902\n",
      "Test: epoch 196 batch 0 loss 0.010231290943920612\n",
      "epoch 196 finished - avarage train loss 0.006710420527238527  avarage test loss 0.01220060687046498\n",
      "Training: epoch 197 batch 0 loss 0.0033994412515312433\n",
      "Training: epoch 197 batch 10 loss 0.007402328308671713\n",
      "Training: epoch 197 batch 20 loss 0.0036899088881909847\n",
      "Test: epoch 197 batch 0 loss 0.011230924166738987\n",
      "epoch 197 finished - avarage train loss 0.006979730767037334  avarage test loss 0.013603274361230433\n",
      "Training: epoch 198 batch 0 loss 0.0048092142678797245\n",
      "Training: epoch 198 batch 10 loss 0.0028296576347202063\n",
      "Training: epoch 198 batch 20 loss 0.006275422405451536\n",
      "Test: epoch 198 batch 0 loss 0.009741604328155518\n",
      "epoch 198 finished - avarage train loss 0.006956546471036714  avarage test loss 0.012010304373688996\n",
      "Training: epoch 199 batch 0 loss 0.003324661171063781\n",
      "Training: epoch 199 batch 10 loss 0.0067005218006670475\n",
      "Training: epoch 199 batch 20 loss 0.0054428549483418465\n",
      "Test: epoch 199 batch 0 loss 0.009456615895032883\n",
      "epoch 199 finished - avarage train loss 0.007009338535336328  avarage test loss 0.011668111314065754\n",
      "Training: epoch 0 batch 0 loss 0.5209597945213318\n",
      "Training: epoch 0 batch 10 loss 0.3772442042827606\n",
      "Training: epoch 0 batch 20 loss 0.7040218114852905\n",
      "Test: epoch 0 batch 0 loss 0.39310842752456665\n",
      "epoch 0 finished - avarage train loss 0.5292682082488619  avarage test loss 0.44584399834275246\n",
      "Training: epoch 1 batch 0 loss 0.5682798027992249\n",
      "Training: epoch 1 batch 10 loss 0.4551073908805847\n",
      "Training: epoch 1 batch 20 loss 0.6816847920417786\n",
      "Test: epoch 1 batch 0 loss 0.39898690581321716\n",
      "epoch 1 finished - avarage train loss 0.5103603241772487  avarage test loss 0.45028701424598694\n",
      "Training: epoch 2 batch 0 loss 0.4064543545246124\n",
      "Training: epoch 2 batch 10 loss 0.37882593274116516\n",
      "Training: epoch 2 batch 20 loss 0.19539473950862885\n",
      "Test: epoch 2 batch 0 loss 0.1896638572216034\n",
      "epoch 2 finished - avarage train loss 0.3766404372350923  avarage test loss 0.19837197288870811\n",
      "Training: epoch 3 batch 0 loss 0.1641761064529419\n",
      "Training: epoch 3 batch 10 loss 0.09404061734676361\n",
      "Training: epoch 3 batch 20 loss 0.0614246241748333\n",
      "Test: epoch 3 batch 0 loss 0.060158178210258484\n",
      "epoch 3 finished - avarage train loss 0.10085873305797577  avarage test loss 0.0670299232006073\n",
      "Training: epoch 4 batch 0 loss 0.04420823976397514\n",
      "Training: epoch 4 batch 10 loss 0.05347106605768204\n",
      "Training: epoch 4 batch 20 loss 0.036390453577041626\n",
      "Test: epoch 4 batch 0 loss 0.035749197006225586\n",
      "epoch 4 finished - avarage train loss 0.04186981319096582  avarage test loss 0.04259673785418272\n",
      "Training: epoch 5 batch 0 loss 0.018953189253807068\n",
      "Training: epoch 5 batch 10 loss 0.03478288650512695\n",
      "Training: epoch 5 batch 20 loss 0.017171863466501236\n",
      "Test: epoch 5 batch 0 loss 0.030153745785355568\n",
      "epoch 5 finished - avarage train loss 0.028499004655870897  avarage test loss 0.03625596035271883\n",
      "Training: epoch 6 batch 0 loss 0.022872446104884148\n",
      "Training: epoch 6 batch 10 loss 0.027100492268800735\n",
      "Training: epoch 6 batch 20 loss 0.021991733461618423\n",
      "Test: epoch 6 batch 0 loss 0.02780458703637123\n",
      "epoch 6 finished - avarage train loss 0.023562335890942608  avarage test loss 0.03451341204345226\n",
      "Training: epoch 7 batch 0 loss 0.024524018168449402\n",
      "Training: epoch 7 batch 10 loss 0.02209055796265602\n",
      "Training: epoch 7 batch 20 loss 0.018316172063350677\n",
      "Test: epoch 7 batch 0 loss 0.028105448931455612\n",
      "epoch 7 finished - avarage train loss 0.02232775233429054  avarage test loss 0.034845036920160055\n",
      "Training: epoch 8 batch 0 loss 0.02852998673915863\n",
      "Training: epoch 8 batch 10 loss 0.015970785170793533\n",
      "Training: epoch 8 batch 20 loss 0.014881481416523457\n",
      "Test: epoch 8 batch 0 loss 0.027223510667681694\n",
      "epoch 8 finished - avarage train loss 0.022575385226257915  avarage test loss 0.034245196264237165\n",
      "Training: epoch 9 batch 0 loss 0.029088737443089485\n",
      "Training: epoch 9 batch 10 loss 0.03212215006351471\n",
      "Training: epoch 9 batch 20 loss 0.02401256375014782\n",
      "Test: epoch 9 batch 0 loss 0.027170654386281967\n",
      "epoch 9 finished - avarage train loss 0.022514328970734417  avarage test loss 0.03410925855860114\n",
      "Training: epoch 10 batch 0 loss 0.031586211174726486\n",
      "Training: epoch 10 batch 10 loss 0.02523738332092762\n",
      "Training: epoch 10 batch 20 loss 0.03458699956536293\n",
      "Test: epoch 10 batch 0 loss 0.02715247869491577\n",
      "epoch 10 finished - avarage train loss 0.022577476559271073  avarage test loss 0.03403924684971571\n",
      "Training: epoch 11 batch 0 loss 0.009560604579746723\n",
      "Training: epoch 11 batch 10 loss 0.02914341352880001\n",
      "Training: epoch 11 batch 20 loss 0.027287526056170464\n",
      "Test: epoch 11 batch 0 loss 0.02721283584833145\n",
      "epoch 11 finished - avarage train loss 0.023258678470577658  avarage test loss 0.034483276307582855\n",
      "Training: epoch 12 batch 0 loss 0.018760045990347862\n",
      "Training: epoch 12 batch 10 loss 0.027954664081335068\n",
      "Training: epoch 12 batch 20 loss 0.01858639530837536\n",
      "Test: epoch 12 batch 0 loss 0.027412589639425278\n",
      "epoch 12 finished - avarage train loss 0.022539601018973465  avarage test loss 0.03466793010011315\n",
      "Training: epoch 13 batch 0 loss 0.023370245471596718\n",
      "Training: epoch 13 batch 10 loss 0.015385159291327\n",
      "Training: epoch 13 batch 20 loss 0.014054232276976109\n",
      "Test: epoch 13 batch 0 loss 0.028924763202667236\n",
      "epoch 13 finished - avarage train loss 0.0236650586192464  avarage test loss 0.03571808338165283\n",
      "Training: epoch 14 batch 0 loss 0.03093213401734829\n",
      "Training: epoch 14 batch 10 loss 0.028283629566431046\n",
      "Training: epoch 14 batch 20 loss 0.020039986819028854\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test: epoch 14 batch 0 loss 0.027350399643182755\n",
      "epoch 14 finished - avarage train loss 0.022039204868006295  avarage test loss 0.03428880963474512\n",
      "Training: epoch 15 batch 0 loss 0.02664966695010662\n",
      "Training: epoch 15 batch 10 loss 0.024600930511951447\n",
      "Training: epoch 15 batch 20 loss 0.020541811361908913\n",
      "Test: epoch 15 batch 0 loss 0.02658833935856819\n",
      "epoch 15 finished - avarage train loss 0.021593870113378967  avarage test loss 0.03391116997227073\n",
      "Training: epoch 16 batch 0 loss 0.020464295521378517\n",
      "Training: epoch 16 batch 10 loss 0.03860482946038246\n",
      "Training: epoch 16 batch 20 loss 0.033013615757226944\n",
      "Test: epoch 16 batch 0 loss 0.026447808369994164\n",
      "epoch 16 finished - avarage train loss 0.022314992829643446  avarage test loss 0.03368378896266222\n",
      "Training: epoch 17 batch 0 loss 0.01849422976374626\n",
      "Training: epoch 17 batch 10 loss 0.031640492379665375\n",
      "Training: epoch 17 batch 20 loss 0.01699414663016796\n",
      "Test: epoch 17 batch 0 loss 0.026181302964687347\n",
      "epoch 17 finished - avarage train loss 0.021520149444454705  avarage test loss 0.03350380389019847\n",
      "Training: epoch 18 batch 0 loss 0.013935655355453491\n",
      "Training: epoch 18 batch 10 loss 0.020240865647792816\n",
      "Training: epoch 18 batch 20 loss 0.0373736135661602\n",
      "Test: epoch 18 batch 0 loss 0.02635040134191513\n",
      "epoch 18 finished - avarage train loss 0.020951687229861474  avarage test loss 0.03380688093602657\n",
      "Training: epoch 19 batch 0 loss 0.01986948400735855\n",
      "Training: epoch 19 batch 10 loss 0.024466240778565407\n",
      "Training: epoch 19 batch 20 loss 0.03839705511927605\n",
      "Test: epoch 19 batch 0 loss 0.02510124072432518\n",
      "epoch 19 finished - avarage train loss 0.020907670002559137  avarage test loss 0.032945468090474606\n",
      "Training: epoch 20 batch 0 loss 0.023009387776255608\n",
      "Training: epoch 20 batch 10 loss 0.023854190483689308\n",
      "Training: epoch 20 batch 20 loss 0.01610533520579338\n",
      "Test: epoch 20 batch 0 loss 0.02218489907681942\n",
      "epoch 20 finished - avarage train loss 0.019993649570849436  avarage test loss 0.030915193259716034\n",
      "Training: epoch 21 batch 0 loss 0.015511446632444859\n",
      "Training: epoch 21 batch 10 loss 0.009500307030975819\n",
      "Training: epoch 21 batch 20 loss 0.01950918324291706\n",
      "Test: epoch 21 batch 0 loss 0.020834723487496376\n",
      "epoch 21 finished - avarage train loss 0.019659578479055702  avarage test loss 0.0304745277389884\n",
      "Training: epoch 22 batch 0 loss 0.01888718083500862\n",
      "Training: epoch 22 batch 10 loss 0.024030249565839767\n",
      "Training: epoch 22 batch 20 loss 0.011374789290130138\n",
      "Test: epoch 22 batch 0 loss 0.022512391209602356\n",
      "epoch 22 finished - avarage train loss 0.018263937070451933  avarage test loss 0.03257548715919256\n",
      "Training: epoch 23 batch 0 loss 0.02214425429701805\n",
      "Training: epoch 23 batch 10 loss 0.018256308510899544\n",
      "Training: epoch 23 batch 20 loss 0.020774897187948227\n",
      "Test: epoch 23 batch 0 loss 0.019860202446579933\n",
      "epoch 23 finished - avarage train loss 0.017597710826145165  avarage test loss 0.03036248404532671\n",
      "Training: epoch 24 batch 0 loss 0.019808407872915268\n",
      "Training: epoch 24 batch 10 loss 0.018638504669070244\n",
      "Training: epoch 24 batch 20 loss 0.0071583399549126625\n",
      "Test: epoch 24 batch 0 loss 0.01791166514158249\n",
      "epoch 24 finished - avarage train loss 0.01552676223218441  avarage test loss 0.02876926911994815\n",
      "Training: epoch 25 batch 0 loss 0.01989833265542984\n",
      "Training: epoch 25 batch 10 loss 0.011825263500213623\n",
      "Training: epoch 25 batch 20 loss 0.011404342018067837\n",
      "Test: epoch 25 batch 0 loss 0.017750954255461693\n",
      "epoch 25 finished - avarage train loss 0.015399127251243797  avarage test loss 0.028790590353310108\n",
      "Training: epoch 26 batch 0 loss 0.00849780160933733\n",
      "Training: epoch 26 batch 10 loss 0.01347767747938633\n",
      "Training: epoch 26 batch 20 loss 0.010430524125695229\n",
      "Test: epoch 26 batch 0 loss 0.014260753989219666\n",
      "epoch 26 finished - avarage train loss 0.013734928171696335  avarage test loss 0.025953568750992417\n",
      "Training: epoch 27 batch 0 loss 0.013081458397209644\n",
      "Training: epoch 27 batch 10 loss 0.007643621880561113\n",
      "Training: epoch 27 batch 20 loss 0.021627960726618767\n",
      "Test: epoch 27 batch 0 loss 0.014670519158244133\n",
      "epoch 27 finished - avarage train loss 0.013736877349558575  avarage test loss 0.025793759152293205\n",
      "Training: epoch 28 batch 0 loss 0.010853078216314316\n",
      "Training: epoch 28 batch 10 loss 0.013605689629912376\n",
      "Training: epoch 28 batch 20 loss 0.007757680956274271\n",
      "Test: epoch 28 batch 0 loss 0.013808703050017357\n",
      "epoch 28 finished - avarage train loss 0.013294434447869145  avarage test loss 0.019565948168747127\n",
      "Training: epoch 29 batch 0 loss 0.008763469755649567\n",
      "Training: epoch 29 batch 10 loss 0.008747615851461887\n",
      "Training: epoch 29 batch 20 loss 0.004438380245119333\n",
      "Test: epoch 29 batch 0 loss 0.01505197212100029\n",
      "epoch 29 finished - avarage train loss 0.01201097779618255  avarage test loss 0.0166615832131356\n",
      "Training: epoch 30 batch 0 loss 0.007448179647326469\n",
      "Training: epoch 30 batch 10 loss 0.008692534640431404\n",
      "Training: epoch 30 batch 20 loss 0.010171264410018921\n",
      "Test: epoch 30 batch 0 loss 0.00932207889854908\n",
      "epoch 30 finished - avarage train loss 0.009985925774250564  avarage test loss 0.013011590344831347\n",
      "Training: epoch 31 batch 0 loss 0.01836015097796917\n",
      "Training: epoch 31 batch 10 loss 0.00442781625315547\n",
      "Training: epoch 31 batch 20 loss 0.007507815025746822\n",
      "Test: epoch 31 batch 0 loss 0.011861913837492466\n",
      "epoch 31 finished - avarage train loss 0.009200232328268988  avarage test loss 0.013490424840711057\n",
      "Training: epoch 32 batch 0 loss 0.0033418687526136637\n",
      "Training: epoch 32 batch 10 loss 0.005377243272960186\n",
      "Training: epoch 32 batch 20 loss 0.005526504945009947\n",
      "Test: epoch 32 batch 0 loss 0.014884382486343384\n",
      "epoch 32 finished - avarage train loss 0.008377447611941346  avarage test loss 0.01774166547693312\n",
      "Training: epoch 33 batch 0 loss 0.011559395119547844\n",
      "Training: epoch 33 batch 10 loss 0.003779057413339615\n",
      "Training: epoch 33 batch 20 loss 0.006397992838174105\n",
      "Test: epoch 33 batch 0 loss 0.01386072114109993\n",
      "epoch 33 finished - avarage train loss 0.008713739426356965  avarage test loss 0.01609389530494809\n",
      "Training: epoch 34 batch 0 loss 0.011061951518058777\n",
      "Training: epoch 34 batch 10 loss 0.0026613478548824787\n",
      "Training: epoch 34 batch 20 loss 0.006078443955630064\n",
      "Test: epoch 34 batch 0 loss 0.011688411235809326\n",
      "epoch 34 finished - avarage train loss 0.006323375482240628  avarage test loss 0.01362459419760853\n",
      "Training: epoch 35 batch 0 loss 0.006324070505797863\n",
      "Training: epoch 35 batch 10 loss 0.005729658529162407\n",
      "Training: epoch 35 batch 20 loss 0.004633293487131596\n",
      "Test: epoch 35 batch 0 loss 0.010977520607411861\n",
      "epoch 35 finished - avarage train loss 0.007522381031243452  avarage test loss 0.013580060447566211\n",
      "Training: epoch 36 batch 0 loss 0.007369696162641048\n",
      "Training: epoch 36 batch 10 loss 0.010119388811290264\n",
      "Training: epoch 36 batch 20 loss 0.009264007210731506\n",
      "Test: epoch 36 batch 0 loss 0.01681852899491787\n",
      "epoch 36 finished - avarage train loss 0.008860522808892459  avarage test loss 0.018407003488391638\n",
      "Training: epoch 37 batch 0 loss 0.004165542311966419\n",
      "Training: epoch 37 batch 10 loss 0.010720906779170036\n",
      "Training: epoch 37 batch 20 loss 0.0050760661251842976\n",
      "Test: epoch 37 batch 0 loss 0.007760456297546625\n",
      "epoch 37 finished - avarage train loss 0.006777655742741351  avarage test loss 0.01235296600498259\n",
      "Training: epoch 38 batch 0 loss 0.005316602066159248\n",
      "Training: epoch 38 batch 10 loss 0.010764582082629204\n",
      "Training: epoch 38 batch 20 loss 0.005559145472943783\n",
      "Test: epoch 38 batch 0 loss 0.011525039561092854\n",
      "epoch 38 finished - avarage train loss 0.008657166356991592  avarage test loss 0.012878606095910072\n",
      "Training: epoch 39 batch 0 loss 0.005789920222014189\n",
      "Training: epoch 39 batch 10 loss 0.005497762002050877\n",
      "Training: epoch 39 batch 20 loss 0.005473296158015728\n",
      "Test: epoch 39 batch 0 loss 0.013104308396577835\n",
      "epoch 39 finished - avarage train loss 0.01150715784651452  avarage test loss 0.0166842935141176\n",
      "Training: epoch 40 batch 0 loss 0.008541248738765717\n",
      "Training: epoch 40 batch 10 loss 0.008032543584704399\n",
      "Training: epoch 40 batch 20 loss 0.00829144287854433\n",
      "Test: epoch 40 batch 0 loss 0.011255327612161636\n",
      "epoch 40 finished - avarage train loss 0.009361844558011869  avarage test loss 0.01369774283375591\n",
      "Training: epoch 41 batch 0 loss 0.009743689559400082\n",
      "Training: epoch 41 batch 10 loss 0.007157271262258291\n",
      "Training: epoch 41 batch 20 loss 0.012155705131590366\n",
      "Test: epoch 41 batch 0 loss 0.01400953158736229\n",
      "epoch 41 finished - avarage train loss 0.008451844035679924  avarage test loss 0.014513549511320889\n",
      "Training: epoch 42 batch 0 loss 0.005575158633291721\n",
      "Training: epoch 42 batch 10 loss 0.010393845848739147\n",
      "Training: epoch 42 batch 20 loss 0.008244303986430168\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test: epoch 42 batch 0 loss 0.01286361739039421\n",
      "epoch 42 finished - avarage train loss 0.0070140947635959966  avarage test loss 0.014533756067976356\n",
      "Training: epoch 43 batch 0 loss 0.006921873893588781\n",
      "Training: epoch 43 batch 10 loss 0.006834674626588821\n",
      "Training: epoch 43 batch 20 loss 0.003560673911124468\n",
      "Test: epoch 43 batch 0 loss 0.01447269320487976\n",
      "epoch 43 finished - avarage train loss 0.007599317219814864  avarage test loss 0.0141586052486673\n",
      "Training: epoch 44 batch 0 loss 0.005067251622676849\n",
      "Training: epoch 44 batch 10 loss 0.006185327190905809\n",
      "Training: epoch 44 batch 20 loss 0.008284264244139194\n",
      "Test: epoch 44 batch 0 loss 0.014119493775069714\n",
      "epoch 44 finished - avarage train loss 0.007874330104296577  avarage test loss 0.015312230214476585\n",
      "Training: epoch 45 batch 0 loss 0.0027616843581199646\n",
      "Training: epoch 45 batch 10 loss 0.0038932485040277243\n",
      "Training: epoch 45 batch 20 loss 0.004073339514434338\n",
      "Test: epoch 45 batch 0 loss 0.012610764242708683\n",
      "epoch 45 finished - avarage train loss 0.007986424041205439  avarage test loss 0.013817037921398878\n",
      "Training: epoch 46 batch 0 loss 0.006334863603115082\n",
      "Training: epoch 46 batch 10 loss 0.0061871763318777084\n",
      "Training: epoch 46 batch 20 loss 0.005381221882998943\n",
      "Test: epoch 46 batch 0 loss 0.011517277918756008\n",
      "epoch 46 finished - avarage train loss 0.007771346148039247  avarage test loss 0.01319695427082479\n",
      "Training: epoch 47 batch 0 loss 0.0059491051360964775\n",
      "Training: epoch 47 batch 10 loss 0.015322445891797543\n",
      "Training: epoch 47 batch 20 loss 0.00818041991442442\n",
      "Test: epoch 47 batch 0 loss 0.01339049730449915\n",
      "epoch 47 finished - avarage train loss 0.006577294861801483  avarage test loss 0.015498781227506697\n",
      "Training: epoch 48 batch 0 loss 0.0033971653319895267\n",
      "Training: epoch 48 batch 10 loss 0.007615929003804922\n",
      "Training: epoch 48 batch 20 loss 0.007852524518966675\n",
      "Test: epoch 48 batch 0 loss 0.013037210330367088\n",
      "epoch 48 finished - avarage train loss 0.006804959595620889  avarage test loss 0.013655731454491615\n",
      "Training: epoch 49 batch 0 loss 0.006525144912302494\n",
      "Training: epoch 49 batch 10 loss 0.005666999612003565\n",
      "Training: epoch 49 batch 20 loss 0.00584326172247529\n",
      "Test: epoch 49 batch 0 loss 0.010377823375165462\n",
      "epoch 49 finished - avarage train loss 0.007665485521007715  avarage test loss 0.015743021620437503\n",
      "Training: epoch 50 batch 0 loss 0.0333629846572876\n",
      "Training: epoch 50 batch 10 loss 0.0052189817652106285\n",
      "Training: epoch 50 batch 20 loss 0.003641711547970772\n",
      "Test: epoch 50 batch 0 loss 0.014338964596390724\n",
      "epoch 50 finished - avarage train loss 0.009427042156136755  avarage test loss 0.015478445682674646\n",
      "Training: epoch 51 batch 0 loss 0.006357783451676369\n",
      "Training: epoch 51 batch 10 loss 0.006825725547969341\n",
      "Training: epoch 51 batch 20 loss 0.00793042778968811\n",
      "Test: epoch 51 batch 0 loss 0.01328221894800663\n",
      "epoch 51 finished - avarage train loss 0.007534917154959564  avarage test loss 0.01394836779218167\n",
      "Training: epoch 52 batch 0 loss 0.002293940866366029\n",
      "Training: epoch 52 batch 10 loss 0.004927142057567835\n",
      "Training: epoch 52 batch 20 loss 0.003945906180888414\n",
      "Test: epoch 52 batch 0 loss 0.013249806128442287\n",
      "epoch 52 finished - avarage train loss 0.007065621670335531  avarage test loss 0.014183185761794448\n",
      "Training: epoch 53 batch 0 loss 0.005967820528894663\n",
      "Training: epoch 53 batch 10 loss 0.008655013516545296\n",
      "Training: epoch 53 batch 20 loss 0.005538060795515776\n",
      "Test: epoch 53 batch 0 loss 0.0157326553016901\n",
      "epoch 53 finished - avarage train loss 0.009042008804028919  avarage test loss 0.015823397785425186\n",
      "Training: epoch 54 batch 0 loss 0.0032024772372096777\n",
      "Training: epoch 54 batch 10 loss 0.0027073132805526257\n",
      "Training: epoch 54 batch 20 loss 0.00640669371932745\n",
      "Test: epoch 54 batch 0 loss 0.009791805408895016\n",
      "epoch 54 finished - avarage train loss 0.0071067431026363165  avarage test loss 0.01229180081281811\n",
      "Training: epoch 55 batch 0 loss 0.005362376105040312\n",
      "Training: epoch 55 batch 10 loss 0.0020614080131053925\n",
      "Training: epoch 55 batch 20 loss 0.004899760242551565\n",
      "Test: epoch 55 batch 0 loss 0.011448100209236145\n",
      "epoch 55 finished - avarage train loss 0.006610406956089468  avarage test loss 0.013392924331128597\n",
      "Training: epoch 56 batch 0 loss 0.007202335633337498\n",
      "Training: epoch 56 batch 10 loss 0.002308968221768737\n",
      "Training: epoch 56 batch 20 loss 0.0038928361609578133\n",
      "Test: epoch 56 batch 0 loss 0.012586291879415512\n",
      "epoch 56 finished - avarage train loss 0.006033215382746582  avarage test loss 0.014211297035217285\n",
      "Training: epoch 57 batch 0 loss 0.010743971914052963\n",
      "Training: epoch 57 batch 10 loss 0.01280753593891859\n",
      "Training: epoch 57 batch 20 loss 0.01424479577690363\n",
      "Test: epoch 57 batch 0 loss 0.013832825236022472\n",
      "epoch 57 finished - avarage train loss 0.00903339363666701  avarage test loss 0.015356127638369799\n",
      "Training: epoch 58 batch 0 loss 0.006159536074846983\n",
      "Training: epoch 58 batch 10 loss 0.0051597002893686295\n",
      "Training: epoch 58 batch 20 loss 0.007565742824226618\n",
      "Test: epoch 58 batch 0 loss 0.011331884190440178\n",
      "epoch 58 finished - avarage train loss 0.007495885249227285  avarage test loss 0.012753768998663872\n",
      "Training: epoch 59 batch 0 loss 0.011888976208865643\n",
      "Training: epoch 59 batch 10 loss 0.004340851213783026\n",
      "Training: epoch 59 batch 20 loss 0.006792084779590368\n",
      "Test: epoch 59 batch 0 loss 0.015185884200036526\n",
      "epoch 59 finished - avarage train loss 0.006407997394301768  avarage test loss 0.014749086694791913\n",
      "Training: epoch 60 batch 0 loss 0.005580459721386433\n",
      "Training: epoch 60 batch 10 loss 0.015117911621928215\n",
      "Training: epoch 60 batch 20 loss 0.004050480667501688\n",
      "Test: epoch 60 batch 0 loss 0.014487088657915592\n",
      "epoch 60 finished - avarage train loss 0.0073602783718499645  avarage test loss 0.01677269977517426\n",
      "Training: epoch 61 batch 0 loss 0.004990491084754467\n",
      "Training: epoch 61 batch 10 loss 0.003484379732981324\n",
      "Training: epoch 61 batch 20 loss 0.005563017446547747\n",
      "Test: epoch 61 batch 0 loss 0.01644725911319256\n",
      "epoch 61 finished - avarage train loss 0.006077985483992459  avarage test loss 0.02038677968084812\n",
      "Training: epoch 62 batch 0 loss 0.009150050580501556\n",
      "Training: epoch 62 batch 10 loss 0.008763549849390984\n",
      "Training: epoch 62 batch 20 loss 0.007479509804397821\n",
      "Test: epoch 62 batch 0 loss 0.013618167489767075\n",
      "epoch 62 finished - avarage train loss 0.010368956694507906  avarage test loss 0.013615304487757385\n",
      "Training: epoch 63 batch 0 loss 0.010052698664367199\n",
      "Training: epoch 63 batch 10 loss 0.002835948718711734\n",
      "Training: epoch 63 batch 20 loss 0.003302458208054304\n",
      "Test: epoch 63 batch 0 loss 0.013715817593038082\n",
      "epoch 63 finished - avarage train loss 0.006480207987900438  avarage test loss 0.014932205551303923\n",
      "Training: epoch 64 batch 0 loss 0.004976394586265087\n",
      "Training: epoch 64 batch 10 loss 0.013243838213384151\n",
      "Training: epoch 64 batch 20 loss 0.0026288593653589487\n",
      "Test: epoch 64 batch 0 loss 0.013893116265535355\n",
      "epoch 64 finished - avarage train loss 0.009200065625542453  avarage test loss 0.015322487335652113\n",
      "Training: epoch 65 batch 0 loss 0.007000857964158058\n",
      "Training: epoch 65 batch 10 loss 0.004993516486138105\n",
      "Training: epoch 65 batch 20 loss 0.004617384634912014\n",
      "Test: epoch 65 batch 0 loss 0.011576616205275059\n",
      "epoch 65 finished - avarage train loss 0.006981150873390765  avarage test loss 0.0131327552953735\n",
      "Training: epoch 66 batch 0 loss 0.003729241667315364\n",
      "Training: epoch 66 batch 10 loss 0.006137209944427013\n",
      "Training: epoch 66 batch 20 loss 0.0039031170308589935\n",
      "Test: epoch 66 batch 0 loss 0.01369072962552309\n",
      "epoch 66 finished - avarage train loss 0.007200030866881897  avarage test loss 0.016004257602617145\n",
      "Training: epoch 67 batch 0 loss 0.004342346917837858\n",
      "Training: epoch 67 batch 10 loss 0.0023166141472756863\n",
      "Training: epoch 67 batch 20 loss 0.00384574756026268\n",
      "Test: epoch 67 batch 0 loss 0.011934957467019558\n",
      "epoch 67 finished - avarage train loss 0.005932803879941589  avarage test loss 0.013019367353990674\n",
      "Training: epoch 68 batch 0 loss 0.0036701580975204706\n",
      "Training: epoch 68 batch 10 loss 0.0022382796742022038\n",
      "Training: epoch 68 batch 20 loss 0.007168305106461048\n",
      "Test: epoch 68 batch 0 loss 0.011365845799446106\n",
      "epoch 68 finished - avarage train loss 0.006693697775360839  avarage test loss 0.012902407790534198\n",
      "Training: epoch 69 batch 0 loss 0.011070510372519493\n",
      "Training: epoch 69 batch 10 loss 0.004783349111676216\n",
      "Training: epoch 69 batch 20 loss 0.002331078751012683\n",
      "Test: epoch 69 batch 0 loss 0.013553403317928314\n",
      "epoch 69 finished - avarage train loss 0.006642598755143839  avarage test loss 0.01808138540945947\n",
      "Training: epoch 70 batch 0 loss 0.0036619657184928656\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training: epoch 70 batch 10 loss 0.005510397721081972\n",
      "Training: epoch 70 batch 20 loss 0.007597100920975208\n",
      "Test: epoch 70 batch 0 loss 0.013901790603995323\n",
      "epoch 70 finished - avarage train loss 0.007914961998126116  avarage test loss 0.014679378480650485\n",
      "Training: epoch 71 batch 0 loss 0.007226783782243729\n",
      "Training: epoch 71 batch 10 loss 0.006240352988243103\n",
      "Training: epoch 71 batch 20 loss 0.012427432462573051\n",
      "Test: epoch 71 batch 0 loss 0.012997888959944248\n",
      "epoch 71 finished - avarage train loss 0.008061490134047023  avarage test loss 0.014485323918052018\n",
      "Training: epoch 72 batch 0 loss 0.004683090373873711\n",
      "Training: epoch 72 batch 10 loss 0.005095050670206547\n",
      "Training: epoch 72 batch 20 loss 0.00597370695322752\n",
      "Test: epoch 72 batch 0 loss 0.01495432946830988\n",
      "epoch 72 finished - avarage train loss 0.008405051651764018  avarage test loss 0.015444082906469703\n",
      "Training: epoch 73 batch 0 loss 0.004022830165922642\n",
      "Training: epoch 73 batch 10 loss 0.010361438617110252\n",
      "Training: epoch 73 batch 20 loss 0.022031867876648903\n",
      "Test: epoch 73 batch 0 loss 0.012134921737015247\n",
      "epoch 73 finished - avarage train loss 0.00820483443937425  avarage test loss 0.013159329304471612\n",
      "Training: epoch 74 batch 0 loss 0.004328753333538771\n",
      "Training: epoch 74 batch 10 loss 0.004815255757421255\n",
      "Training: epoch 74 batch 20 loss 0.008141258731484413\n",
      "Test: epoch 74 batch 0 loss 0.01288539543747902\n",
      "epoch 74 finished - avarage train loss 0.006969896353909682  avarage test loss 0.016246764454990625\n",
      "Training: epoch 75 batch 0 loss 0.004110725596547127\n",
      "Training: epoch 75 batch 10 loss 0.004831776022911072\n",
      "Training: epoch 75 batch 20 loss 0.008913591504096985\n",
      "Test: epoch 75 batch 0 loss 0.011314725503325462\n",
      "epoch 75 finished - avarage train loss 0.007486655521367131  avarage test loss 0.012374836660455912\n",
      "Training: epoch 76 batch 0 loss 0.005284069571644068\n",
      "Training: epoch 76 batch 10 loss 0.005264079663902521\n",
      "Training: epoch 76 batch 20 loss 0.008711585775017738\n",
      "Test: epoch 76 batch 0 loss 0.012826571241021156\n",
      "epoch 76 finished - avarage train loss 0.008818531970910985  avarage test loss 0.017244659597054124\n",
      "Training: epoch 77 batch 0 loss 0.009701737202703953\n",
      "Training: epoch 77 batch 10 loss 0.008371810428798199\n",
      "Training: epoch 77 batch 20 loss 0.010071290656924248\n",
      "Test: epoch 77 batch 0 loss 0.014113948680460453\n",
      "epoch 77 finished - avarage train loss 0.007332551056230119  avarage test loss 0.018129106843844056\n",
      "Training: epoch 78 batch 0 loss 0.005911687854677439\n",
      "Training: epoch 78 batch 10 loss 0.008546569384634495\n",
      "Training: epoch 78 batch 20 loss 0.0050947898998856544\n",
      "Test: epoch 78 batch 0 loss 0.010891403071582317\n",
      "epoch 78 finished - avarage train loss 0.007943895757840625  avarage test loss 0.013258302235044539\n",
      "Training: epoch 79 batch 0 loss 0.006245276890695095\n",
      "Training: epoch 79 batch 10 loss 0.0060803540982306\n",
      "Training: epoch 79 batch 20 loss 0.002576343948021531\n",
      "Test: epoch 79 batch 0 loss 0.012137033976614475\n",
      "epoch 79 finished - avarage train loss 0.006376315783388142  avarage test loss 0.013792061479762197\n",
      "Training: epoch 80 batch 0 loss 0.0029271359089761972\n",
      "Training: epoch 80 batch 10 loss 0.003695669351145625\n",
      "Training: epoch 80 batch 20 loss 0.002918266924098134\n",
      "Test: epoch 80 batch 0 loss 0.011744489893317223\n",
      "epoch 80 finished - avarage train loss 0.006397264989510435  avarage test loss 0.012786835781298578\n",
      "Training: epoch 81 batch 0 loss 0.00397763354703784\n",
      "Training: epoch 81 batch 10 loss 0.008207524195313454\n",
      "Training: epoch 81 batch 20 loss 0.002983438316732645\n",
      "Test: epoch 81 batch 0 loss 0.013947329483926296\n",
      "epoch 81 finished - avarage train loss 0.006369060435300243  avarage test loss 0.017255445360206068\n",
      "Training: epoch 82 batch 0 loss 0.005923745688050985\n",
      "Training: epoch 82 batch 10 loss 0.009947293438017368\n",
      "Training: epoch 82 batch 20 loss 0.005133441183716059\n",
      "Test: epoch 82 batch 0 loss 0.011720172129571438\n",
      "epoch 82 finished - avarage train loss 0.006861147864175768  avarage test loss 0.013034093077294528\n",
      "Training: epoch 83 batch 0 loss 0.009357273578643799\n",
      "Training: epoch 83 batch 10 loss 0.006545142270624638\n",
      "Training: epoch 83 batch 20 loss 0.006313125602900982\n",
      "Test: epoch 83 batch 0 loss 0.010589133016765118\n",
      "epoch 83 finished - avarage train loss 0.006383383237146612  avarage test loss 0.013614483876153827\n",
      "Training: epoch 84 batch 0 loss 0.007821321487426758\n",
      "Training: epoch 84 batch 10 loss 0.005817962344735861\n",
      "Training: epoch 84 batch 20 loss 0.004697049502283335\n",
      "Test: epoch 84 batch 0 loss 0.011780448257923126\n",
      "epoch 84 finished - avarage train loss 0.006696388542909047  avarage test loss 0.012854628497734666\n",
      "Training: epoch 85 batch 0 loss 0.010401752777397633\n",
      "Training: epoch 85 batch 10 loss 0.005186031572520733\n",
      "Training: epoch 85 batch 20 loss 0.005374409258365631\n",
      "Test: epoch 85 batch 0 loss 0.014765577390789986\n",
      "epoch 85 finished - avarage train loss 0.005972995426794836  avarage test loss 0.0162754439515993\n",
      "Training: epoch 86 batch 0 loss 0.006477731745690107\n",
      "Training: epoch 86 batch 10 loss 0.00752611318603158\n",
      "Training: epoch 86 batch 20 loss 0.00912338588386774\n",
      "Test: epoch 86 batch 0 loss 0.012806359678506851\n",
      "epoch 86 finished - avarage train loss 0.009386263522801214  avarage test loss 0.013638536911457777\n",
      "Training: epoch 87 batch 0 loss 0.014043460600078106\n",
      "Training: epoch 87 batch 10 loss 0.01669040136039257\n",
      "Training: epoch 87 batch 20 loss 0.0026697558350861073\n",
      "Test: epoch 87 batch 0 loss 0.011334318667650223\n",
      "epoch 87 finished - avarage train loss 0.006388612195913647  avarage test loss 0.01303904049564153\n",
      "Training: epoch 88 batch 0 loss 0.0046163746155798435\n",
      "Training: epoch 88 batch 10 loss 0.0018638167530298233\n",
      "Training: epoch 88 batch 20 loss 0.005080491304397583\n",
      "Test: epoch 88 batch 0 loss 0.012018928304314613\n",
      "epoch 88 finished - avarage train loss 0.0074954908501742214  avarage test loss 0.014310596860013902\n",
      "Training: epoch 89 batch 0 loss 0.003936098888516426\n",
      "Training: epoch 89 batch 10 loss 0.002744481433182955\n",
      "Training: epoch 89 batch 20 loss 0.00384380086325109\n",
      "Test: epoch 89 batch 0 loss 0.010627790354192257\n",
      "epoch 89 finished - avarage train loss 0.007623300879615648  avarage test loss 0.01308573188725859\n",
      "Training: epoch 90 batch 0 loss 0.004575058817863464\n",
      "Training: epoch 90 batch 10 loss 0.007231751456856728\n",
      "Training: epoch 90 batch 20 loss 0.007760218344628811\n",
      "Test: epoch 90 batch 0 loss 0.011927892453968525\n",
      "epoch 90 finished - avarage train loss 0.008545733249649919  avarage test loss 0.013669964158907533\n",
      "Training: epoch 91 batch 0 loss 0.004036677069962025\n",
      "Training: epoch 91 batch 10 loss 0.0043807439506053925\n",
      "Training: epoch 91 batch 20 loss 0.005030467640608549\n",
      "Test: epoch 91 batch 0 loss 0.010627679526805878\n",
      "epoch 91 finished - avarage train loss 0.006243634535449332  avarage test loss 0.012245966529008001\n",
      "Training: epoch 92 batch 0 loss 0.006464735604822636\n",
      "Training: epoch 92 batch 10 loss 0.0037170022260397673\n",
      "Training: epoch 92 batch 20 loss 0.0061373645439744\n",
      "Test: epoch 92 batch 0 loss 0.014722542837262154\n",
      "epoch 92 finished - avarage train loss 0.007017231791215981  avarage test loss 0.017319094156846404\n",
      "Training: epoch 93 batch 0 loss 0.004420771729201078\n",
      "Training: epoch 93 batch 10 loss 0.0024702120572328568\n",
      "Training: epoch 93 batch 20 loss 0.0025141630321741104\n",
      "Test: epoch 93 batch 0 loss 0.013154219835996628\n",
      "epoch 93 finished - avarage train loss 0.007944022650541416  avarage test loss 0.013855352066457272\n",
      "Training: epoch 94 batch 0 loss 0.010652950033545494\n",
      "Training: epoch 94 batch 10 loss 0.0056891897693276405\n",
      "Training: epoch 94 batch 20 loss 0.003940568771213293\n",
      "Test: epoch 94 batch 0 loss 0.013673697598278522\n",
      "epoch 94 finished - avarage train loss 0.006794603453178344  avarage test loss 0.013794166268780828\n",
      "Training: epoch 95 batch 0 loss 0.00453445129096508\n",
      "Training: epoch 95 batch 10 loss 0.009858684614300728\n",
      "Training: epoch 95 batch 20 loss 0.007384833414107561\n",
      "Test: epoch 95 batch 0 loss 0.013506039045751095\n",
      "epoch 95 finished - avarage train loss 0.007455261986188847  avarage test loss 0.01475849817506969\n",
      "Training: epoch 96 batch 0 loss 0.003999581094831228\n",
      "Training: epoch 96 batch 10 loss 0.007044209633022547\n",
      "Training: epoch 96 batch 20 loss 0.013904247432947159\n",
      "Test: epoch 96 batch 0 loss 0.010897533968091011\n",
      "epoch 96 finished - avarage train loss 0.0067401226080054865  avarage test loss 0.012518814764916897\n",
      "Training: epoch 97 batch 0 loss 0.004680860321968794\n",
      "Training: epoch 97 batch 10 loss 0.00397989759221673\n",
      "Training: epoch 97 batch 20 loss 0.0046948883682489395\n",
      "Test: epoch 97 batch 0 loss 0.012121050618588924\n",
      "epoch 97 finished - avarage train loss 0.006206640451825385  avarage test loss 0.013283945387229323\n",
      "Training: epoch 98 batch 0 loss 0.010056069120764732\n",
      "Training: epoch 98 batch 10 loss 0.008372987620532513\n",
      "Training: epoch 98 batch 20 loss 0.0033983495086431503\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test: epoch 98 batch 0 loss 0.01273228321224451\n",
      "epoch 98 finished - avarage train loss 0.00702630747751943  avarage test loss 0.01318475289735943\n",
      "Training: epoch 99 batch 0 loss 0.005049636121839285\n",
      "Training: epoch 99 batch 10 loss 0.006241001188755035\n",
      "Training: epoch 99 batch 20 loss 0.004670719150453806\n",
      "Test: epoch 99 batch 0 loss 0.013623842969536781\n",
      "epoch 99 finished - avarage train loss 0.006407932342222795  avarage test loss 0.014296116540208459\n",
      "Training: epoch 100 batch 0 loss 0.0034177086781710386\n",
      "Training: epoch 100 batch 10 loss 0.0028445618227124214\n",
      "Training: epoch 100 batch 20 loss 0.010287711396813393\n",
      "Test: epoch 100 batch 0 loss 0.013642407953739166\n",
      "epoch 100 finished - avarage train loss 0.007827603137762896  avarage test loss 0.014786698739044368\n",
      "Training: epoch 101 batch 0 loss 0.00622119614854455\n",
      "Training: epoch 101 batch 10 loss 0.008350587449967861\n",
      "Training: epoch 101 batch 20 loss 0.004257213789969683\n",
      "Test: epoch 101 batch 0 loss 0.011943948455154896\n",
      "epoch 101 finished - avarage train loss 0.008010674292093208  avarage test loss 0.015412054024636745\n",
      "Training: epoch 102 batch 0 loss 0.0028430174570530653\n",
      "Training: epoch 102 batch 10 loss 0.006523419637233019\n",
      "Training: epoch 102 batch 20 loss 0.0030966114718466997\n",
      "Test: epoch 102 batch 0 loss 0.015661247074604034\n",
      "epoch 102 finished - avarage train loss 0.008393045153533077  avarage test loss 0.01598864304833114\n",
      "Training: epoch 103 batch 0 loss 0.006518737878650427\n",
      "Training: epoch 103 batch 10 loss 0.0059395660646259785\n",
      "Training: epoch 103 batch 20 loss 0.0024948338977992535\n",
      "Test: epoch 103 batch 0 loss 0.01196200679987669\n",
      "epoch 103 finished - avarage train loss 0.007420408382915474  avarage test loss 0.013406497309915721\n",
      "Training: epoch 104 batch 0 loss 0.005103228148072958\n",
      "Training: epoch 104 batch 10 loss 0.005063019227236509\n",
      "Training: epoch 104 batch 20 loss 0.002896580845117569\n",
      "Test: epoch 104 batch 0 loss 0.01243407092988491\n",
      "epoch 104 finished - avarage train loss 0.0055761645981592355  avarage test loss 0.013298189616762102\n",
      "Training: epoch 105 batch 0 loss 0.00455473130568862\n",
      "Training: epoch 105 batch 10 loss 0.0057196118868887424\n",
      "Training: epoch 105 batch 20 loss 0.003205547109246254\n",
      "Test: epoch 105 batch 0 loss 0.011564464308321476\n",
      "epoch 105 finished - avarage train loss 0.006122974405900158  avarage test loss 0.014210117980837822\n",
      "Training: epoch 106 batch 0 loss 0.008468542248010635\n",
      "Training: epoch 106 batch 10 loss 0.005409443750977516\n",
      "Training: epoch 106 batch 20 loss 0.002428337000310421\n",
      "Test: epoch 106 batch 0 loss 0.016694316640496254\n",
      "epoch 106 finished - avarage train loss 0.008537149490338975  avarage test loss 0.01760298409499228\n",
      "Training: epoch 107 batch 0 loss 0.005516351666301489\n",
      "Training: epoch 107 batch 10 loss 0.008122552186250687\n",
      "Training: epoch 107 batch 20 loss 0.003983914852142334\n",
      "Test: epoch 107 batch 0 loss 0.01378152146935463\n",
      "epoch 107 finished - avarage train loss 0.007892115539389438  avarage test loss 0.01438340439926833\n",
      "Training: epoch 108 batch 0 loss 0.005738803651183844\n",
      "Training: epoch 108 batch 10 loss 0.003524491796270013\n",
      "Training: epoch 108 batch 20 loss 0.004006220027804375\n",
      "Test: epoch 108 batch 0 loss 0.014661234803497791\n",
      "epoch 108 finished - avarage train loss 0.006879188859000288  avarage test loss 0.014329139143228531\n",
      "Training: epoch 109 batch 0 loss 0.004521103110164404\n",
      "Training: epoch 109 batch 10 loss 0.008329038508236408\n",
      "Training: epoch 109 batch 20 loss 0.0056380461901426315\n",
      "Test: epoch 109 batch 0 loss 0.012494229711592197\n",
      "epoch 109 finished - avarage train loss 0.007560205437114526  avarage test loss 0.013059878372587264\n",
      "Training: epoch 110 batch 0 loss 0.0030295185279101133\n",
      "Training: epoch 110 batch 10 loss 0.0018914472311735153\n",
      "Training: epoch 110 batch 20 loss 0.008972625248134136\n",
      "Test: epoch 110 batch 0 loss 0.010934392921626568\n",
      "epoch 110 finished - avarage train loss 0.00557698233550864  avarage test loss 0.013131483341567218\n",
      "Training: epoch 111 batch 0 loss 0.006204897537827492\n",
      "Training: epoch 111 batch 10 loss 0.003088101278990507\n",
      "Training: epoch 111 batch 20 loss 0.007485091686248779\n",
      "Test: epoch 111 batch 0 loss 0.015507138334214687\n",
      "epoch 111 finished - avarage train loss 0.007672337903480591  avarage test loss 0.01661185221746564\n",
      "Training: epoch 112 batch 0 loss 0.01297912560403347\n",
      "Training: epoch 112 batch 10 loss 0.00974984746426344\n",
      "Training: epoch 112 batch 20 loss 0.005541624501347542\n",
      "Test: epoch 112 batch 0 loss 0.01279244851320982\n",
      "epoch 112 finished - avarage train loss 0.009217698440148398  avarage test loss 0.013363113859668374\n",
      "Training: epoch 113 batch 0 loss 0.005552328657358885\n",
      "Training: epoch 113 batch 10 loss 0.009910156019032001\n",
      "Training: epoch 113 batch 20 loss 0.004748282954096794\n",
      "Test: epoch 113 batch 0 loss 0.016181498765945435\n",
      "epoch 113 finished - avarage train loss 0.0073084903254719644  avarage test loss 0.018300997791811824\n",
      "Training: epoch 114 batch 0 loss 0.011797741055488586\n",
      "Training: epoch 114 batch 10 loss 0.00587871391326189\n",
      "Training: epoch 114 batch 20 loss 0.008432075381278992\n",
      "Test: epoch 114 batch 0 loss 0.013602296821773052\n",
      "epoch 114 finished - avarage train loss 0.007320178744929104  avarage test loss 0.014407786424271762\n",
      "Training: epoch 115 batch 0 loss 0.00553946103900671\n",
      "Training: epoch 115 batch 10 loss 0.0061302026733756065\n",
      "Training: epoch 115 batch 20 loss 0.004326431080698967\n",
      "Test: epoch 115 batch 0 loss 0.013174167834222317\n",
      "epoch 115 finished - avarage train loss 0.0074032741952045214  avarage test loss 0.01487998478114605\n",
      "Training: epoch 116 batch 0 loss 0.004607412498444319\n",
      "Training: epoch 116 batch 10 loss 0.003371558152139187\n",
      "Training: epoch 116 batch 20 loss 0.0014198808930814266\n",
      "Test: epoch 116 batch 0 loss 0.012898934073746204\n",
      "epoch 116 finished - avarage train loss 0.006870798838870792  avarage test loss 0.013813952100463212\n",
      "Training: epoch 117 batch 0 loss 0.004416776355355978\n",
      "Training: epoch 117 batch 10 loss 0.0035664590541273355\n",
      "Training: epoch 117 batch 20 loss 0.0019470062106847763\n",
      "Test: epoch 117 batch 0 loss 0.012893246486783028\n",
      "epoch 117 finished - avarage train loss 0.007844282129522541  avarage test loss 0.013853007112629712\n",
      "Training: epoch 118 batch 0 loss 0.005385573022067547\n",
      "Training: epoch 118 batch 10 loss 0.0030898742843419313\n",
      "Training: epoch 118 batch 20 loss 0.003210751572623849\n",
      "Test: epoch 118 batch 0 loss 0.01134455669671297\n",
      "epoch 118 finished - avarage train loss 0.0070906285565860315  avarage test loss 0.015041809645481408\n",
      "Training: epoch 119 batch 0 loss 0.00902338046580553\n",
      "Training: epoch 119 batch 10 loss 0.004252029117196798\n",
      "Training: epoch 119 batch 20 loss 0.013739489950239658\n",
      "Test: epoch 119 batch 0 loss 0.014588875696063042\n",
      "epoch 119 finished - avarage train loss 0.009381126459880635  avarage test loss 0.015007838956080377\n",
      "Training: epoch 120 batch 0 loss 0.01155308447778225\n",
      "Training: epoch 120 batch 10 loss 0.013831928372383118\n",
      "Training: epoch 120 batch 20 loss 0.00412480253726244\n",
      "Test: epoch 120 batch 0 loss 0.015804020687937737\n",
      "epoch 120 finished - avarage train loss 0.008502106639909846  avarage test loss 0.019562895176932216\n",
      "Training: epoch 121 batch 0 loss 0.007078355178236961\n",
      "Training: epoch 121 batch 10 loss 0.009884495288133621\n",
      "Training: epoch 121 batch 20 loss 0.007674236316233873\n",
      "Test: epoch 121 batch 0 loss 0.011075453832745552\n",
      "epoch 121 finished - avarage train loss 0.00900531976869137  avarage test loss 0.013228992116637528\n",
      "Training: epoch 122 batch 0 loss 0.006415237672626972\n",
      "Training: epoch 122 batch 10 loss 0.006861192639917135\n",
      "Training: epoch 122 batch 20 loss 0.017017638310790062\n",
      "Test: epoch 122 batch 0 loss 0.012769524939358234\n",
      "epoch 122 finished - avarage train loss 0.007892573268377575  avarage test loss 0.013527975417673588\n",
      "Training: epoch 123 batch 0 loss 0.0055943867191672325\n",
      "Training: epoch 123 batch 10 loss 0.006799925118684769\n",
      "Training: epoch 123 batch 20 loss 0.004533390514552593\n",
      "Test: epoch 123 batch 0 loss 0.012422934174537659\n",
      "epoch 123 finished - avarage train loss 0.006854838426706606  avarage test loss 0.013092502136714756\n",
      "Training: epoch 124 batch 0 loss 0.00457193236798048\n",
      "Training: epoch 124 batch 10 loss 0.004804513417184353\n",
      "Training: epoch 124 batch 20 loss 0.002074618823826313\n",
      "Test: epoch 124 batch 0 loss 0.013300522230565548\n",
      "epoch 124 finished - avarage train loss 0.006807066403843206  avarage test loss 0.013571013463661075\n",
      "Training: epoch 125 batch 0 loss 0.0035458370111882687\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training: epoch 125 batch 10 loss 0.004896801896393299\n",
      "Training: epoch 125 batch 20 loss 0.003297502640634775\n",
      "Test: epoch 125 batch 0 loss 0.011377332732081413\n",
      "epoch 125 finished - avarage train loss 0.007265845955959682  avarage test loss 0.013020187499932945\n",
      "Training: epoch 126 batch 0 loss 0.007102527189999819\n",
      "Training: epoch 126 batch 10 loss 0.007487281691282988\n",
      "Training: epoch 126 batch 20 loss 0.003172669792547822\n",
      "Test: epoch 126 batch 0 loss 0.013071655295789242\n",
      "epoch 126 finished - avarage train loss 0.007096261894009237  avarage test loss 0.013558437349274755\n",
      "Training: epoch 127 batch 0 loss 0.014893935061991215\n",
      "Training: epoch 127 batch 10 loss 0.004343593958765268\n",
      "Training: epoch 127 batch 20 loss 0.007905700244009495\n",
      "Test: epoch 127 batch 0 loss 0.013032756745815277\n",
      "epoch 127 finished - avarage train loss 0.0070110765799623114  avarage test loss 0.014232599642127752\n",
      "Training: epoch 128 batch 0 loss 0.005162650253623724\n",
      "Training: epoch 128 batch 10 loss 0.009720468893647194\n",
      "Training: epoch 128 batch 20 loss 0.012300531379878521\n",
      "Test: epoch 128 batch 0 loss 0.013381599448621273\n",
      "epoch 128 finished - avarage train loss 0.00845364827245217  avarage test loss 0.014207102125510573\n",
      "Training: epoch 129 batch 0 loss 0.00857636146247387\n",
      "Training: epoch 129 batch 10 loss 0.008161150850355625\n",
      "Training: epoch 129 batch 20 loss 0.014157655648887157\n",
      "Test: epoch 129 batch 0 loss 0.012208346277475357\n",
      "epoch 129 finished - avarage train loss 0.006601667392934705  avarage test loss 0.013332231785170734\n",
      "Training: epoch 130 batch 0 loss 0.002933454466983676\n",
      "Training: epoch 130 batch 10 loss 0.007236076984554529\n",
      "Training: epoch 130 batch 20 loss 0.0033590623643249273\n",
      "Test: epoch 130 batch 0 loss 0.013237773440778255\n",
      "epoch 130 finished - avarage train loss 0.0076381657031718  avarage test loss 0.01672196900472045\n",
      "Training: epoch 131 batch 0 loss 0.007755732163786888\n",
      "Training: epoch 131 batch 10 loss 0.006298400461673737\n",
      "Training: epoch 131 batch 20 loss 0.003116407198831439\n",
      "Test: epoch 131 batch 0 loss 0.01063135452568531\n",
      "epoch 131 finished - avarage train loss 0.007589774864629425  avarage test loss 0.012437867699190974\n",
      "Training: epoch 132 batch 0 loss 0.006072011776268482\n",
      "Training: epoch 132 batch 10 loss 0.0022862893529236317\n",
      "Training: epoch 132 batch 20 loss 0.014439910650253296\n",
      "Test: epoch 132 batch 0 loss 0.010479101911187172\n",
      "epoch 132 finished - avarage train loss 0.005655871278702699  avarage test loss 0.012202830286696553\n",
      "Training: epoch 133 batch 0 loss 0.006964148487895727\n",
      "Training: epoch 133 batch 10 loss 0.004437581170350313\n",
      "Training: epoch 133 batch 20 loss 0.0026733523700386286\n",
      "Test: epoch 133 batch 0 loss 0.011093897745013237\n",
      "epoch 133 finished - avarage train loss 0.00632336608069981  avarage test loss 0.012763342820107937\n",
      "Training: epoch 134 batch 0 loss 0.006688691210001707\n",
      "Training: epoch 134 batch 10 loss 0.009474240243434906\n",
      "Training: epoch 134 batch 20 loss 0.002001030370593071\n",
      "Test: epoch 134 batch 0 loss 0.010139383375644684\n",
      "epoch 134 finished - avarage train loss 0.005779605997116144  avarage test loss 0.011981562827713788\n",
      "Training: epoch 135 batch 0 loss 0.004375787451863289\n",
      "Training: epoch 135 batch 10 loss 0.008578363806009293\n",
      "Training: epoch 135 batch 20 loss 0.005759809631854296\n",
      "Test: epoch 135 batch 0 loss 0.012885645031929016\n",
      "epoch 135 finished - avarage train loss 0.008645512926360142  avarage test loss 0.014531952678225935\n",
      "Training: epoch 136 batch 0 loss 0.0025692670606076717\n",
      "Training: epoch 136 batch 10 loss 0.0035891816951334476\n",
      "Training: epoch 136 batch 20 loss 0.009795100428164005\n",
      "Test: epoch 136 batch 0 loss 0.012524745427072048\n",
      "epoch 136 finished - avarage train loss 0.007531308793816073  avarage test loss 0.013524958165362477\n",
      "Training: epoch 137 batch 0 loss 0.005703752860426903\n",
      "Training: epoch 137 batch 10 loss 0.009790061973035336\n",
      "Training: epoch 137 batch 20 loss 0.0025197751820087433\n",
      "Test: epoch 137 batch 0 loss 0.016540680080652237\n",
      "epoch 137 finished - avarage train loss 0.006625900141380984  avarage test loss 0.017331189941614866\n",
      "Training: epoch 138 batch 0 loss 0.014674099162220955\n",
      "Training: epoch 138 batch 10 loss 0.00729572307318449\n",
      "Training: epoch 138 batch 20 loss 0.008661177009344101\n",
      "Test: epoch 138 batch 0 loss 0.011526953428983688\n",
      "epoch 138 finished - avarage train loss 0.008845314291177383  avarage test loss 0.014290301944129169\n",
      "Training: epoch 139 batch 0 loss 0.005654514767229557\n",
      "Training: epoch 139 batch 10 loss 0.013689989224076271\n",
      "Training: epoch 139 batch 20 loss 0.0050794631242752075\n",
      "Test: epoch 139 batch 0 loss 0.01174304261803627\n",
      "epoch 139 finished - avarage train loss 0.00998665522462849  avarage test loss 0.013242888497188687\n",
      "Training: epoch 140 batch 0 loss 0.006013855803757906\n",
      "Training: epoch 140 batch 10 loss 0.0026903674006462097\n",
      "Training: epoch 140 batch 20 loss 0.007032536901533604\n",
      "Test: epoch 140 batch 0 loss 0.011179091408848763\n",
      "epoch 140 finished - avarage train loss 0.006646386018536728  avarage test loss 0.012586519587785006\n",
      "Training: epoch 141 batch 0 loss 0.005679015070199966\n",
      "Training: epoch 141 batch 10 loss 0.005702256225049496\n",
      "Training: epoch 141 batch 20 loss 0.009052149951457977\n",
      "Test: epoch 141 batch 0 loss 0.011876404285430908\n",
      "epoch 141 finished - avarage train loss 0.006676534760807608  avarage test loss 0.013485465431585908\n",
      "Training: epoch 142 batch 0 loss 0.008336275815963745\n",
      "Training: epoch 142 batch 10 loss 0.00462567200884223\n",
      "Training: epoch 142 batch 20 loss 0.004862554371356964\n",
      "Test: epoch 142 batch 0 loss 0.0115585345774889\n",
      "epoch 142 finished - avarage train loss 0.007033981687935262  avarage test loss 0.013026618864387274\n",
      "Training: epoch 143 batch 0 loss 0.0014620783040300012\n",
      "Training: epoch 143 batch 10 loss 0.004080172628164291\n",
      "Training: epoch 143 batch 20 loss 0.005706822499632835\n",
      "Test: epoch 143 batch 0 loss 0.01385614462196827\n",
      "epoch 143 finished - avarage train loss 0.006611170074581329  avarage test loss 0.016650369856506586\n",
      "Training: epoch 144 batch 0 loss 0.009144296869635582\n",
      "Training: epoch 144 batch 10 loss 0.006334085017442703\n",
      "Training: epoch 144 batch 20 loss 0.005006189923733473\n",
      "Test: epoch 144 batch 0 loss 0.01314431894570589\n",
      "epoch 144 finished - avarage train loss 0.009841338185400799  avarage test loss 0.01507465832401067\n",
      "Training: epoch 145 batch 0 loss 0.009349733591079712\n",
      "Training: epoch 145 batch 10 loss 0.009781157597899437\n",
      "Training: epoch 145 batch 20 loss 0.0027577721048146486\n",
      "Test: epoch 145 batch 0 loss 0.01145903579890728\n",
      "epoch 145 finished - avarage train loss 0.008103421475770402  avarage test loss 0.013627196080051363\n",
      "Training: epoch 146 batch 0 loss 0.0031515532173216343\n",
      "Training: epoch 146 batch 10 loss 0.006954415235668421\n",
      "Training: epoch 146 batch 20 loss 0.007363937795162201\n",
      "Test: epoch 146 batch 0 loss 0.011323155835270882\n",
      "epoch 146 finished - avarage train loss 0.006624109770075001  avarage test loss 0.012726225890219212\n",
      "Training: epoch 147 batch 0 loss 0.006647478323429823\n",
      "Training: epoch 147 batch 10 loss 0.007829047739505768\n",
      "Training: epoch 147 batch 20 loss 0.004647478461265564\n",
      "Test: epoch 147 batch 0 loss 0.01200722437351942\n",
      "epoch 147 finished - avarage train loss 0.006315177089759502  avarage test loss 0.013065406470559537\n",
      "Training: epoch 148 batch 0 loss 0.0047945561818778515\n",
      "Training: epoch 148 batch 10 loss 0.010251780971884727\n",
      "Training: epoch 148 batch 20 loss 0.0033848960883915424\n",
      "Test: epoch 148 batch 0 loss 0.011425141245126724\n",
      "epoch 148 finished - avarage train loss 0.00708521391136636  avarage test loss 0.012431583716534078\n",
      "Training: epoch 149 batch 0 loss 0.007378927432000637\n",
      "Training: epoch 149 batch 10 loss 0.008626289665699005\n",
      "Training: epoch 149 batch 20 loss 0.021091552451252937\n",
      "Test: epoch 149 batch 0 loss 0.011787095107138157\n",
      "epoch 149 finished - avarage train loss 0.007400217385769918  avarage test loss 0.013124223565682769\n",
      "Training: epoch 150 batch 0 loss 0.0019182286923751235\n",
      "Training: epoch 150 batch 10 loss 0.003058105241507292\n",
      "Training: epoch 150 batch 20 loss 0.00658425921574235\n",
      "Test: epoch 150 batch 0 loss 0.011596772819757462\n",
      "epoch 150 finished - avarage train loss 0.006928789120263837  avarage test loss 0.013057990814559162\n",
      "Training: epoch 151 batch 0 loss 0.003830558620393276\n",
      "Training: epoch 151 batch 10 loss 0.0030600926838815212\n",
      "Training: epoch 151 batch 20 loss 0.00648136530071497\n",
      "Test: epoch 151 batch 0 loss 0.010853949002921581\n",
      "epoch 151 finished - avarage train loss 0.006670815320203787  avarage test loss 0.012257798225618899\n",
      "Training: epoch 152 batch 0 loss 0.003775358898565173\n",
      "Training: epoch 152 batch 10 loss 0.004125846084207296\n",
      "Training: epoch 152 batch 20 loss 0.006510496139526367\n",
      "Test: epoch 152 batch 0 loss 0.012250816449522972\n",
      "epoch 152 finished - avarage train loss 0.006903378260803634  avarage test loss 0.014029910904355347\n",
      "Training: epoch 153 batch 0 loss 0.005419743712991476\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training: epoch 153 batch 10 loss 0.005024712532758713\n",
      "Training: epoch 153 batch 20 loss 0.007366739679127932\n",
      "Test: epoch 153 batch 0 loss 0.010532946325838566\n",
      "epoch 153 finished - avarage train loss 0.006872106592395696  avarage test loss 0.012208072701469064\n",
      "Training: epoch 154 batch 0 loss 0.006310572382062674\n",
      "Training: epoch 154 batch 10 loss 0.0032859055791050196\n",
      "Training: epoch 154 batch 20 loss 0.007196377497166395\n",
      "Test: epoch 154 batch 0 loss 0.0112119410187006\n",
      "epoch 154 finished - avarage train loss 0.008332999591747749  avarage test loss 0.012835605419240892\n",
      "Training: epoch 155 batch 0 loss 0.0028917635791003704\n",
      "Training: epoch 155 batch 10 loss 0.003509540343657136\n",
      "Training: epoch 155 batch 20 loss 0.006144500337541103\n",
      "Test: epoch 155 batch 0 loss 0.011914570815861225\n",
      "epoch 155 finished - avarage train loss 0.006493235344131445  avarage test loss 0.01345455611590296\n",
      "Training: epoch 156 batch 0 loss 0.00387378828600049\n",
      "Training: epoch 156 batch 10 loss 0.003277142997831106\n",
      "Training: epoch 156 batch 20 loss 0.0049938028678298\n",
      "Test: epoch 156 batch 0 loss 0.01257057674229145\n",
      "epoch 156 finished - avarage train loss 0.005848670269940691  avarage test loss 0.014959221589379013\n",
      "Training: epoch 157 batch 0 loss 0.003640481736510992\n",
      "Training: epoch 157 batch 10 loss 0.006388016976416111\n",
      "Training: epoch 157 batch 20 loss 0.024052433669567108\n",
      "Test: epoch 157 batch 0 loss 0.011618033051490784\n",
      "epoch 157 finished - avarage train loss 0.007152030156154571  avarage test loss 0.013088053208775818\n",
      "Training: epoch 158 batch 0 loss 0.005054160952568054\n",
      "Training: epoch 158 batch 10 loss 0.00399977620691061\n",
      "Training: epoch 158 batch 20 loss 0.0028896306175738573\n",
      "Test: epoch 158 batch 0 loss 0.011163354851305485\n",
      "epoch 158 finished - avarage train loss 0.006664095157436256  avarage test loss 0.01255276141455397\n",
      "Training: epoch 159 batch 0 loss 0.005773425102233887\n",
      "Training: epoch 159 batch 10 loss 0.005701761227101088\n",
      "Training: epoch 159 batch 20 loss 0.002793635008856654\n",
      "Test: epoch 159 batch 0 loss 0.011377201415598392\n",
      "epoch 159 finished - avarage train loss 0.007309129345230758  avarage test loss 0.012880418333224952\n",
      "Training: epoch 160 batch 0 loss 0.010179813019931316\n",
      "Training: epoch 160 batch 10 loss 0.007502617780119181\n",
      "Training: epoch 160 batch 20 loss 0.005072417668998241\n",
      "Test: epoch 160 batch 0 loss 0.01191465649753809\n",
      "epoch 160 finished - avarage train loss 0.007529608503883255  avarage test loss 0.0132486573420465\n",
      "Training: epoch 161 batch 0 loss 0.006500855553895235\n",
      "Training: epoch 161 batch 10 loss 0.007860163226723671\n",
      "Training: epoch 161 batch 20 loss 0.008317370899021626\n",
      "Test: epoch 161 batch 0 loss 0.012843195348978043\n",
      "epoch 161 finished - avarage train loss 0.008710337658252182  avarage test loss 0.015772126615047455\n",
      "Training: epoch 162 batch 0 loss 0.007425667252391577\n",
      "Training: epoch 162 batch 10 loss 0.002694185823202133\n",
      "Training: epoch 162 batch 20 loss 0.0026225897017866373\n",
      "Test: epoch 162 batch 0 loss 0.010897573083639145\n",
      "epoch 162 finished - avarage train loss 0.00639691439473295  avarage test loss 0.013652756460942328\n",
      "Training: epoch 163 batch 0 loss 0.012147058732807636\n",
      "Training: epoch 163 batch 10 loss 0.005387598183006048\n",
      "Training: epoch 163 batch 20 loss 0.0065946332179009914\n",
      "Test: epoch 163 batch 0 loss 0.011658328585326672\n",
      "epoch 163 finished - avarage train loss 0.008291377589620393  avarage test loss 0.012863194453530014\n",
      "Training: epoch 164 batch 0 loss 0.003870708402246237\n",
      "Training: epoch 164 batch 10 loss 0.005606158170849085\n",
      "Training: epoch 164 batch 20 loss 0.0067589906975626945\n",
      "Test: epoch 164 batch 0 loss 0.01124484371393919\n",
      "epoch 164 finished - avarage train loss 0.007360501640379943  avarage test loss 0.012333714170381427\n",
      "Training: epoch 165 batch 0 loss 0.005737992934882641\n",
      "Training: epoch 165 batch 10 loss 0.003911106381565332\n",
      "Training: epoch 165 batch 20 loss 0.007563308347016573\n",
      "Test: epoch 165 batch 0 loss 0.01106022298336029\n",
      "epoch 165 finished - avarage train loss 0.005849024429435617  avarage test loss 0.012766362633556128\n",
      "Training: epoch 166 batch 0 loss 0.0026555631775408983\n",
      "Training: epoch 166 batch 10 loss 0.012276655063033104\n",
      "Training: epoch 166 batch 20 loss 0.004122756887227297\n",
      "Test: epoch 166 batch 0 loss 0.012895723804831505\n",
      "epoch 166 finished - avarage train loss 0.007137152928345162  avarage test loss 0.014380370965227485\n",
      "Training: epoch 167 batch 0 loss 0.01679210737347603\n",
      "Training: epoch 167 batch 10 loss 0.0066656009294092655\n",
      "Training: epoch 167 batch 20 loss 0.010526134632527828\n",
      "Test: epoch 167 batch 0 loss 0.014224285259842873\n",
      "epoch 167 finished - avarage train loss 0.00844023417649341  avarage test loss 0.015637129545211792\n",
      "Training: epoch 168 batch 0 loss 0.0068215737119317055\n",
      "Training: epoch 168 batch 10 loss 0.004073599353432655\n",
      "Training: epoch 168 batch 20 loss 0.004536723252385855\n",
      "Test: epoch 168 batch 0 loss 0.011467662639915943\n",
      "epoch 168 finished - avarage train loss 0.006812962057101059  avarage test loss 0.012762357015162706\n",
      "Training: epoch 169 batch 0 loss 0.004652536008507013\n",
      "Training: epoch 169 batch 10 loss 0.0017356915632262826\n",
      "Training: epoch 169 batch 20 loss 0.014365283772349358\n",
      "Test: epoch 169 batch 0 loss 0.012816088274121284\n",
      "epoch 169 finished - avarage train loss 0.007295866396502945  avarage test loss 0.015225851209834218\n",
      "Training: epoch 170 batch 0 loss 0.005850004497915506\n",
      "Training: epoch 170 batch 10 loss 0.00844220444560051\n",
      "Training: epoch 170 batch 20 loss 0.0025825214106589556\n",
      "Test: epoch 170 batch 0 loss 0.011556047946214676\n",
      "epoch 170 finished - avarage train loss 0.007856029366817454  avarage test loss 0.012802563840523362\n",
      "Training: epoch 171 batch 0 loss 0.007203405257314444\n",
      "Training: epoch 171 batch 10 loss 0.005896355025470257\n",
      "Training: epoch 171 batch 20 loss 0.004210923798382282\n",
      "Test: epoch 171 batch 0 loss 0.01086702011525631\n",
      "epoch 171 finished - avarage train loss 0.005738131248327936  avarage test loss 0.012457860982976854\n",
      "Training: epoch 172 batch 0 loss 0.006624804809689522\n",
      "Training: epoch 172 batch 10 loss 0.007883820682764053\n",
      "Training: epoch 172 batch 20 loss 0.005871046334505081\n",
      "Test: epoch 172 batch 0 loss 0.01097156759351492\n",
      "epoch 172 finished - avarage train loss 0.007909671535940263  avarage test loss 0.012347726267762482\n",
      "Training: epoch 173 batch 0 loss 0.0016802289756014943\n",
      "Training: epoch 173 batch 10 loss 0.004315300844609737\n",
      "Training: epoch 173 batch 20 loss 0.005408453289419413\n",
      "Test: epoch 173 batch 0 loss 0.011858801357448101\n",
      "epoch 173 finished - avarage train loss 0.00682683311531256  avarage test loss 0.013437679037451744\n",
      "Training: epoch 174 batch 0 loss 0.00486621493473649\n",
      "Training: epoch 174 batch 10 loss 0.010482089594006538\n",
      "Training: epoch 174 batch 20 loss 0.00590109545737505\n",
      "Test: epoch 174 batch 0 loss 0.01111944392323494\n",
      "epoch 174 finished - avarage train loss 0.006918042609146957  avarage test loss 0.013042779872193933\n",
      "Training: epoch 175 batch 0 loss 0.00982649065554142\n",
      "Training: epoch 175 batch 10 loss 0.004919145256280899\n",
      "Training: epoch 175 batch 20 loss 0.005144944880157709\n",
      "Test: epoch 175 batch 0 loss 0.009776496328413486\n",
      "epoch 175 finished - avarage train loss 0.007024393119881379  avarage test loss 0.011665846221148968\n",
      "Training: epoch 176 batch 0 loss 0.003026368794962764\n",
      "Training: epoch 176 batch 10 loss 0.004379797726869583\n",
      "Training: epoch 176 batch 20 loss 0.001654533320106566\n",
      "Test: epoch 176 batch 0 loss 0.012734190560877323\n",
      "epoch 176 finished - avarage train loss 0.0065796774940501  avarage test loss 0.014258773881010711\n",
      "Training: epoch 177 batch 0 loss 0.00812242366373539\n",
      "Training: epoch 177 batch 10 loss 0.00932438112795353\n",
      "Training: epoch 177 batch 20 loss 0.007913744077086449\n",
      "Test: epoch 177 batch 0 loss 0.011194616556167603\n",
      "epoch 177 finished - avarage train loss 0.00743767798171732  avarage test loss 0.012858224334195256\n",
      "Training: epoch 178 batch 0 loss 0.004377850331366062\n",
      "Training: epoch 178 batch 10 loss 0.002941838698461652\n",
      "Training: epoch 178 batch 20 loss 0.0038603912107646465\n",
      "Test: epoch 178 batch 0 loss 0.01159681472927332\n",
      "epoch 178 finished - avarage train loss 0.007605200215917209  avarage test loss 0.012800858356058598\n",
      "Training: epoch 179 batch 0 loss 0.008359263651072979\n",
      "Training: epoch 179 batch 10 loss 0.0032899905927479267\n",
      "Training: epoch 179 batch 20 loss 0.003748515620827675\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test: epoch 179 batch 0 loss 0.012509479187428951\n",
      "epoch 179 finished - avarage train loss 0.007794249521825334  avarage test loss 0.014207866624929011\n",
      "Training: epoch 180 batch 0 loss 0.005275936797261238\n",
      "Training: epoch 180 batch 10 loss 0.0016190402675420046\n",
      "Training: epoch 180 batch 20 loss 0.003970309626311064\n",
      "Test: epoch 180 batch 0 loss 0.010875753127038479\n",
      "epoch 180 finished - avarage train loss 0.0066390931606292725  avarage test loss 0.012525741709396243\n",
      "Training: epoch 181 batch 0 loss 0.007577927317470312\n",
      "Training: epoch 181 batch 10 loss 0.001838993513956666\n",
      "Training: epoch 181 batch 20 loss 0.0041640643030405045\n",
      "Test: epoch 181 batch 0 loss 0.0124836890026927\n",
      "epoch 181 finished - avarage train loss 0.005514812210157257  avarage test loss 0.013358259340748191\n",
      "Training: epoch 182 batch 0 loss 0.0026116962544620037\n",
      "Training: epoch 182 batch 10 loss 0.003237861907109618\n",
      "Training: epoch 182 batch 20 loss 0.010753778740763664\n",
      "Test: epoch 182 batch 0 loss 0.012651427648961544\n",
      "epoch 182 finished - avarage train loss 0.006978703522251855  avarage test loss 0.013811320648528636\n",
      "Training: epoch 183 batch 0 loss 0.004347682930529118\n",
      "Training: epoch 183 batch 10 loss 0.004569502081722021\n",
      "Training: epoch 183 batch 20 loss 0.0034689141903072596\n",
      "Test: epoch 183 batch 0 loss 0.012965607456862926\n",
      "epoch 183 finished - avarage train loss 0.0057662947962445945  avarage test loss 0.013600829057395458\n",
      "Training: epoch 184 batch 0 loss 0.004988493863493204\n",
      "Training: epoch 184 batch 10 loss 0.00855924841016531\n",
      "Training: epoch 184 batch 20 loss 0.011858772486448288\n",
      "Test: epoch 184 batch 0 loss 0.011895829811692238\n",
      "epoch 184 finished - avarage train loss 0.006206335267052054  avarage test loss 0.013330084853805602\n",
      "Training: epoch 185 batch 0 loss 0.0022368980571627617\n",
      "Training: epoch 185 batch 10 loss 0.008250534534454346\n",
      "Training: epoch 185 batch 20 loss 0.004562082700431347\n",
      "Test: epoch 185 batch 0 loss 0.011670245788991451\n",
      "epoch 185 finished - avarage train loss 0.006970142793103025  avarage test loss 0.013265179470181465\n",
      "Training: epoch 186 batch 0 loss 0.004710867069661617\n",
      "Training: epoch 186 batch 10 loss 0.007464759051799774\n",
      "Training: epoch 186 batch 20 loss 0.0052002002485096455\n",
      "Test: epoch 186 batch 0 loss 0.014383423142135143\n",
      "epoch 186 finished - avarage train loss 0.00839102755156185  avarage test loss 0.017430460546165705\n",
      "Training: epoch 187 batch 0 loss 0.004813353065401316\n",
      "Training: epoch 187 batch 10 loss 0.006755365990102291\n",
      "Training: epoch 187 batch 20 loss 0.0029302227776497602\n",
      "Test: epoch 187 batch 0 loss 0.012948514893651009\n",
      "epoch 187 finished - avarage train loss 0.006256906006581567  avarage test loss 0.014424514956772327\n",
      "Training: epoch 188 batch 0 loss 0.0059725139290094376\n",
      "Training: epoch 188 batch 10 loss 0.003304634476080537\n",
      "Training: epoch 188 batch 20 loss 0.00840481836348772\n",
      "Test: epoch 188 batch 0 loss 0.011917267926037312\n",
      "epoch 188 finished - avarage train loss 0.007321723704708034  avarage test loss 0.013919041957706213\n",
      "Training: epoch 189 batch 0 loss 0.0026536963414400816\n",
      "Training: epoch 189 batch 10 loss 0.007658627349883318\n",
      "Training: epoch 189 batch 20 loss 0.016427068039774895\n",
      "Test: epoch 189 batch 0 loss 0.008813182823359966\n",
      "epoch 189 finished - avarage train loss 0.00771140159461005  avarage test loss 0.01253493595868349\n",
      "Training: epoch 190 batch 0 loss 0.003846297040581703\n",
      "Training: epoch 190 batch 10 loss 0.003438641084358096\n",
      "Training: epoch 190 batch 20 loss 0.007336083333939314\n",
      "Test: epoch 190 batch 0 loss 0.010730652138590813\n",
      "epoch 190 finished - avarage train loss 0.0068190491814487456  avarage test loss 0.012689921888522804\n",
      "Training: epoch 191 batch 0 loss 0.004214535932987928\n",
      "Training: epoch 191 batch 10 loss 0.006510069128125906\n",
      "Training: epoch 191 batch 20 loss 0.007415564730763435\n",
      "Test: epoch 191 batch 0 loss 0.011362324468791485\n",
      "epoch 191 finished - avarage train loss 0.007223729212800491  avarage test loss 0.013661366188898683\n",
      "Training: epoch 192 batch 0 loss 0.002556042978540063\n",
      "Training: epoch 192 batch 10 loss 0.009642286226153374\n",
      "Training: epoch 192 batch 20 loss 0.0031415841076523066\n",
      "Test: epoch 192 batch 0 loss 0.016944510862231255\n",
      "epoch 192 finished - avarage train loss 0.006876125722995092  avarage test loss 0.021428976906463504\n",
      "Training: epoch 193 batch 0 loss 0.02231070026755333\n",
      "Training: epoch 193 batch 10 loss 0.019662922248244286\n",
      "Training: epoch 193 batch 20 loss 0.022206570953130722\n",
      "Test: epoch 193 batch 0 loss 0.018747538328170776\n",
      "epoch 193 finished - avarage train loss 0.016588421955961604  avarage test loss 0.027026739669963717\n",
      "Training: epoch 194 batch 0 loss 0.009238036349415779\n",
      "Training: epoch 194 batch 10 loss 0.003514287294819951\n",
      "Training: epoch 194 batch 20 loss 0.005544356070458889\n",
      "Test: epoch 194 batch 0 loss 0.01001492515206337\n",
      "epoch 194 finished - avarage train loss 0.0076421986791658505  avarage test loss 0.011884034727700055\n",
      "Training: epoch 195 batch 0 loss 0.005813611205667257\n",
      "Training: epoch 195 batch 10 loss 0.006325153633952141\n",
      "Training: epoch 195 batch 20 loss 0.006812048610299826\n",
      "Test: epoch 195 batch 0 loss 0.011600571684539318\n",
      "epoch 195 finished - avarage train loss 0.006497671831688234  avarage test loss 0.01391353108920157\n",
      "Training: epoch 196 batch 0 loss 0.0034960091579705477\n",
      "Training: epoch 196 batch 10 loss 0.009136715903878212\n",
      "Training: epoch 196 batch 20 loss 0.012207874096930027\n",
      "Test: epoch 196 batch 0 loss 0.011463717557489872\n",
      "epoch 196 finished - avarage train loss 0.00628152123957487  avarage test loss 0.013025064021348953\n",
      "Training: epoch 197 batch 0 loss 0.0028959750197827816\n",
      "Training: epoch 197 batch 10 loss 0.005371223669499159\n",
      "Training: epoch 197 batch 20 loss 0.008538732305169106\n",
      "Test: epoch 197 batch 0 loss 0.011049794033169746\n",
      "epoch 197 finished - avarage train loss 0.0067604617255836216  avarage test loss 0.012620588182471693\n",
      "Training: epoch 198 batch 0 loss 0.002680933801457286\n",
      "Training: epoch 198 batch 10 loss 0.0031459303572773933\n",
      "Training: epoch 198 batch 20 loss 0.008444600738584995\n",
      "Test: epoch 198 batch 0 loss 0.012584423646330833\n",
      "epoch 198 finished - avarage train loss 0.005890601499263069  avarage test loss 0.01436343661043793\n",
      "Training: epoch 199 batch 0 loss 0.011009196750819683\n",
      "Training: epoch 199 batch 10 loss 0.002263368107378483\n",
      "Training: epoch 199 batch 20 loss 0.01077909953892231\n",
      "Test: epoch 199 batch 0 loss 0.012392609380185604\n",
      "epoch 199 finished - avarage train loss 0.008017259425130385  avarage test loss 0.013753181206993759\n",
      "Training: epoch 0 batch 0 loss 0.4230725169181824\n",
      "Training: epoch 0 batch 10 loss 0.5118903517723083\n",
      "Training: epoch 0 batch 20 loss 0.5243406891822815\n",
      "Test: epoch 0 batch 0 loss 0.39216819405555725\n",
      "epoch 0 finished - avarage train loss 0.516529399259337  avarage test loss 0.44490570574998856\n",
      "Training: epoch 1 batch 0 loss 0.43428346514701843\n",
      "Training: epoch 1 batch 10 loss 0.413104772567749\n",
      "Training: epoch 1 batch 20 loss 0.5408707857131958\n",
      "Test: epoch 1 batch 0 loss 0.39730194211006165\n",
      "epoch 1 finished - avarage train loss 0.5077251093140964  avarage test loss 0.4365491457283497\n",
      "Training: epoch 2 batch 0 loss 0.4643397331237793\n",
      "Training: epoch 2 batch 10 loss 0.6378558278083801\n",
      "Training: epoch 2 batch 20 loss 0.40669313073158264\n",
      "Test: epoch 2 batch 0 loss 0.3923254609107971\n",
      "epoch 2 finished - avarage train loss 0.49984491642179163  avarage test loss 0.43511081114411354\n",
      "Training: epoch 3 batch 0 loss 0.4709102511405945\n",
      "Training: epoch 3 batch 10 loss 0.5373775959014893\n",
      "Training: epoch 3 batch 20 loss 0.46098801493644714\n",
      "Test: epoch 3 batch 0 loss 0.3971741497516632\n",
      "epoch 3 finished - avarage train loss 0.5181470410577182  avarage test loss 0.4415832534432411\n",
      "Training: epoch 4 batch 0 loss 0.47774332761764526\n",
      "Training: epoch 4 batch 10 loss 0.5985167622566223\n",
      "Training: epoch 4 batch 20 loss 0.6038317084312439\n",
      "Test: epoch 4 batch 0 loss 0.39644140005111694\n",
      "epoch 4 finished - avarage train loss 0.4988401419129865  avarage test loss 0.4464752897620201\n",
      "Training: epoch 5 batch 0 loss 0.6475309729576111\n",
      "Training: epoch 5 batch 10 loss 0.45010071992874146\n",
      "Training: epoch 5 batch 20 loss 0.3980616629123688\n",
      "Test: epoch 5 batch 0 loss 0.39376410841941833\n",
      "epoch 5 finished - avarage train loss 0.526688441120345  avarage test loss 0.43609824404120445\n",
      "Training: epoch 6 batch 0 loss 0.3806612491607666\n",
      "Training: epoch 6 batch 10 loss 0.5239181518554688\n",
      "Training: epoch 6 batch 20 loss 0.44974830746650696\n",
      "Test: epoch 6 batch 0 loss 0.39758366346359253\n",
      "epoch 6 finished - avarage train loss 0.5016072197207089  avarage test loss 0.4398851990699768\n",
      "Training: epoch 7 batch 0 loss 0.49769774079322815\n",
      "Training: epoch 7 batch 10 loss 0.5355104804039001\n",
      "Training: epoch 7 batch 20 loss 0.5028889775276184\n",
      "Test: epoch 7 batch 0 loss 0.3945634663105011\n",
      "epoch 7 finished - avarage train loss 0.5069273391674305  avarage test loss 0.44431882351636887\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training: epoch 8 batch 0 loss 0.4105049967765808\n",
      "Training: epoch 8 batch 10 loss 0.35470500588417053\n",
      "Training: epoch 8 batch 20 loss 0.5756468176841736\n",
      "Test: epoch 8 batch 0 loss 0.3941812813282013\n",
      "epoch 8 finished - avarage train loss 0.5178710551097475  avarage test loss 0.4409320205450058\n",
      "Training: epoch 9 batch 0 loss 0.4633622169494629\n",
      "Training: epoch 9 batch 10 loss 0.5168344974517822\n",
      "Training: epoch 9 batch 20 loss 0.5934770703315735\n",
      "Test: epoch 9 batch 0 loss 0.39130187034606934\n",
      "epoch 9 finished - avarage train loss 0.518624342721084  avarage test loss 0.4416161738336086\n",
      "Training: epoch 10 batch 0 loss 0.5320026874542236\n",
      "Training: epoch 10 batch 10 loss 0.40878379344940186\n",
      "Training: epoch 10 batch 20 loss 0.6714117527008057\n",
      "Test: epoch 10 batch 0 loss 0.39049047231674194\n",
      "epoch 10 finished - avarage train loss 0.5050877312134052  avarage test loss 0.4456508383154869\n",
      "Training: epoch 11 batch 0 loss 0.4498690962791443\n",
      "Training: epoch 11 batch 10 loss 0.6199504733085632\n",
      "Training: epoch 11 batch 20 loss 0.3937270939350128\n",
      "Test: epoch 11 batch 0 loss 0.3739864230155945\n",
      "epoch 11 finished - avarage train loss 0.5176793717104813  avarage test loss 0.414666298776865\n",
      "Training: epoch 12 batch 0 loss 0.4903176724910736\n",
      "Training: epoch 12 batch 10 loss 0.16116133332252502\n",
      "Training: epoch 12 batch 20 loss 0.10137919336557388\n",
      "Test: epoch 12 batch 0 loss 0.08957348763942719\n",
      "epoch 12 finished - avarage train loss 0.19341660149652382  avarage test loss 0.10072165913879871\n",
      "Training: epoch 13 batch 0 loss 0.09607575833797455\n",
      "Training: epoch 13 batch 10 loss 0.05920088291168213\n",
      "Training: epoch 13 batch 20 loss 0.027320824563503265\n",
      "Test: epoch 13 batch 0 loss 0.030170410871505737\n",
      "epoch 13 finished - avarage train loss 0.04114212590301859  avarage test loss 0.0376110952347517\n",
      "Training: epoch 14 batch 0 loss 0.032312940806150436\n",
      "Training: epoch 14 batch 10 loss 0.03522595018148422\n",
      "Training: epoch 14 batch 20 loss 0.02246122807264328\n",
      "Test: epoch 14 batch 0 loss 0.02465284988284111\n",
      "epoch 14 finished - avarage train loss 0.02442290344885711  avarage test loss 0.031229360960423946\n",
      "Training: epoch 15 batch 0 loss 0.025508692488074303\n",
      "Training: epoch 15 batch 10 loss 0.010985813103616238\n",
      "Training: epoch 15 batch 20 loss 0.018520236015319824\n",
      "Test: epoch 15 batch 0 loss 0.02357025444507599\n",
      "epoch 15 finished - avarage train loss 0.01936844826258462  avarage test loss 0.03037313325330615\n",
      "Training: epoch 16 batch 0 loss 0.011634675785899162\n",
      "Training: epoch 16 batch 10 loss 0.026134097948670387\n",
      "Training: epoch 16 batch 20 loss 0.014318203553557396\n",
      "Test: epoch 16 batch 0 loss 0.016966955736279488\n",
      "epoch 16 finished - avarage train loss 0.016667789675231123  avarage test loss 0.023900990141555667\n",
      "Training: epoch 17 batch 0 loss 0.020560504868626595\n",
      "Training: epoch 17 batch 10 loss 0.017590101808309555\n",
      "Training: epoch 17 batch 20 loss 0.02142920345067978\n",
      "Test: epoch 17 batch 0 loss 0.010180236771702766\n",
      "epoch 17 finished - avarage train loss 0.014784297102998042  avarage test loss 0.01595798716880381\n",
      "Training: epoch 18 batch 0 loss 0.009476738050580025\n",
      "Training: epoch 18 batch 10 loss 0.018024491146206856\n",
      "Training: epoch 18 batch 20 loss 0.01163928210735321\n",
      "Test: epoch 18 batch 0 loss 0.01563955470919609\n",
      "epoch 18 finished - avarage train loss 0.011334348630545468  avarage test loss 0.021630276227369905\n",
      "Training: epoch 19 batch 0 loss 0.006852632388472557\n",
      "Training: epoch 19 batch 10 loss 0.005920843221247196\n",
      "Training: epoch 19 batch 20 loss 0.008570428937673569\n",
      "Test: epoch 19 batch 0 loss 0.01272228080779314\n",
      "epoch 19 finished - avarage train loss 0.011053065673030656  avarage test loss 0.016303837997838855\n",
      "Training: epoch 20 batch 0 loss 0.01002445537596941\n",
      "Training: epoch 20 batch 10 loss 0.009090745821595192\n",
      "Training: epoch 20 batch 20 loss 0.01367101538926363\n",
      "Test: epoch 20 batch 0 loss 0.01875263825058937\n",
      "epoch 20 finished - avarage train loss 0.010902906543221968  avarage test loss 0.017533268546685576\n",
      "Training: epoch 21 batch 0 loss 0.00885515846312046\n",
      "Training: epoch 21 batch 10 loss 0.011214287020266056\n",
      "Training: epoch 21 batch 20 loss 0.00458494434133172\n",
      "Test: epoch 21 batch 0 loss 0.008779654279351234\n",
      "epoch 21 finished - avarage train loss 0.007426284341912331  avarage test loss 0.012278530863113701\n",
      "Training: epoch 22 batch 0 loss 0.007770027033984661\n",
      "Training: epoch 22 batch 10 loss 0.004730906803160906\n",
      "Training: epoch 22 batch 20 loss 0.004881648346781731\n",
      "Test: epoch 22 batch 0 loss 0.009633767418563366\n",
      "epoch 22 finished - avarage train loss 0.007620872407830481  avarage test loss 0.01566813513636589\n",
      "Training: epoch 23 batch 0 loss 0.00419202446937561\n",
      "Training: epoch 23 batch 10 loss 0.010869037359952927\n",
      "Training: epoch 23 batch 20 loss 0.006137133110314608\n",
      "Test: epoch 23 batch 0 loss 0.01039043813943863\n",
      "epoch 23 finished - avarage train loss 0.007745705914266151  avarage test loss 0.01264260895550251\n",
      "Training: epoch 24 batch 0 loss 0.006706274580210447\n",
      "Training: epoch 24 batch 10 loss 0.009539532475173473\n",
      "Training: epoch 24 batch 20 loss 0.006847189739346504\n",
      "Test: epoch 24 batch 0 loss 0.010248908773064613\n",
      "epoch 24 finished - avarage train loss 0.008359973078015549  avarage test loss 0.01417084748391062\n",
      "Training: epoch 25 batch 0 loss 0.006075699348002672\n",
      "Training: epoch 25 batch 10 loss 0.008691673167049885\n",
      "Training: epoch 25 batch 20 loss 0.005147358402609825\n",
      "Test: epoch 25 batch 0 loss 0.01316740456968546\n",
      "epoch 25 finished - avarage train loss 0.00841219525720025  avarage test loss 0.01536458870396018\n",
      "Training: epoch 26 batch 0 loss 0.02156369388103485\n",
      "Training: epoch 26 batch 10 loss 0.00445742579177022\n",
      "Training: epoch 26 batch 20 loss 0.010750002227723598\n",
      "Test: epoch 26 batch 0 loss 0.009669431485235691\n",
      "epoch 26 finished - avarage train loss 0.009263357789865855  avarage test loss 0.012490367516875267\n",
      "Training: epoch 27 batch 0 loss 0.005213026888668537\n",
      "Training: epoch 27 batch 10 loss 0.009014752693474293\n",
      "Training: epoch 27 batch 20 loss 0.007562935817986727\n",
      "Test: epoch 27 batch 0 loss 0.011195861734449863\n",
      "epoch 27 finished - avarage train loss 0.007835271246410135  avarage test loss 0.01399091596249491\n",
      "Training: epoch 28 batch 0 loss 0.002782964613288641\n",
      "Training: epoch 28 batch 10 loss 0.003998720087110996\n",
      "Training: epoch 28 batch 20 loss 0.002583419904112816\n",
      "Test: epoch 28 batch 0 loss 0.010605135932564735\n",
      "epoch 28 finished - avarage train loss 0.0064333553088764695  avarage test loss 0.012668882147409022\n",
      "Training: epoch 29 batch 0 loss 0.005603880155831575\n",
      "Training: epoch 29 batch 10 loss 0.010110987350344658\n",
      "Training: epoch 29 batch 20 loss 0.0049186269752681255\n",
      "Test: epoch 29 batch 0 loss 0.011054649949073792\n",
      "epoch 29 finished - avarage train loss 0.007096035349792962  avarage test loss 0.015473274630494416\n",
      "Training: epoch 30 batch 0 loss 0.009818422608077526\n",
      "Training: epoch 30 batch 10 loss 0.003861834993585944\n",
      "Training: epoch 30 batch 20 loss 0.008218381553888321\n",
      "Test: epoch 30 batch 0 loss 0.016202926635742188\n",
      "epoch 30 finished - avarage train loss 0.007486085078261536  avarage test loss 0.022708150325343013\n",
      "Training: epoch 31 batch 0 loss 0.014439510181546211\n",
      "Training: epoch 31 batch 10 loss 0.02712111547589302\n",
      "Training: epoch 31 batch 20 loss 0.01151773426681757\n",
      "Test: epoch 31 batch 0 loss 0.013591086491942406\n",
      "epoch 31 finished - avarage train loss 0.019501200748671746  avarage test loss 0.016018053516745567\n",
      "Training: epoch 32 batch 0 loss 0.00947600044310093\n",
      "Training: epoch 32 batch 10 loss 0.004467567894607782\n",
      "Training: epoch 32 batch 20 loss 0.0032465143594890833\n",
      "Test: epoch 32 batch 0 loss 0.010140516795217991\n",
      "epoch 32 finished - avarage train loss 0.008942724606985676  avarage test loss 0.012814520159736276\n",
      "Training: epoch 33 batch 0 loss 0.0045005884021520615\n",
      "Training: epoch 33 batch 10 loss 0.008756361901760101\n",
      "Training: epoch 33 batch 20 loss 0.004155206494033337\n",
      "Test: epoch 33 batch 0 loss 0.006603648886084557\n",
      "epoch 33 finished - avarage train loss 0.006974143827141359  avarage test loss 0.011700334725901484\n",
      "Training: epoch 34 batch 0 loss 0.012048638425767422\n",
      "Training: epoch 34 batch 10 loss 0.004840975161641836\n",
      "Training: epoch 34 batch 20 loss 0.0030108848586678505\n",
      "Test: epoch 34 batch 0 loss 0.009394055232405663\n",
      "epoch 34 finished - avarage train loss 0.006084847954455121  avarage test loss 0.011678341426886618\n",
      "Training: epoch 35 batch 0 loss 0.008206607773900032\n",
      "Training: epoch 35 batch 10 loss 0.006192974746227264\n",
      "Training: epoch 35 batch 20 loss 0.008824615739285946\n",
      "Test: epoch 35 batch 0 loss 0.009097054600715637\n",
      "epoch 35 finished - avarage train loss 0.007502965942217872  avarage test loss 0.01151199999731034\n",
      "Training: epoch 36 batch 0 loss 0.003138132393360138\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training: epoch 36 batch 10 loss 0.012237388640642166\n",
      "Training: epoch 36 batch 20 loss 0.005538735073059797\n",
      "Test: epoch 36 batch 0 loss 0.011991663835942745\n",
      "epoch 36 finished - avarage train loss 0.006936906266893293  avarage test loss 0.01606442651245743\n",
      "Training: epoch 37 batch 0 loss 0.007049341220408678\n",
      "Training: epoch 37 batch 10 loss 0.006407671142369509\n",
      "Training: epoch 37 batch 20 loss 0.0024961570743471384\n",
      "Test: epoch 37 batch 0 loss 0.009727146476507187\n",
      "epoch 37 finished - avarage train loss 0.006106326669647262  avarage test loss 0.011971995700150728\n",
      "Training: epoch 38 batch 0 loss 0.005870977882295847\n",
      "Training: epoch 38 batch 10 loss 0.0034425663761794567\n",
      "Training: epoch 38 batch 20 loss 0.004363629501312971\n",
      "Test: epoch 38 batch 0 loss 0.010393992066383362\n",
      "epoch 38 finished - avarage train loss 0.006809923300069982  avarage test loss 0.01224895054474473\n",
      "Training: epoch 39 batch 0 loss 0.009279146790504456\n",
      "Training: epoch 39 batch 10 loss 0.006874376907944679\n",
      "Training: epoch 39 batch 20 loss 0.006525713950395584\n",
      "Test: epoch 39 batch 0 loss 0.011610564775764942\n",
      "epoch 39 finished - avarage train loss 0.007657895651485385  avarage test loss 0.013636949588544667\n",
      "Training: epoch 40 batch 0 loss 0.003880032803863287\n",
      "Training: epoch 40 batch 10 loss 0.006610420998185873\n",
      "Training: epoch 40 batch 20 loss 0.007250919006764889\n",
      "Test: epoch 40 batch 0 loss 0.009926768019795418\n",
      "epoch 40 finished - avarage train loss 0.007033674046397209  avarage test loss 0.012545202276669443\n",
      "Training: epoch 41 batch 0 loss 0.015084423124790192\n",
      "Training: epoch 41 batch 10 loss 0.01487109623849392\n",
      "Training: epoch 41 batch 20 loss 0.004436289891600609\n",
      "Test: epoch 41 batch 0 loss 0.010655783116817474\n",
      "epoch 41 finished - avarage train loss 0.008042933571891024  avarage test loss 0.013499844004400074\n",
      "Training: epoch 42 batch 0 loss 0.007432003505527973\n",
      "Training: epoch 42 batch 10 loss 0.01200786978006363\n",
      "Training: epoch 42 batch 20 loss 0.04035617783665657\n",
      "Test: epoch 42 batch 0 loss 0.04718566685914993\n",
      "epoch 42 finished - avarage train loss 0.01948634688421313  avarage test loss 0.04210912436246872\n",
      "Training: epoch 43 batch 0 loss 0.028706679120659828\n",
      "Training: epoch 43 batch 10 loss 0.02325846627354622\n",
      "Training: epoch 43 batch 20 loss 0.01283642090857029\n",
      "Test: epoch 43 batch 0 loss 0.01856042444705963\n",
      "epoch 43 finished - avarage train loss 0.02120527212411679  avarage test loss 0.018816984025761485\n",
      "Training: epoch 44 batch 0 loss 0.005546159110963345\n",
      "Training: epoch 44 batch 10 loss 0.011244844645261765\n",
      "Training: epoch 44 batch 20 loss 0.013126762583851814\n",
      "Test: epoch 44 batch 0 loss 0.01745966263115406\n",
      "epoch 44 finished - avarage train loss 0.01057550436335391  avarage test loss 0.016855641268193722\n",
      "Training: epoch 45 batch 0 loss 0.004845921415835619\n",
      "Training: epoch 45 batch 10 loss 0.003157674800604582\n",
      "Training: epoch 45 batch 20 loss 0.007285722531378269\n",
      "Test: epoch 45 batch 0 loss 0.011287711560726166\n",
      "epoch 45 finished - avarage train loss 0.008572781249752333  avarage test loss 0.01516992028336972\n",
      "Training: epoch 46 batch 0 loss 0.0036228462122380733\n",
      "Training: epoch 46 batch 10 loss 0.0037956407759338617\n",
      "Training: epoch 46 batch 20 loss 0.003596968948841095\n",
      "Test: epoch 46 batch 0 loss 0.010858571156859398\n",
      "epoch 46 finished - avarage train loss 0.007194828490951452  avarage test loss 0.012849542312324047\n",
      "Training: epoch 47 batch 0 loss 0.005760852713137865\n",
      "Training: epoch 47 batch 10 loss 0.004014549311250448\n",
      "Training: epoch 47 batch 20 loss 0.0035770151298493147\n",
      "Test: epoch 47 batch 0 loss 0.011450652964413166\n",
      "epoch 47 finished - avarage train loss 0.007223103356001706  avarage test loss 0.013788249460048974\n",
      "Training: epoch 48 batch 0 loss 0.005272173322737217\n",
      "Training: epoch 48 batch 10 loss 0.0058132647536695\n",
      "Training: epoch 48 batch 20 loss 0.003808837616816163\n",
      "Test: epoch 48 batch 0 loss 0.010580940172076225\n",
      "epoch 48 finished - avarage train loss 0.0068209895639327065  avarage test loss 0.012600093032233417\n",
      "Training: epoch 49 batch 0 loss 0.006571618374437094\n",
      "Training: epoch 49 batch 10 loss 0.0040247696451842785\n",
      "Training: epoch 49 batch 20 loss 0.0036777008790522814\n",
      "Test: epoch 49 batch 0 loss 0.010391226969659328\n",
      "epoch 49 finished - avarage train loss 0.007993396106657797  avarage test loss 0.012657284620217979\n",
      "Training: epoch 50 batch 0 loss 0.0030771244782954454\n",
      "Training: epoch 50 batch 10 loss 0.004963160492479801\n",
      "Training: epoch 50 batch 20 loss 0.0073771169409155846\n",
      "Test: epoch 50 batch 0 loss 0.011142509058117867\n",
      "epoch 50 finished - avarage train loss 0.006851411641350594  avarage test loss 0.013438113499432802\n",
      "Training: epoch 51 batch 0 loss 0.0035244703758507967\n",
      "Training: epoch 51 batch 10 loss 0.009096981957554817\n",
      "Training: epoch 51 batch 20 loss 0.003673785598948598\n",
      "Test: epoch 51 batch 0 loss 0.015288638882339\n",
      "epoch 51 finished - avarage train loss 0.00674476710164213  avarage test loss 0.015835143974982202\n",
      "Training: epoch 52 batch 0 loss 0.0053270733915269375\n",
      "Training: epoch 52 batch 10 loss 0.0023809790145605803\n",
      "Training: epoch 52 batch 20 loss 0.009796847589313984\n",
      "Test: epoch 52 batch 0 loss 0.016750594601035118\n",
      "epoch 52 finished - avarage train loss 0.008420770867438665  avarage test loss 0.018335520988330245\n",
      "Training: epoch 53 batch 0 loss 0.0077525414526462555\n",
      "Training: epoch 53 batch 10 loss 0.010069005191326141\n",
      "Training: epoch 53 batch 20 loss 0.00743093341588974\n",
      "Test: epoch 53 batch 0 loss 0.01250285841524601\n",
      "epoch 53 finished - avarage train loss 0.009524639503195369  avarage test loss 0.015143109951168299\n",
      "Training: epoch 54 batch 0 loss 0.008565445430576801\n",
      "Training: epoch 54 batch 10 loss 0.005124012939631939\n",
      "Training: epoch 54 batch 20 loss 0.0032079676166176796\n",
      "Test: epoch 54 batch 0 loss 0.009858667850494385\n",
      "epoch 54 finished - avarage train loss 0.008582826856331065  avarage test loss 0.011894824798218906\n",
      "Training: epoch 55 batch 0 loss 0.004497375804930925\n",
      "Training: epoch 55 batch 10 loss 0.006115229334682226\n",
      "Training: epoch 55 batch 20 loss 0.0062225800938904285\n",
      "Test: epoch 55 batch 0 loss 0.010448923334479332\n",
      "epoch 55 finished - avarage train loss 0.007133114127570699  avarage test loss 0.012487606727518141\n",
      "Training: epoch 56 batch 0 loss 0.003387061646208167\n",
      "Training: epoch 56 batch 10 loss 0.008076530881226063\n",
      "Training: epoch 56 batch 20 loss 0.006091050338000059\n",
      "Test: epoch 56 batch 0 loss 0.010374919511377811\n",
      "epoch 56 finished - avarage train loss 0.0055963387712836266  avarage test loss 0.01418999524321407\n",
      "Training: epoch 57 batch 0 loss 0.0032679717987775803\n",
      "Training: epoch 57 batch 10 loss 0.009563600644469261\n",
      "Training: epoch 57 batch 20 loss 0.009326407685875893\n",
      "Test: epoch 57 batch 0 loss 0.010098160244524479\n",
      "epoch 57 finished - avarage train loss 0.007170316747164932  avarage test loss 0.012269048253074288\n",
      "Training: epoch 58 batch 0 loss 0.002754397690296173\n",
      "Training: epoch 58 batch 10 loss 0.0031904333736747503\n",
      "Training: epoch 58 batch 20 loss 0.0056325094774365425\n",
      "Test: epoch 58 batch 0 loss 0.00905550830066204\n",
      "epoch 58 finished - avarage train loss 0.007154907123988558  avarage test loss 0.012744534993544221\n",
      "Training: epoch 59 batch 0 loss 0.004299366381019354\n",
      "Training: epoch 59 batch 10 loss 0.004023079294711351\n",
      "Training: epoch 59 batch 20 loss 0.006282917223870754\n",
      "Test: epoch 59 batch 0 loss 0.008950961753726006\n",
      "epoch 59 finished - avarage train loss 0.008085909981987086  avarage test loss 0.014934353297576308\n",
      "Training: epoch 60 batch 0 loss 0.01493300125002861\n",
      "Training: epoch 60 batch 10 loss 0.013571348041296005\n",
      "Training: epoch 60 batch 20 loss 0.008053520694375038\n",
      "Test: epoch 60 batch 0 loss 0.01038373727351427\n",
      "epoch 60 finished - avarage train loss 0.010799412663769105  avarage test loss 0.01571453781798482\n",
      "Training: epoch 61 batch 0 loss 0.006902842782437801\n",
      "Training: epoch 61 batch 10 loss 0.0059546674601733685\n",
      "Training: epoch 61 batch 20 loss 0.003420855151489377\n",
      "Test: epoch 61 batch 0 loss 0.008992134593427181\n",
      "epoch 61 finished - avarage train loss 0.006716290512925078  avarage test loss 0.01126380858477205\n",
      "Training: epoch 62 batch 0 loss 0.003494885051622987\n",
      "Training: epoch 62 batch 10 loss 0.0038628519978374243\n",
      "Training: epoch 62 batch 20 loss 0.006804949138313532\n",
      "Test: epoch 62 batch 0 loss 0.009175358340144157\n",
      "epoch 62 finished - avarage train loss 0.007747515040481913  avarage test loss 0.011352639296092093\n",
      "Training: epoch 63 batch 0 loss 0.004413272254168987\n",
      "Training: epoch 63 batch 10 loss 0.005256555043160915\n",
      "Training: epoch 63 batch 20 loss 0.004278955049812794\n",
      "Test: epoch 63 batch 0 loss 0.009984140284359455\n",
      "epoch 63 finished - avarage train loss 0.006643290928532851  avarage test loss 0.012702319072559476\n",
      "Training: epoch 64 batch 0 loss 0.006983584724366665\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training: epoch 64 batch 10 loss 0.01790531538426876\n",
      "Training: epoch 64 batch 20 loss 0.007444277405738831\n",
      "Test: epoch 64 batch 0 loss 0.009873941540718079\n",
      "epoch 64 finished - avarage train loss 0.006494992388540815  avarage test loss 0.01178134192014113\n",
      "Training: epoch 65 batch 0 loss 0.0028202729299664497\n",
      "Training: epoch 65 batch 10 loss 0.009031744673848152\n",
      "Training: epoch 65 batch 20 loss 0.006097970064729452\n",
      "Test: epoch 65 batch 0 loss 0.010923921130597591\n",
      "epoch 65 finished - avarage train loss 0.006660685770149375  avarage test loss 0.014377690386027098\n",
      "Training: epoch 66 batch 0 loss 0.0025400156155228615\n",
      "Training: epoch 66 batch 10 loss 0.009276427328586578\n",
      "Training: epoch 66 batch 20 loss 0.0024997289292514324\n",
      "Test: epoch 66 batch 0 loss 0.013162199407815933\n",
      "epoch 66 finished - avarage train loss 0.007627323229700841  avarage test loss 0.01514777634292841\n",
      "Training: epoch 67 batch 0 loss 0.00782851129770279\n",
      "Training: epoch 67 batch 10 loss 0.009805864654481411\n",
      "Training: epoch 67 batch 20 loss 0.002083486644551158\n",
      "Test: epoch 67 batch 0 loss 0.012017650529742241\n",
      "epoch 67 finished - avarage train loss 0.014614671749737242  avarage test loss 0.014316618675366044\n",
      "Training: epoch 68 batch 0 loss 0.005084841977804899\n",
      "Training: epoch 68 batch 10 loss 0.006145966239273548\n",
      "Training: epoch 68 batch 20 loss 0.009953855536878109\n",
      "Test: epoch 68 batch 0 loss 0.010819842107594013\n",
      "epoch 68 finished - avarage train loss 0.008631149563809922  avarage test loss 0.013629177119582891\n",
      "Training: epoch 69 batch 0 loss 0.0033440887928009033\n",
      "Training: epoch 69 batch 10 loss 0.0059816595166921616\n",
      "Training: epoch 69 batch 20 loss 0.002761805197224021\n",
      "Test: epoch 69 batch 0 loss 0.010046117007732391\n",
      "epoch 69 finished - avarage train loss 0.007499809368866785  avarage test loss 0.015053900657221675\n",
      "Training: epoch 70 batch 0 loss 0.005185419227927923\n",
      "Training: epoch 70 batch 10 loss 0.005895139183849096\n",
      "Training: epoch 70 batch 20 loss 0.007856030948460102\n",
      "Test: epoch 70 batch 0 loss 0.009739416651427746\n",
      "epoch 70 finished - avarage train loss 0.006536272940514931  avarage test loss 0.013366649625822902\n",
      "Training: epoch 71 batch 0 loss 0.005206447560340166\n",
      "Training: epoch 71 batch 10 loss 0.006548043806105852\n",
      "Training: epoch 71 batch 20 loss 0.004260669462382793\n",
      "Test: epoch 71 batch 0 loss 0.010983948595821857\n",
      "epoch 71 finished - avarage train loss 0.006792331451614355  avarage test loss 0.014124821638688445\n",
      "Training: epoch 72 batch 0 loss 0.00548740616068244\n",
      "Training: epoch 72 batch 10 loss 0.005819442216306925\n",
      "Training: epoch 72 batch 20 loss 0.0059013208374381065\n",
      "Test: epoch 72 batch 0 loss 0.01688823662698269\n",
      "epoch 72 finished - avarage train loss 0.007997825533023169  avarage test loss 0.015645934734493494\n",
      "Training: epoch 73 batch 0 loss 0.0042993417009711266\n",
      "Training: epoch 73 batch 10 loss 0.006148429587483406\n",
      "Training: epoch 73 batch 20 loss 0.0031110444106161594\n",
      "Test: epoch 73 batch 0 loss 0.016610298305749893\n",
      "epoch 73 finished - avarage train loss 0.006936209878466767  avarage test loss 0.01623129693325609\n",
      "Training: epoch 74 batch 0 loss 0.0026653565000742674\n",
      "Training: epoch 74 batch 10 loss 0.006117383949458599\n",
      "Training: epoch 74 batch 20 loss 0.0034048878587782383\n",
      "Test: epoch 74 batch 0 loss 0.011297974735498428\n",
      "epoch 74 finished - avarage train loss 0.008121934598954073  avarage test loss 0.013421681709587574\n",
      "Training: epoch 75 batch 0 loss 0.0034905464854091406\n",
      "Training: epoch 75 batch 10 loss 0.005794575437903404\n",
      "Training: epoch 75 batch 20 loss 0.007180871441960335\n",
      "Test: epoch 75 batch 0 loss 0.01699499785900116\n",
      "epoch 75 finished - avarage train loss 0.007481668907185567  avarage test loss 0.015379062853753567\n",
      "Training: epoch 76 batch 0 loss 0.0034956836607307196\n",
      "Training: epoch 76 batch 10 loss 0.00625133840367198\n",
      "Training: epoch 76 batch 20 loss 0.00370792206376791\n",
      "Test: epoch 76 batch 0 loss 0.010518018156290054\n",
      "epoch 76 finished - avarage train loss 0.006542257386548766  avarage test loss 0.012924991315230727\n",
      "Training: epoch 77 batch 0 loss 0.0037569862324744463\n",
      "Training: epoch 77 batch 10 loss 0.010558081790804863\n",
      "Training: epoch 77 batch 20 loss 0.011471331119537354\n",
      "Test: epoch 77 batch 0 loss 0.010106083936989307\n",
      "epoch 77 finished - avarage train loss 0.006540597544918801  avarage test loss 0.012382410117425025\n",
      "Training: epoch 78 batch 0 loss 0.0019038512837141752\n",
      "Training: epoch 78 batch 10 loss 0.005148099735379219\n",
      "Training: epoch 78 batch 20 loss 0.01058279164135456\n",
      "Test: epoch 78 batch 0 loss 0.010915267281234264\n",
      "epoch 78 finished - avarage train loss 0.006215207977220416  avarage test loss 0.014643080998212099\n",
      "Training: epoch 79 batch 0 loss 0.005571016576141119\n",
      "Training: epoch 79 batch 10 loss 0.0032460240181535482\n",
      "Training: epoch 79 batch 20 loss 0.008892962709069252\n",
      "Test: epoch 79 batch 0 loss 0.010347365401685238\n",
      "epoch 79 finished - avarage train loss 0.007687611730191214  avarage test loss 0.012828961596824229\n",
      "Training: epoch 80 batch 0 loss 0.004875760525465012\n",
      "Training: epoch 80 batch 10 loss 0.014219389297068119\n",
      "Training: epoch 80 batch 20 loss 0.002919011516496539\n",
      "Test: epoch 80 batch 0 loss 0.01445234939455986\n",
      "epoch 80 finished - avarage train loss 0.007109588810949233  avarage test loss 0.015911011723801494\n",
      "Training: epoch 81 batch 0 loss 0.007885439321398735\n",
      "Training: epoch 81 batch 10 loss 0.005960768088698387\n",
      "Training: epoch 81 batch 20 loss 0.004063503351062536\n",
      "Test: epoch 81 batch 0 loss 0.010945599526166916\n",
      "epoch 81 finished - avarage train loss 0.008024231177465669  avarage test loss 0.014162282925099134\n",
      "Training: epoch 82 batch 0 loss 0.005957187153398991\n",
      "Training: epoch 82 batch 10 loss 0.00287442235276103\n",
      "Training: epoch 82 batch 20 loss 0.0029349022079259157\n",
      "Test: epoch 82 batch 0 loss 0.009901490062475204\n",
      "epoch 82 finished - avarage train loss 0.006817482741035778  avarage test loss 0.012299243942834437\n",
      "Training: epoch 83 batch 0 loss 0.013092086650431156\n",
      "Training: epoch 83 batch 10 loss 0.0025013822596520185\n",
      "Training: epoch 83 batch 20 loss 0.004271720070391893\n",
      "Test: epoch 83 batch 0 loss 0.01200648583471775\n",
      "epoch 83 finished - avarage train loss 0.0073972479279699  avarage test loss 0.014309649355709553\n",
      "Training: epoch 84 batch 0 loss 0.0030235215090215206\n",
      "Training: epoch 84 batch 10 loss 0.005095964297652245\n",
      "Training: epoch 84 batch 20 loss 0.0057898638769984245\n",
      "Test: epoch 84 batch 0 loss 0.014627423137426376\n",
      "epoch 84 finished - avarage train loss 0.0066223121248185635  avarage test loss 0.01572092203423381\n",
      "Training: epoch 85 batch 0 loss 0.005777894984930754\n",
      "Training: epoch 85 batch 10 loss 0.010824424214661121\n",
      "Training: epoch 85 batch 20 loss 0.008452998474240303\n",
      "Test: epoch 85 batch 0 loss 0.02081563137471676\n",
      "epoch 85 finished - avarage train loss 0.008021030558979717  avarage test loss 0.02223495056387037\n",
      "Training: epoch 86 batch 0 loss 0.011990539729595184\n",
      "Training: epoch 86 batch 10 loss 0.02050161547958851\n",
      "Training: epoch 86 batch 20 loss 0.007287624757736921\n",
      "Test: epoch 86 batch 0 loss 0.018795419484376907\n",
      "epoch 86 finished - avarage train loss 0.011104458605806375  avarage test loss 0.016951105557382107\n",
      "Training: epoch 87 batch 0 loss 0.007424549199640751\n",
      "Training: epoch 87 batch 10 loss 0.004079424776136875\n",
      "Training: epoch 87 batch 20 loss 0.006755501497536898\n",
      "Test: epoch 87 batch 0 loss 0.01668303646147251\n",
      "epoch 87 finished - avarage train loss 0.006901812346266775  avarage test loss 0.01828581397421658\n",
      "Training: epoch 88 batch 0 loss 0.0034901087637990713\n",
      "Training: epoch 88 batch 10 loss 0.006444535218179226\n",
      "Training: epoch 88 batch 20 loss 0.004619703162461519\n",
      "Test: epoch 88 batch 0 loss 0.010688185691833496\n",
      "epoch 88 finished - avarage train loss 0.006635523850804773  avarage test loss 0.012908660341054201\n",
      "Training: epoch 89 batch 0 loss 0.004083627834916115\n",
      "Training: epoch 89 batch 10 loss 0.003422452136874199\n",
      "Training: epoch 89 batch 20 loss 0.008314255625009537\n",
      "Test: epoch 89 batch 0 loss 0.011627192609012127\n",
      "epoch 89 finished - avarage train loss 0.006673852519678144  avarage test loss 0.013303475803695619\n",
      "Training: epoch 90 batch 0 loss 0.006066239904612303\n",
      "Training: epoch 90 batch 10 loss 0.006143729668110609\n",
      "Training: epoch 90 batch 20 loss 0.00392880430445075\n",
      "Test: epoch 90 batch 0 loss 0.012156294658780098\n",
      "epoch 90 finished - avarage train loss 0.007244001632427861  avarage test loss 0.013583529856987298\n",
      "Training: epoch 91 batch 0 loss 0.007059655152261257\n",
      "Training: epoch 91 batch 10 loss 0.0040777442045509815\n",
      "Training: epoch 91 batch 20 loss 0.009509539231657982\n",
      "Test: epoch 91 batch 0 loss 0.014657995663583279\n",
      "epoch 91 finished - avarage train loss 0.008137925800963723  avarage test loss 0.01834468124434352\n",
      "Training: epoch 92 batch 0 loss 0.010806513950228691\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training: epoch 92 batch 10 loss 0.01305771991610527\n",
      "Training: epoch 92 batch 20 loss 0.003899071831256151\n",
      "Test: epoch 92 batch 0 loss 0.016439324244856834\n",
      "epoch 92 finished - avarage train loss 0.007342616441370598  avarage test loss 0.019454917404800653\n",
      "Training: epoch 93 batch 0 loss 0.006189798470586538\n",
      "Training: epoch 93 batch 10 loss 0.009318780153989792\n",
      "Training: epoch 93 batch 20 loss 0.005671196151524782\n",
      "Test: epoch 93 batch 0 loss 0.017427438870072365\n",
      "epoch 93 finished - avarage train loss 0.008112633384057674  avarage test loss 0.018590956926345825\n",
      "Training: epoch 94 batch 0 loss 0.003305015154182911\n",
      "Training: epoch 94 batch 10 loss 0.004770368803292513\n",
      "Training: epoch 94 batch 20 loss 0.009418126195669174\n",
      "Test: epoch 94 batch 0 loss 0.01995539292693138\n",
      "epoch 94 finished - avarage train loss 0.008354089391449916  avarage test loss 0.020410084864124656\n",
      "Training: epoch 95 batch 0 loss 0.008508075028657913\n",
      "Training: epoch 95 batch 10 loss 0.019510559737682343\n",
      "Training: epoch 95 batch 20 loss 0.006659441627562046\n",
      "Test: epoch 95 batch 0 loss 0.01611991412937641\n",
      "epoch 95 finished - avarage train loss 0.008459013392988208  avarage test loss 0.015426588943228126\n",
      "Training: epoch 96 batch 0 loss 0.0037515894509851933\n",
      "Training: epoch 96 batch 10 loss 0.006328757852315903\n",
      "Training: epoch 96 batch 20 loss 0.0027428120374679565\n",
      "Test: epoch 96 batch 0 loss 0.019388126209378242\n",
      "epoch 96 finished - avarage train loss 0.006776213477333558  avarage test loss 0.01566409505903721\n",
      "Training: epoch 97 batch 0 loss 0.0029306926298886538\n",
      "Training: epoch 97 batch 10 loss 0.0050353799015283585\n",
      "Training: epoch 97 batch 20 loss 0.00501864030957222\n",
      "Test: epoch 97 batch 0 loss 0.017352210357785225\n",
      "epoch 97 finished - avarage train loss 0.006771342475609532  avarage test loss 0.015423576696775854\n",
      "Training: epoch 98 batch 0 loss 0.004506625235080719\n",
      "Training: epoch 98 batch 10 loss 0.0016829381929710507\n",
      "Training: epoch 98 batch 20 loss 0.0073210676200687885\n",
      "Test: epoch 98 batch 0 loss 0.01697881892323494\n",
      "epoch 98 finished - avarage train loss 0.0066588228480119645  avarage test loss 0.015251470496878028\n",
      "Training: epoch 99 batch 0 loss 0.0019084681989625096\n",
      "Training: epoch 99 batch 10 loss 0.004744394216686487\n",
      "Training: epoch 99 batch 20 loss 0.008758481591939926\n",
      "Test: epoch 99 batch 0 loss 0.017350884154438972\n",
      "epoch 99 finished - avarage train loss 0.007998064445363808  avarage test loss 0.015658021089620888\n",
      "Training: epoch 100 batch 0 loss 0.006986057385802269\n",
      "Training: epoch 100 batch 10 loss 0.00716466223821044\n",
      "Training: epoch 100 batch 20 loss 0.0036706174723803997\n",
      "Test: epoch 100 batch 0 loss 0.011230059899389744\n",
      "epoch 100 finished - avarage train loss 0.008312672753978905  avarage test loss 0.012818078510463238\n",
      "Training: epoch 101 batch 0 loss 0.00635588588193059\n",
      "Training: epoch 101 batch 10 loss 0.007688234094530344\n",
      "Training: epoch 101 batch 20 loss 0.007984028197824955\n",
      "Test: epoch 101 batch 0 loss 0.011639128439128399\n",
      "epoch 101 finished - avarage train loss 0.006576260086148977  avarage test loss 0.012957652448676527\n",
      "Training: epoch 102 batch 0 loss 0.0052253687754273415\n",
      "Training: epoch 102 batch 10 loss 0.004608489573001862\n",
      "Training: epoch 102 batch 20 loss 0.00675875972956419\n",
      "Test: epoch 102 batch 0 loss 0.011998921632766724\n",
      "epoch 102 finished - avarage train loss 0.007008287565911125  avarage test loss 0.015125388861633837\n",
      "Training: epoch 103 batch 0 loss 0.013963976874947548\n",
      "Training: epoch 103 batch 10 loss 0.006665692664682865\n",
      "Training: epoch 103 batch 20 loss 0.00572348153218627\n",
      "Test: epoch 103 batch 0 loss 0.010027690790593624\n",
      "epoch 103 finished - avarage train loss 0.008122111196982962  avarage test loss 0.015555754187516868\n",
      "Training: epoch 104 batch 0 loss 0.0034296815283596516\n",
      "Training: epoch 104 batch 10 loss 0.004649207927286625\n",
      "Training: epoch 104 batch 20 loss 0.005334281362593174\n",
      "Test: epoch 104 batch 0 loss 0.008969302289187908\n",
      "epoch 104 finished - avarage train loss 0.007421190184059328  avarage test loss 0.012389609590172768\n",
      "Training: epoch 105 batch 0 loss 0.007287339773029089\n",
      "Training: epoch 105 batch 10 loss 0.003186855698004365\n",
      "Training: epoch 105 batch 20 loss 0.014649282209575176\n",
      "Test: epoch 105 batch 0 loss 0.0092005031183362\n",
      "epoch 105 finished - avarage train loss 0.008596205409487774  avarage test loss 0.012450088048353791\n",
      "Training: epoch 106 batch 0 loss 0.00342742633074522\n",
      "Training: epoch 106 batch 10 loss 0.0026375153101980686\n",
      "Training: epoch 106 batch 20 loss 0.0038463124074041843\n",
      "Test: epoch 106 batch 0 loss 0.010732496157288551\n",
      "epoch 106 finished - avarage train loss 0.007492871581705223  avarage test loss 0.01331728626973927\n",
      "Training: epoch 107 batch 0 loss 0.005574548617005348\n",
      "Training: epoch 107 batch 10 loss 0.0032141581177711487\n",
      "Training: epoch 107 batch 20 loss 0.0072652422823011875\n",
      "Test: epoch 107 batch 0 loss 0.014836672693490982\n",
      "epoch 107 finished - avarage train loss 0.00788738770041101  avarage test loss 0.015358690172433853\n",
      "Training: epoch 108 batch 0 loss 0.004851355217397213\n",
      "Training: epoch 108 batch 10 loss 0.011863594874739647\n",
      "Training: epoch 108 batch 20 loss 0.002535009989514947\n",
      "Test: epoch 108 batch 0 loss 0.015226874500513077\n",
      "epoch 108 finished - avarage train loss 0.007495606334173474  avarage test loss 0.017737421207129955\n",
      "Training: epoch 109 batch 0 loss 0.013031830079853535\n",
      "Training: epoch 109 batch 10 loss 0.0027311500161886215\n",
      "Training: epoch 109 batch 20 loss 0.0032708169892430305\n",
      "Test: epoch 109 batch 0 loss 0.011393231339752674\n",
      "epoch 109 finished - avarage train loss 0.006645403360819508  avarage test loss 0.013585851876996458\n",
      "Training: epoch 110 batch 0 loss 0.010442781262099743\n",
      "Training: epoch 110 batch 10 loss 0.01166221871972084\n",
      "Training: epoch 110 batch 20 loss 0.005292697809636593\n",
      "Test: epoch 110 batch 0 loss 0.011414126493036747\n",
      "epoch 110 finished - avarage train loss 0.006755146197974682  avarage test loss 0.014287912868894637\n",
      "Training: epoch 111 batch 0 loss 0.007763695903122425\n",
      "Training: epoch 111 batch 10 loss 0.006103956140577793\n",
      "Training: epoch 111 batch 20 loss 0.00799847673624754\n",
      "Test: epoch 111 batch 0 loss 0.012452756054699421\n",
      "epoch 111 finished - avarage train loss 0.00771119218351769  avarage test loss 0.0151491864817217\n",
      "Training: epoch 112 batch 0 loss 0.006983289960771799\n",
      "Training: epoch 112 batch 10 loss 0.007636373862624168\n",
      "Training: epoch 112 batch 20 loss 0.0036197167355567217\n",
      "Test: epoch 112 batch 0 loss 0.011164640076458454\n",
      "epoch 112 finished - avarage train loss 0.007747038588698568  avarage test loss 0.013159084017388523\n",
      "Training: epoch 113 batch 0 loss 0.010264248587191105\n",
      "Training: epoch 113 batch 10 loss 0.005883313715457916\n",
      "Training: epoch 113 batch 20 loss 0.007340425159782171\n",
      "Test: epoch 113 batch 0 loss 0.01162763126194477\n",
      "epoch 113 finished - avarage train loss 0.0071281950744190095  avarage test loss 0.012902575428597629\n",
      "Training: epoch 114 batch 0 loss 0.006393404211848974\n",
      "Training: epoch 114 batch 10 loss 0.004461412318050861\n",
      "Training: epoch 114 batch 20 loss 0.006500347517430782\n",
      "Test: epoch 114 batch 0 loss 0.015090260654687881\n",
      "epoch 114 finished - avarage train loss 0.005830806069846811  avarage test loss 0.015476351720280945\n",
      "Training: epoch 115 batch 0 loss 0.006875684950500727\n",
      "Training: epoch 115 batch 10 loss 0.0057660252787172794\n",
      "Training: epoch 115 batch 20 loss 0.009912731125950813\n",
      "Test: epoch 115 batch 0 loss 0.024830935522913933\n",
      "epoch 115 finished - avarage train loss 0.012399532125684721  avarage test loss 0.022661819821223617\n",
      "Training: epoch 116 batch 0 loss 0.010473380796611309\n",
      "Training: epoch 116 batch 10 loss 0.007813572883605957\n",
      "Training: epoch 116 batch 20 loss 0.009973081760108471\n",
      "Test: epoch 116 batch 0 loss 0.0209954921156168\n",
      "epoch 116 finished - avarage train loss 0.015088134284677177  avarage test loss 0.022060742368921638\n",
      "Training: epoch 117 batch 0 loss 0.014692814089357853\n",
      "Training: epoch 117 batch 10 loss 0.005057281814515591\n",
      "Training: epoch 117 batch 20 loss 0.005984400864690542\n",
      "Test: epoch 117 batch 0 loss 0.022404486313462257\n",
      "epoch 117 finished - avarage train loss 0.009328517966486257  avarage test loss 0.018717732978984714\n",
      "Training: epoch 118 batch 0 loss 0.0061352127231657505\n",
      "Training: epoch 118 batch 10 loss 0.0034322349820286036\n",
      "Training: epoch 118 batch 20 loss 0.009135839529335499\n",
      "Test: epoch 118 batch 0 loss 0.019897354766726494\n",
      "epoch 118 finished - avarage train loss 0.007493048761810722  avarage test loss 0.01617579278536141\n",
      "Training: epoch 119 batch 0 loss 0.0032190524507313967\n",
      "Training: epoch 119 batch 10 loss 0.010099870152771473\n",
      "Training: epoch 119 batch 20 loss 0.006456026807427406\n",
      "Test: epoch 119 batch 0 loss 0.02065400406718254\n",
      "epoch 119 finished - avarage train loss 0.007879397438453704  avarage test loss 0.01799195830244571\n",
      "Training: epoch 120 batch 0 loss 0.008801089599728584\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training: epoch 120 batch 10 loss 0.0035610769409686327\n",
      "Training: epoch 120 batch 20 loss 0.006093426141887903\n",
      "Test: epoch 120 batch 0 loss 0.018365472555160522\n",
      "epoch 120 finished - avarage train loss 0.007763528208858494  avarage test loss 0.0175397862913087\n",
      "Training: epoch 121 batch 0 loss 0.002713609952479601\n",
      "Training: epoch 121 batch 10 loss 0.007936115376651287\n",
      "Training: epoch 121 batch 20 loss 0.005708618555217981\n",
      "Test: epoch 121 batch 0 loss 0.01780787669122219\n",
      "epoch 121 finished - avarage train loss 0.006703290502251736  avarage test loss 0.018069628858938813\n",
      "Training: epoch 122 batch 0 loss 0.010220210999250412\n",
      "Training: epoch 122 batch 10 loss 0.007978393696248531\n",
      "Training: epoch 122 batch 20 loss 0.004340717103332281\n",
      "Test: epoch 122 batch 0 loss 0.01840737648308277\n",
      "epoch 122 finished - avarage train loss 0.007082591226709814  avarage test loss 0.01566127175465226\n",
      "Training: epoch 123 batch 0 loss 0.005211465526372194\n",
      "Training: epoch 123 batch 10 loss 0.008810953237116337\n",
      "Training: epoch 123 batch 20 loss 0.004182158969342709\n",
      "Test: epoch 123 batch 0 loss 0.019820217043161392\n",
      "epoch 123 finished - avarage train loss 0.008515920249167187  avarage test loss 0.015914631308987737\n",
      "Training: epoch 124 batch 0 loss 0.009829236194491386\n",
      "Training: epoch 124 batch 10 loss 0.003909529652446508\n",
      "Training: epoch 124 batch 20 loss 0.003824211424216628\n",
      "Test: epoch 124 batch 0 loss 0.019259115681052208\n",
      "epoch 124 finished - avarage train loss 0.006879258712207706  avarage test loss 0.01565339381340891\n",
      "Training: epoch 125 batch 0 loss 0.004941924009472132\n",
      "Training: epoch 125 batch 10 loss 0.002669490408152342\n",
      "Training: epoch 125 batch 20 loss 0.003325502388179302\n",
      "Test: epoch 125 batch 0 loss 0.01991494745016098\n",
      "epoch 125 finished - avarage train loss 0.006781375685963651  avarage test loss 0.01680131594184786\n",
      "Training: epoch 126 batch 0 loss 0.007431101053953171\n",
      "Training: epoch 126 batch 10 loss 0.012373385019600391\n",
      "Training: epoch 126 batch 20 loss 0.005067107733339071\n",
      "Test: epoch 126 batch 0 loss 0.019529135897755623\n",
      "epoch 126 finished - avarage train loss 0.00803243710081382  avarage test loss 0.016084703849628568\n",
      "Training: epoch 127 batch 0 loss 0.003927282989025116\n",
      "Training: epoch 127 batch 10 loss 0.006761916913092136\n",
      "Training: epoch 127 batch 20 loss 0.00843585655093193\n",
      "Test: epoch 127 batch 0 loss 0.019507165998220444\n",
      "epoch 127 finished - avarage train loss 0.006340476917102933  avarage test loss 0.015602135797962546\n",
      "Training: epoch 128 batch 0 loss 0.010070035234093666\n",
      "Training: epoch 128 batch 10 loss 0.003799919970333576\n",
      "Training: epoch 128 batch 20 loss 0.003235552692785859\n",
      "Test: epoch 128 batch 0 loss 0.022582393139600754\n",
      "epoch 128 finished - avarage train loss 0.006976119619954763  avarage test loss 0.02291176002472639\n",
      "Training: epoch 129 batch 0 loss 0.010903972201049328\n",
      "Training: epoch 129 batch 10 loss 0.0077155521139502525\n",
      "Training: epoch 129 batch 20 loss 0.0017807362601161003\n",
      "Test: epoch 129 batch 0 loss 0.0174697432667017\n",
      "epoch 129 finished - avarage train loss 0.008043505331694052  avarage test loss 0.015929601388052106\n",
      "Training: epoch 130 batch 0 loss 0.005372090730816126\n",
      "Training: epoch 130 batch 10 loss 0.013687074184417725\n",
      "Training: epoch 130 batch 20 loss 0.012976884841918945\n",
      "Test: epoch 130 batch 0 loss 0.01837742328643799\n",
      "epoch 130 finished - avarage train loss 0.008665917144189107  avarage test loss 0.01621447817888111\n",
      "Training: epoch 131 batch 0 loss 0.006678420584648848\n",
      "Training: epoch 131 batch 10 loss 0.0036149967927485704\n",
      "Training: epoch 131 batch 20 loss 0.010571660473942757\n",
      "Test: epoch 131 batch 0 loss 0.017287377268075943\n",
      "epoch 131 finished - avarage train loss 0.007028338218364736  avarage test loss 0.015735733439214528\n",
      "Training: epoch 132 batch 0 loss 0.005666822660714388\n",
      "Training: epoch 132 batch 10 loss 0.007749507669359446\n",
      "Training: epoch 132 batch 20 loss 0.004414792638272047\n",
      "Test: epoch 132 batch 0 loss 0.016950692981481552\n",
      "epoch 132 finished - avarage train loss 0.007182708828999051  avarage test loss 0.015373027417808771\n",
      "Training: epoch 133 batch 0 loss 0.007743382826447487\n",
      "Training: epoch 133 batch 10 loss 0.002260876353830099\n",
      "Training: epoch 133 batch 20 loss 0.002045589266344905\n",
      "Test: epoch 133 batch 0 loss 0.016758494079113007\n",
      "epoch 133 finished - avarage train loss 0.007954335928059601  avarage test loss 0.015386027749627829\n",
      "Training: epoch 134 batch 0 loss 0.003310353262349963\n",
      "Training: epoch 134 batch 10 loss 0.004517763387411833\n",
      "Training: epoch 134 batch 20 loss 0.005452870856970549\n",
      "Test: epoch 134 batch 0 loss 0.017895713448524475\n",
      "epoch 134 finished - avarage train loss 0.0065200431834392504  avarage test loss 0.018523230799473822\n",
      "Training: epoch 135 batch 0 loss 0.00915756355971098\n",
      "Training: epoch 135 batch 10 loss 0.005550247151404619\n",
      "Training: epoch 135 batch 20 loss 0.004013019148260355\n",
      "Test: epoch 135 batch 0 loss 0.014477689750492573\n",
      "epoch 135 finished - avarage train loss 0.007104349170072839  avarage test loss 0.01480651437304914\n",
      "Training: epoch 136 batch 0 loss 0.005002249963581562\n",
      "Training: epoch 136 batch 10 loss 0.003919336013495922\n",
      "Training: epoch 136 batch 20 loss 0.004840383306145668\n",
      "Test: epoch 136 batch 0 loss 0.01506232563406229\n",
      "epoch 136 finished - avarage train loss 0.007374849379191111  avarage test loss 0.018208647961728275\n",
      "Training: epoch 137 batch 0 loss 0.00577494828030467\n",
      "Training: epoch 137 batch 10 loss 0.006926674395799637\n",
      "Training: epoch 137 batch 20 loss 0.0032450889702886343\n",
      "Test: epoch 137 batch 0 loss 0.01864345371723175\n",
      "epoch 137 finished - avarage train loss 0.007993946991989324  avarage test loss 0.0179887639824301\n",
      "Training: epoch 138 batch 0 loss 0.005896406248211861\n",
      "Training: epoch 138 batch 10 loss 0.004054852295666933\n",
      "Training: epoch 138 batch 20 loss 0.011385020799934864\n",
      "Test: epoch 138 batch 0 loss 0.016908565536141396\n",
      "epoch 138 finished - avarage train loss 0.008013187128857806  avarage test loss 0.01585627463646233\n",
      "Training: epoch 139 batch 0 loss 0.004790283739566803\n",
      "Training: epoch 139 batch 10 loss 0.004025455564260483\n",
      "Training: epoch 139 batch 20 loss 0.006805074866861105\n",
      "Test: epoch 139 batch 0 loss 0.018307393416762352\n",
      "epoch 139 finished - avarage train loss 0.006855485976898465  avarage test loss 0.01839513110462576\n",
      "Training: epoch 140 batch 0 loss 0.003589192871004343\n",
      "Training: epoch 140 batch 10 loss 0.006976982578635216\n",
      "Training: epoch 140 batch 20 loss 0.002398376353085041\n",
      "Test: epoch 140 batch 0 loss 0.013831323944032192\n",
      "epoch 140 finished - avarage train loss 0.008116264321744955  avarage test loss 0.014528051018714905\n",
      "Training: epoch 141 batch 0 loss 0.001784657477401197\n",
      "Training: epoch 141 batch 10 loss 0.005932140164077282\n",
      "Training: epoch 141 batch 20 loss 0.01810983009636402\n",
      "Test: epoch 141 batch 0 loss 0.013913443312048912\n",
      "epoch 141 finished - avarage train loss 0.009110449512232223  avarage test loss 0.014276797301135957\n",
      "Training: epoch 142 batch 0 loss 0.002268006559461355\n",
      "Training: epoch 142 batch 10 loss 0.009838582947850227\n",
      "Training: epoch 142 batch 20 loss 0.009608903899788857\n",
      "Test: epoch 142 batch 0 loss 0.016406066715717316\n",
      "epoch 142 finished - avarage train loss 0.0068596294631475  avarage test loss 0.015445395489223301\n",
      "Training: epoch 143 batch 0 loss 0.007388116791844368\n",
      "Training: epoch 143 batch 10 loss 0.00646843109279871\n",
      "Training: epoch 143 batch 20 loss 0.004655189346522093\n",
      "Test: epoch 143 batch 0 loss 0.011718486435711384\n",
      "epoch 143 finished - avarage train loss 0.006336911890559412  avarage test loss 0.015265180845744908\n",
      "Training: epoch 144 batch 0 loss 0.006430584006011486\n",
      "Training: epoch 144 batch 10 loss 0.0033458571415394545\n",
      "Training: epoch 144 batch 20 loss 0.009830908849835396\n",
      "Test: epoch 144 batch 0 loss 0.009862575680017471\n",
      "epoch 144 finished - avarage train loss 0.008113310584027705  avarage test loss 0.013523491565138102\n",
      "Training: epoch 145 batch 0 loss 0.0027742490638047457\n",
      "Training: epoch 145 batch 10 loss 0.007802399806678295\n",
      "Training: epoch 145 batch 20 loss 0.008940381929278374\n",
      "Test: epoch 145 batch 0 loss 0.010102277621626854\n",
      "epoch 145 finished - avarage train loss 0.0065941059046649726  avarage test loss 0.012507689651101828\n",
      "Training: epoch 146 batch 0 loss 0.002979044569656253\n",
      "Training: epoch 146 batch 10 loss 0.005588684231042862\n",
      "Training: epoch 146 batch 20 loss 0.00825371965765953\n",
      "Test: epoch 146 batch 0 loss 0.014913045801222324\n",
      "epoch 146 finished - avarage train loss 0.007177316503406599  avarage test loss 0.015305532026104629\n",
      "Training: epoch 147 batch 0 loss 0.0048931436613202095\n",
      "Training: epoch 147 batch 10 loss 0.005025406368076801\n",
      "Training: epoch 147 batch 20 loss 0.002723377663642168\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test: epoch 147 batch 0 loss 0.015156055800616741\n",
      "epoch 147 finished - avarage train loss 0.00500220139029211  avarage test loss 0.017897422309033573\n",
      "Training: epoch 148 batch 0 loss 0.004004799760878086\n",
      "Training: epoch 148 batch 10 loss 0.00811062939465046\n",
      "Training: epoch 148 batch 20 loss 0.004332439042627811\n",
      "Test: epoch 148 batch 0 loss 0.018278921023011208\n",
      "epoch 148 finished - avarage train loss 0.007810126018614091  avarage test loss 0.01826076558791101\n",
      "Training: epoch 149 batch 0 loss 0.008572150021791458\n",
      "Training: epoch 149 batch 10 loss 0.0029636346735060215\n",
      "Training: epoch 149 batch 20 loss 0.00802210159599781\n",
      "Test: epoch 149 batch 0 loss 0.01806590147316456\n",
      "epoch 149 finished - avarage train loss 0.008468974269284257  avarage test loss 0.019816956715658307\n",
      "Training: epoch 150 batch 0 loss 0.009903104044497013\n",
      "Training: epoch 150 batch 10 loss 0.0032159355469048023\n",
      "Training: epoch 150 batch 20 loss 0.0028843714389950037\n",
      "Test: epoch 150 batch 0 loss 0.01877148635685444\n",
      "epoch 150 finished - avarage train loss 0.006795386960408811  avarage test loss 0.01687424094416201\n",
      "Training: epoch 151 batch 0 loss 0.005217559169977903\n",
      "Training: epoch 151 batch 10 loss 0.0024167404044419527\n",
      "Training: epoch 151 batch 20 loss 0.006958823185414076\n",
      "Test: epoch 151 batch 0 loss 0.020100390538573265\n",
      "epoch 151 finished - avarage train loss 0.006447808704627999  avarage test loss 0.015865384717471898\n",
      "Training: epoch 152 batch 0 loss 0.004244898445904255\n",
      "Training: epoch 152 batch 10 loss 0.013921880163252354\n",
      "Training: epoch 152 batch 20 loss 0.005411295220255852\n",
      "Test: epoch 152 batch 0 loss 0.01909787952899933\n",
      "epoch 152 finished - avarage train loss 0.007567606312383352  avarage test loss 0.01673138083424419\n",
      "Training: epoch 153 batch 0 loss 0.0045439256355166435\n",
      "Training: epoch 153 batch 10 loss 0.004923298954963684\n",
      "Training: epoch 153 batch 20 loss 0.0024890382774174213\n",
      "Test: epoch 153 batch 0 loss 0.016630705446004868\n",
      "epoch 153 finished - avarage train loss 0.006372089570002823  avarage test loss 0.015071013942360878\n",
      "Training: epoch 154 batch 0 loss 0.006506376899778843\n",
      "Training: epoch 154 batch 10 loss 0.0073786103166639805\n",
      "Training: epoch 154 batch 20 loss 0.009627370163798332\n",
      "Test: epoch 154 batch 0 loss 0.020051153376698494\n",
      "epoch 154 finished - avarage train loss 0.008192237651232502  avarage test loss 0.022090992191806436\n",
      "Training: epoch 155 batch 0 loss 0.009400637820363045\n",
      "Training: epoch 155 batch 10 loss 0.013261988759040833\n",
      "Training: epoch 155 batch 20 loss 0.0032689631916582584\n",
      "Test: epoch 155 batch 0 loss 0.013035392388701439\n",
      "epoch 155 finished - avarage train loss 0.009357769692573568  avarage test loss 0.01610462344251573\n",
      "Training: epoch 156 batch 0 loss 0.0030808900482952595\n",
      "Training: epoch 156 batch 10 loss 0.012883803807199001\n",
      "Training: epoch 156 batch 20 loss 0.003962996415793896\n",
      "Test: epoch 156 batch 0 loss 0.013263246975839138\n",
      "epoch 156 finished - avarage train loss 0.007576362101425384  avarage test loss 0.015686949249356985\n",
      "Training: epoch 157 batch 0 loss 0.00615179818123579\n",
      "Training: epoch 157 batch 10 loss 0.0045963735319674015\n",
      "Training: epoch 157 batch 20 loss 0.007704786490648985\n",
      "Test: epoch 157 batch 0 loss 0.01201542653143406\n",
      "epoch 157 finished - avarage train loss 0.007403510084761114  avarage test loss 0.01348468184005469\n",
      "Training: epoch 158 batch 0 loss 0.004974213428795338\n",
      "Training: epoch 158 batch 10 loss 0.008362325839698315\n",
      "Training: epoch 158 batch 20 loss 0.008404678665101528\n",
      "Test: epoch 158 batch 0 loss 0.010944626294076443\n",
      "epoch 158 finished - avarage train loss 0.007113262505174197  avarage test loss 0.012720260652713478\n",
      "Training: epoch 159 batch 0 loss 0.006896506529301405\n",
      "Training: epoch 159 batch 10 loss 0.0036480468697845936\n",
      "Training: epoch 159 batch 20 loss 0.003824451472610235\n",
      "Test: epoch 159 batch 0 loss 0.010755554772913456\n",
      "epoch 159 finished - avarage train loss 0.007468256087781027  avarage test loss 0.01418927253689617\n",
      "Training: epoch 160 batch 0 loss 0.006246333010494709\n",
      "Training: epoch 160 batch 10 loss 0.003977200482040644\n",
      "Training: epoch 160 batch 20 loss 0.012743880040943623\n",
      "Test: epoch 160 batch 0 loss 0.014025861397385597\n",
      "epoch 160 finished - avarage train loss 0.008064762693187543  avarage test loss 0.01908708387054503\n",
      "Training: epoch 161 batch 0 loss 0.007351886481046677\n",
      "Training: epoch 161 batch 10 loss 0.005228976719081402\n",
      "Training: epoch 161 batch 20 loss 0.003913004416972399\n",
      "Test: epoch 161 batch 0 loss 0.012743386439979076\n",
      "epoch 161 finished - avarage train loss 0.00714406142300316  avarage test loss 0.015187733341008425\n",
      "Training: epoch 162 batch 0 loss 0.0045614237897098064\n",
      "Training: epoch 162 batch 10 loss 0.008352305740118027\n",
      "Training: epoch 162 batch 20 loss 0.003113901475444436\n",
      "Test: epoch 162 batch 0 loss 0.014328585006296635\n",
      "epoch 162 finished - avarage train loss 0.00863160774240206  avarage test loss 0.018244500271975994\n",
      "Training: epoch 163 batch 0 loss 0.004008047748357058\n",
      "Training: epoch 163 batch 10 loss 0.0104209640994668\n",
      "Training: epoch 163 batch 20 loss 0.005533421412110329\n",
      "Test: epoch 163 batch 0 loss 0.01223251037299633\n",
      "epoch 163 finished - avarage train loss 0.007755964528769255  avarage test loss 0.016381596913561225\n",
      "Training: epoch 164 batch 0 loss 0.003803236410021782\n",
      "Training: epoch 164 batch 10 loss 0.0048333280719816685\n",
      "Training: epoch 164 batch 20 loss 0.0018907621270045638\n",
      "Test: epoch 164 batch 0 loss 0.01074174977838993\n",
      "epoch 164 finished - avarage train loss 0.007932033185076353  avarage test loss 0.012810823041945696\n",
      "Training: epoch 165 batch 0 loss 0.006717727053910494\n",
      "Training: epoch 165 batch 10 loss 0.002528715878725052\n",
      "Training: epoch 165 batch 20 loss 0.0051741753704845905\n",
      "Test: epoch 165 batch 0 loss 0.013225565664470196\n",
      "epoch 165 finished - avarage train loss 0.007890944417309144  avarage test loss 0.01776920515112579\n",
      "Training: epoch 166 batch 0 loss 0.012060366570949554\n",
      "Training: epoch 166 batch 10 loss 0.012256299145519733\n",
      "Training: epoch 166 batch 20 loss 0.01080274861305952\n",
      "Test: epoch 166 batch 0 loss 0.010423462837934494\n",
      "epoch 166 finished - avarage train loss 0.009966803088398844  avarage test loss 0.014520113938488066\n",
      "Training: epoch 167 batch 0 loss 0.014837438240647316\n",
      "Training: epoch 167 batch 10 loss 0.004260677378624678\n",
      "Training: epoch 167 batch 20 loss 0.004044748842716217\n",
      "Test: epoch 167 batch 0 loss 0.010683460161089897\n",
      "epoch 167 finished - avarage train loss 0.0076095528483133895  avarage test loss 0.013162986491806805\n",
      "Training: epoch 168 batch 0 loss 0.004431374371051788\n",
      "Training: epoch 168 batch 10 loss 0.009729133918881416\n",
      "Training: epoch 168 batch 20 loss 0.003917132969945669\n",
      "Test: epoch 168 batch 0 loss 0.011375639587640762\n",
      "epoch 168 finished - avarage train loss 0.006861783291116871  avarage test loss 0.013583149760961533\n",
      "Training: epoch 169 batch 0 loss 0.003379091387614608\n",
      "Training: epoch 169 batch 10 loss 0.011645656079053879\n",
      "Training: epoch 169 batch 20 loss 0.018000060692429543\n",
      "Test: epoch 169 batch 0 loss 0.010517035610973835\n",
      "epoch 169 finished - avarage train loss 0.00728554646323981  avarage test loss 0.012752990471199155\n",
      "Training: epoch 170 batch 0 loss 0.004585969727486372\n",
      "Training: epoch 170 batch 10 loss 0.00518142431974411\n",
      "Training: epoch 170 batch 20 loss 0.0050389268435537815\n",
      "Test: epoch 170 batch 0 loss 0.010570939630270004\n",
      "epoch 170 finished - avarage train loss 0.006883135659555937  avarage test loss 0.013190561905503273\n",
      "Training: epoch 171 batch 0 loss 0.004879076033830643\n",
      "Training: epoch 171 batch 10 loss 0.0056953635066747665\n",
      "Training: epoch 171 batch 20 loss 0.004819138441234827\n",
      "Test: epoch 171 batch 0 loss 0.010571192018687725\n",
      "epoch 171 finished - avarage train loss 0.005749128180845031  avarage test loss 0.012999437982216477\n",
      "Training: epoch 172 batch 0 loss 0.005264006555080414\n",
      "Training: epoch 172 batch 10 loss 0.007474986370652914\n",
      "Training: epoch 172 batch 20 loss 0.008718312717974186\n",
      "Test: epoch 172 batch 0 loss 0.01832081936299801\n",
      "epoch 172 finished - avarage train loss 0.0073699300530655635  avarage test loss 0.01633267872966826\n",
      "Training: epoch 173 batch 0 loss 0.009563321247696877\n",
      "Training: epoch 173 batch 10 loss 0.0067816805094480515\n",
      "Training: epoch 173 batch 20 loss 0.005556540563702583\n",
      "Test: epoch 173 batch 0 loss 0.018617795780301094\n",
      "epoch 173 finished - avarage train loss 0.006955312862028849  avarage test loss 0.01713888847734779\n",
      "Training: epoch 174 batch 0 loss 0.009609820321202278\n",
      "Training: epoch 174 batch 10 loss 0.004521787166595459\n",
      "Training: epoch 174 batch 20 loss 0.008121108636260033\n",
      "Test: epoch 174 batch 0 loss 0.014674022793769836\n",
      "epoch 174 finished - avarage train loss 0.008839833895386806  avarage test loss 0.01493993797339499\n",
      "Training: epoch 175 batch 0 loss 0.004080888815224171\n",
      "Training: epoch 175 batch 10 loss 0.005090177524834871\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training: epoch 175 batch 20 loss 0.009006651118397713\n",
      "Test: epoch 175 batch 0 loss 0.016719123348593712\n",
      "epoch 175 finished - avarage train loss 0.007674498660168771  avarage test loss 0.014988679788075387\n",
      "Training: epoch 176 batch 0 loss 0.0061181760393083096\n",
      "Training: epoch 176 batch 10 loss 0.0063805775716900826\n",
      "Training: epoch 176 batch 20 loss 0.006297662388533354\n",
      "Test: epoch 176 batch 0 loss 0.01651022583246231\n",
      "epoch 176 finished - avarage train loss 0.006463731952204272  avarage test loss 0.015486400458030403\n",
      "Training: epoch 177 batch 0 loss 0.006762901786714792\n",
      "Training: epoch 177 batch 10 loss 0.0073833283968269825\n",
      "Training: epoch 177 batch 20 loss 0.011125686578452587\n",
      "Test: epoch 177 batch 0 loss 0.01154231932014227\n",
      "epoch 177 finished - avarage train loss 0.008292145978916308  avarage test loss 0.015763802686706185\n",
      "Training: epoch 178 batch 0 loss 0.006176547147333622\n",
      "Training: epoch 178 batch 10 loss 0.009078744798898697\n",
      "Training: epoch 178 batch 20 loss 0.003544268198311329\n",
      "Test: epoch 178 batch 0 loss 0.0121888667345047\n",
      "epoch 178 finished - avarage train loss 0.007932838815232289  avarage test loss 0.014357995009049773\n",
      "Training: epoch 179 batch 0 loss 0.003008453408256173\n",
      "Training: epoch 179 batch 10 loss 0.006010967306792736\n",
      "Training: epoch 179 batch 20 loss 0.006020885892212391\n",
      "Test: epoch 179 batch 0 loss 0.012599929235875607\n",
      "epoch 179 finished - avarage train loss 0.006795317392218216  avarage test loss 0.014029006706550717\n",
      "Training: epoch 180 batch 0 loss 0.0034168551210314035\n",
      "Training: epoch 180 batch 10 loss 0.00780629413202405\n",
      "Training: epoch 180 batch 20 loss 0.008129394613206387\n",
      "Test: epoch 180 batch 0 loss 0.012530529871582985\n",
      "epoch 180 finished - avarage train loss 0.006448482644969019  avarage test loss 0.013840627972967923\n",
      "Training: epoch 181 batch 0 loss 0.009348479099571705\n",
      "Training: epoch 181 batch 10 loss 0.0028396546840667725\n",
      "Training: epoch 181 batch 20 loss 0.006754739675670862\n",
      "Test: epoch 181 batch 0 loss 0.011810675263404846\n",
      "epoch 181 finished - avarage train loss 0.007232160315495627  avarage test loss 0.014401201857253909\n",
      "Training: epoch 182 batch 0 loss 0.006054336205124855\n",
      "Training: epoch 182 batch 10 loss 0.005002263002097607\n",
      "Training: epoch 182 batch 20 loss 0.009251459501683712\n",
      "Test: epoch 182 batch 0 loss 0.010973033495247364\n",
      "epoch 182 finished - avarage train loss 0.006363239853867683  avarage test loss 0.012673305114731193\n",
      "Training: epoch 183 batch 0 loss 0.010069437325000763\n",
      "Training: epoch 183 batch 10 loss 0.003726998111233115\n",
      "Training: epoch 183 batch 20 loss 0.008829783648252487\n",
      "Test: epoch 183 batch 0 loss 0.014038951136171818\n",
      "epoch 183 finished - avarage train loss 0.006650238606982447  avarage test loss 0.014388244831934571\n",
      "Training: epoch 184 batch 0 loss 0.006248063407838345\n",
      "Training: epoch 184 batch 10 loss 0.006802069488912821\n",
      "Training: epoch 184 batch 20 loss 0.008460689336061478\n",
      "Test: epoch 184 batch 0 loss 0.012535895220935345\n",
      "epoch 184 finished - avarage train loss 0.0067161008442655715  avarage test loss 0.015791861922480166\n",
      "Training: epoch 185 batch 0 loss 0.009482772089540958\n",
      "Training: epoch 185 batch 10 loss 0.004985931795090437\n",
      "Training: epoch 185 batch 20 loss 0.003500615945085883\n",
      "Test: epoch 185 batch 0 loss 0.016699738800525665\n",
      "epoch 185 finished - avarage train loss 0.007588615215479814  avarage test loss 0.01676660357043147\n",
      "Training: epoch 186 batch 0 loss 0.007375313900411129\n",
      "Training: epoch 186 batch 10 loss 0.005334703251719475\n",
      "Training: epoch 186 batch 20 loss 0.006527463905513287\n",
      "Test: epoch 186 batch 0 loss 0.030657779425382614\n",
      "epoch 186 finished - avarage train loss 0.00799172554678958  avarage test loss 0.028965012170374393\n",
      "Training: epoch 187 batch 0 loss 0.022017668932676315\n",
      "Training: epoch 187 batch 10 loss 0.010096950456500053\n",
      "Training: epoch 187 batch 20 loss 0.007381247356534004\n",
      "Test: epoch 187 batch 0 loss 0.02242354489862919\n",
      "epoch 187 finished - avarage train loss 0.011130386979543957  avarage test loss 0.020767606096342206\n",
      "Training: epoch 188 batch 0 loss 0.012026268988847733\n",
      "Training: epoch 188 batch 10 loss 0.007658287417143583\n",
      "Training: epoch 188 batch 20 loss 0.004113991279155016\n",
      "Test: epoch 188 batch 0 loss 0.017537767067551613\n",
      "epoch 188 finished - avarage train loss 0.0062572641201831146  avarage test loss 0.01714405626989901\n",
      "Training: epoch 189 batch 0 loss 0.002618978964164853\n",
      "Training: epoch 189 batch 10 loss 0.005487259943038225\n",
      "Training: epoch 189 batch 20 loss 0.0014081135159358382\n",
      "Test: epoch 189 batch 0 loss 0.01885509490966797\n",
      "epoch 189 finished - avarage train loss 0.007594335539234352  avarage test loss 0.01727416063658893\n",
      "Training: epoch 190 batch 0 loss 0.003553686197847128\n",
      "Training: epoch 190 batch 10 loss 0.004781925585120916\n",
      "Training: epoch 190 batch 20 loss 0.004159373696893454\n",
      "Test: epoch 190 batch 0 loss 0.020288825035095215\n",
      "epoch 190 finished - avarage train loss 0.007546621519301472  avarage test loss 0.016424992587417364\n",
      "Training: epoch 191 batch 0 loss 0.00835572462528944\n",
      "Training: epoch 191 batch 10 loss 0.0053526186384260654\n",
      "Training: epoch 191 batch 20 loss 0.007875259034335613\n",
      "Test: epoch 191 batch 0 loss 0.019003478810191154\n",
      "epoch 191 finished - avarage train loss 0.006861569552585997  avarage test loss 0.015953029040247202\n",
      "Training: epoch 192 batch 0 loss 0.00432305783033371\n",
      "Training: epoch 192 batch 10 loss 0.007014607544988394\n",
      "Training: epoch 192 batch 20 loss 0.009590781293809414\n",
      "Test: epoch 192 batch 0 loss 0.017292724922299385\n",
      "epoch 192 finished - avarage train loss 0.006584468233431208  avarage test loss 0.015735617373138666\n",
      "Training: epoch 193 batch 0 loss 0.007159455679357052\n",
      "Training: epoch 193 batch 10 loss 0.004585882183164358\n",
      "Training: epoch 193 batch 20 loss 0.012304595671594143\n",
      "Test: epoch 193 batch 0 loss 0.016520243138074875\n",
      "epoch 193 finished - avarage train loss 0.00857102726024158  avarage test loss 0.019247362157329917\n",
      "Training: epoch 194 batch 0 loss 0.004042206332087517\n",
      "Training: epoch 194 batch 10 loss 0.0044260332360863686\n",
      "Training: epoch 194 batch 20 loss 0.00646865414455533\n",
      "Test: epoch 194 batch 0 loss 0.013773082755506039\n",
      "epoch 194 finished - avarage train loss 0.007341098377545332  avarage test loss 0.016822150791995227\n",
      "Training: epoch 195 batch 0 loss 0.006421564146876335\n",
      "Training: epoch 195 batch 10 loss 0.0018011294305324554\n",
      "Training: epoch 195 batch 20 loss 0.005621973425149918\n",
      "Test: epoch 195 batch 0 loss 0.010808455757796764\n",
      "epoch 195 finished - avarage train loss 0.006223581345944569  avarage test loss 0.012767512467689812\n",
      "Training: epoch 196 batch 0 loss 0.002946272259578109\n",
      "Training: epoch 196 batch 10 loss 0.005737760569900274\n",
      "Training: epoch 196 batch 20 loss 0.0036108987405896187\n",
      "Test: epoch 196 batch 0 loss 0.010486487299203873\n",
      "epoch 196 finished - avarage train loss 0.005930608917220399  avarage test loss 0.01353923324495554\n",
      "Training: epoch 197 batch 0 loss 0.007901948876678944\n",
      "Training: epoch 197 batch 10 loss 0.0027772136963903904\n",
      "Training: epoch 197 batch 20 loss 0.006507564801722765\n",
      "Test: epoch 197 batch 0 loss 0.011056945659220219\n",
      "epoch 197 finished - avarage train loss 0.006847263426230899  avarage test loss 0.013199579319916666\n",
      "Training: epoch 198 batch 0 loss 0.0066648125648498535\n",
      "Training: epoch 198 batch 10 loss 0.003005807753652334\n",
      "Training: epoch 198 batch 20 loss 0.0063217561691999435\n",
      "Test: epoch 198 batch 0 loss 0.012130001559853554\n",
      "epoch 198 finished - avarage train loss 0.008352254637806067  avarage test loss 0.016711290692910552\n",
      "Training: epoch 199 batch 0 loss 0.008604948408901691\n",
      "Training: epoch 199 batch 10 loss 0.00283088069409132\n",
      "Training: epoch 199 batch 20 loss 0.013951716013252735\n",
      "Test: epoch 199 batch 0 loss 0.009920651093125343\n",
      "epoch 199 finished - avarage train loss 0.007601130817984712  avarage test loss 0.0143519036937505\n",
      "Training: epoch 0 batch 0 loss 0.5548579692840576\n",
      "Training: epoch 0 batch 10 loss 0.8630601167678833\n",
      "Training: epoch 0 batch 20 loss 0.4682108461856842\n",
      "Test: epoch 0 batch 0 loss 0.4040710926055908\n",
      "epoch 0 finished - avarage train loss 0.5198801406498613  avarage test loss 0.4429030641913414\n",
      "Training: epoch 1 batch 0 loss 0.7810696363449097\n",
      "Training: epoch 1 batch 10 loss 0.6667034029960632\n",
      "Training: epoch 1 batch 20 loss 0.41474515199661255\n",
      "Test: epoch 1 batch 0 loss 0.4067688286304474\n",
      "epoch 1 finished - avarage train loss 0.5085579968731979  avarage test loss 0.45259712636470795\n",
      "Training: epoch 2 batch 0 loss 0.6185779571533203\n",
      "Training: epoch 2 batch 10 loss 0.5970911383628845\n",
      "Training: epoch 2 batch 20 loss 0.38628140091896057\n",
      "Test: epoch 2 batch 0 loss 0.4133513867855072\n",
      "epoch 2 finished - avarage train loss 0.5212722061009243  avarage test loss 0.4614783152937889\n",
      "Training: epoch 3 batch 0 loss 0.3835032284259796\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training: epoch 3 batch 10 loss 0.5369725227355957\n",
      "Training: epoch 3 batch 20 loss 0.5838115811347961\n",
      "Test: epoch 3 batch 0 loss 0.4072277843952179\n",
      "epoch 3 finished - avarage train loss 0.5243918294536656  avarage test loss 0.4519139751791954\n",
      "Training: epoch 4 batch 0 loss 0.7195016741752625\n",
      "Training: epoch 4 batch 10 loss 0.6020079255104065\n",
      "Training: epoch 4 batch 20 loss 0.6366778612136841\n",
      "Test: epoch 4 batch 0 loss 0.39856022596359253\n",
      "epoch 4 finished - avarage train loss 0.5171626086892753  avarage test loss 0.4391242191195488\n",
      "Training: epoch 5 batch 0 loss 0.6971232295036316\n",
      "Training: epoch 5 batch 10 loss 0.37822818756103516\n",
      "Training: epoch 5 batch 20 loss 0.16429324448108673\n",
      "Test: epoch 5 batch 0 loss 0.16068051755428314\n",
      "epoch 5 finished - avarage train loss 0.36737483714161245  avarage test loss 0.16514649987220764\n",
      "Training: epoch 6 batch 0 loss 0.15318834781646729\n",
      "Training: epoch 6 batch 10 loss 0.0829523503780365\n",
      "Training: epoch 6 batch 20 loss 0.060753896832466125\n",
      "Test: epoch 6 batch 0 loss 0.03129521757364273\n",
      "epoch 6 finished - avarage train loss 0.0685135146024926  avarage test loss 0.03792229946702719\n",
      "Training: epoch 7 batch 0 loss 0.03969768434762955\n",
      "Training: epoch 7 batch 10 loss 0.022804200649261475\n",
      "Training: epoch 7 batch 20 loss 0.0215228870511055\n",
      "Test: epoch 7 batch 0 loss 0.02710038237273693\n",
      "epoch 7 finished - avarage train loss 0.026391203258315038  avarage test loss 0.031952238176018\n",
      "Training: epoch 8 batch 0 loss 0.01567240059375763\n",
      "Training: epoch 8 batch 10 loss 0.0257753673940897\n",
      "Training: epoch 8 batch 20 loss 0.023272667080163956\n",
      "Test: epoch 8 batch 0 loss 0.0248298067599535\n",
      "epoch 8 finished - avarage train loss 0.02222059313850156  avarage test loss 0.03035963699221611\n",
      "Training: epoch 9 batch 0 loss 0.014788066036999226\n",
      "Training: epoch 9 batch 10 loss 0.016670335084199905\n",
      "Training: epoch 9 batch 20 loss 0.019997432827949524\n",
      "Test: epoch 9 batch 0 loss 0.024047434329986572\n",
      "epoch 9 finished - avarage train loss 0.02098449448059345  avarage test loss 0.029765246901661158\n",
      "Training: epoch 10 batch 0 loss 0.0187061820179224\n",
      "Training: epoch 10 batch 10 loss 0.014899964444339275\n",
      "Training: epoch 10 batch 20 loss 0.014420743100345135\n",
      "Test: epoch 10 batch 0 loss 0.023882921785116196\n",
      "epoch 10 finished - avarage train loss 0.02058942423298441  avarage test loss 0.029459981247782707\n",
      "Training: epoch 11 batch 0 loss 0.02183086983859539\n",
      "Training: epoch 11 batch 10 loss 0.013734431937336922\n",
      "Training: epoch 11 batch 20 loss 0.020277125760912895\n",
      "Test: epoch 11 batch 0 loss 0.02382132224738598\n",
      "epoch 11 finished - avarage train loss 0.020722904595835454  avarage test loss 0.029397408943623304\n",
      "Training: epoch 12 batch 0 loss 0.021362697705626488\n",
      "Training: epoch 12 batch 10 loss 0.020288817584514618\n",
      "Training: epoch 12 batch 20 loss 0.015553349629044533\n",
      "Test: epoch 12 batch 0 loss 0.02361813746392727\n",
      "epoch 12 finished - avarage train loss 0.02098627457523654  avarage test loss 0.029455791227519512\n",
      "Training: epoch 13 batch 0 loss 0.032832808792591095\n",
      "Training: epoch 13 batch 10 loss 0.025870855897665024\n",
      "Training: epoch 13 batch 20 loss 0.011499911546707153\n",
      "Test: epoch 13 batch 0 loss 0.023141473531723022\n",
      "epoch 13 finished - avarage train loss 0.019766330735051428  avarage test loss 0.02938346890732646\n",
      "Training: epoch 14 batch 0 loss 0.030746741220355034\n",
      "Training: epoch 14 batch 10 loss 0.031430523842573166\n",
      "Training: epoch 14 batch 20 loss 0.007893729954957962\n",
      "Test: epoch 14 batch 0 loss 0.022082524374127388\n",
      "epoch 14 finished - avarage train loss 0.018496157729934  avarage test loss 0.026420150883495808\n",
      "Training: epoch 15 batch 0 loss 0.018420204520225525\n",
      "Training: epoch 15 batch 10 loss 0.02030552737414837\n",
      "Training: epoch 15 batch 20 loss 0.011595350690186024\n",
      "Test: epoch 15 batch 0 loss 0.020641416311264038\n",
      "epoch 15 finished - avarage train loss 0.017023787095115102  avarage test loss 0.022551060188561678\n",
      "Training: epoch 16 batch 0 loss 0.0168706513941288\n",
      "Training: epoch 16 batch 10 loss 0.02943630889058113\n",
      "Training: epoch 16 batch 20 loss 0.01245859730988741\n",
      "Test: epoch 16 batch 0 loss 0.016803381964564323\n",
      "epoch 16 finished - avarage train loss 0.015480578716458946  avarage test loss 0.01927518006414175\n",
      "Training: epoch 17 batch 0 loss 0.01721138507127762\n",
      "Training: epoch 17 batch 10 loss 0.005939684342592955\n",
      "Training: epoch 17 batch 20 loss 0.01140479277819395\n",
      "Test: epoch 17 batch 0 loss 0.017604267224669456\n",
      "epoch 17 finished - avarage train loss 0.015402696191750723  avarage test loss 0.01932082581333816\n",
      "Training: epoch 18 batch 0 loss 0.008938252925872803\n",
      "Training: epoch 18 batch 10 loss 0.01845027692615986\n",
      "Training: epoch 18 batch 20 loss 0.010283509269356728\n",
      "Test: epoch 18 batch 0 loss 0.014799589291214943\n",
      "epoch 18 finished - avarage train loss 0.01395096515851288  avarage test loss 0.02004437241703272\n",
      "Training: epoch 19 batch 0 loss 0.01166711188852787\n",
      "Training: epoch 19 batch 10 loss 0.01649470068514347\n",
      "Training: epoch 19 batch 20 loss 0.015613202936947346\n",
      "Test: epoch 19 batch 0 loss 0.01837090030312538\n",
      "epoch 19 finished - avarage train loss 0.014707888476550579  avarage test loss 0.020500342827290297\n",
      "Training: epoch 20 batch 0 loss 0.012218830175697803\n",
      "Training: epoch 20 batch 10 loss 0.010423230938613415\n",
      "Training: epoch 20 batch 20 loss 0.004955515265464783\n",
      "Test: epoch 20 batch 0 loss 0.014844262972474098\n",
      "epoch 20 finished - avarage train loss 0.01367946860284127  avarage test loss 0.017673902213573456\n",
      "Training: epoch 21 batch 0 loss 0.011182618327438831\n",
      "Training: epoch 21 batch 10 loss 0.008875934407114983\n",
      "Training: epoch 21 batch 20 loss 0.007728212047368288\n",
      "Test: epoch 21 batch 0 loss 0.013221553526818752\n",
      "epoch 21 finished - avarage train loss 0.012201305366410264  avarage test loss 0.017233932856470346\n",
      "Training: epoch 22 batch 0 loss 0.009992117062211037\n",
      "Training: epoch 22 batch 10 loss 0.006236806511878967\n",
      "Training: epoch 22 batch 20 loss 0.011749663390219212\n",
      "Test: epoch 22 batch 0 loss 0.015811068937182426\n",
      "epoch 22 finished - avarage train loss 0.011974485671340391  avarage test loss 0.01888792379759252\n",
      "Training: epoch 23 batch 0 loss 0.00936131738126278\n",
      "Training: epoch 23 batch 10 loss 0.009050301276147366\n",
      "Training: epoch 23 batch 20 loss 0.005452484358102083\n",
      "Test: epoch 23 batch 0 loss 0.014058795757591724\n",
      "epoch 23 finished - avarage train loss 0.011523146776418233  avarage test loss 0.01804994302801788\n",
      "Training: epoch 24 batch 0 loss 0.007941173389554024\n",
      "Training: epoch 24 batch 10 loss 0.005378047004342079\n",
      "Training: epoch 24 batch 20 loss 0.003198730293661356\n",
      "Test: epoch 24 batch 0 loss 0.011937625706195831\n",
      "epoch 24 finished - avarage train loss 0.009143397220057147  avarage test loss 0.014423787826672196\n",
      "Training: epoch 25 batch 0 loss 0.004652815870940685\n",
      "Training: epoch 25 batch 10 loss 0.004148955922573805\n",
      "Training: epoch 25 batch 20 loss 0.004709884524345398\n",
      "Test: epoch 25 batch 0 loss 0.013037962839007378\n",
      "epoch 25 finished - avarage train loss 0.006832531605172774  avarage test loss 0.015095670707523823\n",
      "Training: epoch 26 batch 0 loss 0.007374342065304518\n",
      "Training: epoch 26 batch 10 loss 0.008550516329705715\n",
      "Training: epoch 26 batch 20 loss 0.003564361250028014\n",
      "Test: epoch 26 batch 0 loss 0.00927125383168459\n",
      "epoch 26 finished - avarage train loss 0.009480647028199044  avarage test loss 0.014955409336835146\n",
      "Training: epoch 27 batch 0 loss 0.004268786869943142\n",
      "Training: epoch 27 batch 10 loss 0.00539743434637785\n",
      "Training: epoch 27 batch 20 loss 0.003626030869781971\n",
      "Test: epoch 27 batch 0 loss 0.018221355974674225\n",
      "epoch 27 finished - avarage train loss 0.008165515862919134  avarage test loss 0.02005406399257481\n",
      "Training: epoch 28 batch 0 loss 0.01312193088233471\n",
      "Training: epoch 28 batch 10 loss 0.006451555993407965\n",
      "Training: epoch 28 batch 20 loss 0.006658709608018398\n",
      "Test: epoch 28 batch 0 loss 0.011750474572181702\n",
      "epoch 28 finished - avarage train loss 0.006094501827490227  avarage test loss 0.013643951970152557\n",
      "Training: epoch 29 batch 0 loss 0.005851136054843664\n",
      "Training: epoch 29 batch 10 loss 0.007324492558836937\n",
      "Training: epoch 29 batch 20 loss 0.007615805137902498\n",
      "Test: epoch 29 batch 0 loss 0.014387781731784344\n",
      "epoch 29 finished - avarage train loss 0.007303587120861329  avarage test loss 0.015921304584480822\n",
      "Training: epoch 30 batch 0 loss 0.010149051435291767\n",
      "Training: epoch 30 batch 10 loss 0.008245467208325863\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training: epoch 30 batch 20 loss 0.0075119733810424805\n",
      "Test: epoch 30 batch 0 loss 0.014168295077979565\n",
      "epoch 30 finished - avarage train loss 0.006522033612468633  avarage test loss 0.014318459667265415\n",
      "Training: epoch 31 batch 0 loss 0.007455179002135992\n",
      "Training: epoch 31 batch 10 loss 0.0035380367189645767\n",
      "Training: epoch 31 batch 20 loss 0.006383017171174288\n",
      "Test: epoch 31 batch 0 loss 0.015178226865828037\n",
      "epoch 31 finished - avarage train loss 0.008725923429853443  avarage test loss 0.014737990451976657\n",
      "Training: epoch 32 batch 0 loss 0.010164016857743263\n",
      "Training: epoch 32 batch 10 loss 0.005755733232945204\n",
      "Training: epoch 32 batch 20 loss 0.005051493179053068\n",
      "Test: epoch 32 batch 0 loss 0.013736371882259846\n",
      "epoch 32 finished - avarage train loss 0.007146500468511006  avarage test loss 0.013987551676109433\n",
      "Training: epoch 33 batch 0 loss 0.006076955236494541\n",
      "Training: epoch 33 batch 10 loss 0.003760325023904443\n",
      "Training: epoch 33 batch 20 loss 0.0033736699260771275\n",
      "Test: epoch 33 batch 0 loss 0.01570461131632328\n",
      "epoch 33 finished - avarage train loss 0.00893688193875654  avarage test loss 0.016514143906533718\n",
      "Training: epoch 34 batch 0 loss 0.007031324319541454\n",
      "Training: epoch 34 batch 10 loss 0.0030574409756809473\n",
      "Training: epoch 34 batch 20 loss 0.010013418272137642\n",
      "Test: epoch 34 batch 0 loss 0.012112322263419628\n",
      "epoch 34 finished - avarage train loss 0.009933181789478865  avarage test loss 0.014035398489795625\n",
      "Training: epoch 35 batch 0 loss 0.015273547731339931\n",
      "Training: epoch 35 batch 10 loss 0.004606133792549372\n",
      "Training: epoch 35 batch 20 loss 0.013448075391352177\n",
      "Test: epoch 35 batch 0 loss 0.01563078910112381\n",
      "epoch 35 finished - avarage train loss 0.007876771028506858  avarage test loss 0.01688062632456422\n",
      "Training: epoch 36 batch 0 loss 0.00416457699611783\n",
      "Training: epoch 36 batch 10 loss 0.002833716105669737\n",
      "Training: epoch 36 batch 20 loss 0.0065635633654892445\n",
      "Test: epoch 36 batch 0 loss 0.014449382200837135\n",
      "epoch 36 finished - avarage train loss 0.007641992981320825  avarage test loss 0.01509600377175957\n",
      "Training: epoch 37 batch 0 loss 0.0033648244570940733\n",
      "Training: epoch 37 batch 10 loss 0.005140356253832579\n",
      "Training: epoch 37 batch 20 loss 0.004446573555469513\n",
      "Test: epoch 37 batch 0 loss 0.014873089268803596\n",
      "epoch 37 finished - avarage train loss 0.009453649801234233  avarage test loss 0.015428005252033472\n",
      "Training: epoch 38 batch 0 loss 0.0032105545978993177\n",
      "Training: epoch 38 batch 10 loss 0.00524388812482357\n",
      "Training: epoch 38 batch 20 loss 0.008621709421277046\n",
      "Test: epoch 38 batch 0 loss 0.013448226265609264\n",
      "epoch 38 finished - avarage train loss 0.00724372973440793  avarage test loss 0.017211832338944077\n",
      "Training: epoch 39 batch 0 loss 0.005793506279587746\n",
      "Training: epoch 39 batch 10 loss 0.003745522117242217\n",
      "Training: epoch 39 batch 20 loss 0.002768364269286394\n",
      "Test: epoch 39 batch 0 loss 0.010875469073653221\n",
      "epoch 39 finished - avarage train loss 0.008047008830732826  avarage test loss 0.014127237373031676\n",
      "Training: epoch 40 batch 0 loss 0.008417570032179356\n",
      "Training: epoch 40 batch 10 loss 0.006910394877195358\n",
      "Training: epoch 40 batch 20 loss 0.005683747120201588\n",
      "Test: epoch 40 batch 0 loss 0.012934918515384197\n",
      "epoch 40 finished - avarage train loss 0.006413821994070092  avarage test loss 0.013551549753174186\n",
      "Training: epoch 41 batch 0 loss 0.003485067980363965\n",
      "Training: epoch 41 batch 10 loss 0.004811571910977364\n",
      "Training: epoch 41 batch 20 loss 0.004883125424385071\n",
      "Test: epoch 41 batch 0 loss 0.013246498070657253\n",
      "epoch 41 finished - avarage train loss 0.006817691574065849  avarage test loss 0.013680179137736559\n",
      "Training: epoch 42 batch 0 loss 0.006486847996711731\n",
      "Training: epoch 42 batch 10 loss 0.0038641542196273804\n",
      "Training: epoch 42 batch 20 loss 0.012333549559116364\n",
      "Test: epoch 42 batch 0 loss 0.012355472892522812\n",
      "epoch 42 finished - avarage train loss 0.00558262570472113  avarage test loss 0.013520070351660252\n",
      "Training: epoch 43 batch 0 loss 0.005019718781113625\n",
      "Training: epoch 43 batch 10 loss 0.004241820424795151\n",
      "Training: epoch 43 batch 20 loss 0.012159544974565506\n",
      "Test: epoch 43 batch 0 loss 0.013113059103488922\n",
      "epoch 43 finished - avarage train loss 0.008780989916352877  avarage test loss 0.01426280417945236\n",
      "Training: epoch 44 batch 0 loss 0.009621637873351574\n",
      "Training: epoch 44 batch 10 loss 0.007923449389636517\n",
      "Training: epoch 44 batch 20 loss 0.006580928340554237\n",
      "Test: epoch 44 batch 0 loss 0.013278445228934288\n",
      "epoch 44 finished - avarage train loss 0.008167554672550538  avarage test loss 0.01397259347140789\n",
      "Training: epoch 45 batch 0 loss 0.007742338813841343\n",
      "Training: epoch 45 batch 10 loss 0.006878788582980633\n",
      "Training: epoch 45 batch 20 loss 0.003133603138849139\n",
      "Test: epoch 45 batch 0 loss 0.011454991064965725\n",
      "epoch 45 finished - avarage train loss 0.006727758048359176  avarage test loss 0.01343220949638635\n",
      "Training: epoch 46 batch 0 loss 0.010391120798885822\n",
      "Training: epoch 46 batch 10 loss 0.00927021261304617\n",
      "Training: epoch 46 batch 20 loss 0.0033783966209739447\n",
      "Test: epoch 46 batch 0 loss 0.013872337527573109\n",
      "epoch 46 finished - avarage train loss 0.008703133348247101  avarage test loss 0.01432404259685427\n",
      "Training: epoch 47 batch 0 loss 0.0030032675713300705\n",
      "Training: epoch 47 batch 10 loss 0.00271862861700356\n",
      "Training: epoch 47 batch 20 loss 0.0068999701179564\n",
      "Test: epoch 47 batch 0 loss 0.012496734037995338\n",
      "epoch 47 finished - avarage train loss 0.006666020763202988  avarage test loss 0.013352937763556838\n",
      "Training: epoch 48 batch 0 loss 0.0028854336123913527\n",
      "Training: epoch 48 batch 10 loss 0.005955627653747797\n",
      "Training: epoch 48 batch 20 loss 0.008193936198949814\n",
      "Test: epoch 48 batch 0 loss 0.012260436080396175\n",
      "epoch 48 finished - avarage train loss 0.006942071719095111  avarage test loss 0.01321007579099387\n",
      "Training: epoch 49 batch 0 loss 0.004089690279215574\n",
      "Training: epoch 49 batch 10 loss 0.0048001850955188274\n",
      "Training: epoch 49 batch 20 loss 0.005727526731789112\n",
      "Test: epoch 49 batch 0 loss 0.013319721445441246\n",
      "epoch 49 finished - avarage train loss 0.0069704890323417455  avarage test loss 0.013396427850238979\n",
      "Training: epoch 50 batch 0 loss 0.006892963778227568\n",
      "Training: epoch 50 batch 10 loss 0.006710047833621502\n",
      "Training: epoch 50 batch 20 loss 0.0046567851677536964\n",
      "Test: epoch 50 batch 0 loss 0.014307603240013123\n",
      "epoch 50 finished - avarage train loss 0.006893569715963355  avarage test loss 0.014928244287148118\n",
      "Training: epoch 51 batch 0 loss 0.004987114574760199\n",
      "Training: epoch 51 batch 10 loss 0.005397285334765911\n",
      "Training: epoch 51 batch 20 loss 0.009143638424575329\n",
      "Test: epoch 51 batch 0 loss 0.01482117734849453\n",
      "epoch 51 finished - avarage train loss 0.006902775309722999  avarage test loss 0.015430589206516743\n",
      "Training: epoch 52 batch 0 loss 0.005458216182887554\n",
      "Training: epoch 52 batch 10 loss 0.007258954457938671\n",
      "Training: epoch 52 batch 20 loss 0.005559241399168968\n",
      "Test: epoch 52 batch 0 loss 0.009511366486549377\n",
      "epoch 52 finished - avarage train loss 0.008017057408418122  avarage test loss 0.013213172205723822\n",
      "Training: epoch 53 batch 0 loss 0.00750143826007843\n",
      "Training: epoch 53 batch 10 loss 0.00532212108373642\n",
      "Training: epoch 53 batch 20 loss 0.006101165432482958\n",
      "Test: epoch 53 batch 0 loss 0.010345570743083954\n",
      "epoch 53 finished - avarage train loss 0.006919712839840815  avarage test loss 0.012640275992453098\n",
      "Training: epoch 54 batch 0 loss 0.008595525287091732\n",
      "Training: epoch 54 batch 10 loss 0.014212342910468578\n",
      "Training: epoch 54 batch 20 loss 0.009733131155371666\n",
      "Test: epoch 54 batch 0 loss 0.014582717791199684\n",
      "epoch 54 finished - avarage train loss 0.008028575837419465  avarage test loss 0.015036480966955423\n",
      "Training: epoch 55 batch 0 loss 0.00804795604199171\n",
      "Training: epoch 55 batch 10 loss 0.0053997947834432125\n",
      "Training: epoch 55 batch 20 loss 0.004207840654999018\n",
      "Test: epoch 55 batch 0 loss 0.017093032598495483\n",
      "epoch 55 finished - avarage train loss 0.007813829960751122  avarage test loss 0.018923643277958035\n",
      "Training: epoch 56 batch 0 loss 0.0044654784724116325\n",
      "Training: epoch 56 batch 10 loss 0.009638097137212753\n",
      "Training: epoch 56 batch 20 loss 0.006602074950933456\n",
      "Test: epoch 56 batch 0 loss 0.01575259491801262\n",
      "epoch 56 finished - avarage train loss 0.008649463219375446  avarage test loss 0.016564850346185267\n",
      "Training: epoch 57 batch 0 loss 0.005197605583816767\n",
      "Training: epoch 57 batch 10 loss 0.0041086748242378235\n",
      "Training: epoch 57 batch 20 loss 0.006463485769927502\n",
      "Test: epoch 57 batch 0 loss 0.016045378521084785\n",
      "epoch 57 finished - avarage train loss 0.008628707732362994  avarage test loss 0.018245084676891565\n",
      "Training: epoch 58 batch 0 loss 0.009328175336122513\n",
      "Training: epoch 58 batch 10 loss 0.010526558384299278\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training: epoch 58 batch 20 loss 0.009926711209118366\n",
      "Test: epoch 58 batch 0 loss 0.014525355771183968\n",
      "epoch 58 finished - avarage train loss 0.00827907055520035  avarage test loss 0.01487730024382472\n",
      "Training: epoch 59 batch 0 loss 0.011795115657150745\n",
      "Training: epoch 59 batch 10 loss 0.009458225220441818\n",
      "Training: epoch 59 batch 20 loss 0.00836967397481203\n",
      "Test: epoch 59 batch 0 loss 0.011696206405758858\n",
      "epoch 59 finished - avarage train loss 0.00849276087407408  avarage test loss 0.01269275730010122\n",
      "Training: epoch 60 batch 0 loss 0.003050783183425665\n",
      "Training: epoch 60 batch 10 loss 0.007763159926980734\n",
      "Training: epoch 60 batch 20 loss 0.006247031502425671\n",
      "Test: epoch 60 batch 0 loss 0.010532996617257595\n",
      "epoch 60 finished - avarage train loss 0.00778423227241327  avarage test loss 0.012619917746633291\n",
      "Training: epoch 61 batch 0 loss 0.006343565881252289\n",
      "Training: epoch 61 batch 10 loss 0.007809814065694809\n",
      "Training: epoch 61 batch 20 loss 0.004699734039604664\n",
      "Test: epoch 61 batch 0 loss 0.013189655728638172\n",
      "epoch 61 finished - avarage train loss 0.008985879108438203  avarage test loss 0.014071809593588114\n",
      "Training: epoch 62 batch 0 loss 0.0047306520864367485\n",
      "Training: epoch 62 batch 10 loss 0.005439525470137596\n",
      "Training: epoch 62 batch 20 loss 0.007764375302940607\n",
      "Test: epoch 62 batch 0 loss 0.014404880814254284\n",
      "epoch 62 finished - avarage train loss 0.006975842471202386  avarage test loss 0.0153198930202052\n",
      "Training: epoch 63 batch 0 loss 0.005364056210964918\n",
      "Training: epoch 63 batch 10 loss 0.005942998453974724\n",
      "Training: epoch 63 batch 20 loss 0.0021599787287414074\n",
      "Test: epoch 63 batch 0 loss 0.013611210510134697\n",
      "epoch 63 finished - avarage train loss 0.005865974994055156  avarage test loss 0.014430158655159175\n",
      "Training: epoch 64 batch 0 loss 0.005533992312848568\n",
      "Training: epoch 64 batch 10 loss 0.004645519889891148\n",
      "Training: epoch 64 batch 20 loss 0.0063369967974722385\n",
      "Test: epoch 64 batch 0 loss 0.012555770576000214\n",
      "epoch 64 finished - avarage train loss 0.0065331054642668055  avarage test loss 0.014135096687823534\n",
      "Training: epoch 65 batch 0 loss 0.00436576921492815\n",
      "Training: epoch 65 batch 10 loss 0.00595952058210969\n",
      "Training: epoch 65 batch 20 loss 0.00845428928732872\n",
      "Test: epoch 65 batch 0 loss 0.011579571291804314\n",
      "epoch 65 finished - avarage train loss 0.007651299800209958  avarage test loss 0.013904776540584862\n",
      "Training: epoch 66 batch 0 loss 0.00525529682636261\n",
      "Training: epoch 66 batch 10 loss 0.00457997340708971\n",
      "Training: epoch 66 batch 20 loss 0.0024428307078778744\n",
      "Test: epoch 66 batch 0 loss 0.009558097459375858\n",
      "epoch 66 finished - avarage train loss 0.007279434265440394  avarage test loss 0.013682020013220608\n",
      "Training: epoch 67 batch 0 loss 0.0051221223548054695\n",
      "Training: epoch 67 batch 10 loss 0.0046916743740439415\n",
      "Training: epoch 67 batch 20 loss 0.003018672112375498\n",
      "Test: epoch 67 batch 0 loss 0.013469015248119831\n",
      "epoch 67 finished - avarage train loss 0.006920655864162435  avarage test loss 0.01493016630411148\n",
      "Training: epoch 68 batch 0 loss 0.005895017646253109\n",
      "Training: epoch 68 batch 10 loss 0.006319508887827396\n",
      "Training: epoch 68 batch 20 loss 0.0034452418331056833\n",
      "Test: epoch 68 batch 0 loss 0.010566524229943752\n",
      "epoch 68 finished - avarage train loss 0.007090164476941372  avarage test loss 0.014002525829710066\n",
      "Training: epoch 69 batch 0 loss 0.0037314090877771378\n",
      "Training: epoch 69 batch 10 loss 0.007029506377875805\n",
      "Training: epoch 69 batch 20 loss 0.009873849339783192\n",
      "Test: epoch 69 batch 0 loss 0.010424337349832058\n",
      "epoch 69 finished - avarage train loss 0.00816897405632611  avarage test loss 0.014967171242460608\n",
      "Training: epoch 70 batch 0 loss 0.010322416201233864\n",
      "Training: epoch 70 batch 10 loss 0.007268572226166725\n",
      "Training: epoch 70 batch 20 loss 0.004700659308582544\n",
      "Test: epoch 70 batch 0 loss 0.014307460747659206\n",
      "epoch 70 finished - avarage train loss 0.007454275731639615  avarage test loss 0.015674252761527896\n",
      "Training: epoch 71 batch 0 loss 0.004619554616510868\n",
      "Training: epoch 71 batch 10 loss 0.0171664971858263\n",
      "Training: epoch 71 batch 20 loss 0.007389035075902939\n",
      "Test: epoch 71 batch 0 loss 0.01094397995620966\n",
      "epoch 71 finished - avarage train loss 0.00750122564555756  avarage test loss 0.014135123463347554\n",
      "Training: epoch 72 batch 0 loss 0.003519864985719323\n",
      "Training: epoch 72 batch 10 loss 0.006715667899698019\n",
      "Training: epoch 72 batch 20 loss 0.0037495996803045273\n",
      "Test: epoch 72 batch 0 loss 0.009665277786552906\n",
      "epoch 72 finished - avarage train loss 0.006640365094363946  avarage test loss 0.014470439753495157\n",
      "Training: epoch 73 batch 0 loss 0.0021467162296175957\n",
      "Training: epoch 73 batch 10 loss 0.008829688653349876\n",
      "Training: epoch 73 batch 20 loss 0.009656129404902458\n",
      "Test: epoch 73 batch 0 loss 0.013670390471816063\n",
      "epoch 73 finished - avarage train loss 0.00783773965266501  avarage test loss 0.014929745579138398\n",
      "Training: epoch 74 batch 0 loss 0.009773856028914452\n",
      "Training: epoch 74 batch 10 loss 0.007603462785482407\n",
      "Training: epoch 74 batch 20 loss 0.008851094171404839\n",
      "Test: epoch 74 batch 0 loss 0.03770727291703224\n",
      "epoch 74 finished - avarage train loss 0.009895973092201969  avarage test loss 0.04111331235617399\n",
      "Training: epoch 75 batch 0 loss 0.03437715768814087\n",
      "Training: epoch 75 batch 10 loss 0.01722593791782856\n",
      "Training: epoch 75 batch 20 loss 0.004240924026817083\n",
      "Test: epoch 75 batch 0 loss 0.01539028249680996\n",
      "epoch 75 finished - avarage train loss 0.02433365405183928  avarage test loss 0.01917040511034429\n",
      "Training: epoch 76 batch 0 loss 0.007656958885490894\n",
      "Training: epoch 76 batch 10 loss 0.007638853974640369\n",
      "Training: epoch 76 batch 20 loss 0.0051239836029708385\n",
      "Test: epoch 76 batch 0 loss 0.016259746626019478\n",
      "epoch 76 finished - avarage train loss 0.009075588193433038  avarage test loss 0.017464941600337625\n",
      "Training: epoch 77 batch 0 loss 0.006293915212154388\n",
      "Training: epoch 77 batch 10 loss 0.006422801874577999\n",
      "Training: epoch 77 batch 20 loss 0.011148463934659958\n",
      "Test: epoch 77 batch 0 loss 0.012116476893424988\n",
      "epoch 77 finished - avarage train loss 0.007950501540547302  avarage test loss 0.01805405505001545\n",
      "Training: epoch 78 batch 0 loss 0.016083866357803345\n",
      "Training: epoch 78 batch 10 loss 0.03324294835329056\n",
      "Training: epoch 78 batch 20 loss 0.016315370798110962\n",
      "Test: epoch 78 batch 0 loss 0.016529370099306107\n",
      "epoch 78 finished - avarage train loss 0.022729149152492654  avarage test loss 0.023356115212664008\n",
      "Training: epoch 79 batch 0 loss 0.011687211692333221\n",
      "Training: epoch 79 batch 10 loss 0.012985678389668465\n",
      "Training: epoch 79 batch 20 loss 0.022969573736190796\n",
      "Test: epoch 79 batch 0 loss 0.006445229984819889\n",
      "epoch 79 finished - avarage train loss 0.013722480387137881  avarage test loss 0.013917913427576423\n",
      "Training: epoch 80 batch 0 loss 0.004457010887563229\n",
      "Training: epoch 80 batch 10 loss 0.0058096167631447315\n",
      "Training: epoch 80 batch 20 loss 0.005798409227281809\n",
      "Test: epoch 80 batch 0 loss 0.013612635433673859\n",
      "epoch 80 finished - avarage train loss 0.008646699905010134  avarage test loss 0.015524687943980098\n",
      "Training: epoch 81 batch 0 loss 0.0033644670620560646\n",
      "Training: epoch 81 batch 10 loss 0.0067596095614135265\n",
      "Training: epoch 81 batch 20 loss 0.002499857684597373\n",
      "Test: epoch 81 batch 0 loss 0.013873863033950329\n",
      "epoch 81 finished - avarage train loss 0.0077698138494301456  avarage test loss 0.01690867030993104\n",
      "Training: epoch 82 batch 0 loss 0.00888013280928135\n",
      "Training: epoch 82 batch 10 loss 0.0038679351564496756\n",
      "Training: epoch 82 batch 20 loss 0.002373182214796543\n",
      "Test: epoch 82 batch 0 loss 0.009679771959781647\n",
      "epoch 82 finished - avarage train loss 0.007947947235842204  avarage test loss 0.013786252355203032\n",
      "Training: epoch 83 batch 0 loss 0.0066596027463674545\n",
      "Training: epoch 83 batch 10 loss 0.0023983335122466087\n",
      "Training: epoch 83 batch 20 loss 0.007628345862030983\n",
      "Test: epoch 83 batch 0 loss 0.011026465333998203\n",
      "epoch 83 finished - avarage train loss 0.007453159445190224  avarage test loss 0.012364623602479696\n",
      "Training: epoch 84 batch 0 loss 0.006134861148893833\n",
      "Training: epoch 84 batch 10 loss 0.003132463665679097\n",
      "Training: epoch 84 batch 20 loss 0.015705477446317673\n",
      "Test: epoch 84 batch 0 loss 0.008042645640671253\n",
      "epoch 84 finished - avarage train loss 0.006991935456332205  avarage test loss 0.01491245545912534\n",
      "Training: epoch 85 batch 0 loss 0.009562141261994839\n",
      "Training: epoch 85 batch 10 loss 0.003492899239063263\n",
      "Training: epoch 85 batch 20 loss 0.004856693092733622\n",
      "Test: epoch 85 batch 0 loss 0.008568709716200829\n",
      "epoch 85 finished - avarage train loss 0.007588288602258624  avarage test loss 0.014291333267465234\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training: epoch 86 batch 0 loss 0.0036913841031491756\n",
      "Training: epoch 86 batch 10 loss 0.005813811905682087\n",
      "Training: epoch 86 batch 20 loss 0.0042142379097640514\n",
      "Test: epoch 86 batch 0 loss 0.01023941207677126\n",
      "epoch 86 finished - avarage train loss 0.0068736641732012405  avarage test loss 0.014594594715163112\n",
      "Training: epoch 87 batch 0 loss 0.010221575386822224\n",
      "Training: epoch 87 batch 10 loss 0.0032170447520911694\n",
      "Training: epoch 87 batch 20 loss 0.004520980641245842\n",
      "Test: epoch 87 batch 0 loss 0.009714084677398205\n",
      "epoch 87 finished - avarage train loss 0.006803804743585402  avarage test loss 0.011894322116859257\n",
      "Training: epoch 88 batch 0 loss 0.0043801735155284405\n",
      "Training: epoch 88 batch 10 loss 0.004709809087216854\n",
      "Training: epoch 88 batch 20 loss 0.008663387969136238\n",
      "Test: epoch 88 batch 0 loss 0.007894598878920078\n",
      "epoch 88 finished - avarage train loss 0.007276275379840156  avarage test loss 0.011459972709417343\n",
      "Training: epoch 89 batch 0 loss 0.005627610720694065\n",
      "Training: epoch 89 batch 10 loss 0.01161640789359808\n",
      "Training: epoch 89 batch 20 loss 0.003551402362063527\n",
      "Test: epoch 89 batch 0 loss 0.011775200255215168\n",
      "epoch 89 finished - avarage train loss 0.009098155713029977  avarage test loss 0.01901847287081182\n",
      "Training: epoch 90 batch 0 loss 0.005345713347196579\n",
      "Training: epoch 90 batch 10 loss 0.005194641649723053\n",
      "Training: epoch 90 batch 20 loss 0.009357986971735954\n",
      "Test: epoch 90 batch 0 loss 0.011464269831776619\n",
      "epoch 90 finished - avarage train loss 0.008921140315139601  avarage test loss 0.014205061830580235\n",
      "Training: epoch 91 batch 0 loss 0.007061342243105173\n",
      "Training: epoch 91 batch 10 loss 0.007368487771600485\n",
      "Training: epoch 91 batch 20 loss 0.004745214246213436\n",
      "Test: epoch 91 batch 0 loss 0.010653398931026459\n",
      "epoch 91 finished - avarage train loss 0.007527962599977337  avarage test loss 0.013623756589367986\n",
      "Training: epoch 92 batch 0 loss 0.004677420016378164\n",
      "Training: epoch 92 batch 10 loss 0.004415933974087238\n",
      "Training: epoch 92 batch 20 loss 0.00521425437182188\n",
      "Test: epoch 92 batch 0 loss 0.012043715454638004\n",
      "epoch 92 finished - avarage train loss 0.007237240740354976  avarage test loss 0.01675629278179258\n",
      "Training: epoch 93 batch 0 loss 0.004954391159117222\n",
      "Training: epoch 93 batch 10 loss 0.0026584358420222998\n",
      "Training: epoch 93 batch 20 loss 0.01817709021270275\n",
      "Test: epoch 93 batch 0 loss 0.011097792536020279\n",
      "epoch 93 finished - avarage train loss 0.007303775705654046  avarage test loss 0.012657460174523294\n",
      "Training: epoch 94 batch 0 loss 0.004194959998130798\n",
      "Training: epoch 94 batch 10 loss 0.005910929758101702\n",
      "Training: epoch 94 batch 20 loss 0.004008315037935972\n",
      "Test: epoch 94 batch 0 loss 0.012728976085782051\n",
      "epoch 94 finished - avarage train loss 0.006238214591325357  avarage test loss 0.01448341878131032\n",
      "Training: epoch 95 batch 0 loss 0.004385659005492926\n",
      "Training: epoch 95 batch 10 loss 0.0041643171571195126\n",
      "Training: epoch 95 batch 20 loss 0.003081410890445113\n",
      "Test: epoch 95 batch 0 loss 0.01428605429828167\n",
      "epoch 95 finished - avarage train loss 0.00830250612363733  avarage test loss 0.016960114240646362\n",
      "Training: epoch 96 batch 0 loss 0.009450286626815796\n",
      "Training: epoch 96 batch 10 loss 0.0089309923350811\n",
      "Training: epoch 96 batch 20 loss 0.0070501226000487804\n",
      "Test: epoch 96 batch 0 loss 0.012395971454679966\n",
      "epoch 96 finished - avarage train loss 0.008475278913653616  avarage test loss 0.014164533233270049\n",
      "Training: epoch 97 batch 0 loss 0.005999165587127209\n",
      "Training: epoch 97 batch 10 loss 0.005591710098087788\n",
      "Training: epoch 97 batch 20 loss 0.007340381387621164\n",
      "Test: epoch 97 batch 0 loss 0.012234233319759369\n",
      "epoch 97 finished - avarage train loss 0.007513910748771039  avarage test loss 0.013934017275460064\n",
      "Training: epoch 98 batch 0 loss 0.0031783359590917826\n",
      "Training: epoch 98 batch 10 loss 0.004761699121445417\n",
      "Training: epoch 98 batch 20 loss 0.011645369231700897\n",
      "Test: epoch 98 batch 0 loss 0.011408256366848946\n",
      "epoch 98 finished - avarage train loss 0.0068210098683705615  avarage test loss 0.013288522372022271\n",
      "Training: epoch 99 batch 0 loss 0.009600124321877956\n",
      "Training: epoch 99 batch 10 loss 0.006188277155160904\n",
      "Training: epoch 99 batch 20 loss 0.004576797131448984\n",
      "Test: epoch 99 batch 0 loss 0.013592901639640331\n",
      "epoch 99 finished - avarage train loss 0.0063229389328124195  avarage test loss 0.016120609128847718\n",
      "Training: epoch 100 batch 0 loss 0.010044570080935955\n",
      "Training: epoch 100 batch 10 loss 0.0022109386045485735\n",
      "Training: epoch 100 batch 20 loss 0.015596956945955753\n",
      "Test: epoch 100 batch 0 loss 0.010321242734789848\n",
      "epoch 100 finished - avarage train loss 0.00641329623036215  avarage test loss 0.012964341673068702\n",
      "Training: epoch 101 batch 0 loss 0.012000211514532566\n",
      "Training: epoch 101 batch 10 loss 0.005408336874097586\n",
      "Training: epoch 101 batch 20 loss 0.004565228242427111\n",
      "Test: epoch 101 batch 0 loss 0.011285657994449139\n",
      "epoch 101 finished - avarage train loss 0.008397086860676264  avarage test loss 0.013069145730696619\n",
      "Training: epoch 102 batch 0 loss 0.0032666928600519896\n",
      "Training: epoch 102 batch 10 loss 0.005715581588447094\n",
      "Training: epoch 102 batch 20 loss 0.005472871474921703\n",
      "Test: epoch 102 batch 0 loss 0.011613006703555584\n",
      "epoch 102 finished - avarage train loss 0.007040926833348027  avarage test loss 0.013110483880154788\n",
      "Training: epoch 103 batch 0 loss 0.0119188716635108\n",
      "Training: epoch 103 batch 10 loss 0.007847963832318783\n",
      "Training: epoch 103 batch 20 loss 0.006805920507758856\n",
      "Test: epoch 103 batch 0 loss 0.01224955078214407\n",
      "epoch 103 finished - avarage train loss 0.00746514605245842  avarage test loss 0.013522070832550526\n",
      "Training: epoch 104 batch 0 loss 0.0028974709566682577\n",
      "Training: epoch 104 batch 10 loss 0.002530445111915469\n",
      "Training: epoch 104 batch 20 loss 0.01900475285947323\n",
      "Test: epoch 104 batch 0 loss 0.010770278051495552\n",
      "epoch 104 finished - avarage train loss 0.006392433348208152  avarage test loss 0.013011707342229784\n",
      "Training: epoch 105 batch 0 loss 0.004666418768465519\n",
      "Training: epoch 105 batch 10 loss 0.008153078146278858\n",
      "Training: epoch 105 batch 20 loss 0.0021154044661670923\n",
      "Test: epoch 105 batch 0 loss 0.012335208244621754\n",
      "epoch 105 finished - avarage train loss 0.00774771512229124  avarage test loss 0.013925441424362361\n",
      "Training: epoch 106 batch 0 loss 0.005668961443006992\n",
      "Training: epoch 106 batch 10 loss 0.008229135535657406\n",
      "Training: epoch 106 batch 20 loss 0.006304365117102861\n",
      "Test: epoch 106 batch 0 loss 0.012093599885702133\n",
      "epoch 106 finished - avarage train loss 0.00697897660449661  avarage test loss 0.013289128779433668\n",
      "Training: epoch 107 batch 0 loss 0.0071946256794035435\n",
      "Training: epoch 107 batch 10 loss 0.003748726798221469\n",
      "Training: epoch 107 batch 20 loss 0.005627187434583902\n",
      "Test: epoch 107 batch 0 loss 0.011296015232801437\n",
      "epoch 107 finished - avarage train loss 0.005663808908891575  avarage test loss 0.013455258216708899\n",
      "Training: epoch 108 batch 0 loss 0.006044292822480202\n",
      "Training: epoch 108 batch 10 loss 0.005451356992125511\n",
      "Training: epoch 108 batch 20 loss 0.006153461989015341\n",
      "Test: epoch 108 batch 0 loss 0.012528739869594574\n",
      "epoch 108 finished - avarage train loss 0.005427649100150527  avarage test loss 0.014121737563982606\n",
      "Training: epoch 109 batch 0 loss 0.006165281869471073\n",
      "Training: epoch 109 batch 10 loss 0.0019340881844982505\n",
      "Training: epoch 109 batch 20 loss 0.0023002729285508394\n",
      "Test: epoch 109 batch 0 loss 0.01147160679101944\n",
      "epoch 109 finished - avarage train loss 0.006280913053819075  avarage test loss 0.013158544432371855\n",
      "Training: epoch 110 batch 0 loss 0.0038432267028838396\n",
      "Training: epoch 110 batch 10 loss 0.007467843126505613\n",
      "Training: epoch 110 batch 20 loss 0.00891578383743763\n",
      "Test: epoch 110 batch 0 loss 0.011716685257852077\n",
      "epoch 110 finished - avarage train loss 0.006509101291283451  avarage test loss 0.014085333212278783\n",
      "Training: epoch 111 batch 0 loss 0.006741284858435392\n",
      "Training: epoch 111 batch 10 loss 0.003298619296401739\n",
      "Training: epoch 111 batch 20 loss 0.003148586256429553\n",
      "Test: epoch 111 batch 0 loss 0.013682871125638485\n",
      "epoch 111 finished - avarage train loss 0.006565777135306391  avarage test loss 0.014758225530385971\n",
      "Training: epoch 112 batch 0 loss 0.004542596638202667\n",
      "Training: epoch 112 batch 10 loss 0.010165498591959476\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training: epoch 112 batch 20 loss 0.004779933951795101\n",
      "Test: epoch 112 batch 0 loss 0.016540100798010826\n",
      "epoch 112 finished - avarage train loss 0.008242607823219793  avarage test loss 0.016686599468812346\n",
      "Training: epoch 113 batch 0 loss 0.006017943844199181\n",
      "Training: epoch 113 batch 10 loss 0.0030161752365529537\n",
      "Training: epoch 113 batch 20 loss 0.010998585261404514\n",
      "Test: epoch 113 batch 0 loss 0.015054093673825264\n",
      "epoch 113 finished - avarage train loss 0.008689161839670148  avarage test loss 0.01571472641080618\n",
      "Training: epoch 114 batch 0 loss 0.006498516071587801\n",
      "Training: epoch 114 batch 10 loss 0.014416071586310863\n",
      "Training: epoch 114 batch 20 loss 0.0061358981765806675\n",
      "Test: epoch 114 batch 0 loss 0.014088619500398636\n",
      "epoch 114 finished - avarage train loss 0.007508713992890613  avarage test loss 0.016115690814331174\n",
      "Training: epoch 115 batch 0 loss 0.003903638571500778\n",
      "Training: epoch 115 batch 10 loss 0.005883068311959505\n",
      "Training: epoch 115 batch 20 loss 0.004823003895580769\n",
      "Test: epoch 115 batch 0 loss 0.013022794388234615\n",
      "epoch 115 finished - avarage train loss 0.00606091348614929  avarage test loss 0.014485075720585883\n",
      "Training: epoch 116 batch 0 loss 0.0070817917585372925\n",
      "Training: epoch 116 batch 10 loss 0.006453891284763813\n",
      "Training: epoch 116 batch 20 loss 0.004633226897567511\n",
      "Test: epoch 116 batch 0 loss 0.014160066843032837\n",
      "epoch 116 finished - avarage train loss 0.007331007593793088  avarage test loss 0.015380784403532743\n",
      "Training: epoch 117 batch 0 loss 0.00980317685753107\n",
      "Training: epoch 117 batch 10 loss 0.0041883899830281734\n",
      "Training: epoch 117 batch 20 loss 0.007272656075656414\n",
      "Test: epoch 117 batch 0 loss 0.015601939521729946\n",
      "epoch 117 finished - avarage train loss 0.006883640653164736  avarage test loss 0.015492102480493486\n",
      "Training: epoch 118 batch 0 loss 0.008263947442173958\n",
      "Training: epoch 118 batch 10 loss 0.01247109193354845\n",
      "Training: epoch 118 batch 20 loss 0.008385272696614265\n",
      "Test: epoch 118 batch 0 loss 0.01506924256682396\n",
      "epoch 118 finished - avarage train loss 0.008986499818876899  avarage test loss 0.014603088260628283\n",
      "Training: epoch 119 batch 0 loss 0.0012375694932416081\n",
      "Training: epoch 119 batch 10 loss 0.005937357433140278\n",
      "Training: epoch 119 batch 20 loss 0.006209161132574081\n",
      "Test: epoch 119 batch 0 loss 0.014433213509619236\n",
      "epoch 119 finished - avarage train loss 0.007739238966866557  avarage test loss 0.014708297443576157\n",
      "Training: epoch 120 batch 0 loss 0.004795053042471409\n",
      "Training: epoch 120 batch 10 loss 0.003985175862908363\n",
      "Training: epoch 120 batch 20 loss 0.007929951883852482\n",
      "Test: epoch 120 batch 0 loss 0.01200656034052372\n",
      "epoch 120 finished - avarage train loss 0.00705538329604114  avarage test loss 0.014362221350893378\n",
      "Training: epoch 121 batch 0 loss 0.0047505390830338\n",
      "Training: epoch 121 batch 10 loss 0.0042920964770019054\n",
      "Training: epoch 121 batch 20 loss 0.00869947299361229\n",
      "Test: epoch 121 batch 0 loss 0.01236103568226099\n",
      "epoch 121 finished - avarage train loss 0.00758214097405816  avarage test loss 0.013851171359419823\n",
      "Training: epoch 122 batch 0 loss 0.004062288906425238\n",
      "Training: epoch 122 batch 10 loss 0.0055852713994681835\n",
      "Training: epoch 122 batch 20 loss 0.00427423557266593\n",
      "Test: epoch 122 batch 0 loss 0.012922456488013268\n",
      "epoch 122 finished - avarage train loss 0.007564593419625327  avarage test loss 0.013763064634986222\n",
      "Training: epoch 123 batch 0 loss 0.007335006725043058\n",
      "Training: epoch 123 batch 10 loss 0.002040763385593891\n",
      "Training: epoch 123 batch 20 loss 0.0042451051995158195\n",
      "Test: epoch 123 batch 0 loss 0.011583661660552025\n",
      "epoch 123 finished - avarage train loss 0.007168911734660124  avarage test loss 0.014168158173561096\n",
      "Training: epoch 124 batch 0 loss 0.006598541047424078\n",
      "Training: epoch 124 batch 10 loss 0.005130085162818432\n",
      "Training: epoch 124 batch 20 loss 0.023961832746863365\n",
      "Test: epoch 124 batch 0 loss 0.012092484161257744\n",
      "epoch 124 finished - avarage train loss 0.0072744268169305445  avarage test loss 0.014192397706210613\n",
      "Training: epoch 125 batch 0 loss 0.00641280971467495\n",
      "Training: epoch 125 batch 10 loss 0.004402541555464268\n",
      "Training: epoch 125 batch 20 loss 0.009677293710410595\n",
      "Test: epoch 125 batch 0 loss 0.011286487802863121\n",
      "epoch 125 finished - avarage train loss 0.007734064101080956  avarage test loss 0.012779187760315835\n",
      "Training: epoch 126 batch 0 loss 0.0027147887740284204\n",
      "Training: epoch 126 batch 10 loss 0.003000346478074789\n",
      "Training: epoch 126 batch 20 loss 0.004002346657216549\n",
      "Test: epoch 126 batch 0 loss 0.013008673675358295\n",
      "epoch 126 finished - avarage train loss 0.006881814602569774  avarage test loss 0.014386300113983452\n",
      "Training: epoch 127 batch 0 loss 0.0025415602140128613\n",
      "Training: epoch 127 batch 10 loss 0.016151132062077522\n",
      "Training: epoch 127 batch 20 loss 0.00851069949567318\n",
      "Test: epoch 127 batch 0 loss 0.012954849749803543\n",
      "epoch 127 finished - avarage train loss 0.00693341240222598  avarage test loss 0.014043970848433673\n",
      "Training: epoch 128 batch 0 loss 0.0061020925641059875\n",
      "Training: epoch 128 batch 10 loss 0.004145118407905102\n",
      "Training: epoch 128 batch 20 loss 0.0043800934217870235\n",
      "Test: epoch 128 batch 0 loss 0.012529633939266205\n",
      "epoch 128 finished - avarage train loss 0.005961223975112983  avarage test loss 0.013733633095398545\n",
      "Training: epoch 129 batch 0 loss 0.0071300845593214035\n",
      "Training: epoch 129 batch 10 loss 0.005819002166390419\n",
      "Training: epoch 129 batch 20 loss 0.006827561184763908\n",
      "Test: epoch 129 batch 0 loss 0.012417295016348362\n",
      "epoch 129 finished - avarage train loss 0.008173392314849228  avarage test loss 0.016463774722069502\n",
      "Training: epoch 130 batch 0 loss 0.0040647117421031\n",
      "Training: epoch 130 batch 10 loss 0.008047159761190414\n",
      "Training: epoch 130 batch 20 loss 0.00764197763055563\n",
      "Test: epoch 130 batch 0 loss 0.012705698609352112\n",
      "epoch 130 finished - avarage train loss 0.007735175562315974  avarage test loss 0.013855706318281591\n",
      "Training: epoch 131 batch 0 loss 0.004613388329744339\n",
      "Training: epoch 131 batch 10 loss 0.011747733689844608\n",
      "Training: epoch 131 batch 20 loss 0.006374955642968416\n",
      "Test: epoch 131 batch 0 loss 0.014119434170424938\n",
      "epoch 131 finished - avarage train loss 0.008037257971691674  avarage test loss 0.01773440628312528\n",
      "Training: epoch 132 batch 0 loss 0.0074675921350717545\n",
      "Training: epoch 132 batch 10 loss 0.009860954247415066\n",
      "Training: epoch 132 batch 20 loss 0.005739748943597078\n",
      "Test: epoch 132 batch 0 loss 0.012641096487641335\n",
      "epoch 132 finished - avarage train loss 0.00786957813523196  avarage test loss 0.013804925256408751\n",
      "Training: epoch 133 batch 0 loss 0.0045476751402020454\n",
      "Training: epoch 133 batch 10 loss 0.008533695712685585\n",
      "Training: epoch 133 batch 20 loss 0.010021936148405075\n",
      "Test: epoch 133 batch 0 loss 0.011713598854839802\n",
      "epoch 133 finished - avarage train loss 0.00747100710226544  avarage test loss 0.013146167621016502\n",
      "Training: epoch 134 batch 0 loss 0.005212671589106321\n",
      "Training: epoch 134 batch 10 loss 0.0038107726722955704\n",
      "Training: epoch 134 batch 20 loss 0.0032897042110562325\n",
      "Test: epoch 134 batch 0 loss 0.010297357104718685\n",
      "epoch 134 finished - avarage train loss 0.007764718166134995  avarage test loss 0.013668222585693002\n",
      "Training: epoch 135 batch 0 loss 0.0076266988180577755\n",
      "Training: epoch 135 batch 10 loss 0.009237473830580711\n",
      "Training: epoch 135 batch 20 loss 0.005680676084011793\n",
      "Test: epoch 135 batch 0 loss 0.010568996891379356\n",
      "epoch 135 finished - avarage train loss 0.007907629603166776  avarage test loss 0.012264275923371315\n",
      "Training: epoch 136 batch 0 loss 0.003356774104759097\n",
      "Training: epoch 136 batch 10 loss 0.00979168713092804\n",
      "Training: epoch 136 batch 20 loss 0.0019543280359357595\n",
      "Test: epoch 136 batch 0 loss 0.011412251740694046\n",
      "epoch 136 finished - avarage train loss 0.006349904518895622  avarage test loss 0.014903650851920247\n",
      "Training: epoch 137 batch 0 loss 0.005239913240075111\n",
      "Training: epoch 137 batch 10 loss 0.0076108667999506\n",
      "Training: epoch 137 batch 20 loss 0.005121888592839241\n",
      "Test: epoch 137 batch 0 loss 0.009500863961875439\n",
      "epoch 137 finished - avarage train loss 0.0071514160532889694  avarage test loss 0.012279489892534912\n",
      "Training: epoch 138 batch 0 loss 0.004440969787538052\n",
      "Training: epoch 138 batch 10 loss 0.002537782769650221\n",
      "Training: epoch 138 batch 20 loss 0.004476871807128191\n",
      "Test: epoch 138 batch 0 loss 0.010767453350126743\n",
      "epoch 138 finished - avarage train loss 0.006477162828026661  avarage test loss 0.01372787356376648\n",
      "Training: epoch 139 batch 0 loss 0.0045350706204771996\n",
      "Training: epoch 139 batch 10 loss 0.012117147445678711\n",
      "Training: epoch 139 batch 20 loss 0.014938756823539734\n",
      "Test: epoch 139 batch 0 loss 0.01219157874584198\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 139 finished - avarage train loss 0.008138060802593827  avarage test loss 0.015432045562192798\n",
      "Training: epoch 140 batch 0 loss 0.009916790761053562\n",
      "Training: epoch 140 batch 10 loss 0.011913455091416836\n",
      "Training: epoch 140 batch 20 loss 0.004891552031040192\n",
      "Test: epoch 140 batch 0 loss 0.01492560375481844\n",
      "epoch 140 finished - avarage train loss 0.009344016193765504  avarage test loss 0.019856350729241967\n",
      "Training: epoch 141 batch 0 loss 0.013229617848992348\n",
      "Training: epoch 141 batch 10 loss 0.00857353676110506\n",
      "Training: epoch 141 batch 20 loss 0.006195798981934786\n",
      "Test: epoch 141 batch 0 loss 0.011208927258849144\n",
      "epoch 141 finished - avarage train loss 0.00924449181987037  avarage test loss 0.013468838878907263\n",
      "Training: epoch 142 batch 0 loss 0.004306560382246971\n",
      "Training: epoch 142 batch 10 loss 0.0048219566233456135\n",
      "Training: epoch 142 batch 20 loss 0.006057361140847206\n",
      "Test: epoch 142 batch 0 loss 0.02136503905057907\n",
      "epoch 142 finished - avarage train loss 0.011109175706474945  avarage test loss 0.023812838830053806\n",
      "Training: epoch 143 batch 0 loss 0.014029295183718204\n",
      "Training: epoch 143 batch 10 loss 0.009492585435509682\n",
      "Training: epoch 143 batch 20 loss 0.006058659870177507\n",
      "Test: epoch 143 batch 0 loss 0.014758175238966942\n",
      "epoch 143 finished - avarage train loss 0.009316676778012309  avarage test loss 0.015368573018349707\n",
      "Training: epoch 144 batch 0 loss 0.006210265215486288\n",
      "Training: epoch 144 batch 10 loss 0.00751632871106267\n",
      "Training: epoch 144 batch 20 loss 0.006386987399309874\n",
      "Test: epoch 144 batch 0 loss 0.012643077410757542\n",
      "epoch 144 finished - avarage train loss 0.007785578772168735  avarage test loss 0.013745017116889358\n",
      "Training: epoch 145 batch 0 loss 0.010272186249494553\n",
      "Training: epoch 145 batch 10 loss 0.0034411270171403885\n",
      "Training: epoch 145 batch 20 loss 0.006890715099871159\n",
      "Test: epoch 145 batch 0 loss 0.013314308598637581\n",
      "epoch 145 finished - avarage train loss 0.007816839890939922  avarage test loss 0.015683272387832403\n",
      "Training: epoch 146 batch 0 loss 0.00605218019336462\n",
      "Training: epoch 146 batch 10 loss 0.00714221503585577\n",
      "Training: epoch 146 batch 20 loss 0.008519001305103302\n",
      "Test: epoch 146 batch 0 loss 0.01055676955729723\n",
      "epoch 146 finished - avarage train loss 0.008249458415305307  avarage test loss 0.014410074159968644\n",
      "Training: epoch 147 batch 0 loss 0.006198253482580185\n",
      "Training: epoch 147 batch 10 loss 0.004867082927376032\n",
      "Training: epoch 147 batch 20 loss 0.004958512727171183\n",
      "Test: epoch 147 batch 0 loss 0.011221418157219887\n",
      "epoch 147 finished - avarage train loss 0.00704385984110935  avarage test loss 0.013326674234122038\n",
      "Training: epoch 148 batch 0 loss 0.00547966081649065\n",
      "Training: epoch 148 batch 10 loss 0.00846001785248518\n",
      "Training: epoch 148 batch 20 loss 0.003128412878140807\n",
      "Test: epoch 148 batch 0 loss 0.011388495564460754\n",
      "epoch 148 finished - avarage train loss 0.0071743071753660155  avarage test loss 0.015383438323624432\n",
      "Training: epoch 149 batch 0 loss 0.004946238361299038\n",
      "Training: epoch 149 batch 10 loss 0.014985810033977032\n",
      "Training: epoch 149 batch 20 loss 0.005029238760471344\n",
      "Test: epoch 149 batch 0 loss 0.012356188148260117\n",
      "epoch 149 finished - avarage train loss 0.010604557380529827  avarage test loss 0.016049725003540516\n",
      "Training: epoch 150 batch 0 loss 0.004763719160109758\n",
      "Training: epoch 150 batch 10 loss 0.010445309802889824\n",
      "Training: epoch 150 batch 20 loss 0.002519042696803808\n",
      "Test: epoch 150 batch 0 loss 0.013052811846137047\n",
      "epoch 150 finished - avarage train loss 0.009018959509658402  avarage test loss 0.0150040591834113\n",
      "Training: epoch 151 batch 0 loss 0.00686495378613472\n",
      "Training: epoch 151 batch 10 loss 0.0030122902244329453\n",
      "Training: epoch 151 batch 20 loss 0.0040067159570753574\n",
      "Test: epoch 151 batch 0 loss 0.013523178175091743\n",
      "epoch 151 finished - avarage train loss 0.007004353438568269  avarage test loss 0.014047038392163813\n",
      "Training: epoch 152 batch 0 loss 0.0026064147241413593\n",
      "Training: epoch 152 batch 10 loss 0.010676741600036621\n",
      "Training: epoch 152 batch 20 loss 0.0024734691251069307\n",
      "Test: epoch 152 batch 0 loss 0.01248534582555294\n",
      "epoch 152 finished - avarage train loss 0.006984812930097868  avarage test loss 0.013503930997103453\n",
      "Training: epoch 153 batch 0 loss 0.001772022107616067\n",
      "Training: epoch 153 batch 10 loss 0.0036304916720837355\n",
      "Training: epoch 153 batch 20 loss 0.006159801501780748\n",
      "Test: epoch 153 batch 0 loss 0.012531975284218788\n",
      "epoch 153 finished - avarage train loss 0.0074336098442817555  avarage test loss 0.013389186467975378\n",
      "Training: epoch 154 batch 0 loss 0.005741225555539131\n",
      "Training: epoch 154 batch 10 loss 0.006097012665122747\n",
      "Training: epoch 154 batch 20 loss 0.006638393737375736\n",
      "Test: epoch 154 batch 0 loss 0.013786358758807182\n",
      "epoch 154 finished - avarage train loss 0.006638027703517984  avarage test loss 0.014739263919182122\n",
      "Training: epoch 155 batch 0 loss 0.006103235296905041\n",
      "Training: epoch 155 batch 10 loss 0.007441145367920399\n",
      "Training: epoch 155 batch 20 loss 0.004578785039484501\n",
      "Test: epoch 155 batch 0 loss 0.012731288559734821\n",
      "epoch 155 finished - avarage train loss 0.006637422119042483  avarage test loss 0.013840357423759997\n",
      "Training: epoch 156 batch 0 loss 0.002713080495595932\n",
      "Training: epoch 156 batch 10 loss 0.0033750650472939014\n",
      "Training: epoch 156 batch 20 loss 0.0067448485642671585\n",
      "Test: epoch 156 batch 0 loss 0.016939323395490646\n",
      "epoch 156 finished - avarage train loss 0.00831530336290598  avarage test loss 0.016209879890084267\n",
      "Training: epoch 157 batch 0 loss 0.009583643637597561\n",
      "Training: epoch 157 batch 10 loss 0.0028145560063421726\n",
      "Training: epoch 157 batch 20 loss 0.004725661128759384\n",
      "Test: epoch 157 batch 0 loss 0.017415035516023636\n",
      "epoch 157 finished - avarage train loss 0.007459322737272957  avarage test loss 0.016812896821647882\n",
      "Training: epoch 158 batch 0 loss 0.010839171707630157\n",
      "Training: epoch 158 batch 10 loss 0.011325188912451267\n",
      "Training: epoch 158 batch 20 loss 0.006367900874465704\n",
      "Test: epoch 158 batch 0 loss 0.014744749292731285\n",
      "epoch 158 finished - avarage train loss 0.011179598947537357  avarage test loss 0.015653071110136807\n",
      "Training: epoch 159 batch 0 loss 0.004555291496217251\n",
      "Training: epoch 159 batch 10 loss 0.004603231325745583\n",
      "Training: epoch 159 batch 20 loss 0.00637328065931797\n",
      "Test: epoch 159 batch 0 loss 0.01417157519608736\n",
      "epoch 159 finished - avarage train loss 0.007512600092891732  avarage test loss 0.015192193444818258\n",
      "Training: epoch 160 batch 0 loss 0.004235409200191498\n",
      "Training: epoch 160 batch 10 loss 0.0044503952376544476\n",
      "Training: epoch 160 batch 20 loss 0.00833981018513441\n",
      "Test: epoch 160 batch 0 loss 0.013630291447043419\n",
      "epoch 160 finished - avarage train loss 0.007416856997036214  avarage test loss 0.01420072722248733\n",
      "Training: epoch 161 batch 0 loss 0.0022407846990972757\n",
      "Training: epoch 161 batch 10 loss 0.004942411556839943\n",
      "Training: epoch 161 batch 20 loss 0.005336507223546505\n",
      "Test: epoch 161 batch 0 loss 0.01344098150730133\n",
      "epoch 161 finished - avarage train loss 0.006899852891741642  avarage test loss 0.013902704580686986\n",
      "Training: epoch 162 batch 0 loss 0.013692257925868034\n",
      "Training: epoch 162 batch 10 loss 0.0037526902742683887\n",
      "Training: epoch 162 batch 20 loss 0.006402081809937954\n",
      "Test: epoch 162 batch 0 loss 0.012877162545919418\n",
      "epoch 162 finished - avarage train loss 0.007450293195594488  avarage test loss 0.01369017013348639\n",
      "Training: epoch 163 batch 0 loss 0.007335906848311424\n",
      "Training: epoch 163 batch 10 loss 0.004687101114541292\n",
      "Training: epoch 163 batch 20 loss 0.005599638447165489\n",
      "Test: epoch 163 batch 0 loss 0.011759227141737938\n",
      "epoch 163 finished - avarage train loss 0.007939252127283093  avarage test loss 0.013823666260577738\n",
      "Training: epoch 164 batch 0 loss 0.004492425825446844\n",
      "Training: epoch 164 batch 10 loss 0.0046271770261228085\n",
      "Training: epoch 164 batch 20 loss 0.008732179179787636\n",
      "Test: epoch 164 batch 0 loss 0.013427326455712318\n",
      "epoch 164 finished - avarage train loss 0.006361498963087797  avarage test loss 0.015500382171012461\n",
      "Training: epoch 165 batch 0 loss 0.005606071092188358\n",
      "Training: epoch 165 batch 10 loss 0.006204128265380859\n",
      "Training: epoch 165 batch 20 loss 0.0023404499515891075\n",
      "Test: epoch 165 batch 0 loss 0.01049033273011446\n",
      "epoch 165 finished - avarage train loss 0.009059676820219591  avarage test loss 0.012176274321973324\n",
      "Training: epoch 166 batch 0 loss 0.013268212787806988\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training: epoch 166 batch 10 loss 0.010730179026722908\n",
      "Training: epoch 166 batch 20 loss 0.005183397326618433\n",
      "Test: epoch 166 batch 0 loss 0.010932503268122673\n",
      "epoch 166 finished - avarage train loss 0.007182298220116003  avarage test loss 0.012636228580959141\n",
      "Training: epoch 167 batch 0 loss 0.008489005267620087\n",
      "Training: epoch 167 batch 10 loss 0.003961583133786917\n",
      "Training: epoch 167 batch 20 loss 0.004140205681324005\n",
      "Test: epoch 167 batch 0 loss 0.011736867018043995\n",
      "epoch 167 finished - avarage train loss 0.006569893139510833  avarage test loss 0.014456937555223703\n",
      "Training: epoch 168 batch 0 loss 0.008877956308424473\n",
      "Training: epoch 168 batch 10 loss 0.005674684885889292\n",
      "Training: epoch 168 batch 20 loss 0.0048387497663497925\n",
      "Test: epoch 168 batch 0 loss 0.010586191900074482\n",
      "epoch 168 finished - avarage train loss 0.007419825949031731  avarage test loss 0.012674791971221566\n",
      "Training: epoch 169 batch 0 loss 0.0025188017170876265\n",
      "Training: epoch 169 batch 10 loss 0.005232217721641064\n",
      "Training: epoch 169 batch 20 loss 0.010276840068399906\n",
      "Test: epoch 169 batch 0 loss 0.012967108748853207\n",
      "epoch 169 finished - avarage train loss 0.007012862912860924  avarage test loss 0.014782795682549477\n",
      "Training: epoch 170 batch 0 loss 0.0034755757078528404\n",
      "Training: epoch 170 batch 10 loss 0.006123515777289867\n",
      "Training: epoch 170 batch 20 loss 0.00664606923237443\n",
      "Test: epoch 170 batch 0 loss 0.01153711136430502\n",
      "epoch 170 finished - avarage train loss 0.007294640242089999  avarage test loss 0.013296917080879211\n",
      "Training: epoch 171 batch 0 loss 0.0034376203548163176\n",
      "Training: epoch 171 batch 10 loss 0.011610961519181728\n",
      "Training: epoch 171 batch 20 loss 0.004474052228033543\n",
      "Test: epoch 171 batch 0 loss 0.010751770809292793\n",
      "epoch 171 finished - avarage train loss 0.006282520909986362  avarage test loss 0.013511823606677353\n",
      "Training: epoch 172 batch 0 loss 0.008814607746899128\n",
      "Training: epoch 172 batch 10 loss 0.0017551425844430923\n",
      "Training: epoch 172 batch 20 loss 0.01091874297708273\n",
      "Test: epoch 172 batch 0 loss 0.010498112998902798\n",
      "epoch 172 finished - avarage train loss 0.006720115876242775  avarage test loss 0.012570319580845535\n",
      "Training: epoch 173 batch 0 loss 0.005627739708870649\n",
      "Training: epoch 173 batch 10 loss 0.007315360940992832\n",
      "Training: epoch 173 batch 20 loss 0.004320746753364801\n",
      "Test: epoch 173 batch 0 loss 0.011796681210398674\n",
      "epoch 173 finished - avarage train loss 0.006732133377728791  avarage test loss 0.012976094265468419\n",
      "Training: epoch 174 batch 0 loss 0.005419323220849037\n",
      "Training: epoch 174 batch 10 loss 0.00721628637984395\n",
      "Training: epoch 174 batch 20 loss 0.005138670559972525\n",
      "Test: epoch 174 batch 0 loss 0.011541862972080708\n",
      "epoch 174 finished - avarage train loss 0.006942167717578082  avarage test loss 0.013174444669857621\n",
      "Training: epoch 175 batch 0 loss 0.00230574794113636\n",
      "Training: epoch 175 batch 10 loss 0.005707262549549341\n",
      "Training: epoch 175 batch 20 loss 0.005505944136530161\n",
      "Test: epoch 175 batch 0 loss 0.010821685194969177\n",
      "epoch 175 finished - avarage train loss 0.007236852840488327  avarage test loss 0.012432893156073987\n",
      "Training: epoch 176 batch 0 loss 0.005422092042863369\n",
      "Training: epoch 176 batch 10 loss 0.003939818125218153\n",
      "Training: epoch 176 batch 20 loss 0.0043254722841084\n",
      "Test: epoch 176 batch 0 loss 0.01008678786456585\n",
      "epoch 176 finished - avarage train loss 0.00598192876525994  avarage test loss 0.011873790645040572\n",
      "Training: epoch 177 batch 0 loss 0.003280731150880456\n",
      "Training: epoch 177 batch 10 loss 0.0038173315115273\n",
      "Training: epoch 177 batch 20 loss 0.002559385262429714\n",
      "Test: epoch 177 batch 0 loss 0.012075847014784813\n",
      "epoch 177 finished - avarage train loss 0.005919319770202555  avarage test loss 0.013947890838608146\n",
      "Training: epoch 178 batch 0 loss 0.008356154896318913\n",
      "Training: epoch 178 batch 10 loss 0.01030482817441225\n",
      "Training: epoch 178 batch 20 loss 0.0023838705383241177\n",
      "Test: epoch 178 batch 0 loss 0.011924238875508308\n",
      "epoch 178 finished - avarage train loss 0.00812838875271123  avarage test loss 0.013461049064062536\n",
      "Training: epoch 179 batch 0 loss 0.004459315910935402\n",
      "Training: epoch 179 batch 10 loss 0.010259383358061314\n",
      "Training: epoch 179 batch 20 loss 0.0077377548441290855\n",
      "Test: epoch 179 batch 0 loss 0.013725217431783676\n",
      "epoch 179 finished - avarage train loss 0.008924880217568114  avarage test loss 0.014918636996299028\n",
      "Training: epoch 180 batch 0 loss 0.011121946386992931\n",
      "Training: epoch 180 batch 10 loss 0.007719041313976049\n",
      "Training: epoch 180 batch 20 loss 0.013822561129927635\n",
      "Test: epoch 180 batch 0 loss 0.01211567409336567\n",
      "epoch 180 finished - avarage train loss 0.009297512921279874  avarage test loss 0.013702146941795945\n",
      "Training: epoch 181 batch 0 loss 0.007497831247746944\n",
      "Training: epoch 181 batch 10 loss 0.0035165937151759863\n",
      "Training: epoch 181 batch 20 loss 0.004190846811980009\n",
      "Test: epoch 181 batch 0 loss 0.01171304751187563\n",
      "epoch 181 finished - avarage train loss 0.0055858118877071756  avarage test loss 0.013378847506828606\n",
      "Training: epoch 182 batch 0 loss 0.008110824041068554\n",
      "Training: epoch 182 batch 10 loss 0.0029218026902526617\n",
      "Training: epoch 182 batch 20 loss 0.004683342762291431\n",
      "Test: epoch 182 batch 0 loss 0.010951336473226547\n",
      "epoch 182 finished - avarage train loss 0.0076048625713406965  avarage test loss 0.012727539287880063\n",
      "Training: epoch 183 batch 0 loss 0.009481562301516533\n",
      "Training: epoch 183 batch 10 loss 0.009474902413785458\n",
      "Training: epoch 183 batch 20 loss 0.0040611885488033295\n",
      "Test: epoch 183 batch 0 loss 0.012674466706812382\n",
      "epoch 183 finished - avarage train loss 0.00782474805199509  avarage test loss 0.015839525032788515\n",
      "Training: epoch 184 batch 0 loss 0.0070422678254544735\n",
      "Training: epoch 184 batch 10 loss 0.011237962171435356\n",
      "Training: epoch 184 batch 20 loss 0.009061215445399284\n",
      "Test: epoch 184 batch 0 loss 0.010277480818331242\n",
      "epoch 184 finished - avarage train loss 0.009816787847927931  avarage test loss 0.013301998726092279\n",
      "Training: epoch 185 batch 0 loss 0.005759316962212324\n",
      "Training: epoch 185 batch 10 loss 0.014162356965243816\n",
      "Training: epoch 185 batch 20 loss 0.008629783056676388\n",
      "Test: epoch 185 batch 0 loss 0.013909532688558102\n",
      "epoch 185 finished - avarage train loss 0.009653156623244286  avarage test loss 0.015256699058227241\n",
      "Training: epoch 186 batch 0 loss 0.007017871830612421\n",
      "Training: epoch 186 batch 10 loss 0.004520427901297808\n",
      "Training: epoch 186 batch 20 loss 0.00594689603894949\n",
      "Test: epoch 186 batch 0 loss 0.012317840941250324\n",
      "epoch 186 finished - avarage train loss 0.007758390668233664  avarage test loss 0.013594444026239216\n",
      "Training: epoch 187 batch 0 loss 0.005962575785815716\n",
      "Training: epoch 187 batch 10 loss 0.00590178370475769\n",
      "Training: epoch 187 batch 20 loss 0.0036163092590868473\n",
      "Test: epoch 187 batch 0 loss 0.014938604086637497\n",
      "epoch 187 finished - avarage train loss 0.007275346168798619  avarage test loss 0.015474645886570215\n",
      "Training: epoch 188 batch 0 loss 0.004016535356640816\n",
      "Training: epoch 188 batch 10 loss 0.0055244616232812405\n",
      "Training: epoch 188 batch 20 loss 0.004791986662894487\n",
      "Test: epoch 188 batch 0 loss 0.01242822501808405\n",
      "epoch 188 finished - avarage train loss 0.007449341283565195  avarage test loss 0.014186459011398256\n",
      "Training: epoch 189 batch 0 loss 0.005816334392875433\n",
      "Training: epoch 189 batch 10 loss 0.0037939627654850483\n",
      "Training: epoch 189 batch 20 loss 0.007507941219955683\n",
      "Test: epoch 189 batch 0 loss 0.014449549838900566\n",
      "epoch 189 finished - avarage train loss 0.0068301248107233955  avarage test loss 0.015155426226556301\n",
      "Training: epoch 190 batch 0 loss 0.01023256778717041\n",
      "Training: epoch 190 batch 10 loss 0.010741928592324257\n",
      "Training: epoch 190 batch 20 loss 0.006365716457366943\n",
      "Test: epoch 190 batch 0 loss 0.010466114617884159\n",
      "epoch 190 finished - avarage train loss 0.010200342829820925  avarage test loss 0.014942691777832806\n",
      "Training: epoch 191 batch 0 loss 0.011937633156776428\n",
      "Training: epoch 191 batch 10 loss 0.008050872944295406\n",
      "Training: epoch 191 batch 20 loss 0.004786336328834295\n",
      "Test: epoch 191 batch 0 loss 0.010693985037505627\n",
      "epoch 191 finished - avarage train loss 0.008231476543406988  avarage test loss 0.01429060089867562\n",
      "Training: epoch 192 batch 0 loss 0.009509542025625706\n",
      "Training: epoch 192 batch 10 loss 0.0018510743975639343\n",
      "Training: epoch 192 batch 20 loss 0.004529147874563932\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test: epoch 192 batch 0 loss 0.030125439167022705\n",
      "epoch 192 finished - avarage train loss 0.006948837849857478  avarage test loss 0.03711104532703757\n",
      "Training: epoch 193 batch 0 loss 0.0370161272585392\n",
      "Training: epoch 193 batch 10 loss 0.08782871067523956\n",
      "Training: epoch 193 batch 20 loss 0.02633720636367798\n",
      "Test: epoch 193 batch 0 loss 0.02282845787703991\n",
      "epoch 193 finished - avarage train loss 0.04981977865099907  avarage test loss 0.023181235417723656\n",
      "Training: epoch 194 batch 0 loss 0.02370542660355568\n",
      "Training: epoch 194 batch 10 loss 0.023880837485194206\n",
      "Training: epoch 194 batch 20 loss 0.018250424414873123\n",
      "Test: epoch 194 batch 0 loss 0.014257585629820824\n",
      "epoch 194 finished - avarage train loss 0.01860527938296055  avarage test loss 0.017808564472943544\n",
      "Training: epoch 195 batch 0 loss 0.009015602059662342\n",
      "Training: epoch 195 batch 10 loss 0.019328711554408073\n",
      "Training: epoch 195 batch 20 loss 0.018110932782292366\n",
      "Test: epoch 195 batch 0 loss 0.016670051962137222\n",
      "epoch 195 finished - avarage train loss 0.012319561222503925  avarage test loss 0.01832334860228002\n",
      "Training: epoch 196 batch 0 loss 0.004186517093330622\n",
      "Training: epoch 196 batch 10 loss 0.006103852763772011\n",
      "Training: epoch 196 batch 20 loss 0.007247434929013252\n",
      "Test: epoch 196 batch 0 loss 0.01775839738547802\n",
      "epoch 196 finished - avarage train loss 0.010911409132953348  avarage test loss 0.018665439914911985\n",
      "Training: epoch 197 batch 0 loss 0.012721866369247437\n",
      "Training: epoch 197 batch 10 loss 0.010380261577665806\n",
      "Training: epoch 197 batch 20 loss 0.006489915773272514\n",
      "Test: epoch 197 batch 0 loss 0.015237843617796898\n",
      "epoch 197 finished - avarage train loss 0.011515098334900263  avarage test loss 0.01658055349253118\n",
      "Training: epoch 198 batch 0 loss 0.007285455707460642\n",
      "Training: epoch 198 batch 10 loss 0.005446629598736763\n",
      "Training: epoch 198 batch 20 loss 0.009486367926001549\n",
      "Test: epoch 198 batch 0 loss 0.010509435087442398\n",
      "epoch 198 finished - avarage train loss 0.011801301598035056  avarage test loss 0.01735960296355188\n",
      "Training: epoch 199 batch 0 loss 0.010071313939988613\n",
      "Training: epoch 199 batch 10 loss 0.01136522926390171\n",
      "Training: epoch 199 batch 20 loss 0.004755136091262102\n",
      "Test: epoch 199 batch 0 loss 0.015315142460167408\n",
      "epoch 199 finished - avarage train loss 0.008404678552700528  avarage test loss 0.016156203346326947\n"
     ]
    }
   ],
   "source": [
    "median_loss, losses = get_final_median_loss(df, num_trials)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.011459972709417343"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "median_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.011936204391531646,\n",
       " 0.011919335345737636,\n",
       " 0.011285196291282773,\n",
       " 0.012202803627587855,\n",
       " 0.010931709315627813,\n",
       " 0.011369480635039508,\n",
       " 0.011665846221148968,\n",
       " 0.01126380858477205,\n",
       " 0.011459972709417343]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>x2</th>\n",
       "      <th>x1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.199210</td>\n",
       "      <td>-1.242773</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.099672</td>\n",
       "      <td>1.030061</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-1.093608</td>\n",
       "      <td>-0.196601</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.788205</td>\n",
       "      <td>1.525318</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.431947</td>\n",
       "      <td>0.611544</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         x2        x1\n",
       "0 -0.199210 -1.242773\n",
       "1  0.099672  1.030061\n",
       "2 -1.093608 -0.196601\n",
       "3  1.788205  1.525318\n",
       "4  0.431947  0.611544"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[['x2', 'x1']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training: epoch 0 batch 0 loss 0.5293440222740173\n",
      "Training: epoch 0 batch 10 loss 0.6268832087516785\n",
      "Training: epoch 0 batch 20 loss 0.5240335464477539\n",
      "Test: epoch 0 batch 0 loss 0.4775336980819702\n",
      "epoch 0 finished - avarage train loss 0.5224528497663038  avarage test loss 0.5293320342898369\n",
      "Training: epoch 1 batch 0 loss 0.46376171708106995\n",
      "Training: epoch 1 batch 10 loss 0.45183244347572327\n",
      "Training: epoch 1 batch 20 loss 0.6972669363021851\n",
      "Test: epoch 1 batch 0 loss 0.4282788336277008\n",
      "epoch 1 finished - avarage train loss 0.5176612940327875  avarage test loss 0.5128885433077812\n",
      "Training: epoch 2 batch 0 loss 0.44308531284332275\n",
      "Training: epoch 2 batch 10 loss 0.6235655546188354\n",
      "Training: epoch 2 batch 20 loss 0.52065509557724\n",
      "Test: epoch 2 batch 0 loss 0.44362887740135193\n",
      "epoch 2 finished - avarage train loss 0.5215667239550886  avarage test loss 0.5231263786554337\n",
      "Training: epoch 3 batch 0 loss 0.6598615050315857\n",
      "Training: epoch 3 batch 10 loss 0.45495253801345825\n",
      "Training: epoch 3 batch 20 loss 0.5390751361846924\n",
      "Test: epoch 3 batch 0 loss 0.43952110409736633\n",
      "epoch 3 finished - avarage train loss 0.5148164434679623  avarage test loss 0.5212086588144302\n",
      "Training: epoch 4 batch 0 loss 0.5739450454711914\n",
      "Training: epoch 4 batch 10 loss 0.6270421743392944\n",
      "Training: epoch 4 batch 20 loss 0.5614476799964905\n",
      "Test: epoch 4 batch 0 loss 0.42670106887817383\n",
      "epoch 4 finished - avarage train loss 0.5056601392811743  avarage test loss 0.5171499773859978\n",
      "Training: epoch 5 batch 0 loss 0.602340579032898\n",
      "Training: epoch 5 batch 10 loss 0.5443271994590759\n",
      "Training: epoch 5 batch 20 loss 0.652946412563324\n",
      "Test: epoch 5 batch 0 loss 0.41698122024536133\n",
      "epoch 5 finished - avarage train loss 0.5024792043299511  avarage test loss 0.5131180509924889\n",
      "Training: epoch 6 batch 0 loss 0.5581084489822388\n",
      "Training: epoch 6 batch 10 loss 0.4491235315799713\n",
      "Training: epoch 6 batch 20 loss 0.4494321644306183\n",
      "Test: epoch 6 batch 0 loss 0.4252566993236542\n",
      "epoch 6 finished - avarage train loss 0.5146036250837918  avarage test loss 0.5069134756922722\n",
      "Training: epoch 7 batch 0 loss 0.5707193613052368\n",
      "Training: epoch 7 batch 10 loss 0.5566163659095764\n",
      "Training: epoch 7 batch 20 loss 0.6062909364700317\n",
      "Test: epoch 7 batch 0 loss 0.4354490339756012\n",
      "epoch 7 finished - avarage train loss 0.5226516209799668  avarage test loss 0.5231605097651482\n",
      "Training: epoch 8 batch 0 loss 0.5088419318199158\n",
      "Training: epoch 8 batch 10 loss 0.45207664370536804\n",
      "Training: epoch 8 batch 20 loss 0.5311642289161682\n",
      "Test: epoch 8 batch 0 loss 0.43356743454933167\n",
      "epoch 8 finished - avarage train loss 0.5269781042789591  avarage test loss 0.5230886936187744\n",
      "Training: epoch 9 batch 0 loss 0.40706050395965576\n",
      "Training: epoch 9 batch 10 loss 0.44271087646484375\n",
      "Training: epoch 9 batch 20 loss 0.49367570877075195\n",
      "Test: epoch 9 batch 0 loss 0.4275214374065399\n",
      "epoch 9 finished - avarage train loss 0.5075991945020084  avarage test loss 0.5065852850675583\n",
      "Training: epoch 10 batch 0 loss 0.49882417917251587\n",
      "Training: epoch 10 batch 10 loss 0.5900009870529175\n",
      "Training: epoch 10 batch 20 loss 0.33886539936065674\n",
      "Test: epoch 10 batch 0 loss 0.4155884385108948\n",
      "epoch 10 finished - avarage train loss 0.5145798716051825  avarage test loss 0.5031707361340523\n",
      "Training: epoch 11 batch 0 loss 0.5380767583847046\n",
      "Training: epoch 11 batch 10 loss 0.5971953272819519\n",
      "Training: epoch 11 batch 20 loss 0.38957351446151733\n",
      "Test: epoch 11 batch 0 loss 0.425003319978714\n",
      "epoch 11 finished - avarage train loss 0.4986743888464467  avarage test loss 0.5048235654830933\n",
      "Training: epoch 12 batch 0 loss 0.6479243040084839\n",
      "Training: epoch 12 batch 10 loss 0.6467232704162598\n",
      "Training: epoch 12 batch 20 loss 0.4356440305709839\n",
      "Test: epoch 12 batch 0 loss 0.4252757132053375\n",
      "epoch 12 finished - avarage train loss 0.5021201947639728  avarage test loss 0.5098009333014488\n",
      "Training: epoch 13 batch 0 loss 0.4300469756126404\n",
      "Training: epoch 13 batch 10 loss 0.5590668320655823\n",
      "Training: epoch 13 batch 20 loss 0.5917236804962158\n",
      "Test: epoch 13 batch 0 loss 0.4155207574367523\n",
      "epoch 13 finished - avarage train loss 0.5057778563992731  avarage test loss 0.503650926053524\n",
      "Training: epoch 14 batch 0 loss 0.5681018829345703\n",
      "Training: epoch 14 batch 10 loss 0.6349452137947083\n",
      "Training: epoch 14 batch 20 loss 0.6342625021934509\n",
      "Test: epoch 14 batch 0 loss 0.4262300133705139\n",
      "epoch 14 finished - avarage train loss 0.511589253770894  avarage test loss 0.5090366452932358\n",
      "Training: epoch 15 batch 0 loss 0.4624607264995575\n",
      "Training: epoch 15 batch 10 loss 0.5080530643463135\n",
      "Training: epoch 15 batch 20 loss 0.3888091742992401\n",
      "Test: epoch 15 batch 0 loss 0.4241676330566406\n",
      "epoch 15 finished - avarage train loss 0.5066906902296789  avarage test loss 0.5065749958157539\n",
      "Training: epoch 16 batch 0 loss 0.4857688248157501\n",
      "Training: epoch 16 batch 10 loss 0.6901178359985352\n",
      "Training: epoch 16 batch 20 loss 0.6161574125289917\n",
      "Test: epoch 16 batch 0 loss 0.42512667179107666\n",
      "epoch 16 finished - avarage train loss 0.5145540607386622  avarage test loss 0.5050332248210907\n",
      "Training: epoch 17 batch 0 loss 0.44432705640792847\n",
      "Training: epoch 17 batch 10 loss 0.5967875123023987\n",
      "Training: epoch 17 batch 20 loss 0.5121127963066101\n",
      "Test: epoch 17 batch 0 loss 0.4340139329433441\n",
      "epoch 17 finished - avarage train loss 0.5172968455429735  avarage test loss 0.5175513103604317\n",
      "Training: epoch 18 batch 0 loss 0.47661149501800537\n",
      "Training: epoch 18 batch 10 loss 0.4544101655483246\n",
      "Training: epoch 18 batch 20 loss 0.4203166663646698\n",
      "Test: epoch 18 batch 0 loss 0.42450881004333496\n",
      "epoch 18 finished - avarage train loss 0.5192725463160153  avarage test loss 0.5073756277561188\n",
      "Training: epoch 19 batch 0 loss 0.3990466892719269\n",
      "Training: epoch 19 batch 10 loss 0.6015803217887878\n",
      "Training: epoch 19 batch 20 loss 0.6976674199104309\n",
      "Test: epoch 19 batch 0 loss 0.42150574922561646\n",
      "epoch 19 finished - avarage train loss 0.5037440534295707  avarage test loss 0.5079526528716087\n",
      "Training: epoch 20 batch 0 loss 0.5825079083442688\n",
      "Training: epoch 20 batch 10 loss 0.4215944707393646\n",
      "Training: epoch 20 batch 20 loss 0.5190436244010925\n",
      "Test: epoch 20 batch 0 loss 0.3096426725387573\n",
      "epoch 20 finished - avarage train loss 0.48370631244675866  avarage test loss 0.4080798774957657\n",
      "Training: epoch 21 batch 0 loss 0.2896040976047516\n",
      "Training: epoch 21 batch 10 loss 0.2844521999359131\n",
      "Training: epoch 21 batch 20 loss 0.25023743510246277\n",
      "Test: epoch 21 batch 0 loss 0.09643516689538956\n",
      "epoch 21 finished - avarage train loss 0.2502856169795168  avarage test loss 0.16590464673936367\n",
      "Training: epoch 22 batch 0 loss 0.2029801905155182\n",
      "Training: epoch 22 batch 10 loss 0.11066883057355881\n",
      "Training: epoch 22 batch 20 loss 0.08464440703392029\n",
      "Test: epoch 22 batch 0 loss 0.057655200362205505\n",
      "epoch 22 finished - avarage train loss 0.11537108865791354  avarage test loss 0.07766229845583439\n",
      "Training: epoch 23 batch 0 loss 0.07168067246675491\n",
      "Training: epoch 23 batch 10 loss 0.09012816846370697\n",
      "Training: epoch 23 batch 20 loss 0.04265398159623146\n",
      "Test: epoch 23 batch 0 loss 0.03787393122911453\n",
      "epoch 23 finished - avarage train loss 0.04954537556603037  avarage test loss 0.04512758832424879\n",
      "Training: epoch 24 batch 0 loss 0.05079954117536545\n",
      "Training: epoch 24 batch 10 loss 0.034320320934057236\n",
      "Training: epoch 24 batch 20 loss 0.012978214770555496\n",
      "Test: epoch 24 batch 0 loss 0.005926212761551142\n",
      "epoch 24 finished - avarage train loss 0.022810371753213734  avarage test loss 0.016896309214644134\n",
      "Training: epoch 25 batch 0 loss 0.010793942958116531\n",
      "Training: epoch 25 batch 10 loss 0.008328709751367569\n",
      "Training: epoch 25 batch 20 loss 0.00953679345548153\n",
      "Test: epoch 25 batch 0 loss 0.0074036321602761745\n",
      "epoch 25 finished - avarage train loss 0.010796067786627802  avarage test loss 0.01979364932049066\n",
      "Training: epoch 26 batch 0 loss 0.008527765050530434\n",
      "Training: epoch 26 batch 10 loss 0.008855302818119526\n",
      "Training: epoch 26 batch 20 loss 0.01384793035686016\n",
      "Test: epoch 26 batch 0 loss 0.002939181635156274\n",
      "epoch 26 finished - avarage train loss 0.010308395324531814  avarage test loss 0.014823644130956382\n",
      "Training: epoch 27 batch 0 loss 0.003441199893131852\n",
      "Training: epoch 27 batch 10 loss 0.007549291010946035\n",
      "Training: epoch 27 batch 20 loss 0.0038772893603891134\n",
      "Test: epoch 27 batch 0 loss 0.0038045502733439207\n",
      "epoch 27 finished - avarage train loss 0.008663392095858681  avarage test loss 0.013387420389335603\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training: epoch 28 batch 0 loss 0.005791204050183296\n",
      "Training: epoch 28 batch 10 loss 0.009249537251889706\n",
      "Training: epoch 28 batch 20 loss 0.0032975487411022186\n",
      "Test: epoch 28 batch 0 loss 0.006020783446729183\n",
      "epoch 28 finished - avarage train loss 0.008175457000796651  avarage test loss 0.018153004115447402\n",
      "Training: epoch 29 batch 0 loss 0.008491779677569866\n",
      "Training: epoch 29 batch 10 loss 0.0055030519142746925\n",
      "Training: epoch 29 batch 20 loss 0.007952672429382801\n",
      "Test: epoch 29 batch 0 loss 0.0035558755043894053\n",
      "epoch 29 finished - avarage train loss 0.009325813871390861  avarage test loss 0.01634820323670283\n",
      "Training: epoch 30 batch 0 loss 0.006720810197293758\n",
      "Training: epoch 30 batch 10 loss 0.005007367115467787\n",
      "Training: epoch 30 batch 20 loss 0.004300526808947325\n",
      "Test: epoch 30 batch 0 loss 0.0034101635683327913\n",
      "epoch 30 finished - avarage train loss 0.008504314596989545  avarage test loss 0.014756400778423995\n",
      "Training: epoch 31 batch 0 loss 0.005792582407593727\n",
      "Training: epoch 31 batch 10 loss 0.0052839056588709354\n",
      "Training: epoch 31 batch 20 loss 0.006944854278117418\n",
      "Test: epoch 31 batch 0 loss 0.005998274311423302\n",
      "epoch 31 finished - avarage train loss 0.008962079778249407  avarage test loss 0.013949171407148242\n",
      "Training: epoch 32 batch 0 loss 0.006784246768802404\n",
      "Training: epoch 32 batch 10 loss 0.0063958014361560345\n",
      "Training: epoch 32 batch 20 loss 0.006887459196150303\n",
      "Test: epoch 32 batch 0 loss 0.0035697505809366703\n",
      "epoch 32 finished - avarage train loss 0.008243113057687879  avarage test loss 0.014670489123091102\n",
      "Training: epoch 33 batch 0 loss 0.004425778519362211\n",
      "Training: epoch 33 batch 10 loss 0.00729213934391737\n",
      "Training: epoch 33 batch 20 loss 0.008122778497636318\n",
      "Test: epoch 33 batch 0 loss 0.0035225204192101955\n",
      "epoch 33 finished - avarage train loss 0.007460334563049777  avarage test loss 0.013582304702140391\n",
      "Training: epoch 34 batch 0 loss 0.010071856901049614\n",
      "Training: epoch 34 batch 10 loss 0.006912043318152428\n",
      "Training: epoch 34 batch 20 loss 0.006327318027615547\n",
      "Test: epoch 34 batch 0 loss 0.004391816444694996\n",
      "epoch 34 finished - avarage train loss 0.009950574005729166  avarage test loss 0.014032679027877748\n",
      "Training: epoch 35 batch 0 loss 0.0038329942617565393\n",
      "Training: epoch 35 batch 10 loss 0.008254983462393284\n",
      "Training: epoch 35 batch 20 loss 0.009061705321073532\n",
      "Test: epoch 35 batch 0 loss 0.003918684087693691\n",
      "epoch 35 finished - avarage train loss 0.009377199160898554  avarage test loss 0.014998706290498376\n",
      "Training: epoch 36 batch 0 loss 0.0061995950527489185\n",
      "Training: epoch 36 batch 10 loss 0.007623921148478985\n",
      "Training: epoch 36 batch 20 loss 0.010845526121556759\n",
      "Test: epoch 36 batch 0 loss 0.007144354283809662\n",
      "epoch 36 finished - avarage train loss 0.008439595770925797  avarage test loss 0.018380396533757448\n",
      "Training: epoch 37 batch 0 loss 0.006220897193998098\n",
      "Training: epoch 37 batch 10 loss 0.008699744939804077\n",
      "Training: epoch 37 batch 20 loss 0.005922006908804178\n",
      "Test: epoch 37 batch 0 loss 0.0029607154428958893\n",
      "epoch 37 finished - avarage train loss 0.008292628074835601  avarage test loss 0.013091917731799185\n",
      "Training: epoch 38 batch 0 loss 0.008909315802156925\n",
      "Training: epoch 38 batch 10 loss 0.006625928916037083\n",
      "Training: epoch 38 batch 20 loss 0.007586117368191481\n",
      "Test: epoch 38 batch 0 loss 0.00380640453658998\n",
      "epoch 38 finished - avarage train loss 0.008354895210278959  avarage test loss 0.015250030730385333\n",
      "Training: epoch 39 batch 0 loss 0.0034403521567583084\n",
      "Training: epoch 39 batch 10 loss 0.006100090686231852\n",
      "Training: epoch 39 batch 20 loss 0.005516346078366041\n",
      "Test: epoch 39 batch 0 loss 0.003965078387409449\n",
      "epoch 39 finished - avarage train loss 0.01006689611501221  avarage test loss 0.016077272011898458\n",
      "Training: epoch 40 batch 0 loss 0.010208728723227978\n",
      "Training: epoch 40 batch 10 loss 0.012460820376873016\n",
      "Training: epoch 40 batch 20 loss 0.006043017841875553\n",
      "Test: epoch 40 batch 0 loss 0.0036696610040962696\n",
      "epoch 40 finished - avarage train loss 0.008323211182743824  avarage test loss 0.014342535403557122\n",
      "Training: epoch 41 batch 0 loss 0.009172074496746063\n",
      "Training: epoch 41 batch 10 loss 0.008027937263250351\n",
      "Training: epoch 41 batch 20 loss 0.008750241249799728\n",
      "Test: epoch 41 batch 0 loss 0.0038406432140618563\n",
      "epoch 41 finished - avarage train loss 0.00843403762977185  avarage test loss 0.01301229459932074\n",
      "Training: epoch 42 batch 0 loss 0.00650410819798708\n",
      "Training: epoch 42 batch 10 loss 0.010190133936703205\n",
      "Training: epoch 42 batch 20 loss 0.009930160827934742\n",
      "Test: epoch 42 batch 0 loss 0.0067360964603722095\n",
      "epoch 42 finished - avarage train loss 0.008241664313165278  avarage test loss 0.018223482300527394\n",
      "Training: epoch 43 batch 0 loss 0.009178336709737778\n",
      "Training: epoch 43 batch 10 loss 0.010136118158698082\n",
      "Training: epoch 43 batch 20 loss 0.004334027413278818\n",
      "Test: epoch 43 batch 0 loss 0.004646590910851955\n",
      "epoch 43 finished - avarage train loss 0.011163891694540608  avarage test loss 0.013558886363171041\n",
      "Training: epoch 44 batch 0 loss 0.010424752719700336\n",
      "Training: epoch 44 batch 10 loss 0.0078092594631016254\n",
      "Training: epoch 44 batch 20 loss 0.00816990714520216\n",
      "Test: epoch 44 batch 0 loss 0.0049546281807124615\n",
      "epoch 44 finished - avarage train loss 0.01016915193758905  avarage test loss 0.01545185036957264\n",
      "Training: epoch 45 batch 0 loss 0.008293695747852325\n",
      "Training: epoch 45 batch 10 loss 0.00577121414244175\n",
      "Training: epoch 45 batch 20 loss 0.006401608232408762\n",
      "Test: epoch 45 batch 0 loss 0.004401789978146553\n",
      "epoch 45 finished - avarage train loss 0.008727685821338975  avarage test loss 0.01538705441635102\n",
      "Training: epoch 46 batch 0 loss 0.01182333379983902\n",
      "Training: epoch 46 batch 10 loss 0.009877150878310204\n",
      "Training: epoch 46 batch 20 loss 0.018521403893828392\n",
      "Test: epoch 46 batch 0 loss 0.004540319554507732\n",
      "epoch 46 finished - avarage train loss 0.0098135763747168  avarage test loss 0.01408805896062404\n",
      "Training: epoch 47 batch 0 loss 0.014198708347976208\n",
      "Training: epoch 47 batch 10 loss 0.008357042446732521\n",
      "Training: epoch 47 batch 20 loss 0.006798956543207169\n",
      "Test: epoch 47 batch 0 loss 0.0032608904875814915\n",
      "epoch 47 finished - avarage train loss 0.009171373331277021  avarage test loss 0.014055885141715407\n",
      "Training: epoch 48 batch 0 loss 0.008396593853831291\n",
      "Training: epoch 48 batch 10 loss 0.006308373529464006\n",
      "Training: epoch 48 batch 20 loss 0.009173254482448101\n",
      "Test: epoch 48 batch 0 loss 0.004226657096296549\n",
      "epoch 48 finished - avarage train loss 0.0082330956557316  avarage test loss 0.016464557382278144\n",
      "Training: epoch 49 batch 0 loss 0.01144394651055336\n",
      "Training: epoch 49 batch 10 loss 0.00470331497490406\n",
      "Training: epoch 49 batch 20 loss 0.007106799632310867\n",
      "Test: epoch 49 batch 0 loss 0.004403337836265564\n",
      "epoch 49 finished - avarage train loss 0.009213310706525528  avarage test loss 0.01568128098733723\n",
      "Training: epoch 50 batch 0 loss 0.012231433764100075\n",
      "Training: epoch 50 batch 10 loss 0.009275050833821297\n",
      "Training: epoch 50 batch 20 loss 0.010650557465851307\n",
      "Test: epoch 50 batch 0 loss 0.0035488465800881386\n",
      "epoch 50 finished - avarage train loss 0.009957729830759866  avarage test loss 0.014621331822127104\n",
      "Training: epoch 51 batch 0 loss 0.008373278193175793\n",
      "Training: epoch 51 batch 10 loss 0.007785291876643896\n",
      "Training: epoch 51 batch 20 loss 0.009024778380990028\n",
      "Test: epoch 51 batch 0 loss 0.003614414483308792\n",
      "epoch 51 finished - avarage train loss 0.010447761365051928  avarage test loss 0.013851000112481415\n",
      "Training: epoch 52 batch 0 loss 0.007235514931380749\n",
      "Training: epoch 52 batch 10 loss 0.006159236654639244\n",
      "Training: epoch 52 batch 20 loss 0.009157727472484112\n",
      "Test: epoch 52 batch 0 loss 0.005873177200555801\n",
      "epoch 52 finished - avarage train loss 0.009226471458658063  avarage test loss 0.0156147051602602\n",
      "Training: epoch 53 batch 0 loss 0.01572447456419468\n",
      "Training: epoch 53 batch 10 loss 0.008959133177995682\n",
      "Training: epoch 53 batch 20 loss 0.0058597382158041\n",
      "Test: epoch 53 batch 0 loss 0.005666378419846296\n",
      "epoch 53 finished - avarage train loss 0.00912649865294325  avarage test loss 0.013910382869653404\n",
      "Training: epoch 54 batch 0 loss 0.008953113108873367\n",
      "Training: epoch 54 batch 10 loss 0.01242797914892435\n",
      "Training: epoch 54 batch 20 loss 0.0038595320656895638\n",
      "Test: epoch 54 batch 0 loss 0.005846688989549875\n",
      "epoch 54 finished - avarage train loss 0.008872961818144239  avarage test loss 0.015172815532423556\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training: epoch 55 batch 0 loss 0.007301707286387682\n",
      "Training: epoch 55 batch 10 loss 0.010514733381569386\n",
      "Training: epoch 55 batch 20 loss 0.005645349621772766\n",
      "Test: epoch 55 batch 0 loss 0.004382119048386812\n",
      "epoch 55 finished - avarage train loss 0.008697752102181829  avarage test loss 0.013568968395702541\n",
      "Training: epoch 56 batch 0 loss 0.0049989609979093075\n",
      "Training: epoch 56 batch 10 loss 0.010814409703016281\n",
      "Training: epoch 56 batch 20 loss 0.006711605004966259\n",
      "Test: epoch 56 batch 0 loss 0.007709646597504616\n",
      "epoch 56 finished - avarage train loss 0.0078086514218614015  avarage test loss 0.01837832317687571\n",
      "Training: epoch 57 batch 0 loss 0.010523590259253979\n",
      "Training: epoch 57 batch 10 loss 0.012051045894622803\n",
      "Training: epoch 57 batch 20 loss 0.011607416905462742\n",
      "Test: epoch 57 batch 0 loss 0.005980341695249081\n",
      "epoch 57 finished - avarage train loss 0.011649928200219211  avarage test loss 0.014270613668486476\n",
      "Training: epoch 58 batch 0 loss 0.007232956122606993\n",
      "Training: epoch 58 batch 10 loss 0.008981766179203987\n",
      "Training: epoch 58 batch 20 loss 0.005627802107483149\n",
      "Test: epoch 58 batch 0 loss 0.005176224280148745\n",
      "epoch 58 finished - avarage train loss 0.009392079094360614  avarage test loss 0.013170850928872824\n",
      "Training: epoch 59 batch 0 loss 0.004721470642834902\n",
      "Training: epoch 59 batch 10 loss 0.01105900201946497\n",
      "Training: epoch 59 batch 20 loss 0.0065995161421597\n",
      "Test: epoch 59 batch 0 loss 0.007097110152244568\n",
      "epoch 59 finished - avarage train loss 0.007622628432602204  avarage test loss 0.017347741057164967\n",
      "Training: epoch 60 batch 0 loss 0.011410455219447613\n",
      "Training: epoch 60 batch 10 loss 0.0063735926523804665\n",
      "Training: epoch 60 batch 20 loss 0.02056126296520233\n",
      "Test: epoch 60 batch 0 loss 0.006044308189302683\n",
      "epoch 60 finished - avarage train loss 0.011468285855291218  avarage test loss 0.014673041296191514\n",
      "Training: epoch 61 batch 0 loss 0.009407507255673409\n",
      "Training: epoch 61 batch 10 loss 0.0075876424089074135\n",
      "Training: epoch 61 batch 20 loss 0.005062394309788942\n",
      "Test: epoch 61 batch 0 loss 0.0044867489486932755\n",
      "epoch 61 finished - avarage train loss 0.008104214163754007  avarage test loss 0.014347718213684857\n",
      "Training: epoch 62 batch 0 loss 0.010049011558294296\n",
      "Training: epoch 62 batch 10 loss 0.004807730205357075\n",
      "Training: epoch 62 batch 20 loss 0.0083652688190341\n",
      "Test: epoch 62 batch 0 loss 0.004715236369520426\n",
      "epoch 62 finished - avarage train loss 0.008032063744833758  avarage test loss 0.013662299024872482\n",
      "Training: epoch 63 batch 0 loss 0.01123039610683918\n",
      "Training: epoch 63 batch 10 loss 0.006684301421046257\n",
      "Training: epoch 63 batch 20 loss 0.00799629744142294\n",
      "Test: epoch 63 batch 0 loss 0.006356063764542341\n",
      "epoch 63 finished - avarage train loss 0.008371292059470353  avarage test loss 0.01578748703468591\n",
      "Training: epoch 64 batch 0 loss 0.006224148906767368\n",
      "Training: epoch 64 batch 10 loss 0.008754756301641464\n",
      "Training: epoch 64 batch 20 loss 0.012760728597640991\n",
      "Test: epoch 64 batch 0 loss 0.004482030402868986\n",
      "epoch 64 finished - avarage train loss 0.009355471341003632  avarage test loss 0.014199910569004714\n",
      "Training: epoch 65 batch 0 loss 0.005791830364614725\n",
      "Training: epoch 65 batch 10 loss 0.00893705990165472\n",
      "Training: epoch 65 batch 20 loss 0.009753784164786339\n",
      "Test: epoch 65 batch 0 loss 0.004276351071894169\n",
      "epoch 65 finished - avarage train loss 0.008785102933902165  avarage test loss 0.01425938995089382\n",
      "Training: epoch 66 batch 0 loss 0.005252337083220482\n",
      "Training: epoch 66 batch 10 loss 0.006863745395094156\n",
      "Training: epoch 66 batch 20 loss 0.009206719696521759\n",
      "Test: epoch 66 batch 0 loss 0.003638722700998187\n",
      "epoch 66 finished - avarage train loss 0.007787927455300915  avarage test loss 0.013050889072474092\n",
      "Training: epoch 67 batch 0 loss 0.006992613896727562\n",
      "Training: epoch 67 batch 10 loss 0.0058753155171871185\n",
      "Training: epoch 67 batch 20 loss 0.007500743959099054\n",
      "Test: epoch 67 batch 0 loss 0.004124821163713932\n",
      "epoch 67 finished - avarage train loss 0.008278556392881376  avarage test loss 0.012932391022332013\n",
      "Training: epoch 68 batch 0 loss 0.008321997709572315\n",
      "Training: epoch 68 batch 10 loss 0.01134533155709505\n",
      "Training: epoch 68 batch 20 loss 0.004904978442937136\n",
      "Test: epoch 68 batch 0 loss 0.007123807445168495\n",
      "epoch 68 finished - avarage train loss 0.009649351094688835  avarage test loss 0.01668576302472502\n",
      "Training: epoch 69 batch 0 loss 0.010882554575800896\n",
      "Training: epoch 69 batch 10 loss 0.012348835356533527\n",
      "Training: epoch 69 batch 20 loss 0.003906624391674995\n",
      "Test: epoch 69 batch 0 loss 0.003708639182150364\n",
      "epoch 69 finished - avarage train loss 0.009427925925059566  avarage test loss 0.013110620784573257\n",
      "Training: epoch 70 batch 0 loss 0.006317464634776115\n",
      "Training: epoch 70 batch 10 loss 0.009189804084599018\n",
      "Training: epoch 70 batch 20 loss 0.005726308561861515\n",
      "Test: epoch 70 batch 0 loss 0.003941913601011038\n",
      "epoch 70 finished - avarage train loss 0.007849215682403281  avarage test loss 0.013869086862541735\n",
      "Training: epoch 71 batch 0 loss 0.020485853776335716\n",
      "Training: epoch 71 batch 10 loss 0.005556223448365927\n",
      "Training: epoch 71 batch 20 loss 0.006776311434805393\n",
      "Test: epoch 71 batch 0 loss 0.0045289043337106705\n",
      "epoch 71 finished - avarage train loss 0.008971807193653336  avarage test loss 0.013493034522980452\n",
      "Training: epoch 72 batch 0 loss 0.008607983589172363\n",
      "Training: epoch 72 batch 10 loss 0.005582648329436779\n",
      "Training: epoch 72 batch 20 loss 0.0049019912257790565\n",
      "Test: epoch 72 batch 0 loss 0.0038056618068367243\n",
      "epoch 72 finished - avarage train loss 0.007879631601852077  avarage test loss 0.013304478663485497\n",
      "Training: epoch 73 batch 0 loss 0.005063403397798538\n",
      "Training: epoch 73 batch 10 loss 0.009708873927593231\n",
      "Training: epoch 73 batch 20 loss 0.005792011506855488\n",
      "Test: epoch 73 batch 0 loss 0.004648921079933643\n",
      "epoch 73 finished - avarage train loss 0.007589638474044101  avarage test loss 0.014100808650255203\n",
      "Training: epoch 74 batch 0 loss 0.010480286553502083\n",
      "Training: epoch 74 batch 10 loss 0.00354530056938529\n",
      "Training: epoch 74 batch 20 loss 0.007436564192175865\n",
      "Test: epoch 74 batch 0 loss 0.003980905748903751\n",
      "epoch 74 finished - avarage train loss 0.00792989387690764  avarage test loss 0.013745793490670621\n",
      "Training: epoch 75 batch 0 loss 0.007032069843262434\n",
      "Training: epoch 75 batch 10 loss 0.0077221328392624855\n",
      "Training: epoch 75 batch 20 loss 0.003014469286426902\n",
      "Test: epoch 75 batch 0 loss 0.004300110973417759\n",
      "epoch 75 finished - avarage train loss 0.008085868931536016  avarage test loss 0.015061130630783737\n",
      "Training: epoch 76 batch 0 loss 0.008539844304323196\n",
      "Training: epoch 76 batch 10 loss 0.005839963909238577\n",
      "Training: epoch 76 batch 20 loss 0.007740856613963842\n",
      "Test: epoch 76 batch 0 loss 0.008292408660054207\n",
      "epoch 76 finished - avarage train loss 0.008768842266551379  avarage test loss 0.017441606149077415\n",
      "Training: epoch 77 batch 0 loss 0.01707996428012848\n",
      "Training: epoch 77 batch 10 loss 0.007301181089133024\n",
      "Training: epoch 77 batch 20 loss 0.0077794757671654224\n",
      "Test: epoch 77 batch 0 loss 0.0041223797015845776\n",
      "epoch 77 finished - avarage train loss 0.010054560297907427  avarage test loss 0.013049870962277055\n",
      "Training: epoch 78 batch 0 loss 0.006163603626191616\n",
      "Training: epoch 78 batch 10 loss 0.006976969074457884\n",
      "Training: epoch 78 batch 20 loss 0.007355258334428072\n",
      "Test: epoch 78 batch 0 loss 0.0038161366246640682\n",
      "epoch 78 finished - avarage train loss 0.008223192075845497  avarage test loss 0.013403287972323596\n",
      "Training: epoch 79 batch 0 loss 0.009265847504138947\n",
      "Training: epoch 79 batch 10 loss 0.008524764329195023\n",
      "Training: epoch 79 batch 20 loss 0.01160272117704153\n",
      "Test: epoch 79 batch 0 loss 0.00460616173222661\n",
      "epoch 79 finished - avarage train loss 0.008784217816167351  avarage test loss 0.01300839992472902\n",
      "Training: epoch 80 batch 0 loss 0.014379926957190037\n",
      "Training: epoch 80 batch 10 loss 0.014840520918369293\n",
      "Training: epoch 80 batch 20 loss 0.00786377489566803\n",
      "Test: epoch 80 batch 0 loss 0.004912461154162884\n",
      "epoch 80 finished - avarage train loss 0.009580915055141366  avarage test loss 0.013877477264031768\n",
      "Training: epoch 81 batch 0 loss 0.004566183779388666\n",
      "Training: epoch 81 batch 10 loss 0.005938115529716015\n",
      "Training: epoch 81 batch 20 loss 0.004874700680375099\n",
      "Test: epoch 81 batch 0 loss 0.004569020587950945\n",
      "epoch 81 finished - avarage train loss 0.008862848329389918  avarage test loss 0.013296876102685928\n",
      "Training: epoch 82 batch 0 loss 0.002709123305976391\n",
      "Training: epoch 82 batch 10 loss 0.005116429645568132\n",
      "Training: epoch 82 batch 20 loss 0.010151373222470284\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test: epoch 82 batch 0 loss 0.00523136742413044\n",
      "epoch 82 finished - avarage train loss 0.007996657863259315  avarage test loss 0.014731432893313468\n",
      "Training: epoch 83 batch 0 loss 0.007097173947840929\n",
      "Training: epoch 83 batch 10 loss 0.012866683304309845\n",
      "Training: epoch 83 batch 20 loss 0.00964576005935669\n",
      "Test: epoch 83 batch 0 loss 0.006633077748119831\n",
      "epoch 83 finished - avarage train loss 0.008662316956204074  avarage test loss 0.015486346557736397\n",
      "Training: epoch 84 batch 0 loss 0.011023740284144878\n",
      "Training: epoch 84 batch 10 loss 0.004733086097985506\n",
      "Training: epoch 84 batch 20 loss 0.004877698607742786\n",
      "Test: epoch 84 batch 0 loss 0.005893656052649021\n",
      "epoch 84 finished - avarage train loss 0.007982889061857915  avarage test loss 0.014180152444168925\n",
      "Training: epoch 85 batch 0 loss 0.006152545101940632\n",
      "Training: epoch 85 batch 10 loss 0.00558087881654501\n",
      "Training: epoch 85 batch 20 loss 0.005233673378825188\n",
      "Test: epoch 85 batch 0 loss 0.003985796123743057\n",
      "epoch 85 finished - avarage train loss 0.007672599211720557  avarage test loss 0.01391542749479413\n",
      "Training: epoch 86 batch 0 loss 0.005009328480809927\n",
      "Training: epoch 86 batch 10 loss 0.006057048216462135\n",
      "Training: epoch 86 batch 20 loss 0.004904257133603096\n",
      "Test: epoch 86 batch 0 loss 0.0038142986595630646\n",
      "epoch 86 finished - avarage train loss 0.007612435909887326  avarage test loss 0.013035367825068533\n",
      "Training: epoch 87 batch 0 loss 0.005129644647240639\n",
      "Training: epoch 87 batch 10 loss 0.009361037984490395\n",
      "Training: epoch 87 batch 20 loss 0.016355721279978752\n",
      "Test: epoch 87 batch 0 loss 0.004218036308884621\n",
      "epoch 87 finished - avarage train loss 0.007852182675409934  avarage test loss 0.014099236344918609\n",
      "Training: epoch 88 batch 0 loss 0.008600465953350067\n",
      "Training: epoch 88 batch 10 loss 0.005934536457061768\n",
      "Training: epoch 88 batch 20 loss 0.007075862493366003\n",
      "Test: epoch 88 batch 0 loss 0.004755353555083275\n",
      "epoch 88 finished - avarage train loss 0.008732460230460455  avarage test loss 0.0136151920305565\n",
      "Training: epoch 89 batch 0 loss 0.008942143060266972\n",
      "Training: epoch 89 batch 10 loss 0.00859502051025629\n",
      "Training: epoch 89 batch 20 loss 0.008463754318654537\n",
      "Test: epoch 89 batch 0 loss 0.004538476932793856\n",
      "epoch 89 finished - avarage train loss 0.00787055099800486  avarage test loss 0.013785754912532866\n",
      "Training: epoch 90 batch 0 loss 0.0025946462992578745\n",
      "Training: epoch 90 batch 10 loss 0.004058763850480318\n",
      "Training: epoch 90 batch 20 loss 0.013956654816865921\n",
      "Test: epoch 90 batch 0 loss 0.006785125937312841\n",
      "epoch 90 finished - avarage train loss 0.008970216422438108  avarage test loss 0.015480166883207858\n",
      "Training: epoch 91 batch 0 loss 0.008722521364688873\n",
      "Training: epoch 91 batch 10 loss 0.006293646991252899\n",
      "Training: epoch 91 batch 20 loss 0.006974508054554462\n",
      "Test: epoch 91 batch 0 loss 0.007926929742097855\n",
      "epoch 91 finished - avarage train loss 0.008398519266910594  avarage test loss 0.01851075212471187\n",
      "Training: epoch 92 batch 0 loss 0.009471905417740345\n",
      "Training: epoch 92 batch 10 loss 0.015686282888054848\n",
      "Training: epoch 92 batch 20 loss 0.012001395225524902\n",
      "Test: epoch 92 batch 0 loss 0.006624308880418539\n",
      "epoch 92 finished - avarage train loss 0.015657535930774336  avarage test loss 0.01686795603018254\n",
      "Training: epoch 93 batch 0 loss 0.013519160449504852\n",
      "Training: epoch 93 batch 10 loss 0.007317447103559971\n",
      "Training: epoch 93 batch 20 loss 0.0055201356299221516\n",
      "Test: epoch 93 batch 0 loss 0.004813728854060173\n",
      "epoch 93 finished - avarage train loss 0.009310444142540982  avarage test loss 0.013331306749023497\n",
      "Training: epoch 94 batch 0 loss 0.009857545606791973\n",
      "Training: epoch 94 batch 10 loss 0.011704414151608944\n",
      "Training: epoch 94 batch 20 loss 0.008128300309181213\n",
      "Test: epoch 94 batch 0 loss 0.0058473851531744\n",
      "epoch 94 finished - avarage train loss 0.009429064290276888  avarage test loss 0.014527736348100007\n",
      "Training: epoch 95 batch 0 loss 0.006926774512976408\n",
      "Training: epoch 95 batch 10 loss 0.008883791975677013\n",
      "Training: epoch 95 batch 20 loss 0.006209349259734154\n",
      "Test: epoch 95 batch 0 loss 0.004382826387882233\n",
      "epoch 95 finished - avarage train loss 0.008342142473777821  avarage test loss 0.013551834388636053\n",
      "Training: epoch 96 batch 0 loss 0.005616994109004736\n",
      "Training: epoch 96 batch 10 loss 0.007733380887657404\n",
      "Training: epoch 96 batch 20 loss 0.006964369677007198\n",
      "Test: epoch 96 batch 0 loss 0.004239526577293873\n",
      "epoch 96 finished - avarage train loss 0.009761679146824211  avarage test loss 0.013000571983866394\n",
      "Training: epoch 97 batch 0 loss 0.005522137042135\n",
      "Training: epoch 97 batch 10 loss 0.006435679737478495\n",
      "Training: epoch 97 batch 20 loss 0.008282648399472237\n",
      "Test: epoch 97 batch 0 loss 0.005656911991536617\n",
      "epoch 97 finished - avarage train loss 0.008696772109974047  avarage test loss 0.01587218907661736\n",
      "Training: epoch 98 batch 0 loss 0.009117077104747295\n",
      "Training: epoch 98 batch 10 loss 0.0028633615002036095\n",
      "Training: epoch 98 batch 20 loss 0.009312240406870842\n",
      "Test: epoch 98 batch 0 loss 0.005860993638634682\n",
      "epoch 98 finished - avarage train loss 0.009507154451747393  avarage test loss 0.015046002808958292\n",
      "Training: epoch 99 batch 0 loss 0.009902077727019787\n",
      "Training: epoch 99 batch 10 loss 0.009698334150016308\n",
      "Training: epoch 99 batch 20 loss 0.006832721643149853\n",
      "Test: epoch 99 batch 0 loss 0.003935038577765226\n",
      "epoch 99 finished - avarage train loss 0.008387297713037195  avarage test loss 0.013854835764504969\n",
      "Training: epoch 100 batch 0 loss 0.009453753009438515\n",
      "Training: epoch 100 batch 10 loss 0.004663896281272173\n",
      "Training: epoch 100 batch 20 loss 0.007008294574916363\n",
      "Test: epoch 100 batch 0 loss 0.003869389882311225\n",
      "epoch 100 finished - avarage train loss 0.008214771506729824  avarage test loss 0.012857320834882557\n",
      "Training: epoch 101 batch 0 loss 0.006240741349756718\n",
      "Training: epoch 101 batch 10 loss 0.008906785398721695\n",
      "Training: epoch 101 batch 20 loss 0.004446874372661114\n",
      "Test: epoch 101 batch 0 loss 0.004640521481633186\n",
      "epoch 101 finished - avarage train loss 0.008589025827702778  avarage test loss 0.013808519230224192\n",
      "Training: epoch 102 batch 0 loss 0.008416463620960712\n",
      "Training: epoch 102 batch 10 loss 0.008975734934210777\n",
      "Training: epoch 102 batch 20 loss 0.014836692251265049\n",
      "Test: epoch 102 batch 0 loss 0.005355291534215212\n",
      "epoch 102 finished - avarage train loss 0.010330826204655499  avarage test loss 0.013961147400550544\n",
      "Training: epoch 103 batch 0 loss 0.007004696410149336\n",
      "Training: epoch 103 batch 10 loss 0.003876124508678913\n",
      "Training: epoch 103 batch 20 loss 0.005601017270237207\n",
      "Test: epoch 103 batch 0 loss 0.004278842359781265\n",
      "epoch 103 finished - avarage train loss 0.00882050197507287  avarage test loss 0.013254643301479518\n",
      "Training: epoch 104 batch 0 loss 0.00517235929146409\n",
      "Training: epoch 104 batch 10 loss 0.00670628109946847\n",
      "Training: epoch 104 batch 20 loss 0.00486205006018281\n",
      "Test: epoch 104 batch 0 loss 0.004101831000298262\n",
      "epoch 104 finished - avarage train loss 0.007197546804773396  avarage test loss 0.013039317331276834\n",
      "Training: epoch 105 batch 0 loss 0.006326876115053892\n",
      "Training: epoch 105 batch 10 loss 0.006309833377599716\n",
      "Training: epoch 105 batch 20 loss 0.008561675436794758\n",
      "Test: epoch 105 batch 0 loss 0.004349885508418083\n",
      "epoch 105 finished - avarage train loss 0.008317273029864862  avarage test loss 0.013118891161866486\n",
      "Training: epoch 106 batch 0 loss 0.00660546962171793\n",
      "Training: epoch 106 batch 10 loss 0.008920624852180481\n",
      "Training: epoch 106 batch 20 loss 0.0052068778313696384\n",
      "Test: epoch 106 batch 0 loss 0.005914788693189621\n",
      "epoch 106 finished - avarage train loss 0.008651285970198184  avarage test loss 0.014602863346226513\n",
      "Training: epoch 107 batch 0 loss 0.005637595895677805\n",
      "Training: epoch 107 batch 10 loss 0.0074851796962320805\n",
      "Training: epoch 107 batch 20 loss 0.008923668414354324\n",
      "Test: epoch 107 batch 0 loss 0.005078143440186977\n",
      "epoch 107 finished - avarage train loss 0.009077152060665962  avarage test loss 0.014285456389188766\n",
      "Training: epoch 108 batch 0 loss 0.007184646558016539\n",
      "Training: epoch 108 batch 10 loss 0.005922367796301842\n",
      "Training: epoch 108 batch 20 loss 0.008025341667234898\n",
      "Test: epoch 108 batch 0 loss 0.0049009984359145164\n",
      "epoch 108 finished - avarage train loss 0.008660859540747157  avarage test loss 0.013134288368746638\n",
      "Training: epoch 109 batch 0 loss 0.009022812359035015\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training: epoch 109 batch 10 loss 0.003769970964640379\n",
      "Training: epoch 109 batch 20 loss 0.0054791695438325405\n",
      "Test: epoch 109 batch 0 loss 0.004962466191500425\n",
      "epoch 109 finished - avarage train loss 0.009258111344714617  avarage test loss 0.013149886624887586\n",
      "Training: epoch 110 batch 0 loss 0.006849371828138828\n",
      "Training: epoch 110 batch 10 loss 0.01022367738187313\n",
      "Training: epoch 110 batch 20 loss 0.015269486233592033\n",
      "Test: epoch 110 batch 0 loss 0.003693175036460161\n",
      "epoch 110 finished - avarage train loss 0.008589323216663867  avarage test loss 0.01262878708075732\n",
      "Training: epoch 111 batch 0 loss 0.005675601307302713\n",
      "Training: epoch 111 batch 10 loss 0.010704094544053078\n",
      "Training: epoch 111 batch 20 loss 0.010515669360756874\n",
      "Test: epoch 111 batch 0 loss 0.003887514816597104\n",
      "epoch 111 finished - avarage train loss 0.008873836643158876  avarage test loss 0.012871288519818336\n",
      "Training: epoch 112 batch 0 loss 0.006301570683717728\n",
      "Training: epoch 112 batch 10 loss 0.009698281064629555\n",
      "Training: epoch 112 batch 20 loss 0.008786397986114025\n",
      "Test: epoch 112 batch 0 loss 0.004300044849514961\n",
      "epoch 112 finished - avarage train loss 0.008334620518545652  avarage test loss 0.013555199606344104\n",
      "Training: epoch 113 batch 0 loss 0.0042467983439564705\n",
      "Training: epoch 113 batch 10 loss 0.008158858865499496\n",
      "Training: epoch 113 batch 20 loss 0.007388323079794645\n",
      "Test: epoch 113 batch 0 loss 0.004841960035264492\n",
      "epoch 113 finished - avarage train loss 0.00807351345228481  avarage test loss 0.014098165556788445\n",
      "Training: epoch 114 batch 0 loss 0.005729243624955416\n",
      "Training: epoch 114 batch 10 loss 0.00828512478619814\n",
      "Training: epoch 114 batch 20 loss 0.009132545441389084\n",
      "Test: epoch 114 batch 0 loss 0.0051429844461381435\n",
      "epoch 114 finished - avarage train loss 0.009376876112394806  avarage test loss 0.01337772246915847\n",
      "Training: epoch 115 batch 0 loss 0.004844429437071085\n",
      "Training: epoch 115 batch 10 loss 0.005074744578450918\n",
      "Training: epoch 115 batch 20 loss 0.008270157501101494\n",
      "Test: epoch 115 batch 0 loss 0.004780590534210205\n",
      "epoch 115 finished - avarage train loss 0.007667001545557688  avarage test loss 0.01314692897722125\n",
      "Training: epoch 116 batch 0 loss 0.009544271044433117\n",
      "Training: epoch 116 batch 10 loss 0.004758397117257118\n",
      "Training: epoch 116 batch 20 loss 0.014708740636706352\n",
      "Test: epoch 116 batch 0 loss 0.005188923329114914\n",
      "epoch 116 finished - avarage train loss 0.00786350547849875  avarage test loss 0.013724623713642359\n",
      "Training: epoch 117 batch 0 loss 0.002992951078340411\n",
      "Training: epoch 117 batch 10 loss 0.007091757841408253\n",
      "Training: epoch 117 batch 20 loss 0.006053075194358826\n",
      "Test: epoch 117 batch 0 loss 0.008136301301419735\n",
      "epoch 117 finished - avarage train loss 0.008538607387514463  avarage test loss 0.017294076271355152\n",
      "Training: epoch 118 batch 0 loss 0.012656010687351227\n",
      "Training: epoch 118 batch 10 loss 0.017807191237807274\n",
      "Training: epoch 118 batch 20 loss 0.009187444113194942\n",
      "Test: epoch 118 batch 0 loss 0.014049271121621132\n",
      "epoch 118 finished - avarage train loss 0.015218351421685055  avarage test loss 0.02040197537280619\n",
      "Training: epoch 119 batch 0 loss 0.009521406143903732\n",
      "Training: epoch 119 batch 10 loss 0.01807454787194729\n",
      "Training: epoch 119 batch 20 loss 0.007780440617352724\n",
      "Test: epoch 119 batch 0 loss 0.006328735966235399\n",
      "epoch 119 finished - avarage train loss 0.011820444756926134  avarage test loss 0.01390621846076101\n",
      "Training: epoch 120 batch 0 loss 0.008485139347612858\n",
      "Training: epoch 120 batch 10 loss 0.011069695465266705\n",
      "Training: epoch 120 batch 20 loss 0.006553879473358393\n",
      "Test: epoch 120 batch 0 loss 0.007902885787189007\n",
      "epoch 120 finished - avarage train loss 0.008624731426544744  avarage test loss 0.015231690951623023\n",
      "Training: epoch 121 batch 0 loss 0.010136543773114681\n",
      "Training: epoch 121 batch 10 loss 0.00834194291383028\n",
      "Training: epoch 121 batch 20 loss 0.0033833631314337254\n",
      "Test: epoch 121 batch 0 loss 0.0056111873127520084\n",
      "epoch 121 finished - avarage train loss 0.009504681501280645  avarage test loss 0.014050514088012278\n",
      "Training: epoch 122 batch 0 loss 0.007533926051110029\n",
      "Training: epoch 122 batch 10 loss 0.005221416242420673\n",
      "Training: epoch 122 batch 20 loss 0.004988906905055046\n",
      "Test: epoch 122 batch 0 loss 0.006324628368020058\n",
      "epoch 122 finished - avarage train loss 0.008246125067295185  avarage test loss 0.014645059825852513\n",
      "Training: epoch 123 batch 0 loss 0.0060908980667591095\n",
      "Training: epoch 123 batch 10 loss 0.007342916447669268\n",
      "Training: epoch 123 batch 20 loss 0.007605711463838816\n",
      "Test: epoch 123 batch 0 loss 0.006125043146312237\n",
      "epoch 123 finished - avarage train loss 0.008306926838925173  avarage test loss 0.014424781897105277\n",
      "Training: epoch 124 batch 0 loss 0.009410643950104713\n",
      "Training: epoch 124 batch 10 loss 0.006370867136865854\n",
      "Training: epoch 124 batch 20 loss 0.008883430622518063\n",
      "Test: epoch 124 batch 0 loss 0.006252202205359936\n",
      "epoch 124 finished - avarage train loss 0.008508514195423702  avarage test loss 0.01517789566423744\n",
      "Training: epoch 125 batch 0 loss 0.010488185100257397\n",
      "Training: epoch 125 batch 10 loss 0.007466931827366352\n",
      "Training: epoch 125 batch 20 loss 0.004615152254700661\n",
      "Test: epoch 125 batch 0 loss 0.004258111119270325\n",
      "epoch 125 finished - avarage train loss 0.008535003618754703  avarage test loss 0.013065811479464173\n",
      "Training: epoch 126 batch 0 loss 0.005233177915215492\n",
      "Training: epoch 126 batch 10 loss 0.011525675654411316\n",
      "Training: epoch 126 batch 20 loss 0.002452314365655184\n",
      "Test: epoch 126 batch 0 loss 0.005368547048419714\n",
      "epoch 126 finished - avarage train loss 0.008230715901751456  avarage test loss 0.014665233553387225\n",
      "Training: epoch 127 batch 0 loss 0.0048196278512477875\n",
      "Training: epoch 127 batch 10 loss 0.013829856179654598\n",
      "Training: epoch 127 batch 20 loss 0.009093008935451508\n",
      "Test: epoch 127 batch 0 loss 0.00541371013969183\n",
      "epoch 127 finished - avarage train loss 0.009517459206861156  avarage test loss 0.013687581638805568\n",
      "Training: epoch 128 batch 0 loss 0.004314664285629988\n",
      "Training: epoch 128 batch 10 loss 0.005966448225080967\n",
      "Training: epoch 128 batch 20 loss 0.0051715499721467495\n",
      "Test: epoch 128 batch 0 loss 0.0048021129332482815\n",
      "epoch 128 finished - avarage train loss 0.008717856996146769  avarage test loss 0.01311483106110245\n",
      "Training: epoch 129 batch 0 loss 0.003998213913291693\n",
      "Training: epoch 129 batch 10 loss 0.0046821096912026405\n",
      "Training: epoch 129 batch 20 loss 0.008468656800687313\n",
      "Test: epoch 129 batch 0 loss 0.005288706161081791\n",
      "epoch 129 finished - avarage train loss 0.007830630875481614  avarage test loss 0.014311727019958198\n",
      "Training: epoch 130 batch 0 loss 0.0060660215094685555\n",
      "Training: epoch 130 batch 10 loss 0.008525480516254902\n",
      "Training: epoch 130 batch 20 loss 0.01299530453979969\n",
      "Test: epoch 130 batch 0 loss 0.004308256786316633\n",
      "epoch 130 finished - avarage train loss 0.007347551302920128  avarage test loss 0.014950769138522446\n",
      "Training: epoch 131 batch 0 loss 0.006414053495973349\n",
      "Training: epoch 131 batch 10 loss 0.0038555325008928776\n",
      "Training: epoch 131 batch 20 loss 0.008395038545131683\n",
      "Test: epoch 131 batch 0 loss 0.004590943921357393\n",
      "epoch 131 finished - avarage train loss 0.008061929100096739  avarage test loss 0.015889817965216935\n",
      "Training: epoch 132 batch 0 loss 0.008866868913173676\n",
      "Training: epoch 132 batch 10 loss 0.011145005002617836\n",
      "Training: epoch 132 batch 20 loss 0.007286897394806147\n",
      "Test: epoch 132 batch 0 loss 0.005112514831125736\n",
      "epoch 132 finished - avarage train loss 0.008295067023999732  avarage test loss 0.013757665175944567\n",
      "Training: epoch 133 batch 0 loss 0.005149833392351866\n",
      "Training: epoch 133 batch 10 loss 0.0035475052427500486\n",
      "Training: epoch 133 batch 20 loss 0.007733562029898167\n",
      "Test: epoch 133 batch 0 loss 0.005269026383757591\n",
      "epoch 133 finished - avarage train loss 0.008388901667284041  avarage test loss 0.014005252975039184\n",
      "Training: epoch 134 batch 0 loss 0.006031444296240807\n",
      "Training: epoch 134 batch 10 loss 0.016252489760518074\n",
      "Training: epoch 134 batch 20 loss 0.005163211841136217\n",
      "Test: epoch 134 batch 0 loss 0.005934528540819883\n",
      "epoch 134 finished - avarage train loss 0.010008038005566803  avarage test loss 0.015594733296893537\n",
      "Training: epoch 135 batch 0 loss 0.005355166736990213\n",
      "Training: epoch 135 batch 10 loss 0.006865545641630888\n",
      "Training: epoch 135 batch 20 loss 0.004943191073834896\n",
      "Test: epoch 135 batch 0 loss 0.0056188371963799\n",
      "epoch 135 finished - avarage train loss 0.0075512716129165275  avarage test loss 0.013937342446297407\n",
      "Training: epoch 136 batch 0 loss 0.005534821655601263\n",
      "Training: epoch 136 batch 10 loss 0.01043102890253067\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training: epoch 136 batch 20 loss 0.008701851591467857\n",
      "Test: epoch 136 batch 0 loss 0.004296599421650171\n",
      "epoch 136 finished - avarage train loss 0.007991470613292065  avarage test loss 0.013118166360072792\n",
      "Training: epoch 137 batch 0 loss 0.0052534714341163635\n",
      "Training: epoch 137 batch 10 loss 0.010694914497435093\n",
      "Training: epoch 137 batch 20 loss 0.02069183439016342\n",
      "Test: epoch 137 batch 0 loss 0.005077124107629061\n",
      "epoch 137 finished - avarage train loss 0.008729156508142578  avarage test loss 0.013308811467140913\n",
      "Training: epoch 138 batch 0 loss 0.005685857031494379\n",
      "Training: epoch 138 batch 10 loss 0.010262823663651943\n",
      "Training: epoch 138 batch 20 loss 0.006241523195058107\n",
      "Test: epoch 138 batch 0 loss 0.005317665636539459\n",
      "epoch 138 finished - avarage train loss 0.008024882669721184  avarage test loss 0.014076163759455085\n",
      "Training: epoch 139 batch 0 loss 0.006490813102573156\n",
      "Training: epoch 139 batch 10 loss 0.010662194341421127\n",
      "Training: epoch 139 batch 20 loss 0.008205085061490536\n",
      "Test: epoch 139 batch 0 loss 0.005732466466724873\n",
      "epoch 139 finished - avarage train loss 0.00952103407250653  avarage test loss 0.013705471646972\n",
      "Training: epoch 140 batch 0 loss 0.007959146052598953\n",
      "Training: epoch 140 batch 10 loss 0.00484683271497488\n",
      "Training: epoch 140 batch 20 loss 0.010162800550460815\n",
      "Test: epoch 140 batch 0 loss 0.004659152124077082\n",
      "epoch 140 finished - avarage train loss 0.008618679122421247  avarage test loss 0.012928578187711537\n",
      "Training: epoch 141 batch 0 loss 0.0056487517431378365\n",
      "Training: epoch 141 batch 10 loss 0.00590763334184885\n",
      "Training: epoch 141 batch 20 loss 0.004600897431373596\n",
      "Test: epoch 141 batch 0 loss 0.004586572293192148\n",
      "epoch 141 finished - avarage train loss 0.007434199532044345  avarage test loss 0.013305844157002866\n",
      "Training: epoch 142 batch 0 loss 0.00596754252910614\n",
      "Training: epoch 142 batch 10 loss 0.00583975575864315\n",
      "Training: epoch 142 batch 20 loss 0.004542785231024027\n",
      "Test: epoch 142 batch 0 loss 0.003761207452043891\n",
      "epoch 142 finished - avarage train loss 0.008129689143970609  avarage test loss 0.01294028846314177\n",
      "Training: epoch 143 batch 0 loss 0.007632290944457054\n",
      "Training: epoch 143 batch 10 loss 0.005814854055643082\n",
      "Training: epoch 143 batch 20 loss 0.006551519967615604\n",
      "Test: epoch 143 batch 0 loss 0.0041293189860880375\n",
      "epoch 143 finished - avarage train loss 0.007908102301559571  avarage test loss 0.012966787675395608\n",
      "Training: epoch 144 batch 0 loss 0.006274322047829628\n",
      "Training: epoch 144 batch 10 loss 0.006553132086992264\n",
      "Training: epoch 144 batch 20 loss 0.004443924408406019\n",
      "Test: epoch 144 batch 0 loss 0.005740704946219921\n",
      "epoch 144 finished - avarage train loss 0.008273565900865299  avarage test loss 0.013637749711051583\n",
      "Training: epoch 145 batch 0 loss 0.008667352609336376\n",
      "Training: epoch 145 batch 10 loss 0.005081052426248789\n",
      "Training: epoch 145 batch 20 loss 0.006679143290966749\n",
      "Test: epoch 145 batch 0 loss 0.0046847183257341385\n",
      "epoch 145 finished - avarage train loss 0.008651423797910583  avarage test loss 0.013321316218934953\n",
      "Training: epoch 146 batch 0 loss 0.004367891699075699\n",
      "Training: epoch 146 batch 10 loss 0.006215241737663746\n",
      "Training: epoch 146 batch 20 loss 0.011813792400062084\n",
      "Test: epoch 146 batch 0 loss 0.004749961197376251\n",
      "epoch 146 finished - avarage train loss 0.00955823234057632  avarage test loss 0.015059354831464589\n",
      "Training: epoch 147 batch 0 loss 0.009290162473917007\n",
      "Training: epoch 147 batch 10 loss 0.006767823360860348\n",
      "Training: epoch 147 batch 20 loss 0.005822924897074699\n",
      "Test: epoch 147 batch 0 loss 0.003656911663711071\n",
      "epoch 147 finished - avarage train loss 0.00717226366094988  avarage test loss 0.013509735115803778\n",
      "Training: epoch 148 batch 0 loss 0.008751612156629562\n",
      "Training: epoch 148 batch 10 loss 0.010608936659991741\n",
      "Training: epoch 148 batch 20 loss 0.009693220257759094\n",
      "Test: epoch 148 batch 0 loss 0.006769163534045219\n",
      "epoch 148 finished - avarage train loss 0.009022815947838384  avarage test loss 0.01563922897912562\n",
      "Training: epoch 149 batch 0 loss 0.006192113738507032\n",
      "Training: epoch 149 batch 10 loss 0.004939878359436989\n",
      "Training: epoch 149 batch 20 loss 0.004546198528259993\n",
      "Test: epoch 149 batch 0 loss 0.006110544316470623\n",
      "epoch 149 finished - avarage train loss 0.00900159840439928  avarage test loss 0.015952882473357022\n",
      "Training: epoch 150 batch 0 loss 0.010950599797070026\n",
      "Training: epoch 150 batch 10 loss 0.007683733012527227\n",
      "Training: epoch 150 batch 20 loss 0.005782864522188902\n",
      "Test: epoch 150 batch 0 loss 0.004036415368318558\n",
      "epoch 150 finished - avarage train loss 0.009857636659630927  avarage test loss 0.013277062098495662\n",
      "Training: epoch 151 batch 0 loss 0.010508616454899311\n",
      "Training: epoch 151 batch 10 loss 0.008533989079296589\n",
      "Training: epoch 151 batch 20 loss 0.005953651387244463\n",
      "Test: epoch 151 batch 0 loss 0.004267815500497818\n",
      "epoch 151 finished - avarage train loss 0.0066441311741825835  avarage test loss 0.013263594475574791\n",
      "Training: epoch 152 batch 0 loss 0.00987984798848629\n",
      "Training: epoch 152 batch 10 loss 0.0033297280315309763\n",
      "Training: epoch 152 batch 20 loss 0.007916436530649662\n",
      "Test: epoch 152 batch 0 loss 0.0040236786007881165\n",
      "epoch 152 finished - avarage train loss 0.008947540708849656  avarage test loss 0.012924002949148417\n",
      "Training: epoch 153 batch 0 loss 0.00454585300758481\n",
      "Training: epoch 153 batch 10 loss 0.0049408501945436\n",
      "Training: epoch 153 batch 20 loss 0.00805334560573101\n",
      "Test: epoch 153 batch 0 loss 0.004482623655349016\n",
      "epoch 153 finished - avarage train loss 0.00793019627574189  avarage test loss 0.01295273716095835\n",
      "Training: epoch 154 batch 0 loss 0.003111829049885273\n",
      "Training: epoch 154 batch 10 loss 0.004391390830278397\n",
      "Training: epoch 154 batch 20 loss 0.006702299229800701\n",
      "Test: epoch 154 batch 0 loss 0.0051898290403187275\n",
      "epoch 154 finished - avarage train loss 0.007961098273316848  avarage test loss 0.01414362306240946\n",
      "Training: epoch 155 batch 0 loss 0.006993279326707125\n",
      "Training: epoch 155 batch 10 loss 0.005031561944633722\n",
      "Training: epoch 155 batch 20 loss 0.006657544523477554\n",
      "Test: epoch 155 batch 0 loss 0.007473691366612911\n",
      "epoch 155 finished - avarage train loss 0.008470462137383634  avarage test loss 0.01582624006550759\n",
      "Training: epoch 156 batch 0 loss 0.0073004430159926414\n",
      "Training: epoch 156 batch 10 loss 0.007970061153173447\n",
      "Training: epoch 156 batch 20 loss 0.00722392788156867\n",
      "Test: epoch 156 batch 0 loss 0.008092163130640984\n",
      "epoch 156 finished - avarage train loss 0.008925060002968228  avarage test loss 0.018370559439063072\n",
      "Training: epoch 157 batch 0 loss 0.00841678399592638\n",
      "Training: epoch 157 batch 10 loss 0.005007621832191944\n",
      "Training: epoch 157 batch 20 loss 0.005185763351619244\n",
      "Test: epoch 157 batch 0 loss 0.004096046090126038\n",
      "epoch 157 finished - avarage train loss 0.00805577550661461  avarage test loss 0.013412665692158043\n",
      "Training: epoch 158 batch 0 loss 0.010459852404892445\n",
      "Training: epoch 158 batch 10 loss 0.007043271325528622\n",
      "Training: epoch 158 batch 20 loss 0.006839720532298088\n",
      "Test: epoch 158 batch 0 loss 0.004296752158552408\n",
      "epoch 158 finished - avarage train loss 0.00864948374058666  avarage test loss 0.013342020916752517\n",
      "Training: epoch 159 batch 0 loss 0.007307886146008968\n",
      "Training: epoch 159 batch 10 loss 0.005631906911730766\n",
      "Training: epoch 159 batch 20 loss 0.005739476066082716\n",
      "Test: epoch 159 batch 0 loss 0.00414885301142931\n",
      "epoch 159 finished - avarage train loss 0.00789427026656681  avarage test loss 0.013290088041685522\n",
      "Training: epoch 160 batch 0 loss 0.012799754738807678\n",
      "Training: epoch 160 batch 10 loss 0.008341459557414055\n",
      "Training: epoch 160 batch 20 loss 0.00593017740175128\n",
      "Test: epoch 160 batch 0 loss 0.004734461661428213\n",
      "epoch 160 finished - avarage train loss 0.008749986621390643  avarage test loss 0.013437420129776001\n",
      "Training: epoch 161 batch 0 loss 0.0061201550997793674\n",
      "Training: epoch 161 batch 10 loss 0.01467110775411129\n",
      "Training: epoch 161 batch 20 loss 0.00583272660151124\n",
      "Test: epoch 161 batch 0 loss 0.004904557950794697\n",
      "epoch 161 finished - avarage train loss 0.008339031214087174  avarage test loss 0.013481619418598711\n",
      "Training: epoch 162 batch 0 loss 0.004443350713700056\n",
      "Training: epoch 162 batch 10 loss 0.009366273880004883\n",
      "Training: epoch 162 batch 20 loss 0.0057532270438969135\n",
      "Test: epoch 162 batch 0 loss 0.005213610362261534\n",
      "epoch 162 finished - avarage train loss 0.007572565380291179  avarage test loss 0.013769497512839735\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training: epoch 163 batch 0 loss 0.007691794075071812\n",
      "Training: epoch 163 batch 10 loss 0.006308326497673988\n",
      "Training: epoch 163 batch 20 loss 0.006189089268445969\n",
      "Test: epoch 163 batch 0 loss 0.004062384366989136\n",
      "epoch 163 finished - avarage train loss 0.008113132460556668  avarage test loss 0.012848137295804918\n",
      "Training: epoch 164 batch 0 loss 0.008148293942213058\n",
      "Training: epoch 164 batch 10 loss 0.005432106554508209\n",
      "Training: epoch 164 batch 20 loss 0.008176052942872047\n",
      "Test: epoch 164 batch 0 loss 0.005469474010169506\n",
      "epoch 164 finished - avarage train loss 0.007757110320481247  avarage test loss 0.013934784568846226\n",
      "Training: epoch 165 batch 0 loss 0.006771901622414589\n",
      "Training: epoch 165 batch 10 loss 0.004644098691642284\n",
      "Training: epoch 165 batch 20 loss 0.004248298704624176\n",
      "Test: epoch 165 batch 0 loss 0.003876050002872944\n",
      "epoch 165 finished - avarage train loss 0.007698579418376602  avarage test loss 0.01373467780649662\n",
      "Training: epoch 166 batch 0 loss 0.004263767972588539\n",
      "Training: epoch 166 batch 10 loss 0.008678590878844261\n",
      "Training: epoch 166 batch 20 loss 0.005877309013158083\n",
      "Test: epoch 166 batch 0 loss 0.004463355056941509\n",
      "epoch 166 finished - avarage train loss 0.007968132127204845  avarage test loss 0.013689935440197587\n",
      "Training: epoch 167 batch 0 loss 0.010588785633444786\n",
      "Training: epoch 167 batch 10 loss 0.009044271893799305\n",
      "Training: epoch 167 batch 20 loss 0.010532627813518047\n",
      "Test: epoch 167 batch 0 loss 0.0038934098556637764\n",
      "epoch 167 finished - avarage train loss 0.008467735546030876  avarage test loss 0.01291999057866633\n",
      "Training: epoch 168 batch 0 loss 0.009000515565276146\n",
      "Training: epoch 168 batch 10 loss 0.007977300323545933\n",
      "Training: epoch 168 batch 20 loss 0.0027388655580580235\n",
      "Test: epoch 168 batch 0 loss 0.00425101350992918\n",
      "epoch 168 finished - avarage train loss 0.008530495832834778  avarage test loss 0.013515026308596134\n",
      "Training: epoch 169 batch 0 loss 0.010031801648437977\n",
      "Training: epoch 169 batch 10 loss 0.008306851610541344\n",
      "Training: epoch 169 batch 20 loss 0.009073791094124317\n",
      "Test: epoch 169 batch 0 loss 0.0036296139005571604\n",
      "epoch 169 finished - avarage train loss 0.007567035170785826  avarage test loss 0.013349351531360298\n",
      "Training: epoch 170 batch 0 loss 0.010981404222548008\n",
      "Training: epoch 170 batch 10 loss 0.004371174145489931\n",
      "Training: epoch 170 batch 20 loss 0.005953519605100155\n",
      "Test: epoch 170 batch 0 loss 0.0032920727971941233\n",
      "epoch 170 finished - avarage train loss 0.008775328318106717  avarage test loss 0.012972681608516723\n",
      "Training: epoch 171 batch 0 loss 0.004358064383268356\n",
      "Training: epoch 171 batch 10 loss 0.008011551573872566\n",
      "Training: epoch 171 batch 20 loss 0.006725984159857035\n",
      "Test: epoch 171 batch 0 loss 0.0054345508106052876\n",
      "epoch 171 finished - avarage train loss 0.008949552116722896  avarage test loss 0.01921231357846409\n",
      "Training: epoch 172 batch 0 loss 0.012249327264726162\n",
      "Training: epoch 172 batch 10 loss 0.005508147645741701\n",
      "Training: epoch 172 batch 20 loss 0.005452857818454504\n",
      "Test: epoch 172 batch 0 loss 0.005510929971933365\n",
      "epoch 172 finished - avarage train loss 0.007643298390482007  avarage test loss 0.019162589451298118\n",
      "Training: epoch 173 batch 0 loss 0.007318406831473112\n",
      "Training: epoch 173 batch 10 loss 0.008989408612251282\n",
      "Training: epoch 173 batch 20 loss 0.008301887661218643\n",
      "Test: epoch 173 batch 0 loss 0.004305844660848379\n",
      "epoch 173 finished - avarage train loss 0.008410991314027843  avarage test loss 0.013426553457975388\n",
      "Training: epoch 174 batch 0 loss 0.005008342210203409\n",
      "Training: epoch 174 batch 10 loss 0.00672598322853446\n",
      "Training: epoch 174 batch 20 loss 0.003123791888356209\n",
      "Test: epoch 174 batch 0 loss 0.003444147761911154\n",
      "epoch 174 finished - avarage train loss 0.006479101793455152  avarage test loss 0.012874266831204295\n",
      "Training: epoch 175 batch 0 loss 0.013152931816875935\n",
      "Training: epoch 175 batch 10 loss 0.004382675979286432\n",
      "Training: epoch 175 batch 20 loss 0.008665998466312885\n",
      "Test: epoch 175 batch 0 loss 0.004177878610789776\n",
      "epoch 175 finished - avarage train loss 0.008242144442067063  avarage test loss 0.01350615790579468\n",
      "Training: epoch 176 batch 0 loss 0.00927890557795763\n",
      "Training: epoch 176 batch 10 loss 0.007271096110343933\n",
      "Training: epoch 176 batch 20 loss 0.007932613603770733\n",
      "Test: epoch 176 batch 0 loss 0.005806907080113888\n",
      "epoch 176 finished - avarage train loss 0.009232610753127214  avarage test loss 0.01461138529703021\n",
      "Training: epoch 177 batch 0 loss 0.009921060875058174\n",
      "Training: epoch 177 batch 10 loss 0.005957009736448526\n",
      "Training: epoch 177 batch 20 loss 0.00871219765394926\n",
      "Test: epoch 177 batch 0 loss 0.005684723611921072\n",
      "epoch 177 finished - avarage train loss 0.009127902043662194  avarage test loss 0.013794457190670073\n",
      "Training: epoch 178 batch 0 loss 0.005259851925075054\n",
      "Training: epoch 178 batch 10 loss 0.006105329375714064\n",
      "Training: epoch 178 batch 20 loss 0.005794045049697161\n",
      "Test: epoch 178 batch 0 loss 0.0056957220658659935\n",
      "epoch 178 finished - avarage train loss 0.00826779099438211  avarage test loss 0.017574629397131503\n",
      "Training: epoch 179 batch 0 loss 0.0043570175766944885\n",
      "Training: epoch 179 batch 10 loss 0.009331432171165943\n",
      "Training: epoch 179 batch 20 loss 0.011971699073910713\n",
      "Test: epoch 179 batch 0 loss 0.00494573637843132\n",
      "epoch 179 finished - avarage train loss 0.008570721613821285  avarage test loss 0.013852262985892594\n",
      "Training: epoch 180 batch 0 loss 0.005264775827527046\n",
      "Training: epoch 180 batch 10 loss 0.007986615411937237\n",
      "Training: epoch 180 batch 20 loss 0.004270466975867748\n",
      "Test: epoch 180 batch 0 loss 0.0037020347081124783\n",
      "epoch 180 finished - avarage train loss 0.007894237943250558  avarage test loss 0.013096573296934366\n",
      "Training: epoch 181 batch 0 loss 0.0037712466437369585\n",
      "Training: epoch 181 batch 10 loss 0.004714434500783682\n",
      "Training: epoch 181 batch 20 loss 0.006004611495882273\n",
      "Test: epoch 181 batch 0 loss 0.004390362650156021\n",
      "epoch 181 finished - avarage train loss 0.0074461739638756064  avarage test loss 0.013680592062883079\n",
      "Training: epoch 182 batch 0 loss 0.005928654223680496\n",
      "Training: epoch 182 batch 10 loss 0.012659148313105106\n",
      "Training: epoch 182 batch 20 loss 0.006544936448335648\n",
      "Test: epoch 182 batch 0 loss 0.005795912817120552\n",
      "epoch 182 finished - avarage train loss 0.00901802691855821  avarage test loss 0.01484270440414548\n",
      "Training: epoch 183 batch 0 loss 0.009051281958818436\n",
      "Training: epoch 183 batch 10 loss 0.007694073021411896\n",
      "Training: epoch 183 batch 20 loss 0.004908518400043249\n",
      "Test: epoch 183 batch 0 loss 0.006275341846048832\n",
      "epoch 183 finished - avarage train loss 0.008163584613016453  avarage test loss 0.015743786934763193\n",
      "Training: epoch 184 batch 0 loss 0.008420874364674091\n",
      "Training: epoch 184 batch 10 loss 0.007422554772347212\n",
      "Training: epoch 184 batch 20 loss 0.003436763072386384\n",
      "Test: epoch 184 batch 0 loss 0.003684817813336849\n",
      "epoch 184 finished - avarage train loss 0.008413081258085781  avarage test loss 0.014604555675759912\n",
      "Training: epoch 185 batch 0 loss 0.007228867616504431\n",
      "Training: epoch 185 batch 10 loss 0.007650702726095915\n",
      "Training: epoch 185 batch 20 loss 0.004639316350221634\n",
      "Test: epoch 185 batch 0 loss 0.00419026892632246\n",
      "epoch 185 finished - avarage train loss 0.0069912094783423275  avarage test loss 0.01432732050307095\n",
      "Training: epoch 186 batch 0 loss 0.005395490676164627\n",
      "Training: epoch 186 batch 10 loss 0.013173762708902359\n",
      "Training: epoch 186 batch 20 loss 0.00916050560772419\n",
      "Test: epoch 186 batch 0 loss 0.0059225065633654594\n",
      "epoch 186 finished - avarage train loss 0.00854963184623369  avarage test loss 0.015862262342125177\n",
      "Training: epoch 187 batch 0 loss 0.007790613919496536\n",
      "Training: epoch 187 batch 10 loss 0.009280398488044739\n",
      "Training: epoch 187 batch 20 loss 0.00569719634950161\n",
      "Test: epoch 187 batch 0 loss 0.005793694872409105\n",
      "epoch 187 finished - avarage train loss 0.008358394381879219  avarage test loss 0.01761613553389907\n",
      "Training: epoch 188 batch 0 loss 0.0049244435504078865\n",
      "Training: epoch 188 batch 10 loss 0.005667448043823242\n",
      "Training: epoch 188 batch 20 loss 0.00410466967150569\n",
      "Test: epoch 188 batch 0 loss 0.00676422193646431\n",
      "epoch 188 finished - avarage train loss 0.00942543319201675  avarage test loss 0.021513970219530165\n",
      "Training: epoch 189 batch 0 loss 0.005997083615511656\n",
      "Training: epoch 189 batch 10 loss 0.009436533786356449\n",
      "Training: epoch 189 batch 20 loss 0.006343816872686148\n",
      "Test: epoch 189 batch 0 loss 0.008275059051811695\n",
      "epoch 189 finished - avarage train loss 0.007659591605951046  avarage test loss 0.02260547468904406\n",
      "Training: epoch 190 batch 0 loss 0.009621997363865376\n",
      "Training: epoch 190 batch 10 loss 0.011666453443467617\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training: epoch 190 batch 20 loss 0.007868722081184387\n",
      "Test: epoch 190 batch 0 loss 0.007521740160882473\n",
      "epoch 190 finished - avarage train loss 0.008880324260299576  avarage test loss 0.020299173891544342\n",
      "Training: epoch 191 batch 0 loss 0.009060378186404705\n",
      "Training: epoch 191 batch 10 loss 0.009607413783669472\n",
      "Training: epoch 191 batch 20 loss 0.010686575435101986\n",
      "Test: epoch 191 batch 0 loss 0.006675742566585541\n",
      "epoch 191 finished - avarage train loss 0.01308788596813021  avarage test loss 0.01733887125737965\n",
      "Training: epoch 192 batch 0 loss 0.007280596531927586\n",
      "Training: epoch 192 batch 10 loss 0.007708217017352581\n",
      "Training: epoch 192 batch 20 loss 0.0033848548773676157\n",
      "Test: epoch 192 batch 0 loss 0.004817230626940727\n",
      "epoch 192 finished - avarage train loss 0.008893011137843132  avarage test loss 0.013117702677845955\n",
      "Training: epoch 193 batch 0 loss 0.004731409251689911\n",
      "Training: epoch 193 batch 10 loss 0.006226290948688984\n",
      "Training: epoch 193 batch 20 loss 0.0064855399541556835\n",
      "Test: epoch 193 batch 0 loss 0.004114625509828329\n",
      "epoch 193 finished - avarage train loss 0.006521720190307704  avarage test loss 0.012835992150940001\n",
      "Training: epoch 194 batch 0 loss 0.00825186725705862\n",
      "Training: epoch 194 batch 10 loss 0.005701282527297735\n",
      "Training: epoch 194 batch 20 loss 0.006751345004886389\n",
      "Test: epoch 194 batch 0 loss 0.005204828456044197\n",
      "epoch 194 finished - avarage train loss 0.006987453494542118  avarage test loss 0.014282142161391675\n",
      "Training: epoch 195 batch 0 loss 0.00634285993874073\n",
      "Training: epoch 195 batch 10 loss 0.01202708575874567\n",
      "Training: epoch 195 batch 20 loss 0.008022120222449303\n",
      "Test: epoch 195 batch 0 loss 0.007099427282810211\n",
      "epoch 195 finished - avarage train loss 0.011478392535756374  avarage test loss 0.018606253433972597\n",
      "Training: epoch 196 batch 0 loss 0.008978147059679031\n",
      "Training: epoch 196 batch 10 loss 0.006068794056773186\n",
      "Training: epoch 196 batch 20 loss 0.012942525558173656\n",
      "Test: epoch 196 batch 0 loss 0.004677135031670332\n",
      "epoch 196 finished - avarage train loss 0.008392514420095188  avarage test loss 0.014095099410042167\n",
      "Training: epoch 197 batch 0 loss 0.00444043381139636\n",
      "Training: epoch 197 batch 10 loss 0.00840071588754654\n",
      "Training: epoch 197 batch 20 loss 0.005910995416343212\n",
      "Test: epoch 197 batch 0 loss 0.006053997669368982\n",
      "epoch 197 finished - avarage train loss 0.007747097053276053  avarage test loss 0.020717499661259353\n",
      "Training: epoch 198 batch 0 loss 0.007313392590731382\n",
      "Training: epoch 198 batch 10 loss 0.004390507470816374\n",
      "Training: epoch 198 batch 20 loss 0.013753265142440796\n",
      "Test: epoch 198 batch 0 loss 0.004723254591226578\n",
      "epoch 198 finished - avarage train loss 0.007676093574163729  avarage test loss 0.013994654174894094\n",
      "Training: epoch 199 batch 0 loss 0.00874752551317215\n",
      "Training: epoch 199 batch 10 loss 0.00663856090977788\n",
      "Training: epoch 199 batch 20 loss 0.008627263829112053\n",
      "Test: epoch 199 batch 0 loss 0.004434663336724043\n",
      "epoch 199 finished - avarage train loss 0.007508419052665603  avarage test loss 0.01359630806837231\n",
      "Training: epoch 0 batch 0 loss 0.5853729844093323\n",
      "Training: epoch 0 batch 10 loss 0.5296275019645691\n",
      "Training: epoch 0 batch 20 loss 0.5462851524353027\n",
      "Test: epoch 0 batch 0 loss 0.41759803891181946\n",
      "epoch 0 finished - avarage train loss 0.5375005335643374  avarage test loss 0.5203802138566971\n",
      "Training: epoch 1 batch 0 loss 0.392652690410614\n",
      "Training: epoch 1 batch 10 loss 0.7544998526573181\n",
      "Training: epoch 1 batch 20 loss 0.5935722589492798\n",
      "Test: epoch 1 batch 0 loss 0.4222927987575531\n",
      "epoch 1 finished - avarage train loss 0.534204456312903  avarage test loss 0.5103442743420601\n",
      "Training: epoch 2 batch 0 loss 0.5386241674423218\n",
      "Training: epoch 2 batch 10 loss 0.5100141763687134\n",
      "Training: epoch 2 batch 20 loss 0.4704626500606537\n",
      "Test: epoch 2 batch 0 loss 0.41546061635017395\n",
      "epoch 2 finished - avarage train loss 0.5097455176813849  avarage test loss 0.504950761795044\n",
      "Training: epoch 3 batch 0 loss 0.585946798324585\n",
      "Training: epoch 3 batch 10 loss 0.5834425687789917\n",
      "Training: epoch 3 batch 20 loss 0.5401694774627686\n",
      "Test: epoch 3 batch 0 loss 0.42760637402534485\n",
      "epoch 3 finished - avarage train loss 0.5149214216347399  avarage test loss 0.5143917351961136\n",
      "Training: epoch 4 batch 0 loss 0.40149059891700745\n",
      "Training: epoch 4 batch 10 loss 0.5503549575805664\n",
      "Training: epoch 4 batch 20 loss 0.6481829285621643\n",
      "Test: epoch 4 batch 0 loss 0.42617329955101013\n",
      "epoch 4 finished - avarage train loss 0.5253047460112078  avarage test loss 0.5094475671648979\n",
      "Training: epoch 5 batch 0 loss 0.7264875769615173\n",
      "Training: epoch 5 batch 10 loss 0.4592122435569763\n",
      "Training: epoch 5 batch 20 loss 0.3610927164554596\n",
      "Test: epoch 5 batch 0 loss 0.42725998163223267\n",
      "epoch 5 finished - avarage train loss 0.5163195729255676  avarage test loss 0.5030867159366608\n",
      "Training: epoch 6 batch 0 loss 0.6045952439308167\n",
      "Training: epoch 6 batch 10 loss 0.4252629578113556\n",
      "Training: epoch 6 batch 20 loss 0.5578004717826843\n",
      "Test: epoch 6 batch 0 loss 0.41737598180770874\n",
      "epoch 6 finished - avarage train loss 0.5091674225083713  avarage test loss 0.5020845383405685\n",
      "Training: epoch 7 batch 0 loss 0.5271771550178528\n",
      "Training: epoch 7 batch 10 loss 0.49960842728614807\n",
      "Training: epoch 7 batch 20 loss 0.5015349984169006\n",
      "Test: epoch 7 batch 0 loss 0.20343826711177826\n",
      "epoch 7 finished - avarage train loss 0.4864761171669796  avarage test loss 0.2593109495937824\n",
      "Training: epoch 8 batch 0 loss 0.25629329681396484\n",
      "Training: epoch 8 batch 10 loss 0.05350775271654129\n",
      "Training: epoch 8 batch 20 loss 0.031980231404304504\n",
      "Test: epoch 8 batch 0 loss 0.029932931065559387\n",
      "epoch 8 finished - avarage train loss 0.0736874311134733  avarage test loss 0.03648209758102894\n",
      "Training: epoch 9 batch 0 loss 0.033455245196819305\n",
      "Training: epoch 9 batch 10 loss 0.01804647408425808\n",
      "Training: epoch 9 batch 20 loss 0.017095038667321205\n",
      "Test: epoch 9 batch 0 loss 0.01834193244576454\n",
      "epoch 9 finished - avarage train loss 0.025617101807789557  avarage test loss 0.024766084272414446\n",
      "Training: epoch 10 batch 0 loss 0.02617480978369713\n",
      "Training: epoch 10 batch 10 loss 0.022765884175896645\n",
      "Training: epoch 10 batch 20 loss 0.023682473227381706\n",
      "Test: epoch 10 batch 0 loss 0.015743374824523926\n",
      "epoch 10 finished - avarage train loss 0.022448577629081135  avarage test loss 0.023115884978324175\n",
      "Training: epoch 11 batch 0 loss 0.01437306497246027\n",
      "Training: epoch 11 batch 10 loss 0.01788720116019249\n",
      "Training: epoch 11 batch 20 loss 0.021348582580685616\n",
      "Test: epoch 11 batch 0 loss 0.01104445569217205\n",
      "epoch 11 finished - avarage train loss 0.01892117272805551  avarage test loss 0.020150663098320365\n",
      "Training: epoch 12 batch 0 loss 0.01789628528058529\n",
      "Training: epoch 12 batch 10 loss 0.018847549334168434\n",
      "Training: epoch 12 batch 20 loss 0.01611124537885189\n",
      "Test: epoch 12 batch 0 loss 0.01033602561801672\n",
      "epoch 12 finished - avarage train loss 0.01594586268966568  avarage test loss 0.01784852030687034\n",
      "Training: epoch 13 batch 0 loss 0.022218653932213783\n",
      "Training: epoch 13 batch 10 loss 0.021462038159370422\n",
      "Training: epoch 13 batch 20 loss 0.009194610640406609\n",
      "Test: epoch 13 batch 0 loss 0.0073264529928565025\n",
      "epoch 13 finished - avarage train loss 0.014105650564206058  avarage test loss 0.016989528900012374\n",
      "Training: epoch 14 batch 0 loss 0.008295283652842045\n",
      "Training: epoch 14 batch 10 loss 0.020615359768271446\n",
      "Training: epoch 14 batch 20 loss 0.010263609699904919\n",
      "Test: epoch 14 batch 0 loss 0.008739381097257137\n",
      "epoch 14 finished - avarage train loss 0.012716550974111104  avarage test loss 0.01680595730431378\n",
      "Training: epoch 15 batch 0 loss 0.01418809499591589\n",
      "Training: epoch 15 batch 10 loss 0.011196194216609001\n",
      "Training: epoch 15 batch 20 loss 0.004882325418293476\n",
      "Test: epoch 15 batch 0 loss 0.007730567362159491\n",
      "epoch 15 finished - avarage train loss 0.010152477883444777  avarage test loss 0.01649913494475186\n",
      "Training: epoch 16 batch 0 loss 0.00603687996044755\n",
      "Training: epoch 16 batch 10 loss 0.012496959418058395\n",
      "Training: epoch 16 batch 20 loss 0.010145772248506546\n",
      "Test: epoch 16 batch 0 loss 0.008956337347626686\n",
      "epoch 16 finished - avarage train loss 0.01184757487398797  avarage test loss 0.017177298199385405\n",
      "Training: epoch 17 batch 0 loss 0.010104220360517502\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training: epoch 17 batch 10 loss 0.009651171043515205\n",
      "Training: epoch 17 batch 20 loss 0.012717073783278465\n",
      "Test: epoch 17 batch 0 loss 0.008892903104424477\n",
      "epoch 17 finished - avarage train loss 0.01009348898740678  avarage test loss 0.01724818500224501\n",
      "Training: epoch 18 batch 0 loss 0.004005901049822569\n",
      "Training: epoch 18 batch 10 loss 0.009467856027185917\n",
      "Training: epoch 18 batch 20 loss 0.0075819590128958225\n",
      "Test: epoch 18 batch 0 loss 0.008222747594118118\n",
      "epoch 18 finished - avarage train loss 0.010946444653231522  avarage test loss 0.016579750808887184\n",
      "Training: epoch 19 batch 0 loss 0.006400011945515871\n",
      "Training: epoch 19 batch 10 loss 0.00631409278139472\n",
      "Training: epoch 19 batch 20 loss 0.007074066437780857\n",
      "Test: epoch 19 batch 0 loss 0.005647680722177029\n",
      "epoch 19 finished - avarage train loss 0.009170561595338172  avarage test loss 0.01621404872275889\n",
      "Training: epoch 20 batch 0 loss 0.00649871863424778\n",
      "Training: epoch 20 batch 10 loss 0.004987779539078474\n",
      "Training: epoch 20 batch 20 loss 0.01454975362867117\n",
      "Test: epoch 20 batch 0 loss 0.006552649196237326\n",
      "epoch 20 finished - avarage train loss 0.008534046933697215  avarage test loss 0.016638781991787255\n",
      "Training: epoch 21 batch 0 loss 0.010532999411225319\n",
      "Training: epoch 21 batch 10 loss 0.012601920403540134\n",
      "Training: epoch 21 batch 20 loss 0.0063150906935334206\n",
      "Test: epoch 21 batch 0 loss 0.008380495943129063\n",
      "epoch 21 finished - avarage train loss 0.008597119093133971  avarage test loss 0.016387765062972903\n",
      "Training: epoch 22 batch 0 loss 0.007610412314534187\n",
      "Training: epoch 22 batch 10 loss 0.007281912956386805\n",
      "Training: epoch 22 batch 20 loss 0.007283554878085852\n",
      "Test: epoch 22 batch 0 loss 0.006922806613147259\n",
      "epoch 22 finished - avarage train loss 0.009395581421217528  avarage test loss 0.015485147014260292\n",
      "Training: epoch 23 batch 0 loss 0.00794855784624815\n",
      "Training: epoch 23 batch 10 loss 0.012260595336556435\n",
      "Training: epoch 23 batch 20 loss 0.01269494742155075\n",
      "Test: epoch 23 batch 0 loss 0.007592559792101383\n",
      "epoch 23 finished - avarage train loss 0.008654036310276595  avarage test loss 0.016912720748223364\n",
      "Training: epoch 24 batch 0 loss 0.0056054359301924706\n",
      "Training: epoch 24 batch 10 loss 0.0075115421786904335\n",
      "Training: epoch 24 batch 20 loss 0.009382696822285652\n",
      "Test: epoch 24 batch 0 loss 0.0069583929143846035\n",
      "epoch 24 finished - avarage train loss 0.008474217623407984  avarage test loss 0.016056887223385274\n",
      "Training: epoch 25 batch 0 loss 0.0042868442833423615\n",
      "Training: epoch 25 batch 10 loss 0.005167090333998203\n",
      "Training: epoch 25 batch 20 loss 0.005846960470080376\n",
      "Test: epoch 25 batch 0 loss 0.006017426960170269\n",
      "epoch 25 finished - avarage train loss 0.007151312877199259  avarage test loss 0.015141630312427878\n",
      "Training: epoch 26 batch 0 loss 0.007645605131983757\n",
      "Training: epoch 26 batch 10 loss 0.004355240613222122\n",
      "Training: epoch 26 batch 20 loss 0.006592604797333479\n",
      "Test: epoch 26 batch 0 loss 0.0058884331956505775\n",
      "epoch 26 finished - avarage train loss 0.008019175982616585  avarage test loss 0.01553601457271725\n",
      "Training: epoch 27 batch 0 loss 0.008303862996399403\n",
      "Training: epoch 27 batch 10 loss 0.00887781847268343\n",
      "Training: epoch 27 batch 20 loss 0.006060855463147163\n",
      "Test: epoch 27 batch 0 loss 0.011704759672284126\n",
      "epoch 27 finished - avarage train loss 0.008472674606560633  avarage test loss 0.022935799788683653\n",
      "Training: epoch 28 batch 0 loss 0.014335855841636658\n",
      "Training: epoch 28 batch 10 loss 0.019396604970097542\n",
      "Training: epoch 28 batch 20 loss 0.006643342785537243\n",
      "Test: epoch 28 batch 0 loss 0.015769172459840775\n",
      "epoch 28 finished - avarage train loss 0.013493836247201624  avarage test loss 0.02402572426944971\n",
      "Training: epoch 29 batch 0 loss 0.010547582991421223\n",
      "Training: epoch 29 batch 10 loss 0.008519914932549\n",
      "Training: epoch 29 batch 20 loss 0.0033893391955643892\n",
      "Test: epoch 29 batch 0 loss 0.01057820487767458\n",
      "epoch 29 finished - avarage train loss 0.009126809481852528  avarage test loss 0.016932104248553514\n",
      "Training: epoch 30 batch 0 loss 0.01109212078154087\n",
      "Training: epoch 30 batch 10 loss 0.007501055486500263\n",
      "Training: epoch 30 batch 20 loss 0.006426698062568903\n",
      "Test: epoch 30 batch 0 loss 0.004461624193936586\n",
      "epoch 30 finished - avarage train loss 0.009816443932981327  avarage test loss 0.015584484790451825\n",
      "Training: epoch 31 batch 0 loss 0.00760178966447711\n",
      "Training: epoch 31 batch 10 loss 0.014605616219341755\n",
      "Training: epoch 31 batch 20 loss 0.005468994379043579\n",
      "Test: epoch 31 batch 0 loss 0.004912082571536303\n",
      "epoch 31 finished - avarage train loss 0.008032340893586135  avarage test loss 0.014026265009306371\n",
      "Training: epoch 32 batch 0 loss 0.005699189379811287\n",
      "Training: epoch 32 batch 10 loss 0.005628264509141445\n",
      "Training: epoch 32 batch 20 loss 0.004848166834563017\n",
      "Test: epoch 32 batch 0 loss 0.005130277015268803\n",
      "epoch 32 finished - avarage train loss 0.007708638617447738  avarage test loss 0.015146438381634653\n",
      "Training: epoch 33 batch 0 loss 0.007634500041604042\n",
      "Training: epoch 33 batch 10 loss 0.006715708877891302\n",
      "Training: epoch 33 batch 20 loss 0.008459503762423992\n",
      "Test: epoch 33 batch 0 loss 0.0041114334017038345\n",
      "epoch 33 finished - avarage train loss 0.007629202229195628  avarage test loss 0.015085729071870446\n",
      "Training: epoch 34 batch 0 loss 0.0037692543119192123\n",
      "Training: epoch 34 batch 10 loss 0.005416201427578926\n",
      "Training: epoch 34 batch 20 loss 0.012002945877611637\n",
      "Test: epoch 34 batch 0 loss 0.0036482566501945257\n",
      "epoch 34 finished - avarage train loss 0.007261937548374307  avarage test loss 0.014939745480660349\n",
      "Training: epoch 35 batch 0 loss 0.0054741366766393185\n",
      "Training: epoch 35 batch 10 loss 0.00740025844424963\n",
      "Training: epoch 35 batch 20 loss 0.008300327695906162\n",
      "Test: epoch 35 batch 0 loss 0.0043175057508051395\n",
      "epoch 35 finished - avarage train loss 0.0077625942288030835  avarage test loss 0.015506202820688486\n",
      "Training: epoch 36 batch 0 loss 0.007665728218853474\n",
      "Training: epoch 36 batch 10 loss 0.004914020653814077\n",
      "Training: epoch 36 batch 20 loss 0.005024676211178303\n",
      "Test: epoch 36 batch 0 loss 0.004900711588561535\n",
      "epoch 36 finished - avarage train loss 0.008044429934294573  avarage test loss 0.015780683374032378\n",
      "Training: epoch 37 batch 0 loss 0.007467808201909065\n",
      "Training: epoch 37 batch 10 loss 0.006914892699569464\n",
      "Training: epoch 37 batch 20 loss 0.00518295681104064\n",
      "Test: epoch 37 batch 0 loss 0.006102524232119322\n",
      "epoch 37 finished - avarage train loss 0.008959768155332783  avarage test loss 0.01769875327590853\n",
      "Training: epoch 38 batch 0 loss 0.007817419245839119\n",
      "Training: epoch 38 batch 10 loss 0.006556478329002857\n",
      "Training: epoch 38 batch 20 loss 0.008066202513873577\n",
      "Test: epoch 38 batch 0 loss 0.004667157307267189\n",
      "epoch 38 finished - avarage train loss 0.008196052203982555  avarage test loss 0.014974693302065134\n",
      "Training: epoch 39 batch 0 loss 0.007812116295099258\n",
      "Training: epoch 39 batch 10 loss 0.006411585491150618\n",
      "Training: epoch 39 batch 20 loss 0.0044930302537977695\n",
      "Test: epoch 39 batch 0 loss 0.003627475118264556\n",
      "epoch 39 finished - avarage train loss 0.008079868243557626  avarage test loss 0.015389802574645728\n",
      "Training: epoch 40 batch 0 loss 0.00986553356051445\n",
      "Training: epoch 40 batch 10 loss 0.007257026620209217\n",
      "Training: epoch 40 batch 20 loss 0.010861511342227459\n",
      "Test: epoch 40 batch 0 loss 0.003842822276055813\n",
      "epoch 40 finished - avarage train loss 0.00803249855204646  avarage test loss 0.01594513247255236\n",
      "Training: epoch 41 batch 0 loss 0.00800305139273405\n",
      "Training: epoch 41 batch 10 loss 0.004707024898380041\n",
      "Training: epoch 41 batch 20 loss 0.008290667086839676\n",
      "Test: epoch 41 batch 0 loss 0.003218860598281026\n",
      "epoch 41 finished - avarage train loss 0.008262501660220582  avarage test loss 0.01442016480723396\n",
      "Training: epoch 42 batch 0 loss 0.00479087932035327\n",
      "Training: epoch 42 batch 10 loss 0.008698748424649239\n",
      "Training: epoch 42 batch 20 loss 0.008369858376681805\n",
      "Test: epoch 42 batch 0 loss 0.00435086851939559\n",
      "epoch 42 finished - avarage train loss 0.007382718091509466  avarage test loss 0.015194320934824646\n",
      "Training: epoch 43 batch 0 loss 0.009167451411485672\n",
      "Training: epoch 43 batch 10 loss 0.005706686060875654\n",
      "Training: epoch 43 batch 20 loss 0.008344046771526337\n",
      "Test: epoch 43 batch 0 loss 0.00355997821316123\n",
      "epoch 43 finished - avarage train loss 0.008279208479256466  avarage test loss 0.015198982669971883\n",
      "Training: epoch 44 batch 0 loss 0.004893424455076456\n",
      "Training: epoch 44 batch 10 loss 0.010888670571148396\n",
      "Training: epoch 44 batch 20 loss 0.006573189049959183\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test: epoch 44 batch 0 loss 0.0031384185422211885\n",
      "epoch 44 finished - avarage train loss 0.007123454738857931  avarage test loss 0.015098688949365169\n",
      "Training: epoch 45 batch 0 loss 0.00539800338447094\n",
      "Training: epoch 45 batch 10 loss 0.004239120054990053\n",
      "Training: epoch 45 batch 20 loss 0.007694003637880087\n",
      "Test: epoch 45 batch 0 loss 0.003135927487164736\n",
      "epoch 45 finished - avarage train loss 0.008204721628913078  avarage test loss 0.014607248362153769\n",
      "Training: epoch 46 batch 0 loss 0.003698354121297598\n",
      "Training: epoch 46 batch 10 loss 0.005601851269602776\n",
      "Training: epoch 46 batch 20 loss 0.009561946615576744\n",
      "Test: epoch 46 batch 0 loss 0.0051267268136143684\n",
      "epoch 46 finished - avarage train loss 0.00797817047589041  avarage test loss 0.01758128753863275\n",
      "Training: epoch 47 batch 0 loss 0.0059083811938762665\n",
      "Training: epoch 47 batch 10 loss 0.00591032300144434\n",
      "Training: epoch 47 batch 20 loss 0.011022979393601418\n",
      "Test: epoch 47 batch 0 loss 0.01180053036659956\n",
      "epoch 47 finished - avarage train loss 0.009207655699795177  avarage test loss 0.016784142469987273\n",
      "Training: epoch 48 batch 0 loss 0.014307031407952309\n",
      "Training: epoch 48 batch 10 loss 0.005742758512496948\n",
      "Training: epoch 48 batch 20 loss 0.004632248543202877\n",
      "Test: epoch 48 batch 0 loss 0.003083096584305167\n",
      "epoch 48 finished - avarage train loss 0.009019574359187791  avarage test loss 0.013192253012675792\n",
      "Training: epoch 49 batch 0 loss 0.005459214095026255\n",
      "Training: epoch 49 batch 10 loss 0.003446787828579545\n",
      "Training: epoch 49 batch 20 loss 0.00980017427355051\n",
      "Test: epoch 49 batch 0 loss 0.0026509633753448725\n",
      "epoch 49 finished - avarage train loss 0.008505795913716328  avarage test loss 0.014622565766330808\n",
      "Training: epoch 50 batch 0 loss 0.009115759283304214\n",
      "Training: epoch 50 batch 10 loss 0.004843815229833126\n",
      "Training: epoch 50 batch 20 loss 0.00987050961703062\n",
      "Test: epoch 50 batch 0 loss 0.010404464788734913\n",
      "epoch 50 finished - avarage train loss 0.007623101793358038  avarage test loss 0.01848320080898702\n",
      "Training: epoch 51 batch 0 loss 0.017815526574850082\n",
      "Training: epoch 51 batch 10 loss 0.007078586146235466\n",
      "Training: epoch 51 batch 20 loss 0.006641514599323273\n",
      "Test: epoch 51 batch 0 loss 0.004171703476458788\n",
      "epoch 51 finished - avarage train loss 0.011549675210925012  avarage test loss 0.015096511342562735\n",
      "Training: epoch 52 batch 0 loss 0.005670823622494936\n",
      "Training: epoch 52 batch 10 loss 0.007739974185824394\n",
      "Training: epoch 52 batch 20 loss 0.002178255235776305\n",
      "Test: epoch 52 batch 0 loss 0.003837266005575657\n",
      "epoch 52 finished - avarage train loss 0.0073236280907716216  avarage test loss 0.013513771235011518\n",
      "Training: epoch 53 batch 0 loss 0.005239966791123152\n",
      "Training: epoch 53 batch 10 loss 0.009519778192043304\n",
      "Training: epoch 53 batch 20 loss 0.011369648389518261\n",
      "Test: epoch 53 batch 0 loss 0.002692011184990406\n",
      "epoch 53 finished - avarage train loss 0.008071466943184877  avarage test loss 0.013731518178246915\n",
      "Training: epoch 54 batch 0 loss 0.004945863038301468\n",
      "Training: epoch 54 batch 10 loss 0.0042103794403374195\n",
      "Training: epoch 54 batch 20 loss 0.004713509231805801\n",
      "Test: epoch 54 batch 0 loss 0.0036279482301324606\n",
      "epoch 54 finished - avarage train loss 0.006757631191405757  avarage test loss 0.013707847625482827\n",
      "Training: epoch 55 batch 0 loss 0.008405107073485851\n",
      "Training: epoch 55 batch 10 loss 0.0075443703681230545\n",
      "Training: epoch 55 batch 20 loss 0.00851699709892273\n",
      "Test: epoch 55 batch 0 loss 0.004034887067973614\n",
      "epoch 55 finished - avarage train loss 0.007195122331252386  avarage test loss 0.015578705468215048\n",
      "Training: epoch 56 batch 0 loss 0.005806928966194391\n",
      "Training: epoch 56 batch 10 loss 0.0057011814787983894\n",
      "Training: epoch 56 batch 20 loss 0.011309618130326271\n",
      "Test: epoch 56 batch 0 loss 0.003888840554282069\n",
      "epoch 56 finished - avarage train loss 0.008389705454866433  avarage test loss 0.016164426517207175\n",
      "Training: epoch 57 batch 0 loss 0.010119885206222534\n",
      "Training: epoch 57 batch 10 loss 0.007928065955638885\n",
      "Training: epoch 57 batch 20 loss 0.0037465551868081093\n",
      "Test: epoch 57 batch 0 loss 0.0029368700925260782\n",
      "epoch 57 finished - avarage train loss 0.008562131102421674  avarage test loss 0.013967139588203281\n",
      "Training: epoch 58 batch 0 loss 0.004072262439876795\n",
      "Training: epoch 58 batch 10 loss 0.006615552585572004\n",
      "Training: epoch 58 batch 20 loss 0.00553694274276495\n",
      "Test: epoch 58 batch 0 loss 0.003924058750271797\n",
      "epoch 58 finished - avarage train loss 0.007228578946649514  avarage test loss 0.015329528832808137\n",
      "Training: epoch 59 batch 0 loss 0.005085096228867769\n",
      "Training: epoch 59 batch 10 loss 0.0032041582744568586\n",
      "Training: epoch 59 batch 20 loss 0.0059432582929730415\n",
      "Test: epoch 59 batch 0 loss 0.003256352851167321\n",
      "epoch 59 finished - avarage train loss 0.008331154874558079  avarage test loss 0.014945865666959435\n",
      "Training: epoch 60 batch 0 loss 0.006136460229754448\n",
      "Training: epoch 60 batch 10 loss 0.005537822376936674\n",
      "Training: epoch 60 batch 20 loss 0.004191096406430006\n",
      "Test: epoch 60 batch 0 loss 0.003245533909648657\n",
      "epoch 60 finished - avarage train loss 0.00782996839201399  avarage test loss 0.012825475889258087\n",
      "Training: epoch 61 batch 0 loss 0.006803853437304497\n",
      "Training: epoch 61 batch 10 loss 0.007015848066657782\n",
      "Training: epoch 61 batch 20 loss 0.01248069666326046\n",
      "Test: epoch 61 batch 0 loss 0.004213953856378794\n",
      "epoch 61 finished - avarage train loss 0.008574698666303322  avarage test loss 0.012491888832300901\n",
      "Training: epoch 62 batch 0 loss 0.003658478846773505\n",
      "Training: epoch 62 batch 10 loss 0.006414530798792839\n",
      "Training: epoch 62 batch 20 loss 0.0034534302540123463\n",
      "Test: epoch 62 batch 0 loss 0.003460415406152606\n",
      "epoch 62 finished - avarage train loss 0.008012731029684174  avarage test loss 0.01390705822268501\n",
      "Training: epoch 63 batch 0 loss 0.0064141410402953625\n",
      "Training: epoch 63 batch 10 loss 0.009268206544220448\n",
      "Training: epoch 63 batch 20 loss 0.011231821030378342\n",
      "Test: epoch 63 batch 0 loss 0.002964335260912776\n",
      "epoch 63 finished - avarage train loss 0.007378753957113829  avarage test loss 0.012355111364740878\n",
      "Training: epoch 64 batch 0 loss 0.0037158275954425335\n",
      "Training: epoch 64 batch 10 loss 0.0044171689078211784\n",
      "Training: epoch 64 batch 20 loss 0.006352821830660105\n",
      "Test: epoch 64 batch 0 loss 0.0040481495670974255\n",
      "epoch 64 finished - avarage train loss 0.008003003718652603  avarage test loss 0.012595475767739117\n",
      "Training: epoch 65 batch 0 loss 0.003030266147106886\n",
      "Training: epoch 65 batch 10 loss 0.00335294590331614\n",
      "Training: epoch 65 batch 20 loss 0.004947214853018522\n",
      "Test: epoch 65 batch 0 loss 0.015072748064994812\n",
      "epoch 65 finished - avarage train loss 0.008066922169307182  avarage test loss 0.027940476778894663\n",
      "Training: epoch 66 batch 0 loss 0.022418320178985596\n",
      "Training: epoch 66 batch 10 loss 0.011232711374759674\n",
      "Training: epoch 66 batch 20 loss 0.018935490399599075\n",
      "Test: epoch 66 batch 0 loss 0.014514081180095673\n",
      "epoch 66 finished - avarage train loss 0.016677751607293713  avarage test loss 0.026089472929015756\n",
      "Training: epoch 67 batch 0 loss 0.011247423477470875\n",
      "Training: epoch 67 batch 10 loss 0.00616704486310482\n",
      "Training: epoch 67 batch 20 loss 0.008166044019162655\n",
      "Test: epoch 67 batch 0 loss 0.006653454154729843\n",
      "epoch 67 finished - avarage train loss 0.009853581111107406  avarage test loss 0.02007158356718719\n",
      "Training: epoch 68 batch 0 loss 0.0037659509107470512\n",
      "Training: epoch 68 batch 10 loss 0.005662256386131048\n",
      "Training: epoch 68 batch 20 loss 0.011134910397231579\n",
      "Test: epoch 68 batch 0 loss 0.027707917615771294\n",
      "epoch 68 finished - avarage train loss 0.00851555152571407  avarage test loss 0.04653020156547427\n",
      "Training: epoch 69 batch 0 loss 0.032971035689115524\n",
      "Training: epoch 69 batch 10 loss 0.02965945564210415\n",
      "Training: epoch 69 batch 20 loss 0.024215564131736755\n",
      "Test: epoch 69 batch 0 loss 0.025206729769706726\n",
      "epoch 69 finished - avarage train loss 0.025466706314734345  avarage test loss 0.03230674425140023\n",
      "Training: epoch 70 batch 0 loss 0.018130360171198845\n",
      "Training: epoch 70 batch 10 loss 0.013348893262445927\n",
      "Training: epoch 70 batch 20 loss 0.01981448009610176\n",
      "Test: epoch 70 batch 0 loss 0.01610180363059044\n",
      "epoch 70 finished - avarage train loss 0.0189051298617289  avarage test loss 0.024470796110108495\n",
      "Training: epoch 71 batch 0 loss 0.007999035529792309\n",
      "Training: epoch 71 batch 10 loss 0.005703887436538935\n",
      "Training: epoch 71 batch 20 loss 0.003807127708569169\n",
      "Test: epoch 71 batch 0 loss 0.0031608964782208204\n",
      "epoch 71 finished - avarage train loss 0.011068578592726383  avarage test loss 0.012520007963757962\n",
      "Training: epoch 72 batch 0 loss 0.0037604111712425947\n",
      "Training: epoch 72 batch 10 loss 0.006568169221282005\n",
      "Training: epoch 72 batch 20 loss 0.00901483092457056\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test: epoch 72 batch 0 loss 0.005946813616901636\n",
      "epoch 72 finished - avarage train loss 0.0069954374821150096  avarage test loss 0.015290401061065495\n",
      "Training: epoch 73 batch 0 loss 0.007489213719964027\n",
      "Training: epoch 73 batch 10 loss 0.00814700499176979\n",
      "Training: epoch 73 batch 20 loss 0.005092829465866089\n",
      "Test: epoch 73 batch 0 loss 0.008004294708371162\n",
      "epoch 73 finished - avarage train loss 0.010228683353112689  avarage test loss 0.021319374907761812\n",
      "Training: epoch 74 batch 0 loss 0.00618826225399971\n",
      "Training: epoch 74 batch 10 loss 0.008122572675347328\n",
      "Training: epoch 74 batch 20 loss 0.004444424528628588\n",
      "Test: epoch 74 batch 0 loss 0.0035696416161954403\n",
      "epoch 74 finished - avarage train loss 0.007441087055887128  avarage test loss 0.012875361135229468\n",
      "Training: epoch 75 batch 0 loss 0.003918676637113094\n",
      "Training: epoch 75 batch 10 loss 0.00643600057810545\n",
      "Training: epoch 75 batch 20 loss 0.005342977121472359\n",
      "Test: epoch 75 batch 0 loss 0.0027763615362346172\n",
      "epoch 75 finished - avarage train loss 0.008928720742977899  avarage test loss 0.0125033522490412\n",
      "Training: epoch 76 batch 0 loss 0.010851689614355564\n",
      "Training: epoch 76 batch 10 loss 0.00543932756409049\n",
      "Training: epoch 76 batch 20 loss 0.008203395642340183\n",
      "Test: epoch 76 batch 0 loss 0.0022692224010825157\n",
      "epoch 76 finished - avarage train loss 0.00849068352695683  avarage test loss 0.012263277196325362\n",
      "Training: epoch 77 batch 0 loss 0.0022968128323554993\n",
      "Training: epoch 77 batch 10 loss 0.006378541234880686\n",
      "Training: epoch 77 batch 20 loss 0.0056516071781516075\n",
      "Test: epoch 77 batch 0 loss 0.002321105683222413\n",
      "epoch 77 finished - avarage train loss 0.0081605593400911  avarage test loss 0.012784436519723386\n",
      "Training: epoch 78 batch 0 loss 0.004712617956101894\n",
      "Training: epoch 78 batch 10 loss 0.008748520165681839\n",
      "Training: epoch 78 batch 20 loss 0.007239834405481815\n",
      "Test: epoch 78 batch 0 loss 0.0028724069707095623\n",
      "epoch 78 finished - avarage train loss 0.006587415508090936  avarage test loss 0.013466907548718154\n",
      "Training: epoch 79 batch 0 loss 0.005156697239726782\n",
      "Training: epoch 79 batch 10 loss 0.005214951001107693\n",
      "Training: epoch 79 batch 20 loss 0.005537488963454962\n",
      "Test: epoch 79 batch 0 loss 0.0024870983324944973\n",
      "epoch 79 finished - avarage train loss 0.007806056773225809  avarage test loss 0.012540102354250848\n",
      "Training: epoch 80 batch 0 loss 0.008364634588360786\n",
      "Training: epoch 80 batch 10 loss 0.0036986684426665306\n",
      "Training: epoch 80 batch 20 loss 0.005358010996133089\n",
      "Test: epoch 80 batch 0 loss 0.0035542487166821957\n",
      "epoch 80 finished - avarage train loss 0.007856840460464871  avarage test loss 0.01580558263231069\n",
      "Training: epoch 81 batch 0 loss 0.005914755165576935\n",
      "Training: epoch 81 batch 10 loss 0.00634795194491744\n",
      "Training: epoch 81 batch 20 loss 0.006069684401154518\n",
      "Test: epoch 81 batch 0 loss 0.0032826226670295\n",
      "epoch 81 finished - avarage train loss 0.006694022760774684  avarage test loss 0.016456648299936205\n",
      "Training: epoch 82 batch 0 loss 0.00739815691486001\n",
      "Training: epoch 82 batch 10 loss 0.0069676055572927\n",
      "Training: epoch 82 batch 20 loss 0.0032520336098968983\n",
      "Test: epoch 82 batch 0 loss 0.003148290328681469\n",
      "epoch 82 finished - avarage train loss 0.007453673992884056  avarage test loss 0.01650791836436838\n",
      "Training: epoch 83 batch 0 loss 0.007060414645820856\n",
      "Training: epoch 83 batch 10 loss 0.010431299917399883\n",
      "Training: epoch 83 batch 20 loss 0.0033158408477902412\n",
      "Test: epoch 83 batch 0 loss 0.004759902134537697\n",
      "epoch 83 finished - avarage train loss 0.006180286688473204  avarage test loss 0.015606551547534764\n",
      "Training: epoch 84 batch 0 loss 0.006080636754631996\n",
      "Training: epoch 84 batch 10 loss 0.013546105474233627\n",
      "Training: epoch 84 batch 20 loss 0.007138517219573259\n",
      "Test: epoch 84 batch 0 loss 0.00408666068688035\n",
      "epoch 84 finished - avarage train loss 0.009051912399972308  avarage test loss 0.017728293198160827\n",
      "Training: epoch 85 batch 0 loss 0.0033490557689219713\n",
      "Training: epoch 85 batch 10 loss 0.0033264649100601673\n",
      "Training: epoch 85 batch 20 loss 0.0035912077873945236\n",
      "Test: epoch 85 batch 0 loss 0.003044445300474763\n",
      "epoch 85 finished - avarage train loss 0.006768132848986264  avarage test loss 0.014219878532458097\n",
      "Training: epoch 86 batch 0 loss 0.008259215392172337\n",
      "Training: epoch 86 batch 10 loss 0.004258260130882263\n",
      "Training: epoch 86 batch 20 loss 0.006400867830961943\n",
      "Test: epoch 86 batch 0 loss 0.0030130785889923573\n",
      "epoch 86 finished - avarage train loss 0.008191205842163542  avarage test loss 0.015819755499251187\n",
      "Training: epoch 87 batch 0 loss 0.008408784866333008\n",
      "Training: epoch 87 batch 10 loss 0.006828371435403824\n",
      "Training: epoch 87 batch 20 loss 0.0031135370954871178\n",
      "Test: epoch 87 batch 0 loss 0.005034969188272953\n",
      "epoch 87 finished - avarage train loss 0.007323987741858281  avarage test loss 0.01736750826239586\n",
      "Training: epoch 88 batch 0 loss 0.0064336503855884075\n",
      "Training: epoch 88 batch 10 loss 0.008688163012266159\n",
      "Training: epoch 88 batch 20 loss 0.005562330596148968\n",
      "Test: epoch 88 batch 0 loss 0.005575342103838921\n",
      "epoch 88 finished - avarage train loss 0.007184248586217391  avarage test loss 0.019751097075641155\n",
      "Training: epoch 89 batch 0 loss 0.01252762321382761\n",
      "Training: epoch 89 batch 10 loss 0.003378777066245675\n",
      "Training: epoch 89 batch 20 loss 0.004321209620684385\n",
      "Test: epoch 89 batch 0 loss 0.004048679955303669\n",
      "epoch 89 finished - avarage train loss 0.006665942684650935  avarage test loss 0.01735706254839897\n",
      "Training: epoch 90 batch 0 loss 0.01111944392323494\n",
      "Training: epoch 90 batch 10 loss 0.0036902728024870157\n",
      "Training: epoch 90 batch 20 loss 0.003753907745704055\n",
      "Test: epoch 90 batch 0 loss 0.0036721962969750166\n",
      "epoch 90 finished - avarage train loss 0.007966498386692899  avarage test loss 0.016049558820668608\n",
      "Training: epoch 91 batch 0 loss 0.014108509756624699\n",
      "Training: epoch 91 batch 10 loss 0.003669500583782792\n",
      "Training: epoch 91 batch 20 loss 0.010277575813233852\n",
      "Test: epoch 91 batch 0 loss 0.004728490952402353\n",
      "epoch 91 finished - avarage train loss 0.008164530708293977  avarage test loss 0.018139620660804212\n",
      "Training: epoch 92 batch 0 loss 0.006755318958312273\n",
      "Training: epoch 92 batch 10 loss 0.004732553381472826\n",
      "Training: epoch 92 batch 20 loss 0.005120380315929651\n",
      "Test: epoch 92 batch 0 loss 0.0034580291248857975\n",
      "epoch 92 finished - avarage train loss 0.00834177516336585  avarage test loss 0.013996982714161277\n",
      "Training: epoch 93 batch 0 loss 0.006809947080910206\n",
      "Training: epoch 93 batch 10 loss 0.004340943414717913\n",
      "Training: epoch 93 batch 20 loss 0.003201487474143505\n",
      "Test: epoch 93 batch 0 loss 0.004207085818052292\n",
      "epoch 93 finished - avarage train loss 0.0076511094651730924  avarage test loss 0.013278401922434568\n",
      "Training: epoch 94 batch 0 loss 0.0030932731460779905\n",
      "Training: epoch 94 batch 10 loss 0.0032216617837548256\n",
      "Training: epoch 94 batch 20 loss 0.006720676552504301\n",
      "Test: epoch 94 batch 0 loss 0.004852598067373037\n",
      "epoch 94 finished - avarage train loss 0.007336583542476954  avarage test loss 0.013726592296734452\n",
      "Training: epoch 95 batch 0 loss 0.009066140279173851\n",
      "Training: epoch 95 batch 10 loss 0.007149553392082453\n",
      "Training: epoch 95 batch 20 loss 0.008782553486526012\n",
      "Test: epoch 95 batch 0 loss 0.003738116007298231\n",
      "epoch 95 finished - avarage train loss 0.006085947180038382  avarage test loss 0.013803579611703753\n",
      "Training: epoch 96 batch 0 loss 0.006013527046889067\n",
      "Training: epoch 96 batch 10 loss 0.0062093306332826614\n",
      "Training: epoch 96 batch 20 loss 0.004187482409179211\n",
      "Test: epoch 96 batch 0 loss 0.004241982474923134\n",
      "epoch 96 finished - avarage train loss 0.006878282152244757  avarage test loss 0.015797342057339847\n",
      "Training: epoch 97 batch 0 loss 0.00591849721968174\n",
      "Training: epoch 97 batch 10 loss 0.020568307489156723\n",
      "Training: epoch 97 batch 20 loss 0.006292261183261871\n",
      "Test: epoch 97 batch 0 loss 0.003239853773266077\n",
      "epoch 97 finished - avarage train loss 0.008511545843091505  avarage test loss 0.015539017389528453\n",
      "Training: epoch 98 batch 0 loss 0.007425590418279171\n",
      "Training: epoch 98 batch 10 loss 0.005313791334629059\n",
      "Training: epoch 98 batch 20 loss 0.0040627229027450085\n",
      "Test: epoch 98 batch 0 loss 0.005035370122641325\n",
      "epoch 98 finished - avarage train loss 0.007059131026396464  avarage test loss 0.018583400873467326\n",
      "Training: epoch 99 batch 0 loss 0.006910312455147505\n",
      "Training: epoch 99 batch 10 loss 0.008677313104271889\n",
      "Training: epoch 99 batch 20 loss 0.008951561525464058\n",
      "Test: epoch 99 batch 0 loss 0.005519099999219179\n",
      "epoch 99 finished - avarage train loss 0.006826064228240786  avarage test loss 0.01923391444142908\n",
      "Training: epoch 100 batch 0 loss 0.010044345632195473\n",
      "Training: epoch 100 batch 10 loss 0.01137175876647234\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training: epoch 100 batch 20 loss 0.007553288713097572\n",
      "Test: epoch 100 batch 0 loss 0.00572212366387248\n",
      "epoch 100 finished - avarage train loss 0.008530583240669864  avarage test loss 0.013828772120177746\n",
      "Training: epoch 101 batch 0 loss 0.01232279185205698\n",
      "Training: epoch 101 batch 10 loss 0.004799723159521818\n",
      "Training: epoch 101 batch 20 loss 0.006898434832692146\n",
      "Test: epoch 101 batch 0 loss 0.006270189303904772\n",
      "epoch 101 finished - avarage train loss 0.008766945604042247  avarage test loss 0.01894607685972005\n",
      "Training: epoch 102 batch 0 loss 0.005929609760642052\n",
      "Training: epoch 102 batch 10 loss 0.011608349159359932\n",
      "Training: epoch 102 batch 20 loss 0.0022076196037232876\n",
      "Test: epoch 102 batch 0 loss 0.006778651848435402\n",
      "epoch 102 finished - avarage train loss 0.010251647086235983  avarage test loss 0.020241261227056384\n",
      "Training: epoch 103 batch 0 loss 0.0078119682148098946\n",
      "Training: epoch 103 batch 10 loss 0.007623052690178156\n",
      "Training: epoch 103 batch 20 loss 0.003229798749089241\n",
      "Test: epoch 103 batch 0 loss 0.0047176796942949295\n",
      "epoch 103 finished - avarage train loss 0.007993095377781268  avarage test loss 0.015713814413174987\n",
      "Training: epoch 104 batch 0 loss 0.008885687217116356\n",
      "Training: epoch 104 batch 10 loss 0.013581735081970692\n",
      "Training: epoch 104 batch 20 loss 0.005658073350787163\n",
      "Test: epoch 104 batch 0 loss 0.005601700861006975\n",
      "epoch 104 finished - avarage train loss 0.009224696269515774  avarage test loss 0.013269261689856648\n",
      "Training: epoch 105 batch 0 loss 0.00581302959471941\n",
      "Training: epoch 105 batch 10 loss 0.012421848252415657\n",
      "Training: epoch 105 batch 20 loss 0.008337251842021942\n",
      "Test: epoch 105 batch 0 loss 0.0043707117438316345\n",
      "epoch 105 finished - avarage train loss 0.01024421415259612  avarage test loss 0.013741403818130493\n",
      "Training: epoch 106 batch 0 loss 0.00582390371710062\n",
      "Training: epoch 106 batch 10 loss 0.011366200633347034\n",
      "Training: epoch 106 batch 20 loss 0.005006507970392704\n",
      "Test: epoch 106 batch 0 loss 0.0038678599521517754\n",
      "epoch 106 finished - avarage train loss 0.008993274564373082  avarage test loss 0.012892703642137349\n",
      "Training: epoch 107 batch 0 loss 0.006812118459492922\n",
      "Training: epoch 107 batch 10 loss 0.00806634034961462\n",
      "Training: epoch 107 batch 20 loss 0.014652954414486885\n",
      "Test: epoch 107 batch 0 loss 0.006815842352807522\n",
      "epoch 107 finished - avarage train loss 0.008170066602078491  avarage test loss 0.016767058987170458\n",
      "Training: epoch 108 batch 0 loss 0.011943995952606201\n",
      "Training: epoch 108 batch 10 loss 0.00457155704498291\n",
      "Training: epoch 108 batch 20 loss 0.007289662957191467\n",
      "Test: epoch 108 batch 0 loss 0.004520101938396692\n",
      "epoch 108 finished - avarage train loss 0.009555925695418284  avarage test loss 0.014310492086224258\n",
      "Training: epoch 109 batch 0 loss 0.007247508969157934\n",
      "Training: epoch 109 batch 10 loss 0.003295239992439747\n",
      "Training: epoch 109 batch 20 loss 0.01125285029411316\n",
      "Test: epoch 109 batch 0 loss 0.004012871067970991\n",
      "epoch 109 finished - avarage train loss 0.008300638581015941  avarage test loss 0.013647751533426344\n",
      "Training: epoch 110 batch 0 loss 0.005480644293129444\n",
      "Training: epoch 110 batch 10 loss 0.008408566936850548\n",
      "Training: epoch 110 batch 20 loss 0.006255903281271458\n",
      "Test: epoch 110 batch 0 loss 0.005645569413900375\n",
      "epoch 110 finished - avarage train loss 0.007660250773589159  avarage test loss 0.014058590168133378\n",
      "Training: epoch 111 batch 0 loss 0.005384240299463272\n",
      "Training: epoch 111 batch 10 loss 0.008225349709391594\n",
      "Training: epoch 111 batch 20 loss 0.007927082479000092\n",
      "Test: epoch 111 batch 0 loss 0.0038757207803428173\n",
      "epoch 111 finished - avarage train loss 0.008084210679577342  avarage test loss 0.012925034738145769\n",
      "Training: epoch 112 batch 0 loss 0.007488368544727564\n",
      "Training: epoch 112 batch 10 loss 0.00843318086117506\n",
      "Training: epoch 112 batch 20 loss 0.006123351864516735\n",
      "Test: epoch 112 batch 0 loss 0.0035169553011655807\n",
      "epoch 112 finished - avarage train loss 0.00788983277141534  avarage test loss 0.012898100132588297\n",
      "Training: epoch 113 batch 0 loss 0.00955028273165226\n",
      "Training: epoch 113 batch 10 loss 0.009627481922507286\n",
      "Training: epoch 113 batch 20 loss 0.006092394236475229\n",
      "Test: epoch 113 batch 0 loss 0.004871916491538286\n",
      "epoch 113 finished - avarage train loss 0.007949475391671575  avarage test loss 0.013360891491174698\n",
      "Training: epoch 114 batch 0 loss 0.008713538758456707\n",
      "Training: epoch 114 batch 10 loss 0.004418096039444208\n",
      "Training: epoch 114 batch 20 loss 0.004099634476006031\n",
      "Test: epoch 114 batch 0 loss 0.0035748016089200974\n",
      "epoch 114 finished - avarage train loss 0.007921955098622832  avarage test loss 0.012830767489504069\n",
      "Training: epoch 115 batch 0 loss 0.006250469479709864\n",
      "Training: epoch 115 batch 10 loss 0.005213651806116104\n",
      "Training: epoch 115 batch 20 loss 0.0023230493534356356\n",
      "Test: epoch 115 batch 0 loss 0.004030102398246527\n",
      "epoch 115 finished - avarage train loss 0.00752438806350632  avarage test loss 0.013131479499861598\n",
      "Training: epoch 116 batch 0 loss 0.007999002933502197\n",
      "Training: epoch 116 batch 10 loss 0.007453010883182287\n",
      "Training: epoch 116 batch 20 loss 0.015100337564945221\n",
      "Test: epoch 116 batch 0 loss 0.0037978091277182102\n",
      "epoch 116 finished - avarage train loss 0.00795639748685062  avarage test loss 0.01311952865216881\n",
      "Training: epoch 117 batch 0 loss 0.008792825043201447\n",
      "Training: epoch 117 batch 10 loss 0.00785121787339449\n",
      "Training: epoch 117 batch 20 loss 0.0057729696854949\n",
      "Test: epoch 117 batch 0 loss 0.003952846396714449\n",
      "epoch 117 finished - avarage train loss 0.007393809605454062  avarage test loss 0.013040418736636639\n",
      "Training: epoch 118 batch 0 loss 0.006907499395310879\n",
      "Training: epoch 118 batch 10 loss 0.005390652921050787\n",
      "Training: epoch 118 batch 20 loss 0.00501454621553421\n",
      "Test: epoch 118 batch 0 loss 0.004216537345200777\n",
      "epoch 118 finished - avarage train loss 0.008389244875710073  avarage test loss 0.012862519070040435\n",
      "Training: epoch 119 batch 0 loss 0.00740203307941556\n",
      "Training: epoch 119 batch 10 loss 0.004669434390962124\n",
      "Training: epoch 119 batch 20 loss 0.005539275705814362\n",
      "Test: epoch 119 batch 0 loss 0.004793236963450909\n",
      "epoch 119 finished - avarage train loss 0.0076322519252526345  avarage test loss 0.014944002265110612\n",
      "Training: epoch 120 batch 0 loss 0.006304045673459768\n",
      "Training: epoch 120 batch 10 loss 0.010705336928367615\n",
      "Training: epoch 120 batch 20 loss 0.01008164044469595\n",
      "Test: epoch 120 batch 0 loss 0.005844515282660723\n",
      "epoch 120 finished - avarage train loss 0.008440248041959673  avarage test loss 0.014020164031535387\n",
      "Training: epoch 121 batch 0 loss 0.005389610771089792\n",
      "Training: epoch 121 batch 10 loss 0.005196793004870415\n",
      "Training: epoch 121 batch 20 loss 0.006426484324038029\n",
      "Test: epoch 121 batch 0 loss 0.004318152088671923\n",
      "epoch 121 finished - avarage train loss 0.009092328564167536  avarage test loss 0.013648383435793221\n",
      "Training: epoch 122 batch 0 loss 0.0052915820851922035\n",
      "Training: epoch 122 batch 10 loss 0.006327365525066853\n",
      "Training: epoch 122 batch 20 loss 0.004664917476475239\n",
      "Test: epoch 122 batch 0 loss 0.004266280215233564\n",
      "epoch 122 finished - avarage train loss 0.007400610211208977  avarage test loss 0.01275097276084125\n",
      "Training: epoch 123 batch 0 loss 0.004012672230601311\n",
      "Training: epoch 123 batch 10 loss 0.01144503615796566\n",
      "Training: epoch 123 batch 20 loss 0.005400126799941063\n",
      "Test: epoch 123 batch 0 loss 0.003915522247552872\n",
      "epoch 123 finished - avarage train loss 0.008411704530489856  avarage test loss 0.013073741924017668\n",
      "Training: epoch 124 batch 0 loss 0.00455921795219183\n",
      "Training: epoch 124 batch 10 loss 0.0046311370097100735\n",
      "Training: epoch 124 batch 20 loss 0.005791232455521822\n",
      "Test: epoch 124 batch 0 loss 0.004868877120316029\n",
      "epoch 124 finished - avarage train loss 0.00784633483673478  avarage test loss 0.014436586992815137\n",
      "Training: epoch 125 batch 0 loss 0.007539663463830948\n",
      "Training: epoch 125 batch 10 loss 0.005093891639262438\n",
      "Training: epoch 125 batch 20 loss 0.0069400412030518055\n",
      "Test: epoch 125 batch 0 loss 0.003839311422780156\n",
      "epoch 125 finished - avarage train loss 0.007190315109067436  avarage test loss 0.012496929382905364\n",
      "Training: epoch 126 batch 0 loss 0.00798027217388153\n",
      "Training: epoch 126 batch 10 loss 0.006087274290621281\n",
      "Training: epoch 126 batch 20 loss 0.009342401288449764\n",
      "Test: epoch 126 batch 0 loss 0.0037183240056037903\n",
      "epoch 126 finished - avarage train loss 0.008805000801281682  avarage test loss 0.01288978214142844\n",
      "Training: epoch 127 batch 0 loss 0.006793041247874498\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training: epoch 127 batch 10 loss 0.004577524494379759\n",
      "Training: epoch 127 batch 20 loss 0.004021253436803818\n",
      "Test: epoch 127 batch 0 loss 0.004032457713037729\n",
      "epoch 127 finished - avarage train loss 0.006794157462066104  avarage test loss 0.012613927654456347\n",
      "Training: epoch 128 batch 0 loss 0.009845432825386524\n",
      "Training: epoch 128 batch 10 loss 0.0057280403561890125\n",
      "Training: epoch 128 batch 20 loss 0.007858079858124256\n",
      "Test: epoch 128 batch 0 loss 0.004162491764873266\n",
      "epoch 128 finished - avarage train loss 0.009494036355794504  avarage test loss 0.012650999356992543\n",
      "Training: epoch 129 batch 0 loss 0.007032371591776609\n",
      "Training: epoch 129 batch 10 loss 0.009051277302205563\n",
      "Training: epoch 129 batch 20 loss 0.003842607606202364\n",
      "Test: epoch 129 batch 0 loss 0.0046204752288758755\n",
      "epoch 129 finished - avarage train loss 0.007313688775396039  avarage test loss 0.014628028264269233\n",
      "Training: epoch 130 batch 0 loss 0.0032355613075196743\n",
      "Training: epoch 130 batch 10 loss 0.00536169670522213\n",
      "Training: epoch 130 batch 20 loss 0.00431938236579299\n",
      "Test: epoch 130 batch 0 loss 0.008043836802244186\n",
      "epoch 130 finished - avarage train loss 0.00848924836689799  avarage test loss 0.01732649584300816\n",
      "Training: epoch 131 batch 0 loss 0.007493765093386173\n",
      "Training: epoch 131 batch 10 loss 0.011985964141786098\n",
      "Training: epoch 131 batch 20 loss 0.005626905709505081\n",
      "Test: epoch 131 batch 0 loss 0.004051005467772484\n",
      "epoch 131 finished - avarage train loss 0.009341580006454525  avarage test loss 0.013389791129156947\n",
      "Training: epoch 132 batch 0 loss 0.004584191367030144\n",
      "Training: epoch 132 batch 10 loss 0.0023963761050254107\n",
      "Training: epoch 132 batch 20 loss 0.006558122579008341\n",
      "Test: epoch 132 batch 0 loss 0.0038695756811648607\n",
      "epoch 132 finished - avarage train loss 0.008474818832273114  avarage test loss 0.012560911185573786\n",
      "Training: epoch 133 batch 0 loss 0.0023880312219262123\n",
      "Training: epoch 133 batch 10 loss 0.003033615881577134\n",
      "Training: epoch 133 batch 20 loss 0.006183349993079901\n",
      "Test: epoch 133 batch 0 loss 0.005035206209868193\n",
      "epoch 133 finished - avarage train loss 0.008562808527193707  avarage test loss 0.013360496028326452\n",
      "Training: epoch 134 batch 0 loss 0.0075302524492144585\n",
      "Training: epoch 134 batch 10 loss 0.0038070736918598413\n",
      "Training: epoch 134 batch 20 loss 0.011236779391765594\n",
      "Test: epoch 134 batch 0 loss 0.00399888725951314\n",
      "epoch 134 finished - avarage train loss 0.009246979824042526  avarage test loss 0.013068621163256466\n",
      "Training: epoch 135 batch 0 loss 0.007887814193964005\n",
      "Training: epoch 135 batch 10 loss 0.004459029994904995\n",
      "Training: epoch 135 batch 20 loss 0.006285666488111019\n",
      "Test: epoch 135 batch 0 loss 0.003981336019933224\n",
      "epoch 135 finished - avarage train loss 0.007247852980448255  avarage test loss 0.0127356993034482\n",
      "Training: epoch 136 batch 0 loss 0.007239962927997112\n",
      "Training: epoch 136 batch 10 loss 0.00482682790607214\n",
      "Training: epoch 136 batch 20 loss 0.006524727214127779\n",
      "Test: epoch 136 batch 0 loss 0.00507235387340188\n",
      "epoch 136 finished - avarage train loss 0.006865214283481754  avarage test loss 0.014094882877543569\n",
      "Training: epoch 137 batch 0 loss 0.008008452132344246\n",
      "Training: epoch 137 batch 10 loss 0.007764473091810942\n",
      "Training: epoch 137 batch 20 loss 0.0075169638730585575\n",
      "Test: epoch 137 batch 0 loss 0.005021023564040661\n",
      "epoch 137 finished - avarage train loss 0.008182012652657155  avarage test loss 0.01363730919547379\n",
      "Training: epoch 138 batch 0 loss 0.006207536440342665\n",
      "Training: epoch 138 batch 10 loss 0.005994338076561689\n",
      "Training: epoch 138 batch 20 loss 0.005023574456572533\n",
      "Test: epoch 138 batch 0 loss 0.004803897812962532\n",
      "epoch 138 finished - avarage train loss 0.007685398763238356  avarage test loss 0.01501587184611708\n",
      "Training: epoch 139 batch 0 loss 0.008741019293665886\n",
      "Training: epoch 139 batch 10 loss 0.010670739226043224\n",
      "Training: epoch 139 batch 20 loss 0.004632764495909214\n",
      "Test: epoch 139 batch 0 loss 0.004742836579680443\n",
      "epoch 139 finished - avarage train loss 0.008674125973905983  avarage test loss 0.013255235855467618\n",
      "Training: epoch 140 batch 0 loss 0.01048680953681469\n",
      "Training: epoch 140 batch 10 loss 0.010990630835294724\n",
      "Training: epoch 140 batch 20 loss 0.010044819675385952\n",
      "Test: epoch 140 batch 0 loss 0.004296463448554277\n",
      "epoch 140 finished - avarage train loss 0.007582911940548441  avarage test loss 0.013156206579878926\n",
      "Training: epoch 141 batch 0 loss 0.00571130495518446\n",
      "Training: epoch 141 batch 10 loss 0.011833030730485916\n",
      "Training: epoch 141 batch 20 loss 0.003143773414194584\n",
      "Test: epoch 141 batch 0 loss 0.004467823542654514\n",
      "epoch 141 finished - avarage train loss 0.006948353497889535  avarage test loss 0.012805123173166066\n",
      "Training: epoch 142 batch 0 loss 0.0105842724442482\n",
      "Training: epoch 142 batch 10 loss 0.007846608757972717\n",
      "Training: epoch 142 batch 20 loss 0.009854508563876152\n",
      "Test: epoch 142 batch 0 loss 0.004937873221933842\n",
      "epoch 142 finished - avarage train loss 0.007472966299874002  avarage test loss 0.013250158866867423\n",
      "Training: epoch 143 batch 0 loss 0.003898210357874632\n",
      "Training: epoch 143 batch 10 loss 0.0047492110170423985\n",
      "Training: epoch 143 batch 20 loss 0.005351698957383633\n",
      "Test: epoch 143 batch 0 loss 0.006261531729251146\n",
      "epoch 143 finished - avarage train loss 0.007914683974251664  avarage test loss 0.014272137777879834\n",
      "Training: epoch 144 batch 0 loss 0.0137075399979949\n",
      "Training: epoch 144 batch 10 loss 0.00546198571100831\n",
      "Training: epoch 144 batch 20 loss 0.009292872622609138\n",
      "Test: epoch 144 batch 0 loss 0.004903968423604965\n",
      "epoch 144 finished - avarage train loss 0.007490903652947524  avarage test loss 0.013575381366536021\n",
      "Training: epoch 145 batch 0 loss 0.01220819354057312\n",
      "Training: epoch 145 batch 10 loss 0.007532309740781784\n",
      "Training: epoch 145 batch 20 loss 0.0059117707423865795\n",
      "Test: epoch 145 batch 0 loss 0.008768266998231411\n",
      "epoch 145 finished - avarage train loss 0.008248433671442085  avarage test loss 0.01818952616304159\n",
      "Training: epoch 146 batch 0 loss 0.012567941099405289\n",
      "Training: epoch 146 batch 10 loss 0.008841688744723797\n",
      "Training: epoch 146 batch 20 loss 0.006434715818613768\n",
      "Test: epoch 146 batch 0 loss 0.005310702137649059\n",
      "epoch 146 finished - avarage train loss 0.008597933237665686  avarage test loss 0.013795992592349648\n",
      "Training: epoch 147 batch 0 loss 0.0096516078338027\n",
      "Training: epoch 147 batch 10 loss 0.00437514204531908\n",
      "Training: epoch 147 batch 20 loss 0.003865425009280443\n",
      "Test: epoch 147 batch 0 loss 0.004086520988494158\n",
      "epoch 147 finished - avarage train loss 0.0070884550905561654  avarage test loss 0.012617937580216676\n",
      "Training: epoch 148 batch 0 loss 0.006462613586336374\n",
      "Training: epoch 148 batch 10 loss 0.004589763469994068\n",
      "Training: epoch 148 batch 20 loss 0.0067188916727900505\n",
      "Test: epoch 148 batch 0 loss 0.004422966856509447\n",
      "epoch 148 finished - avarage train loss 0.0072073200316136255  avarage test loss 0.012743524042889476\n",
      "Training: epoch 149 batch 0 loss 0.007289517670869827\n",
      "Training: epoch 149 batch 10 loss 0.007257386576384306\n",
      "Training: epoch 149 batch 20 loss 0.007043633610010147\n",
      "Test: epoch 149 batch 0 loss 0.003917779307812452\n",
      "epoch 149 finished - avarage train loss 0.00846265179329905  avarage test loss 0.012549502542242408\n",
      "Training: epoch 150 batch 0 loss 0.004665230866521597\n",
      "Training: epoch 150 batch 10 loss 0.005219081416726112\n",
      "Training: epoch 150 batch 20 loss 0.00558113306760788\n",
      "Test: epoch 150 batch 0 loss 0.0039052744396030903\n",
      "epoch 150 finished - avarage train loss 0.007373634725809097  avarage test loss 0.012768195709213614\n",
      "Training: epoch 151 batch 0 loss 0.004617455415427685\n",
      "Training: epoch 151 batch 10 loss 0.006147522944957018\n",
      "Training: epoch 151 batch 20 loss 0.007583442144095898\n",
      "Test: epoch 151 batch 0 loss 0.004013507626950741\n",
      "epoch 151 finished - avarage train loss 0.007808336658917111  avarage test loss 0.012908741424325854\n",
      "Training: epoch 152 batch 0 loss 0.0052948701195418835\n",
      "Training: epoch 152 batch 10 loss 0.005974754225462675\n",
      "Training: epoch 152 batch 20 loss 0.004073204938322306\n",
      "Test: epoch 152 batch 0 loss 0.004453426226973534\n",
      "epoch 152 finished - avarage train loss 0.007902797175057489  avarage test loss 0.012818607094231993\n",
      "Training: epoch 153 batch 0 loss 0.01075541041791439\n",
      "Training: epoch 153 batch 10 loss 0.005217339377850294\n",
      "Training: epoch 153 batch 20 loss 0.005487443879246712\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test: epoch 153 batch 0 loss 0.0038461415097117424\n",
      "epoch 153 finished - avarage train loss 0.008205766323953867  avarage test loss 0.012595276348292828\n",
      "Training: epoch 154 batch 0 loss 0.0060681067407131195\n",
      "Training: epoch 154 batch 10 loss 0.006359025835990906\n",
      "Training: epoch 154 batch 20 loss 0.006758254952728748\n",
      "Test: epoch 154 batch 0 loss 0.004625380039215088\n",
      "epoch 154 finished - avarage train loss 0.007734676831048624  avarage test loss 0.013128704042173922\n",
      "Training: epoch 155 batch 0 loss 0.0058425371535122395\n",
      "Training: epoch 155 batch 10 loss 0.0042416611686348915\n",
      "Training: epoch 155 batch 20 loss 0.004030046984553337\n",
      "Test: epoch 155 batch 0 loss 0.005139654036611319\n",
      "epoch 155 finished - avarage train loss 0.007126043590813361  avarage test loss 0.014304881216958165\n",
      "Training: epoch 156 batch 0 loss 0.009408578276634216\n",
      "Training: epoch 156 batch 10 loss 0.004739502910524607\n",
      "Training: epoch 156 batch 20 loss 0.009258300997316837\n",
      "Test: epoch 156 batch 0 loss 0.0037603983655571938\n",
      "epoch 156 finished - avarage train loss 0.010203805214179486  avarage test loss 0.013571273302659392\n",
      "Training: epoch 157 batch 0 loss 0.008104750886559486\n",
      "Training: epoch 157 batch 10 loss 0.0054754153825342655\n",
      "Training: epoch 157 batch 20 loss 0.005885772872716188\n",
      "Test: epoch 157 batch 0 loss 0.004324852488934994\n",
      "epoch 157 finished - avarage train loss 0.008238353838758736  avarage test loss 0.013108031474985182\n",
      "Training: epoch 158 batch 0 loss 0.0022878125309944153\n",
      "Training: epoch 158 batch 10 loss 0.0038546156138181686\n",
      "Training: epoch 158 batch 20 loss 0.008466709405183792\n",
      "Test: epoch 158 batch 0 loss 0.005070413928478956\n",
      "epoch 158 finished - avarage train loss 0.0075909600689493375  avarage test loss 0.01329461089335382\n",
      "Training: epoch 159 batch 0 loss 0.004187727812677622\n",
      "Training: epoch 159 batch 10 loss 0.006630263291299343\n",
      "Training: epoch 159 batch 20 loss 0.0030468148179352283\n",
      "Test: epoch 159 batch 0 loss 0.0037905883509665728\n",
      "epoch 159 finished - avarage train loss 0.0073275430738155185  avarage test loss 0.013050519803073257\n",
      "Training: epoch 160 batch 0 loss 0.004327551927417517\n",
      "Training: epoch 160 batch 10 loss 0.011083727702498436\n",
      "Training: epoch 160 batch 20 loss 0.005060539115220308\n",
      "Test: epoch 160 batch 0 loss 0.005490545183420181\n",
      "epoch 160 finished - avarage train loss 0.008265387877051172  avarage test loss 0.01476906577590853\n",
      "Training: epoch 161 batch 0 loss 0.010902496986091137\n",
      "Training: epoch 161 batch 10 loss 0.008155526593327522\n",
      "Training: epoch 161 batch 20 loss 0.005596596747636795\n",
      "Test: epoch 161 batch 0 loss 0.004534831736236811\n",
      "epoch 161 finished - avarage train loss 0.009789910135341102  avarage test loss 0.012838299677241594\n",
      "Training: epoch 162 batch 0 loss 0.005584720056504011\n",
      "Training: epoch 162 batch 10 loss 0.0038000333588570356\n",
      "Training: epoch 162 batch 20 loss 0.005752793978899717\n",
      "Test: epoch 162 batch 0 loss 0.0037050298415124416\n",
      "epoch 162 finished - avarage train loss 0.00722154312574401  avarage test loss 0.012599650537595153\n",
      "Training: epoch 163 batch 0 loss 0.004960104823112488\n",
      "Training: epoch 163 batch 10 loss 0.004938662052154541\n",
      "Training: epoch 163 batch 20 loss 0.0046691629104316235\n",
      "Test: epoch 163 batch 0 loss 0.008008729666471481\n",
      "epoch 163 finished - avarage train loss 0.007402583836288801  avarage test loss 0.016587064834311604\n",
      "Training: epoch 164 batch 0 loss 0.007537277415394783\n",
      "Training: epoch 164 batch 10 loss 0.00888411607593298\n",
      "Training: epoch 164 batch 20 loss 0.006087129935622215\n",
      "Test: epoch 164 batch 0 loss 0.005726147908717394\n",
      "epoch 164 finished - avarage train loss 0.008702925037078816  avarage test loss 0.013428525417111814\n",
      "Training: epoch 165 batch 0 loss 0.015082948841154575\n",
      "Training: epoch 165 batch 10 loss 0.0026016701012849808\n",
      "Training: epoch 165 batch 20 loss 0.010635733604431152\n",
      "Test: epoch 165 batch 0 loss 0.005757943261414766\n",
      "epoch 165 finished - avarage train loss 0.008014363044037902  avarage test loss 0.013733640545979142\n",
      "Training: epoch 166 batch 0 loss 0.004665323533117771\n",
      "Training: epoch 166 batch 10 loss 0.00495529267936945\n",
      "Training: epoch 166 batch 20 loss 0.00745262810960412\n",
      "Test: epoch 166 batch 0 loss 0.003909417428076267\n",
      "epoch 166 finished - avarage train loss 0.007449172292675437  avarage test loss 0.012537575734313577\n",
      "Training: epoch 167 batch 0 loss 0.01394303422421217\n",
      "Training: epoch 167 batch 10 loss 0.007187108509242535\n",
      "Training: epoch 167 batch 20 loss 0.009479263797402382\n",
      "Test: epoch 167 batch 0 loss 0.004318860825151205\n",
      "epoch 167 finished - avarage train loss 0.008419035163162083  avarage test loss 0.012535503483377397\n",
      "Training: epoch 168 batch 0 loss 0.006509940605610609\n",
      "Training: epoch 168 batch 10 loss 0.004004249349236488\n",
      "Training: epoch 168 batch 20 loss 0.0015857589896768332\n",
      "Test: epoch 168 batch 0 loss 0.004114238079637289\n",
      "epoch 168 finished - avarage train loss 0.006715118531779996  avarage test loss 0.01247917185537517\n",
      "Training: epoch 169 batch 0 loss 0.006814355030655861\n",
      "Training: epoch 169 batch 10 loss 0.004542047623544931\n",
      "Training: epoch 169 batch 20 loss 0.007542067673057318\n",
      "Test: epoch 169 batch 0 loss 0.004070879425853491\n",
      "epoch 169 finished - avarage train loss 0.007896358596867529  avarage test loss 0.012397797545418143\n",
      "Training: epoch 170 batch 0 loss 0.0035578496754169464\n",
      "Training: epoch 170 batch 10 loss 0.007099995389580727\n",
      "Training: epoch 170 batch 20 loss 0.00528944656252861\n",
      "Test: epoch 170 batch 0 loss 0.004271455574780703\n",
      "epoch 170 finished - avarage train loss 0.008169578950338322  avarage test loss 0.012537301110569388\n",
      "Training: epoch 171 batch 0 loss 0.003921983763575554\n",
      "Training: epoch 171 batch 10 loss 0.005898457020521164\n",
      "Training: epoch 171 batch 20 loss 0.006333214696496725\n",
      "Test: epoch 171 batch 0 loss 0.004033127333968878\n",
      "epoch 171 finished - avarage train loss 0.00909694154939518  avarage test loss 0.012471505964640528\n",
      "Training: epoch 172 batch 0 loss 0.009988774545490742\n",
      "Training: epoch 172 batch 10 loss 0.007127724122256041\n",
      "Training: epoch 172 batch 20 loss 0.007591070607304573\n",
      "Test: epoch 172 batch 0 loss 0.004295186139643192\n",
      "epoch 172 finished - avarage train loss 0.007318881918387166  avarage test loss 0.013343592057935894\n",
      "Training: epoch 173 batch 0 loss 0.01217894721776247\n",
      "Training: epoch 173 batch 10 loss 0.005832106806337833\n",
      "Training: epoch 173 batch 20 loss 0.004996396601200104\n",
      "Test: epoch 173 batch 0 loss 0.0038412907160818577\n",
      "epoch 173 finished - avarage train loss 0.007322002900764346  avarage test loss 0.012435226468369365\n",
      "Training: epoch 174 batch 0 loss 0.005870174616575241\n",
      "Training: epoch 174 batch 10 loss 0.0030271278228610754\n",
      "Training: epoch 174 batch 20 loss 0.0040608556009829044\n",
      "Test: epoch 174 batch 0 loss 0.004249583464115858\n",
      "epoch 174 finished - avarage train loss 0.0077163989270298645  avarage test loss 0.012967387912794948\n",
      "Training: epoch 175 batch 0 loss 0.006357396021485329\n",
      "Training: epoch 175 batch 10 loss 0.003182683838531375\n",
      "Training: epoch 175 batch 20 loss 0.006824230309575796\n",
      "Test: epoch 175 batch 0 loss 0.0036417986266314983\n",
      "epoch 175 finished - avarage train loss 0.007735601634365217  avarage test loss 0.01248243183363229\n",
      "Training: epoch 176 batch 0 loss 0.00906759686768055\n",
      "Training: epoch 176 batch 10 loss 0.004953958094120026\n",
      "Training: epoch 176 batch 20 loss 0.004839134402573109\n",
      "Test: epoch 176 batch 0 loss 0.005159544292837381\n",
      "epoch 176 finished - avarage train loss 0.007694735689538306  avarage test loss 0.013499613967724144\n",
      "Training: epoch 177 batch 0 loss 0.007619835436344147\n",
      "Training: epoch 177 batch 10 loss 0.006647690664976835\n",
      "Training: epoch 177 batch 20 loss 0.008071968331933022\n",
      "Test: epoch 177 batch 0 loss 0.004728286527097225\n",
      "epoch 177 finished - avarage train loss 0.008293414223489576  avarage test loss 0.014103113324381411\n",
      "Training: epoch 178 batch 0 loss 0.0036812806501984596\n",
      "Training: epoch 178 batch 10 loss 0.0056097423657774925\n",
      "Training: epoch 178 batch 20 loss 0.008599603548645973\n",
      "Test: epoch 178 batch 0 loss 0.004126147832721472\n",
      "epoch 178 finished - avarage train loss 0.007213739630091807  avarage test loss 0.01243981352308765\n",
      "Training: epoch 179 batch 0 loss 0.005452940706163645\n",
      "Training: epoch 179 batch 10 loss 0.004748436156660318\n",
      "Training: epoch 179 batch 20 loss 0.005043979734182358\n",
      "Test: epoch 179 batch 0 loss 0.00446406751871109\n",
      "epoch 179 finished - avarage train loss 0.00882141836437172  avarage test loss 0.01282447692938149\n",
      "Training: epoch 180 batch 0 loss 0.008496754802763462\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training: epoch 180 batch 10 loss 0.005342478863894939\n",
      "Training: epoch 180 batch 20 loss 0.006712507456541061\n",
      "Test: epoch 180 batch 0 loss 0.008325852453708649\n",
      "epoch 180 finished - avarage train loss 0.008027016345796913  avarage test loss 0.01758594438433647\n",
      "Training: epoch 181 batch 0 loss 0.009923292323946953\n",
      "Training: epoch 181 batch 10 loss 0.013892974704504013\n",
      "Training: epoch 181 batch 20 loss 0.006256705150008202\n",
      "Test: epoch 181 batch 0 loss 0.005592614412307739\n",
      "epoch 181 finished - avarage train loss 0.009707593856828994  avarage test loss 0.015569633338600397\n",
      "Training: epoch 182 batch 0 loss 0.006881684064865112\n",
      "Training: epoch 182 batch 10 loss 0.006441992707550526\n",
      "Training: epoch 182 batch 20 loss 0.006414992269128561\n",
      "Test: epoch 182 batch 0 loss 0.004273072816431522\n",
      "epoch 182 finished - avarage train loss 0.007891638084025732  avarage test loss 0.012598235392943025\n",
      "Training: epoch 183 batch 0 loss 0.004562428221106529\n",
      "Training: epoch 183 batch 10 loss 0.005173282697796822\n",
      "Training: epoch 183 batch 20 loss 0.0065030730329453945\n",
      "Test: epoch 183 batch 0 loss 0.005728942342102528\n",
      "epoch 183 finished - avarage train loss 0.0076602121879314556  avarage test loss 0.014128706301562488\n",
      "Training: epoch 184 batch 0 loss 0.0162982065230608\n",
      "Training: epoch 184 batch 10 loss 0.006106714252382517\n",
      "Training: epoch 184 batch 20 loss 0.005913307890295982\n",
      "Test: epoch 184 batch 0 loss 0.004308047238737345\n",
      "epoch 184 finished - avarage train loss 0.0077818636220462365  avarage test loss 0.012601529131643474\n",
      "Training: epoch 185 batch 0 loss 0.004884494934231043\n",
      "Training: epoch 185 batch 10 loss 0.003755145473405719\n",
      "Training: epoch 185 batch 20 loss 0.010426423512399197\n",
      "Test: epoch 185 batch 0 loss 0.005304330959916115\n",
      "epoch 185 finished - avarage train loss 0.007663735413345797  avarage test loss 0.013369821303058416\n",
      "Training: epoch 186 batch 0 loss 0.007087825797498226\n",
      "Training: epoch 186 batch 10 loss 0.006780443713068962\n",
      "Training: epoch 186 batch 20 loss 0.005829876754432917\n",
      "Test: epoch 186 batch 0 loss 0.005401073954999447\n",
      "epoch 186 finished - avarage train loss 0.008120626516254812  avarage test loss 0.013458441477268934\n",
      "Training: epoch 187 batch 0 loss 0.012285887263715267\n",
      "Training: epoch 187 batch 10 loss 0.00968444999307394\n",
      "Training: epoch 187 batch 20 loss 0.005958884488791227\n",
      "Test: epoch 187 batch 0 loss 0.004699111916124821\n",
      "epoch 187 finished - avarage train loss 0.007963480339546141  avarage test loss 0.013066495303064585\n",
      "Training: epoch 188 batch 0 loss 0.008057564496994019\n",
      "Training: epoch 188 batch 10 loss 0.007118667010217905\n",
      "Training: epoch 188 batch 20 loss 0.0071515184827148914\n",
      "Test: epoch 188 batch 0 loss 0.005087127909064293\n",
      "epoch 188 finished - avarage train loss 0.008312123530576455  avarage test loss 0.01420266879722476\n",
      "Training: epoch 189 batch 0 loss 0.007930153980851173\n",
      "Training: epoch 189 batch 10 loss 0.007581927813589573\n",
      "Training: epoch 189 batch 20 loss 0.0110618956387043\n",
      "Test: epoch 189 batch 0 loss 0.0041776602156460285\n",
      "epoch 189 finished - avarage train loss 0.00837564307008067  avarage test loss 0.012767162988893688\n",
      "Training: epoch 190 batch 0 loss 0.009502594359219074\n",
      "Training: epoch 190 batch 10 loss 0.004270374774932861\n",
      "Training: epoch 190 batch 20 loss 0.0051997569389641285\n",
      "Test: epoch 190 batch 0 loss 0.004989459179341793\n",
      "epoch 190 finished - avarage train loss 0.006981480360866107  avarage test loss 0.013267871574498713\n",
      "Training: epoch 191 batch 0 loss 0.005884572863578796\n",
      "Training: epoch 191 batch 10 loss 0.014334690757095814\n",
      "Training: epoch 191 batch 20 loss 0.008387754671275616\n",
      "Test: epoch 191 batch 0 loss 0.0037872621323913336\n",
      "epoch 191 finished - avarage train loss 0.007793220105291954  avarage test loss 0.012828669336158782\n",
      "Training: epoch 192 batch 0 loss 0.007032074499875307\n",
      "Training: epoch 192 batch 10 loss 0.0037972470745444298\n",
      "Training: epoch 192 batch 20 loss 0.00330357626080513\n",
      "Test: epoch 192 batch 0 loss 0.005193416960537434\n",
      "epoch 192 finished - avarage train loss 0.006966261161041671  avarage test loss 0.013723154668696225\n",
      "Training: epoch 193 batch 0 loss 0.013392150402069092\n",
      "Training: epoch 193 batch 10 loss 0.01055392436683178\n",
      "Training: epoch 193 batch 20 loss 0.003265171777456999\n",
      "Test: epoch 193 batch 0 loss 0.004530391655862331\n",
      "epoch 193 finished - avarage train loss 0.008208767623351565  avarage test loss 0.01324785640463233\n",
      "Training: epoch 194 batch 0 loss 0.010006096214056015\n",
      "Training: epoch 194 batch 10 loss 0.003741417545825243\n",
      "Training: epoch 194 batch 20 loss 0.004860821180045605\n",
      "Test: epoch 194 batch 0 loss 0.004646322689950466\n",
      "epoch 194 finished - avarage train loss 0.007234651322380222  avarage test loss 0.013073817826807499\n",
      "Training: epoch 195 batch 0 loss 0.004569666460156441\n",
      "Training: epoch 195 batch 10 loss 0.010035655461251736\n",
      "Training: epoch 195 batch 20 loss 0.006756477989256382\n",
      "Test: epoch 195 batch 0 loss 0.004037186037749052\n",
      "epoch 195 finished - avarage train loss 0.00852954226289073  avarage test loss 0.012754526629578322\n",
      "Training: epoch 196 batch 0 loss 0.009456997737288475\n",
      "Training: epoch 196 batch 10 loss 0.0048750462010502815\n",
      "Training: epoch 196 batch 20 loss 0.0072725084610283375\n",
      "Test: epoch 196 batch 0 loss 0.004324822220951319\n",
      "epoch 196 finished - avarage train loss 0.00782648024374041  avarage test loss 0.013977382564917207\n",
      "Training: epoch 197 batch 0 loss 0.005611194763332605\n",
      "Training: epoch 197 batch 10 loss 0.006560084875673056\n",
      "Training: epoch 197 batch 20 loss 0.0037720936816185713\n",
      "Test: epoch 197 batch 0 loss 0.005374799948185682\n",
      "epoch 197 finished - avarage train loss 0.007551509292860483  avarage test loss 0.013551001786254346\n",
      "Training: epoch 198 batch 0 loss 0.005161478649824858\n",
      "Training: epoch 198 batch 10 loss 0.00650390749797225\n",
      "Training: epoch 198 batch 20 loss 0.007856393232941628\n",
      "Test: epoch 198 batch 0 loss 0.0038963027764111757\n",
      "epoch 198 finished - avarage train loss 0.007291207395108609  avarage test loss 0.013026411470491439\n",
      "Training: epoch 199 batch 0 loss 0.004166800994426012\n",
      "Training: epoch 199 batch 10 loss 0.006795190274715424\n",
      "Training: epoch 199 batch 20 loss 0.005446614697575569\n",
      "Test: epoch 199 batch 0 loss 0.004329856485128403\n",
      "epoch 199 finished - avarage train loss 0.007194005442654778  avarage test loss 0.012780503078829497\n",
      "Training: epoch 0 batch 0 loss 0.5689870119094849\n",
      "Training: epoch 0 batch 10 loss 0.5964903831481934\n",
      "Training: epoch 0 batch 20 loss 0.4401160478591919\n",
      "Test: epoch 0 batch 0 loss 0.4422934055328369\n",
      "epoch 0 finished - avarage train loss 0.5439721438391455  avarage test loss 0.5242450758814812\n",
      "Training: epoch 1 batch 0 loss 0.5607456564903259\n",
      "Training: epoch 1 batch 10 loss 0.686597466468811\n",
      "Training: epoch 1 batch 20 loss 0.5070798993110657\n",
      "Test: epoch 1 batch 0 loss 0.43324142694473267\n",
      "epoch 1 finished - avarage train loss 0.5084620714187622  avarage test loss 0.5179993510246277\n",
      "Training: epoch 2 batch 0 loss 0.42158007621765137\n",
      "Training: epoch 2 batch 10 loss 0.5018696188926697\n",
      "Training: epoch 2 batch 20 loss 0.527289867401123\n",
      "Test: epoch 2 batch 0 loss 0.42252451181411743\n",
      "epoch 2 finished - avarage train loss 0.5190699634880855  avarage test loss 0.5052650272846222\n",
      "Training: epoch 3 batch 0 loss 0.5891655087471008\n",
      "Training: epoch 3 batch 10 loss 0.49309059977531433\n",
      "Training: epoch 3 batch 20 loss 0.1536611020565033\n",
      "Test: epoch 3 batch 0 loss 0.08353355526924133\n",
      "epoch 3 finished - avarage train loss 0.3306815166925562  avarage test loss 0.07868759427219629\n",
      "Training: epoch 4 batch 0 loss 0.06501417607069016\n",
      "Training: epoch 4 batch 10 loss 0.055885929614305496\n",
      "Training: epoch 4 batch 20 loss 0.038159117102622986\n",
      "Test: epoch 4 batch 0 loss 0.02681219018995762\n",
      "epoch 4 finished - avarage train loss 0.04546479700968183  avarage test loss 0.03539211628958583\n",
      "Training: epoch 5 batch 0 loss 0.0196318868547678\n",
      "Training: epoch 5 batch 10 loss 0.014665655791759491\n",
      "Training: epoch 5 batch 20 loss 0.01370991300791502\n",
      "Test: epoch 5 batch 0 loss 0.017827190458774567\n",
      "epoch 5 finished - avarage train loss 0.015158016507610166  avarage test loss 0.029117297148332\n",
      "Training: epoch 6 batch 0 loss 0.019808119162917137\n",
      "Training: epoch 6 batch 10 loss 0.01274598203599453\n",
      "Training: epoch 6 batch 20 loss 0.004969666246324778\n",
      "Test: epoch 6 batch 0 loss 0.013900631107389927\n",
      "epoch 6 finished - avarage train loss 0.011564919066711747  avarage test loss 0.025377083336934447\n",
      "Training: epoch 7 batch 0 loss 0.007245175540447235\n",
      "Training: epoch 7 batch 10 loss 0.00851476565003395\n",
      "Training: epoch 7 batch 20 loss 0.008241460658609867\n",
      "Test: epoch 7 batch 0 loss 0.010177789255976677\n",
      "epoch 7 finished - avarage train loss 0.010128795426210454  avarage test loss 0.023837400134652853\n",
      "Training: epoch 8 batch 0 loss 0.01049420703202486\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training: epoch 8 batch 10 loss 0.014582318253815174\n",
      "Training: epoch 8 batch 20 loss 0.0060138762928545475\n",
      "Test: epoch 8 batch 0 loss 0.00881548784673214\n",
      "epoch 8 finished - avarage train loss 0.010066163148088702  avarage test loss 0.02312761591747403\n",
      "Training: epoch 9 batch 0 loss 0.008148816414177418\n",
      "Training: epoch 9 batch 10 loss 0.00853328499943018\n",
      "Training: epoch 9 batch 20 loss 0.006919106934219599\n",
      "Test: epoch 9 batch 0 loss 0.008120762184262276\n",
      "epoch 9 finished - avarage train loss 0.0091249273672443  avarage test loss 0.022681349655613303\n",
      "Training: epoch 10 batch 0 loss 0.005641126073896885\n",
      "Training: epoch 10 batch 10 loss 0.005941620096564293\n",
      "Training: epoch 10 batch 20 loss 0.004584039561450481\n",
      "Test: epoch 10 batch 0 loss 0.0075587560422718525\n",
      "epoch 10 finished - avarage train loss 0.008552128449082375  avarage test loss 0.02223614777904004\n",
      "Training: epoch 11 batch 0 loss 0.006287856493145227\n",
      "Training: epoch 11 batch 10 loss 0.009958758018910885\n",
      "Training: epoch 11 batch 20 loss 0.005655313841998577\n",
      "Test: epoch 11 batch 0 loss 0.007169014308601618\n",
      "epoch 11 finished - avarage train loss 0.008772730843388829  avarage test loss 0.022064413875341415\n",
      "Training: epoch 12 batch 0 loss 0.006335384212434292\n",
      "Training: epoch 12 batch 10 loss 0.0038514016196131706\n",
      "Training: epoch 12 batch 20 loss 0.011988605372607708\n",
      "Test: epoch 12 batch 0 loss 0.006625966634601355\n",
      "epoch 12 finished - avarage train loss 0.008193109762565843  avarage test loss 0.021614776458591223\n",
      "Training: epoch 13 batch 0 loss 0.010964911431074142\n",
      "Training: epoch 13 batch 10 loss 0.004216326866298914\n",
      "Training: epoch 13 batch 20 loss 0.006670706439763308\n",
      "Test: epoch 13 batch 0 loss 0.006218249443918467\n",
      "epoch 13 finished - avarage train loss 0.008418929377763435  avarage test loss 0.01845682819839567\n",
      "Training: epoch 14 batch 0 loss 0.008082814514636993\n",
      "Training: epoch 14 batch 10 loss 0.008049213327467442\n",
      "Training: epoch 14 batch 20 loss 0.007571229245513678\n",
      "Test: epoch 14 batch 0 loss 0.006080317310988903\n",
      "epoch 14 finished - avarage train loss 0.007669104343472883  avarage test loss 0.02015437837690115\n",
      "Training: epoch 15 batch 0 loss 0.007011990062892437\n",
      "Training: epoch 15 batch 10 loss 0.006144525017589331\n",
      "Training: epoch 15 batch 20 loss 0.005468445364385843\n",
      "Test: epoch 15 batch 0 loss 0.006118157412856817\n",
      "epoch 15 finished - avarage train loss 0.008296526454646012  avarage test loss 0.019309776951558888\n",
      "Training: epoch 16 batch 0 loss 0.00436431635171175\n",
      "Training: epoch 16 batch 10 loss 0.009045521728694439\n",
      "Training: epoch 16 batch 20 loss 0.007886398583650589\n",
      "Test: epoch 16 batch 0 loss 0.0054468028247356415\n",
      "epoch 16 finished - avarage train loss 0.008005550524605245  avarage test loss 0.015915724681690335\n",
      "Training: epoch 17 batch 0 loss 0.007811712101101875\n",
      "Training: epoch 17 batch 10 loss 0.007282373029738665\n",
      "Training: epoch 17 batch 20 loss 0.004642937798053026\n",
      "Test: epoch 17 batch 0 loss 0.006361839361488819\n",
      "epoch 17 finished - avarage train loss 0.0068454875867685365  avarage test loss 0.014617913286201656\n",
      "Training: epoch 18 batch 0 loss 0.009086817502975464\n",
      "Training: epoch 18 batch 10 loss 0.003743072273209691\n",
      "Training: epoch 18 batch 20 loss 0.00837588869035244\n",
      "Test: epoch 18 batch 0 loss 0.005400742404162884\n",
      "epoch 18 finished - avarage train loss 0.007145903185652247  avarage test loss 0.013797150691971183\n",
      "Training: epoch 19 batch 0 loss 0.003840101882815361\n",
      "Training: epoch 19 batch 10 loss 0.006227233447134495\n",
      "Training: epoch 19 batch 20 loss 0.009818320162594318\n",
      "Test: epoch 19 batch 0 loss 0.005206055007874966\n",
      "epoch 19 finished - avarage train loss 0.009234873915155387  avarage test loss 0.01737146300729364\n",
      "Training: epoch 20 batch 0 loss 0.006434503011405468\n",
      "Training: epoch 20 batch 10 loss 0.008641781285405159\n",
      "Training: epoch 20 batch 20 loss 0.004457372706383467\n",
      "Test: epoch 20 batch 0 loss 0.0057685477659106255\n",
      "epoch 20 finished - avarage train loss 0.00787974759165583  avarage test loss 0.01770703517831862\n",
      "Training: epoch 21 batch 0 loss 0.00417797127738595\n",
      "Training: epoch 21 batch 10 loss 0.010180283337831497\n",
      "Training: epoch 21 batch 20 loss 0.00724298320710659\n",
      "Test: epoch 21 batch 0 loss 0.005581313744187355\n",
      "epoch 21 finished - avarage train loss 0.008807568786794255  avarage test loss 0.013744849944487214\n",
      "Training: epoch 22 batch 0 loss 0.007005940657109022\n",
      "Training: epoch 22 batch 10 loss 0.006841825321316719\n",
      "Training: epoch 22 batch 20 loss 0.016779514029622078\n",
      "Test: epoch 22 batch 0 loss 0.004511578008532524\n",
      "epoch 22 finished - avarage train loss 0.00761581697212211  avarage test loss 0.014842963893897831\n",
      "Training: epoch 23 batch 0 loss 0.004987090360373259\n",
      "Training: epoch 23 batch 10 loss 0.004968860652297735\n",
      "Training: epoch 23 batch 20 loss 0.007204324938356876\n",
      "Test: epoch 23 batch 0 loss 0.007261354010552168\n",
      "epoch 23 finished - avarage train loss 0.008417975976420888  avarage test loss 0.01606345025356859\n",
      "Training: epoch 24 batch 0 loss 0.011312509886920452\n",
      "Training: epoch 24 batch 10 loss 0.005455877631902695\n",
      "Training: epoch 24 batch 20 loss 0.006794297602027655\n",
      "Test: epoch 24 batch 0 loss 0.006250903941690922\n",
      "epoch 24 finished - avarage train loss 0.008876127094544214  avarage test loss 0.016037968336604536\n",
      "Training: epoch 25 batch 0 loss 0.007427247241139412\n",
      "Training: epoch 25 batch 10 loss 0.007057413458824158\n",
      "Training: epoch 25 batch 20 loss 0.0077160499058663845\n",
      "Test: epoch 25 batch 0 loss 0.005538636818528175\n",
      "epoch 25 finished - avarage train loss 0.008207226172089577  avarage test loss 0.016567050712183118\n",
      "Training: epoch 26 batch 0 loss 0.010131384246051311\n",
      "Training: epoch 26 batch 10 loss 0.009382585063576698\n",
      "Training: epoch 26 batch 20 loss 0.006898470222949982\n",
      "Test: epoch 26 batch 0 loss 0.004635816439986229\n",
      "epoch 26 finished - avarage train loss 0.008859650512900332  avarage test loss 0.01625083980616182\n",
      "Training: epoch 27 batch 0 loss 0.006866848096251488\n",
      "Training: epoch 27 batch 10 loss 0.009361258707940578\n",
      "Training: epoch 27 batch 20 loss 0.005749558098614216\n",
      "Test: epoch 27 batch 0 loss 0.006207773461937904\n",
      "epoch 27 finished - avarage train loss 0.008007166545902347  avarage test loss 0.014677427592687309\n",
      "Training: epoch 28 batch 0 loss 0.007135485298931599\n",
      "Training: epoch 28 batch 10 loss 0.00801097322255373\n",
      "Training: epoch 28 batch 20 loss 0.005675238557159901\n",
      "Test: epoch 28 batch 0 loss 0.004249232821166515\n",
      "epoch 28 finished - avarage train loss 0.009326808154582977  avarage test loss 0.014892842154949903\n",
      "Training: epoch 29 batch 0 loss 0.008593326434493065\n",
      "Training: epoch 29 batch 10 loss 0.008962634019553661\n",
      "Training: epoch 29 batch 20 loss 0.006615479476749897\n",
      "Test: epoch 29 batch 0 loss 0.004702053964138031\n",
      "epoch 29 finished - avarage train loss 0.00803966075181961  avarage test loss 0.01719449902884662\n",
      "Training: epoch 30 batch 0 loss 0.004851764999330044\n",
      "Training: epoch 30 batch 10 loss 0.011521770618855953\n",
      "Training: epoch 30 batch 20 loss 0.005410933401435614\n",
      "Test: epoch 30 batch 0 loss 0.0048699090257287025\n",
      "epoch 30 finished - avarage train loss 0.007771683375126329  avarage test loss 0.015641822712495923\n",
      "Training: epoch 31 batch 0 loss 0.009416702203452587\n",
      "Training: epoch 31 batch 10 loss 0.008067520335316658\n",
      "Training: epoch 31 batch 20 loss 0.006095612421631813\n",
      "Test: epoch 31 batch 0 loss 0.004554950166493654\n",
      "epoch 31 finished - avarage train loss 0.007412042075382738  avarage test loss 0.013939744210802019\n",
      "Training: epoch 32 batch 0 loss 0.007032213266938925\n",
      "Training: epoch 32 batch 10 loss 0.003797810059040785\n",
      "Training: epoch 32 batch 20 loss 0.005295926705002785\n",
      "Test: epoch 32 batch 0 loss 0.004925546236336231\n",
      "epoch 32 finished - avarage train loss 0.00782698398488092  avarage test loss 0.01370511925779283\n",
      "Training: epoch 33 batch 0 loss 0.004683421924710274\n",
      "Training: epoch 33 batch 10 loss 0.010472088120877743\n",
      "Training: epoch 33 batch 20 loss 0.006682003848254681\n",
      "Test: epoch 33 batch 0 loss 0.003412123303860426\n",
      "epoch 33 finished - avarage train loss 0.00834334021883792  avarage test loss 0.013490499346517026\n",
      "Training: epoch 34 batch 0 loss 0.009968788363039494\n",
      "Training: epoch 34 batch 10 loss 0.00905389990657568\n",
      "Training: epoch 34 batch 20 loss 0.007829640991985798\n",
      "Test: epoch 34 batch 0 loss 0.0055470895022153854\n",
      "epoch 34 finished - avarage train loss 0.008755006907700464  avarage test loss 0.01379425695631653\n",
      "Training: epoch 35 batch 0 loss 0.0048174671828746796\n",
      "Training: epoch 35 batch 10 loss 0.007610484026372433\n",
      "Training: epoch 35 batch 20 loss 0.0063436091877520084\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test: epoch 35 batch 0 loss 0.0038100816309452057\n",
      "epoch 35 finished - avarage train loss 0.007506467546496926  avarage test loss 0.01319323480129242\n",
      "Training: epoch 36 batch 0 loss 0.004309484269469976\n",
      "Training: epoch 36 batch 10 loss 0.005989952478557825\n",
      "Training: epoch 36 batch 20 loss 0.01644711382687092\n",
      "Test: epoch 36 batch 0 loss 0.004683393053710461\n",
      "epoch 36 finished - avarage train loss 0.008213538323240033  avarage test loss 0.013882943545468152\n",
      "Training: epoch 37 batch 0 loss 0.005777741316705942\n",
      "Training: epoch 37 batch 10 loss 0.00597483292222023\n",
      "Training: epoch 37 batch 20 loss 0.007727921940386295\n",
      "Test: epoch 37 batch 0 loss 0.005650319624692202\n",
      "epoch 37 finished - avarage train loss 0.00920429400264703  avarage test loss 0.013841407955624163\n",
      "Training: epoch 38 batch 0 loss 0.00757082924246788\n",
      "Training: epoch 38 batch 10 loss 0.01406325027346611\n",
      "Training: epoch 38 batch 20 loss 0.006508007645606995\n",
      "Test: epoch 38 batch 0 loss 0.0058243898674845695\n",
      "epoch 38 finished - avarage train loss 0.007782812406919126  avarage test loss 0.015600137412548065\n",
      "Training: epoch 39 batch 0 loss 0.007873916998505592\n",
      "Training: epoch 39 batch 10 loss 0.007341904100030661\n",
      "Training: epoch 39 batch 20 loss 0.007905175909399986\n",
      "Test: epoch 39 batch 0 loss 0.004382776096463203\n",
      "epoch 39 finished - avarage train loss 0.009055503001757738  avarage test loss 0.013473769882693887\n",
      "Training: epoch 40 batch 0 loss 0.0058845169842243195\n",
      "Training: epoch 40 batch 10 loss 0.014727935194969177\n",
      "Training: epoch 40 batch 20 loss 0.009156639687716961\n",
      "Test: epoch 40 batch 0 loss 0.006347130052745342\n",
      "epoch 40 finished - avarage train loss 0.007999248224599608  avarage test loss 0.014317295281216502\n",
      "Training: epoch 41 batch 0 loss 0.005278674885630608\n",
      "Training: epoch 41 batch 10 loss 0.009489995427429676\n",
      "Training: epoch 41 batch 20 loss 0.00406488636508584\n",
      "Test: epoch 41 batch 0 loss 0.0033093246165663004\n",
      "epoch 41 finished - avarage train loss 0.008393298585674372  avarage test loss 0.013200335146393627\n",
      "Training: epoch 42 batch 0 loss 0.009748834185302258\n",
      "Training: epoch 42 batch 10 loss 0.008219750598073006\n",
      "Training: epoch 42 batch 20 loss 0.004211690742522478\n",
      "Test: epoch 42 batch 0 loss 0.004428233951330185\n",
      "epoch 42 finished - avarage train loss 0.008294961656090515  avarage test loss 0.01315731240902096\n",
      "Training: epoch 43 batch 0 loss 0.0062117911875247955\n",
      "Training: epoch 43 batch 10 loss 0.007311360444873571\n",
      "Training: epoch 43 batch 20 loss 0.004676627926528454\n",
      "Test: epoch 43 batch 0 loss 0.003530173795297742\n",
      "epoch 43 finished - avarage train loss 0.008764856262132525  avarage test loss 0.015325250511523336\n",
      "Training: epoch 44 batch 0 loss 0.007410218007862568\n",
      "Training: epoch 44 batch 10 loss 0.003912066109478474\n",
      "Training: epoch 44 batch 20 loss 0.007796528749167919\n",
      "Test: epoch 44 batch 0 loss 0.004622200503945351\n",
      "epoch 44 finished - avarage train loss 0.008558950981061006  avarage test loss 0.015440054354257882\n",
      "Training: epoch 45 batch 0 loss 0.010530092753469944\n",
      "Training: epoch 45 batch 10 loss 0.005015779286623001\n",
      "Training: epoch 45 batch 20 loss 0.006372146774083376\n",
      "Test: epoch 45 batch 0 loss 0.005377595312893391\n",
      "epoch 45 finished - avarage train loss 0.008595707192053569  avarage test loss 0.019788461504504085\n",
      "Training: epoch 46 batch 0 loss 0.006767540704458952\n",
      "Training: epoch 46 batch 10 loss 0.006383688189089298\n",
      "Training: epoch 46 batch 20 loss 0.004125459119677544\n",
      "Test: epoch 46 batch 0 loss 0.005859001539647579\n",
      "epoch 46 finished - avarage train loss 0.007715142428361136  avarage test loss 0.021593446959741414\n",
      "Training: epoch 47 batch 0 loss 0.0034016401041299105\n",
      "Training: epoch 47 batch 10 loss 0.005792297422885895\n",
      "Training: epoch 47 batch 20 loss 0.007922999560832977\n",
      "Test: epoch 47 batch 0 loss 0.005026065278798342\n",
      "epoch 47 finished - avarage train loss 0.007927028622863621  avarage test loss 0.016906654345802963\n",
      "Training: epoch 48 batch 0 loss 0.007317911833524704\n",
      "Training: epoch 48 batch 10 loss 0.00765842292457819\n",
      "Training: epoch 48 batch 20 loss 0.00487386342138052\n",
      "Test: epoch 48 batch 0 loss 0.006065942347049713\n",
      "epoch 48 finished - avarage train loss 0.008046490621977839  avarage test loss 0.015613176859915257\n",
      "Training: epoch 49 batch 0 loss 0.01144214067608118\n",
      "Training: epoch 49 batch 10 loss 0.007397168315947056\n",
      "Training: epoch 49 batch 20 loss 0.008561075665056705\n",
      "Test: epoch 49 batch 0 loss 0.008157305419445038\n",
      "epoch 49 finished - avarage train loss 0.010321449151198411  avarage test loss 0.01593644847162068\n",
      "Training: epoch 50 batch 0 loss 0.014637168496847153\n",
      "Training: epoch 50 batch 10 loss 0.0047553726471960545\n",
      "Training: epoch 50 batch 20 loss 0.009908548556268215\n",
      "Test: epoch 50 batch 0 loss 0.004574634600430727\n",
      "epoch 50 finished - avarage train loss 0.008112379494283733  avarage test loss 0.016008802107535303\n",
      "Training: epoch 51 batch 0 loss 0.006864029448479414\n",
      "Training: epoch 51 batch 10 loss 0.00898771546781063\n",
      "Training: epoch 51 batch 20 loss 0.004931490868330002\n",
      "Test: epoch 51 batch 0 loss 0.0049240184016525745\n",
      "epoch 51 finished - avarage train loss 0.007435295192910166  avarage test loss 0.014049226650968194\n",
      "Training: epoch 52 batch 0 loss 0.0032501420937478542\n",
      "Training: epoch 52 batch 10 loss 0.004729804117232561\n",
      "Training: epoch 52 batch 20 loss 0.0054209064692258835\n",
      "Test: epoch 52 batch 0 loss 0.003687020158395171\n",
      "epoch 52 finished - avarage train loss 0.007728622046074477  avarage test loss 0.013985362311359495\n",
      "Training: epoch 53 batch 0 loss 0.003080416237935424\n",
      "Training: epoch 53 batch 10 loss 0.006209324114024639\n",
      "Training: epoch 53 batch 20 loss 0.008624903857707977\n",
      "Test: epoch 53 batch 0 loss 0.006470542401075363\n",
      "epoch 53 finished - avarage train loss 0.008408634321250278  avarage test loss 0.016162975691258907\n",
      "Training: epoch 54 batch 0 loss 0.00832507573068142\n",
      "Training: epoch 54 batch 10 loss 0.007625112310051918\n",
      "Training: epoch 54 batch 20 loss 0.004062808584421873\n",
      "Test: epoch 54 batch 0 loss 0.00606904923915863\n",
      "epoch 54 finished - avarage train loss 0.008239009489851266  avarage test loss 0.01435178832616657\n",
      "Training: epoch 55 batch 0 loss 0.009896750561892986\n",
      "Training: epoch 55 batch 10 loss 0.009125012904405594\n",
      "Training: epoch 55 batch 20 loss 0.014498438686132431\n",
      "Test: epoch 55 batch 0 loss 0.006166854873299599\n",
      "epoch 55 finished - avarage train loss 0.011317103763978029  avarage test loss 0.014279688009992242\n",
      "Training: epoch 56 batch 0 loss 0.004942749161273241\n",
      "Training: epoch 56 batch 10 loss 0.008670108392834663\n",
      "Training: epoch 56 batch 20 loss 0.007230426650494337\n",
      "Test: epoch 56 batch 0 loss 0.0033133940305560827\n",
      "epoch 56 finished - avarage train loss 0.00793036146506924  avarage test loss 0.014123031927738339\n",
      "Training: epoch 57 batch 0 loss 0.011766149662435055\n",
      "Training: epoch 57 batch 10 loss 0.005799717269837856\n",
      "Training: epoch 57 batch 20 loss 0.006527556572109461\n",
      "Test: epoch 57 batch 0 loss 0.004090168513357639\n",
      "epoch 57 finished - avarage train loss 0.006605620841206661  avarage test loss 0.01471855747513473\n",
      "Training: epoch 58 batch 0 loss 0.006246349308639765\n",
      "Training: epoch 58 batch 10 loss 0.003923920914530754\n",
      "Training: epoch 58 batch 20 loss 0.008734973147511482\n",
      "Test: epoch 58 batch 0 loss 0.004770317580550909\n",
      "epoch 58 finished - avarage train loss 0.007762159140587881  avarage test loss 0.013473207014612854\n",
      "Training: epoch 59 batch 0 loss 0.012560024857521057\n",
      "Training: epoch 59 batch 10 loss 0.004288846626877785\n",
      "Training: epoch 59 batch 20 loss 0.005708878394216299\n",
      "Test: epoch 59 batch 0 loss 0.003965351730585098\n",
      "epoch 59 finished - avarage train loss 0.008773739738710996  avarage test loss 0.014067045878618956\n",
      "Training: epoch 60 batch 0 loss 0.007536177523434162\n",
      "Training: epoch 60 batch 10 loss 0.0044940803200006485\n",
      "Training: epoch 60 batch 20 loss 0.0046165334060788155\n",
      "Test: epoch 60 batch 0 loss 0.005381342023611069\n",
      "epoch 60 finished - avarage train loss 0.008092615239579102  avarage test loss 0.017579283798113465\n",
      "Training: epoch 61 batch 0 loss 0.010407760739326477\n",
      "Training: epoch 61 batch 10 loss 0.022257260978221893\n",
      "Training: epoch 61 batch 20 loss 0.00714231887832284\n",
      "Test: epoch 61 batch 0 loss 0.008599469438195229\n",
      "epoch 61 finished - avarage train loss 0.011886468671005347  avarage test loss 0.022868603467941284\n",
      "Training: epoch 62 batch 0 loss 0.006315730512142181\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training: epoch 62 batch 10 loss 0.007439347915351391\n",
      "Training: epoch 62 batch 20 loss 0.006898888386785984\n",
      "Test: epoch 62 batch 0 loss 0.008075762540102005\n",
      "epoch 62 finished - avarage train loss 0.00845767626667331  avarage test loss 0.023558280430734158\n",
      "Training: epoch 63 batch 0 loss 0.01017520297318697\n",
      "Training: epoch 63 batch 10 loss 0.0077039883472025394\n",
      "Training: epoch 63 batch 20 loss 0.01012689620256424\n",
      "Test: epoch 63 batch 0 loss 0.00864451564848423\n",
      "epoch 63 finished - avarage train loss 0.010664458948605019  avarage test loss 0.02327447454445064\n",
      "Training: epoch 64 batch 0 loss 0.00814005732536316\n",
      "Training: epoch 64 batch 10 loss 0.007279310841113329\n",
      "Training: epoch 64 batch 20 loss 0.011462309397757053\n",
      "Test: epoch 64 batch 0 loss 0.009574760682880878\n",
      "epoch 64 finished - avarage train loss 0.009123935878019908  avarage test loss 0.023653742857277393\n",
      "Training: epoch 65 batch 0 loss 0.005282023921608925\n",
      "Training: epoch 65 batch 10 loss 0.016821712255477905\n",
      "Training: epoch 65 batch 20 loss 0.006416275631636381\n",
      "Test: epoch 65 batch 0 loss 0.00745747284963727\n",
      "epoch 65 finished - avarage train loss 0.008704944056105512  avarage test loss 0.022454217076301575\n",
      "Training: epoch 66 batch 0 loss 0.010152377188205719\n",
      "Training: epoch 66 batch 10 loss 0.01384175755083561\n",
      "Training: epoch 66 batch 20 loss 0.01343714538961649\n",
      "Test: epoch 66 batch 0 loss 0.0076914941892027855\n",
      "epoch 66 finished - avarage train loss 0.008541663161254135  avarage test loss 0.02179349900688976\n",
      "Training: epoch 67 batch 0 loss 0.0043328264728188515\n",
      "Training: epoch 67 batch 10 loss 0.0025094475131481886\n",
      "Training: epoch 67 batch 20 loss 0.004798493348062038\n",
      "Test: epoch 67 batch 0 loss 0.007604195736348629\n",
      "epoch 67 finished - avarage train loss 0.008827735501309407  avarage test loss 0.02199781546369195\n",
      "Training: epoch 68 batch 0 loss 0.006429523229598999\n",
      "Training: epoch 68 batch 10 loss 0.007193687371909618\n",
      "Training: epoch 68 batch 20 loss 0.005322080571204424\n",
      "Test: epoch 68 batch 0 loss 0.00963530596345663\n",
      "epoch 68 finished - avarage train loss 0.009047518564580843  avarage test loss 0.0230936425505206\n",
      "Training: epoch 69 batch 0 loss 0.004822042770683765\n",
      "Training: epoch 69 batch 10 loss 0.009711143560707569\n",
      "Training: epoch 69 batch 20 loss 0.005927657708525658\n",
      "Test: epoch 69 batch 0 loss 0.008910441771149635\n",
      "epoch 69 finished - avarage train loss 0.008463852091467586  avarage test loss 0.022833418799564242\n",
      "Training: epoch 70 batch 0 loss 0.004986289422959089\n",
      "Training: epoch 70 batch 10 loss 0.005305142141878605\n",
      "Training: epoch 70 batch 20 loss 0.007992814294993877\n",
      "Test: epoch 70 batch 0 loss 0.006451460067182779\n",
      "epoch 70 finished - avarage train loss 0.0071876749861985445  avarage test loss 0.021452268585562706\n",
      "Training: epoch 71 batch 0 loss 0.004960893653333187\n",
      "Training: epoch 71 batch 10 loss 0.007677770219743252\n",
      "Training: epoch 71 batch 20 loss 0.009237007237970829\n",
      "Test: epoch 71 batch 0 loss 0.005210184026509523\n",
      "epoch 71 finished - avarage train loss 0.008068902096871671  avarage test loss 0.020434333593584597\n",
      "Training: epoch 72 batch 0 loss 0.006835335865616798\n",
      "Training: epoch 72 batch 10 loss 0.0035907397978007793\n",
      "Training: epoch 72 batch 20 loss 0.007021751254796982\n",
      "Test: epoch 72 batch 0 loss 0.00596782099455595\n",
      "epoch 72 finished - avarage train loss 0.007734687926633091  avarage test loss 0.019811807433143258\n",
      "Training: epoch 73 batch 0 loss 0.010119861923158169\n",
      "Training: epoch 73 batch 10 loss 0.0049325404688715935\n",
      "Training: epoch 73 batch 20 loss 0.006002322304993868\n",
      "Test: epoch 73 batch 0 loss 0.005058528855443001\n",
      "epoch 73 finished - avarage train loss 0.007912125004905051  avarage test loss 0.014004596509039402\n",
      "Training: epoch 74 batch 0 loss 0.005893528461456299\n",
      "Training: epoch 74 batch 10 loss 0.007297506555914879\n",
      "Training: epoch 74 batch 20 loss 0.004803841467946768\n",
      "Test: epoch 74 batch 0 loss 0.005131328944116831\n",
      "epoch 74 finished - avarage train loss 0.007152739253922783  avarage test loss 0.015337418881244957\n",
      "Training: epoch 75 batch 0 loss 0.006192835513502359\n",
      "Training: epoch 75 batch 10 loss 0.006116384640336037\n",
      "Training: epoch 75 batch 20 loss 0.012429522350430489\n",
      "Test: epoch 75 batch 0 loss 0.00431255204603076\n",
      "epoch 75 finished - avarage train loss 0.007586757845148958  avarage test loss 0.015645180945284665\n",
      "Training: epoch 76 batch 0 loss 0.004694017581641674\n",
      "Training: epoch 76 batch 10 loss 0.0069596245884895325\n",
      "Training: epoch 76 batch 20 loss 0.008439059369266033\n",
      "Test: epoch 76 batch 0 loss 0.005235223565250635\n",
      "epoch 76 finished - avarage train loss 0.007826437764190909  avarage test loss 0.01634158508386463\n",
      "Training: epoch 77 batch 0 loss 0.010770113207399845\n",
      "Training: epoch 77 batch 10 loss 0.012053797021508217\n",
      "Training: epoch 77 batch 20 loss 0.006904143840074539\n",
      "Test: epoch 77 batch 0 loss 0.005263447295874357\n",
      "epoch 77 finished - avarage train loss 0.009260960862618583  avarage test loss 0.017807864933274686\n",
      "Training: epoch 78 batch 0 loss 0.009391460567712784\n",
      "Training: epoch 78 batch 10 loss 0.00827553030103445\n",
      "Training: epoch 78 batch 20 loss 0.006937634199857712\n",
      "Test: epoch 78 batch 0 loss 0.005547805689275265\n",
      "epoch 78 finished - avarage train loss 0.007991122483307946  avarage test loss 0.021022431668825448\n",
      "Training: epoch 79 batch 0 loss 0.00891813077032566\n",
      "Training: epoch 79 batch 10 loss 0.005485336296260357\n",
      "Training: epoch 79 batch 20 loss 0.005315459333360195\n",
      "Test: epoch 79 batch 0 loss 0.0056340256705880165\n",
      "epoch 79 finished - avarage train loss 0.0087731394531398  avarage test loss 0.021145515609532595\n",
      "Training: epoch 80 batch 0 loss 0.004491390194743872\n",
      "Training: epoch 80 batch 10 loss 0.006222774740308523\n",
      "Training: epoch 80 batch 20 loss 0.007432315032929182\n",
      "Test: epoch 80 batch 0 loss 0.004398223478347063\n",
      "epoch 80 finished - avarage train loss 0.007219177638662273  avarage test loss 0.016054813051596284\n",
      "Training: epoch 81 batch 0 loss 0.006175382062792778\n",
      "Training: epoch 81 batch 10 loss 0.009817349724471569\n",
      "Training: epoch 81 batch 20 loss 0.005402310751378536\n",
      "Test: epoch 81 batch 0 loss 0.006312090903520584\n",
      "epoch 81 finished - avarage train loss 0.008172960341747465  avarage test loss 0.01679398654960096\n",
      "Training: epoch 82 batch 0 loss 0.010907405987381935\n",
      "Training: epoch 82 batch 10 loss 0.01012328639626503\n",
      "Training: epoch 82 batch 20 loss 0.007829464972019196\n",
      "Test: epoch 82 batch 0 loss 0.005937311798334122\n",
      "epoch 82 finished - avarage train loss 0.011443616998992089  avarage test loss 0.01686407532542944\n",
      "Training: epoch 83 batch 0 loss 0.007271138019859791\n",
      "Training: epoch 83 batch 10 loss 0.005949195008724928\n",
      "Training: epoch 83 batch 20 loss 0.011009728536009789\n",
      "Test: epoch 83 batch 0 loss 0.0037838220596313477\n",
      "epoch 83 finished - avarage train loss 0.008194668764441177  avarage test loss 0.016277753747999668\n",
      "Training: epoch 84 batch 0 loss 0.004548582714051008\n",
      "Training: epoch 84 batch 10 loss 0.006409988738596439\n",
      "Training: epoch 84 batch 20 loss 0.006701707374304533\n",
      "Test: epoch 84 batch 0 loss 0.0034366813488304615\n",
      "epoch 84 finished - avarage train loss 0.007686580756101115  avarage test loss 0.015594330383464694\n",
      "Training: epoch 85 batch 0 loss 0.0036012749187648296\n",
      "Training: epoch 85 batch 10 loss 0.011786268092691898\n",
      "Training: epoch 85 batch 20 loss 0.005546163767576218\n",
      "Test: epoch 85 batch 0 loss 0.0036889563780277967\n",
      "epoch 85 finished - avarage train loss 0.00711224932255673  avarage test loss 0.015261280408594757\n",
      "Training: epoch 86 batch 0 loss 0.004666950553655624\n",
      "Training: epoch 86 batch 10 loss 0.0045486451126635075\n",
      "Training: epoch 86 batch 20 loss 0.005431328900158405\n",
      "Test: epoch 86 batch 0 loss 0.005158695392310619\n",
      "epoch 86 finished - avarage train loss 0.007729387199827309  avarage test loss 0.019348435569554567\n",
      "Training: epoch 87 batch 0 loss 0.005539809353649616\n",
      "Training: epoch 87 batch 10 loss 0.006750927306711674\n",
      "Training: epoch 87 batch 20 loss 0.01018850039690733\n",
      "Test: epoch 87 batch 0 loss 0.006363155320286751\n",
      "epoch 87 finished - avarage train loss 0.00823671353081691  avarage test loss 0.021508593577891588\n",
      "Training: epoch 88 batch 0 loss 0.0034556607715785503\n",
      "Training: epoch 88 batch 10 loss 0.004733679816126823\n",
      "Training: epoch 88 batch 20 loss 0.005965352989733219\n",
      "Test: epoch 88 batch 0 loss 0.006224881391972303\n",
      "epoch 88 finished - avarage train loss 0.00804804616350809  avarage test loss 0.021416375180706382\n",
      "Training: epoch 89 batch 0 loss 0.010036900639533997\n",
      "Training: epoch 89 batch 10 loss 0.0058221532963216305\n",
      "Training: epoch 89 batch 20 loss 0.0032560189720243216\n",
      "Test: epoch 89 batch 0 loss 0.006081663072109222\n",
      "epoch 89 finished - avarage train loss 0.008160099636056814  avarage test loss 0.02098681079223752\n",
      "Training: epoch 90 batch 0 loss 0.0038248540367931128\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training: epoch 90 batch 10 loss 0.00552541296929121\n",
      "Training: epoch 90 batch 20 loss 0.006689517758786678\n",
      "Test: epoch 90 batch 0 loss 0.009184125810861588\n",
      "epoch 90 finished - avarage train loss 0.007445482432778026  avarage test loss 0.02297931502107531\n",
      "Training: epoch 91 batch 0 loss 0.0057099442929029465\n",
      "Training: epoch 91 batch 10 loss 0.004722289741039276\n",
      "Training: epoch 91 batch 20 loss 0.004235384054481983\n",
      "Test: epoch 91 batch 0 loss 0.009098370559513569\n",
      "epoch 91 finished - avarage train loss 0.00757613942701498  avarage test loss 0.023469664971344173\n",
      "Training: epoch 92 batch 0 loss 0.006770743057131767\n",
      "Training: epoch 92 batch 10 loss 0.012090828269720078\n",
      "Training: epoch 92 batch 20 loss 0.007478110957890749\n",
      "Test: epoch 92 batch 0 loss 0.013542569242417812\n",
      "epoch 92 finished - avarage train loss 0.009703226515959049  avarage test loss 0.02483344846405089\n",
      "Training: epoch 93 batch 0 loss 0.007343067787587643\n",
      "Training: epoch 93 batch 10 loss 0.008608445525169373\n",
      "Training: epoch 93 batch 20 loss 0.00456532696262002\n",
      "Test: epoch 93 batch 0 loss 0.010948690585792065\n",
      "epoch 93 finished - avarage train loss 0.007327682459084639  avarage test loss 0.023146543535403907\n",
      "Training: epoch 94 batch 0 loss 0.005454845726490021\n",
      "Training: epoch 94 batch 10 loss 0.009263966232538223\n",
      "Training: epoch 94 batch 20 loss 0.008358778432011604\n",
      "Test: epoch 94 batch 0 loss 0.008671681396663189\n",
      "epoch 94 finished - avarage train loss 0.007807599139753087  avarage test loss 0.02248507214244455\n",
      "Training: epoch 95 batch 0 loss 0.00519415270537138\n",
      "Training: epoch 95 batch 10 loss 0.011125616729259491\n",
      "Training: epoch 95 batch 20 loss 0.006196596659719944\n",
      "Test: epoch 95 batch 0 loss 0.0065425094217062\n",
      "epoch 95 finished - avarage train loss 0.008246990948401648  avarage test loss 0.021495540742762387\n",
      "Training: epoch 96 batch 0 loss 0.006044880487024784\n",
      "Training: epoch 96 batch 10 loss 0.003831112291663885\n",
      "Training: epoch 96 batch 20 loss 0.003157196333631873\n",
      "Test: epoch 96 batch 0 loss 0.00629306910559535\n",
      "epoch 96 finished - avarage train loss 0.008281737814882192  avarage test loss 0.021204920951277018\n",
      "Training: epoch 97 batch 0 loss 0.00797115545719862\n",
      "Training: epoch 97 batch 10 loss 0.005925373639911413\n",
      "Training: epoch 97 batch 20 loss 0.008151385933160782\n",
      "Test: epoch 97 batch 0 loss 0.006040601991117001\n",
      "epoch 97 finished - avarage train loss 0.007198701146990061  avarage test loss 0.020997829851694405\n",
      "Training: epoch 98 batch 0 loss 0.005590606480836868\n",
      "Training: epoch 98 batch 10 loss 0.005031360778957605\n",
      "Training: epoch 98 batch 20 loss 0.007893035188317299\n",
      "Test: epoch 98 batch 0 loss 0.005262716207653284\n",
      "epoch 98 finished - avarage train loss 0.006876064127246882  avarage test loss 0.013754302868619561\n",
      "Training: epoch 99 batch 0 loss 0.00396757060661912\n",
      "Training: epoch 99 batch 10 loss 0.004219508729875088\n",
      "Training: epoch 99 batch 20 loss 0.004529691766947508\n",
      "Test: epoch 99 batch 0 loss 0.0038750059902668\n",
      "epoch 99 finished - avarage train loss 0.0064713321947332085  avarage test loss 0.012966789654456079\n",
      "Training: epoch 100 batch 0 loss 0.0037213845644146204\n",
      "Training: epoch 100 batch 10 loss 0.011441154405474663\n",
      "Training: epoch 100 batch 20 loss 0.007561818230897188\n",
      "Test: epoch 100 batch 0 loss 0.0056047262623906136\n",
      "epoch 100 finished - avarage train loss 0.00880789357751351  avarage test loss 0.014753362163901329\n",
      "Training: epoch 101 batch 0 loss 0.010478952899575233\n",
      "Training: epoch 101 batch 10 loss 0.007832149975001812\n",
      "Training: epoch 101 batch 20 loss 0.0064049032516777515\n",
      "Test: epoch 101 batch 0 loss 0.0044709015637636185\n",
      "epoch 101 finished - avarage train loss 0.007387046389089063  avarage test loss 0.01595093810465187\n",
      "Training: epoch 102 batch 0 loss 0.004530041012912989\n",
      "Training: epoch 102 batch 10 loss 0.01014908216893673\n",
      "Training: epoch 102 batch 20 loss 0.008261059410870075\n",
      "Test: epoch 102 batch 0 loss 0.004127167630940676\n",
      "epoch 102 finished - avarage train loss 0.009472192481866685  avarage test loss 0.015316357254050672\n",
      "Training: epoch 103 batch 0 loss 0.006520295515656471\n",
      "Training: epoch 103 batch 10 loss 0.007631482556462288\n",
      "Training: epoch 103 batch 20 loss 0.00777212530374527\n",
      "Test: epoch 103 batch 0 loss 0.006571309175342321\n",
      "epoch 103 finished - avarage train loss 0.008812836837023497  avarage test loss 0.0187686838908121\n",
      "Training: epoch 104 batch 0 loss 0.010362895205616951\n",
      "Training: epoch 104 batch 10 loss 0.0071851881220936775\n",
      "Training: epoch 104 batch 20 loss 0.004633129108697176\n",
      "Test: epoch 104 batch 0 loss 0.004519855137914419\n",
      "epoch 104 finished - avarage train loss 0.007936542925970822  avarage test loss 0.013327896245755255\n",
      "Training: epoch 105 batch 0 loss 0.005229413043707609\n",
      "Training: epoch 105 batch 10 loss 0.0063529061153531075\n",
      "Training: epoch 105 batch 20 loss 0.006052881013602018\n",
      "Test: epoch 105 batch 0 loss 0.004933068994432688\n",
      "epoch 105 finished - avarage train loss 0.00724152575150646  avarage test loss 0.015197423985227942\n",
      "Training: epoch 106 batch 0 loss 0.007044896483421326\n",
      "Training: epoch 106 batch 10 loss 0.0076396409422159195\n",
      "Training: epoch 106 batch 20 loss 0.0060089705511927605\n",
      "Test: epoch 106 batch 0 loss 0.008931403048336506\n",
      "epoch 106 finished - avarage train loss 0.007312026998863138  avarage test loss 0.016026404802687466\n",
      "Training: epoch 107 batch 0 loss 0.006914402823895216\n",
      "Training: epoch 107 batch 10 loss 0.0065015461295843124\n",
      "Training: epoch 107 batch 20 loss 0.011669730767607689\n",
      "Test: epoch 107 batch 0 loss 0.006704683415591717\n",
      "epoch 107 finished - avarage train loss 0.00734876036033805  avarage test loss 0.014083020738326013\n",
      "Training: epoch 108 batch 0 loss 0.007280145771801472\n",
      "Training: epoch 108 batch 10 loss 0.006981472950428724\n",
      "Training: epoch 108 batch 20 loss 0.005519889295101166\n",
      "Test: epoch 108 batch 0 loss 0.004023552872240543\n",
      "epoch 108 finished - avarage train loss 0.007905386219700349  avarage test loss 0.014389734133146703\n",
      "Training: epoch 109 batch 0 loss 0.005667413584887981\n",
      "Training: epoch 109 batch 10 loss 0.0074730743654072285\n",
      "Training: epoch 109 batch 20 loss 0.004838093183934689\n",
      "Test: epoch 109 batch 0 loss 0.01399805024266243\n",
      "epoch 109 finished - avarage train loss 0.0072584496249027295  avarage test loss 0.027744325809180737\n",
      "Training: epoch 110 batch 0 loss 0.022598734125494957\n",
      "Training: epoch 110 batch 10 loss 0.022464117035269737\n",
      "Training: epoch 110 batch 20 loss 0.012464433908462524\n",
      "Test: epoch 110 batch 0 loss 0.0068469904363155365\n",
      "epoch 110 finished - avarage train loss 0.02060119478545826  avarage test loss 0.018102645641192794\n",
      "Training: epoch 111 batch 0 loss 0.00739208934828639\n",
      "Training: epoch 111 batch 10 loss 0.028231216594576836\n",
      "Training: epoch 111 batch 20 loss 0.0167501512914896\n",
      "Test: epoch 111 batch 0 loss 0.006533978506922722\n",
      "epoch 111 finished - avarage train loss 0.015063439166687172  avarage test loss 0.022869592998176813\n",
      "Training: epoch 112 batch 0 loss 0.007641941774636507\n",
      "Training: epoch 112 batch 10 loss 0.004269497003406286\n",
      "Training: epoch 112 batch 20 loss 0.005702082067728043\n",
      "Test: epoch 112 batch 0 loss 0.007659995462745428\n",
      "epoch 112 finished - avarage train loss 0.00816053768684124  avarage test loss 0.022564359358511865\n",
      "Training: epoch 113 batch 0 loss 0.006748558953404427\n",
      "Training: epoch 113 batch 10 loss 0.010524301789700985\n",
      "Training: epoch 113 batch 20 loss 0.005702436435967684\n",
      "Test: epoch 113 batch 0 loss 0.0071956398896873\n",
      "epoch 113 finished - avarage train loss 0.008413197745665395  avarage test loss 0.022853614878840744\n",
      "Training: epoch 114 batch 0 loss 0.004217612557113171\n",
      "Training: epoch 114 batch 10 loss 0.00891474261879921\n",
      "Training: epoch 114 batch 20 loss 0.006866864860057831\n",
      "Test: epoch 114 batch 0 loss 0.007641913369297981\n",
      "epoch 114 finished - avarage train loss 0.008017161178627405  avarage test loss 0.022412228398025036\n",
      "Training: epoch 115 batch 0 loss 0.006919861771166325\n",
      "Training: epoch 115 batch 10 loss 0.007720492780208588\n",
      "Training: epoch 115 batch 20 loss 0.0065438393503427505\n",
      "Test: epoch 115 batch 0 loss 0.005165221635252237\n",
      "epoch 115 finished - avarage train loss 0.007347508783227411  avarage test loss 0.019730792846530676\n",
      "Training: epoch 116 batch 0 loss 0.00459064356982708\n",
      "Training: epoch 116 batch 10 loss 0.002781698713079095\n",
      "Training: epoch 116 batch 20 loss 0.004428367130458355\n",
      "Test: epoch 116 batch 0 loss 0.0049223885871469975\n",
      "epoch 116 finished - avarage train loss 0.006270751164391123  avarage test loss 0.01863820489961654\n",
      "Training: epoch 117 batch 0 loss 0.005439018830657005\n",
      "Training: epoch 117 batch 10 loss 0.004481195006519556\n",
      "Training: epoch 117 batch 20 loss 0.0048795584589242935\n",
      "Test: epoch 117 batch 0 loss 0.004383129067718983\n",
      "epoch 117 finished - avarage train loss 0.007636871413681014  avarage test loss 0.014965277630835772\n",
      "Training: epoch 118 batch 0 loss 0.010098811239004135\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training: epoch 118 batch 10 loss 0.006492760963737965\n",
      "Training: epoch 118 batch 20 loss 0.00890207476913929\n",
      "Test: epoch 118 batch 0 loss 0.003786279121413827\n",
      "epoch 118 finished - avarage train loss 0.00821124624204019  avarage test loss 0.014286187652032822\n",
      "Training: epoch 119 batch 0 loss 0.008733933791518211\n",
      "Training: epoch 119 batch 10 loss 0.004049414768815041\n",
      "Training: epoch 119 batch 20 loss 0.004462327342480421\n",
      "Test: epoch 119 batch 0 loss 0.004687032662332058\n",
      "epoch 119 finished - avarage train loss 0.007211692446585873  avarage test loss 0.01434877491556108\n",
      "Training: epoch 120 batch 0 loss 0.0041925362311303616\n",
      "Training: epoch 120 batch 10 loss 0.010465376079082489\n",
      "Training: epoch 120 batch 20 loss 0.007633397821336985\n",
      "Test: epoch 120 batch 0 loss 0.0037108680699020624\n",
      "epoch 120 finished - avarage train loss 0.008721668385611525  avarage test loss 0.01463261543540284\n",
      "Training: epoch 121 batch 0 loss 0.004020574036985636\n",
      "Training: epoch 121 batch 10 loss 0.0031030739191919565\n",
      "Training: epoch 121 batch 20 loss 0.0035017714835703373\n",
      "Test: epoch 121 batch 0 loss 0.005170035641640425\n",
      "epoch 121 finished - avarage train loss 0.006639433117454936  avarage test loss 0.015986880869604647\n",
      "Training: epoch 122 batch 0 loss 0.006593621801584959\n",
      "Training: epoch 122 batch 10 loss 0.006949855480343103\n",
      "Training: epoch 122 batch 20 loss 0.010671485215425491\n",
      "Test: epoch 122 batch 0 loss 0.005122418515384197\n",
      "epoch 122 finished - avarage train loss 0.010581144356522066  avarage test loss 0.015398619347251952\n",
      "Training: epoch 123 batch 0 loss 0.01018782053142786\n",
      "Training: epoch 123 batch 10 loss 0.012355058453977108\n",
      "Training: epoch 123 batch 20 loss 0.004692126531153917\n",
      "Test: epoch 123 batch 0 loss 0.00457263458520174\n",
      "epoch 123 finished - avarage train loss 0.007843127016556161  avarage test loss 0.013857478159479797\n",
      "Training: epoch 124 batch 0 loss 0.005047133658081293\n",
      "Training: epoch 124 batch 10 loss 0.010776503011584282\n",
      "Training: epoch 124 batch 20 loss 0.0066148098558187485\n",
      "Test: epoch 124 batch 0 loss 0.005052199121564627\n",
      "epoch 124 finished - avarage train loss 0.0076586483768990325  avarage test loss 0.015538117033429444\n",
      "Training: epoch 125 batch 0 loss 0.004371560178697109\n",
      "Training: epoch 125 batch 10 loss 0.004303404595702887\n",
      "Training: epoch 125 batch 20 loss 0.010462041012942791\n",
      "Test: epoch 125 batch 0 loss 0.005773060489445925\n",
      "epoch 125 finished - avarage train loss 0.007204910282623665  avarage test loss 0.01639436313416809\n",
      "Training: epoch 126 batch 0 loss 0.0034764371812343597\n",
      "Training: epoch 126 batch 10 loss 0.005617289803922176\n",
      "Training: epoch 126 batch 20 loss 0.00553130405023694\n",
      "Test: epoch 126 batch 0 loss 0.004723707679659128\n",
      "epoch 126 finished - avarage train loss 0.0076943875033536864  avarage test loss 0.013701317366212606\n",
      "Training: epoch 127 batch 0 loss 0.005704754497855902\n",
      "Training: epoch 127 batch 10 loss 0.008517622016370296\n",
      "Training: epoch 127 batch 20 loss 0.0054807816632092\n",
      "Test: epoch 127 batch 0 loss 0.004413499031215906\n",
      "epoch 127 finished - avarage train loss 0.008540849498440993  avarage test loss 0.014959427644498646\n",
      "Training: epoch 128 batch 0 loss 0.006988281384110451\n",
      "Training: epoch 128 batch 10 loss 0.010421836748719215\n",
      "Training: epoch 128 batch 20 loss 0.0035704730544239283\n",
      "Test: epoch 128 batch 0 loss 0.0045187645591795444\n",
      "epoch 128 finished - avarage train loss 0.0075589922931173755  avarage test loss 0.014548013568855822\n",
      "Training: epoch 129 batch 0 loss 0.005676080007106066\n",
      "Training: epoch 129 batch 10 loss 0.00961646344512701\n",
      "Training: epoch 129 batch 20 loss 0.0038219462148845196\n",
      "Test: epoch 129 batch 0 loss 0.005156844854354858\n",
      "epoch 129 finished - avarage train loss 0.008088188109405595  avarage test loss 0.013194031897000968\n",
      "Training: epoch 130 batch 0 loss 0.006190587300807238\n",
      "Training: epoch 130 batch 10 loss 0.006005486939102411\n",
      "Training: epoch 130 batch 20 loss 0.004857346881181002\n",
      "Test: epoch 130 batch 0 loss 0.006075229030102491\n",
      "epoch 130 finished - avarage train loss 0.008411313985185376  avarage test loss 0.015282820328138769\n",
      "Training: epoch 131 batch 0 loss 0.005879919044673443\n",
      "Training: epoch 131 batch 10 loss 0.009923810139298439\n",
      "Training: epoch 131 batch 20 loss 0.007621120661497116\n",
      "Test: epoch 131 batch 0 loss 0.004974029492586851\n",
      "epoch 131 finished - avarage train loss 0.007606439950779594  avarage test loss 0.013546989299356937\n",
      "Training: epoch 132 batch 0 loss 0.006925720721483231\n",
      "Training: epoch 132 batch 10 loss 0.003459170926362276\n",
      "Training: epoch 132 batch 20 loss 0.003821393009275198\n",
      "Test: epoch 132 batch 0 loss 0.0064922235906124115\n",
      "epoch 132 finished - avarage train loss 0.007867656800703242  avarage test loss 0.014050434809178114\n",
      "Training: epoch 133 batch 0 loss 0.008360981941223145\n",
      "Training: epoch 133 batch 10 loss 0.0066047427244484425\n",
      "Training: epoch 133 batch 20 loss 0.005980225279927254\n",
      "Test: epoch 133 batch 0 loss 0.004378917161375284\n",
      "epoch 133 finished - avarage train loss 0.007517630354790339  avarage test loss 0.013548292685300112\n",
      "Training: epoch 134 batch 0 loss 0.005841546226292849\n",
      "Training: epoch 134 batch 10 loss 0.003596469759941101\n",
      "Training: epoch 134 batch 20 loss 0.005557082127779722\n",
      "Test: epoch 134 batch 0 loss 0.008566949516534805\n",
      "epoch 134 finished - avarage train loss 0.00745580670552264  avarage test loss 0.02314991015009582\n",
      "Training: epoch 135 batch 0 loss 0.00803969707340002\n",
      "Training: epoch 135 batch 10 loss 0.011725400574505329\n",
      "Training: epoch 135 batch 20 loss 0.008619681932032108\n",
      "Test: epoch 135 batch 0 loss 0.01048189215362072\n",
      "epoch 135 finished - avarage train loss 0.008792812398088903  avarage test loss 0.026126923970878124\n",
      "Training: epoch 136 batch 0 loss 0.011104023084044456\n",
      "Training: epoch 136 batch 10 loss 0.008029957301914692\n",
      "Training: epoch 136 batch 20 loss 0.006849522236734629\n",
      "Test: epoch 136 batch 0 loss 0.004901000764220953\n",
      "epoch 136 finished - avarage train loss 0.009595605319943922  avarage test loss 0.019257674110122025\n",
      "Training: epoch 137 batch 0 loss 0.009089612402021885\n",
      "Training: epoch 137 batch 10 loss 0.005598956719040871\n",
      "Training: epoch 137 batch 20 loss 0.003433331148698926\n",
      "Test: epoch 137 batch 0 loss 0.00483680609613657\n",
      "epoch 137 finished - avarage train loss 0.006716003488941953  avarage test loss 0.013135530869476497\n",
      "Training: epoch 138 batch 0 loss 0.003506119828671217\n",
      "Training: epoch 138 batch 10 loss 0.003556037088856101\n",
      "Training: epoch 138 batch 20 loss 0.004658673424273729\n",
      "Test: epoch 138 batch 0 loss 0.005007489584386349\n",
      "epoch 138 finished - avarage train loss 0.007161497186612466  avarage test loss 0.014193637878634036\n",
      "Training: epoch 139 batch 0 loss 0.009899036027491093\n",
      "Training: epoch 139 batch 10 loss 0.005733052734285593\n",
      "Training: epoch 139 batch 20 loss 0.004931390285491943\n",
      "Test: epoch 139 batch 0 loss 0.005217850673943758\n",
      "epoch 139 finished - avarage train loss 0.007158913826248769  avarage test loss 0.015238033491186798\n",
      "Training: epoch 140 batch 0 loss 0.004179284907877445\n",
      "Training: epoch 140 batch 10 loss 0.007189087104052305\n",
      "Training: epoch 140 batch 20 loss 0.0051745823584496975\n",
      "Test: epoch 140 batch 0 loss 0.007154228165745735\n",
      "epoch 140 finished - avarage train loss 0.0072602708021114615  avarage test loss 0.0221966136014089\n",
      "Training: epoch 141 batch 0 loss 0.0048239692114293575\n",
      "Training: epoch 141 batch 10 loss 0.006567442789673805\n",
      "Training: epoch 141 batch 20 loss 0.0035316029097884893\n",
      "Test: epoch 141 batch 0 loss 0.006464641075581312\n",
      "epoch 141 finished - avarage train loss 0.007627173383110042  avarage test loss 0.02237259689718485\n",
      "Training: epoch 142 batch 0 loss 0.006631749216467142\n",
      "Training: epoch 142 batch 10 loss 0.004015078768134117\n",
      "Training: epoch 142 batch 20 loss 0.005797265563160181\n",
      "Test: epoch 142 batch 0 loss 0.006013661157339811\n",
      "epoch 142 finished - avarage train loss 0.007311597594927098  avarage test loss 0.021581566892564297\n",
      "Training: epoch 143 batch 0 loss 0.004388597793877125\n",
      "Training: epoch 143 batch 10 loss 0.006662808358669281\n",
      "Training: epoch 143 batch 20 loss 0.005567902233451605\n",
      "Test: epoch 143 batch 0 loss 0.004608275834470987\n",
      "epoch 143 finished - avarage train loss 0.008172904839739203  avarage test loss 0.020586855011060834\n",
      "Training: epoch 144 batch 0 loss 0.006810403428971767\n",
      "Training: epoch 144 batch 10 loss 0.005623213015496731\n",
      "Training: epoch 144 batch 20 loss 0.00581357954069972\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test: epoch 144 batch 0 loss 0.004726208746433258\n",
      "epoch 144 finished - avarage train loss 0.007398734487400487  avarage test loss 0.021208164165727794\n",
      "Training: epoch 145 batch 0 loss 0.005070777144283056\n",
      "Training: epoch 145 batch 10 loss 0.005322599317878485\n",
      "Training: epoch 145 batch 20 loss 0.004630254115909338\n",
      "Test: epoch 145 batch 0 loss 0.005804566200822592\n",
      "epoch 145 finished - avarage train loss 0.008052875119229329  avarage test loss 0.021429861430078745\n",
      "Training: epoch 146 batch 0 loss 0.006291582249104977\n",
      "Training: epoch 146 batch 10 loss 0.007567958906292915\n",
      "Training: epoch 146 batch 20 loss 0.003499345388263464\n",
      "Test: epoch 146 batch 0 loss 0.004691697657108307\n",
      "epoch 146 finished - avarage train loss 0.00769482699126519  avarage test loss 0.01947069214656949\n",
      "Training: epoch 147 batch 0 loss 0.004613231867551804\n",
      "Training: epoch 147 batch 10 loss 0.007376303430646658\n",
      "Training: epoch 147 batch 20 loss 0.005556642077863216\n",
      "Test: epoch 147 batch 0 loss 0.005308658815920353\n",
      "epoch 147 finished - avarage train loss 0.007243441682758516  avarage test loss 0.020754212513566017\n",
      "Training: epoch 148 batch 0 loss 0.006588817574083805\n",
      "Training: epoch 148 batch 10 loss 0.006454698741436005\n",
      "Training: epoch 148 batch 20 loss 0.0046309856697916985\n",
      "Test: epoch 148 batch 0 loss 0.005240302067250013\n",
      "epoch 148 finished - avarage train loss 0.006543260252360126  avarage test loss 0.019172898144461215\n",
      "Training: epoch 149 batch 0 loss 0.006599796935915947\n",
      "Training: epoch 149 batch 10 loss 0.0033072573132812977\n",
      "Training: epoch 149 batch 20 loss 0.004124451894313097\n",
      "Test: epoch 149 batch 0 loss 0.004952602554112673\n",
      "epoch 149 finished - avarage train loss 0.006831718504364635  avarage test loss 0.016319189919158816\n",
      "Training: epoch 150 batch 0 loss 0.005356819834560156\n",
      "Training: epoch 150 batch 10 loss 0.0054044416174292564\n",
      "Training: epoch 150 batch 20 loss 0.00419132923707366\n",
      "Test: epoch 150 batch 0 loss 0.004740022588521242\n",
      "epoch 150 finished - avarage train loss 0.0070586162699579165  avarage test loss 0.01927447016350925\n",
      "Training: epoch 151 batch 0 loss 0.012665336951613426\n",
      "Training: epoch 151 batch 10 loss 0.006515728775411844\n",
      "Training: epoch 151 batch 20 loss 0.004101066384464502\n",
      "Test: epoch 151 batch 0 loss 0.004650296177715063\n",
      "epoch 151 finished - avarage train loss 0.007495147738091904  avarage test loss 0.019252910395152867\n",
      "Training: epoch 152 batch 0 loss 0.0030840279068797827\n",
      "Training: epoch 152 batch 10 loss 0.004039905034005642\n",
      "Training: epoch 152 batch 20 loss 0.006392388604581356\n",
      "Test: epoch 152 batch 0 loss 0.004316308535635471\n",
      "epoch 152 finished - avarage train loss 0.005823154367731306  avarage test loss 0.013433530810289085\n",
      "Training: epoch 153 batch 0 loss 0.00577215850353241\n",
      "Training: epoch 153 batch 10 loss 0.00996353104710579\n",
      "Training: epoch 153 batch 20 loss 0.004362196195870638\n",
      "Test: epoch 153 batch 0 loss 0.004716445691883564\n",
      "epoch 153 finished - avarage train loss 0.006436225147275576  avarage test loss 0.013801940949633718\n",
      "Training: epoch 154 batch 0 loss 0.005729067604988813\n",
      "Training: epoch 154 batch 10 loss 0.003729037707671523\n",
      "Training: epoch 154 batch 20 loss 0.008692420087754726\n",
      "Test: epoch 154 batch 0 loss 0.004662257619202137\n",
      "epoch 154 finished - avarage train loss 0.008476659872344342  avarage test loss 0.015570036484859884\n",
      "Training: epoch 155 batch 0 loss 0.009871282614767551\n",
      "Training: epoch 155 batch 10 loss 0.003100327216088772\n",
      "Training: epoch 155 batch 20 loss 0.004375535063445568\n",
      "Test: epoch 155 batch 0 loss 0.004394649062305689\n",
      "epoch 155 finished - avarage train loss 0.007824356033434642  avarage test loss 0.017017181729897857\n",
      "Training: epoch 156 batch 0 loss 0.007851111702620983\n",
      "Training: epoch 156 batch 10 loss 0.00801931507885456\n",
      "Training: epoch 156 batch 20 loss 0.005615598522126675\n",
      "Test: epoch 156 batch 0 loss 0.004055475816130638\n",
      "epoch 156 finished - avarage train loss 0.007909725653007627  avarage test loss 0.013637396623380482\n",
      "Training: epoch 157 batch 0 loss 0.005550103727728128\n",
      "Training: epoch 157 batch 10 loss 0.004509487189352512\n",
      "Training: epoch 157 batch 20 loss 0.006390371359884739\n",
      "Test: epoch 157 batch 0 loss 0.0036020551342517138\n",
      "epoch 157 finished - avarage train loss 0.007529085839231466  avarage test loss 0.01420527946902439\n",
      "Training: epoch 158 batch 0 loss 0.006856042426079512\n",
      "Training: epoch 158 batch 10 loss 0.005258234217762947\n",
      "Training: epoch 158 batch 20 loss 0.005951202940195799\n",
      "Test: epoch 158 batch 0 loss 0.0037946950178593397\n",
      "epoch 158 finished - avarage train loss 0.008147969729556092  avarage test loss 0.0154492492438294\n",
      "Training: epoch 159 batch 0 loss 0.00834723562002182\n",
      "Training: epoch 159 batch 10 loss 0.004731056746095419\n",
      "Training: epoch 159 batch 20 loss 0.003064702497795224\n",
      "Test: epoch 159 batch 0 loss 0.005949880462139845\n",
      "epoch 159 finished - avarage train loss 0.007941699426235824  avarage test loss 0.017457099398598075\n",
      "Training: epoch 160 batch 0 loss 0.008572581224143505\n",
      "Training: epoch 160 batch 10 loss 0.004114963114261627\n",
      "Training: epoch 160 batch 20 loss 0.009825536981225014\n",
      "Test: epoch 160 batch 0 loss 0.004976699594408274\n",
      "epoch 160 finished - avarage train loss 0.007366101008615103  avarage test loss 0.018855543457902968\n",
      "Training: epoch 161 batch 0 loss 0.003752759424969554\n",
      "Training: epoch 161 batch 10 loss 0.006044223438948393\n",
      "Training: epoch 161 batch 20 loss 0.005129547789692879\n",
      "Test: epoch 161 batch 0 loss 0.006322209257632494\n",
      "epoch 161 finished - avarage train loss 0.006870280260798232  avarage test loss 0.02171693870332092\n",
      "Training: epoch 162 batch 0 loss 0.004373786970973015\n",
      "Training: epoch 162 batch 10 loss 0.004641454201191664\n",
      "Training: epoch 162 batch 20 loss 0.01680176705121994\n",
      "Test: epoch 162 batch 0 loss 0.005266051273792982\n",
      "epoch 162 finished - avarage train loss 0.008298274563175851  avarage test loss 0.018686941009946167\n",
      "Training: epoch 163 batch 0 loss 0.006867293734103441\n",
      "Training: epoch 163 batch 10 loss 0.005162553861737251\n",
      "Training: epoch 163 batch 20 loss 0.004219917114824057\n",
      "Test: epoch 163 batch 0 loss 0.004492281004786491\n",
      "epoch 163 finished - avarage train loss 0.0065671692377534405  avarage test loss 0.015439692186191678\n",
      "Training: epoch 164 batch 0 loss 0.005494618788361549\n",
      "Training: epoch 164 batch 10 loss 0.00573776988312602\n",
      "Training: epoch 164 batch 20 loss 0.0072837830521166325\n",
      "Test: epoch 164 batch 0 loss 0.003828331595286727\n",
      "epoch 164 finished - avarage train loss 0.007788628668941814  avarage test loss 0.014590292994398624\n",
      "Training: epoch 165 batch 0 loss 0.012058204971253872\n",
      "Training: epoch 165 batch 10 loss 0.009994041174650192\n",
      "Training: epoch 165 batch 20 loss 0.0060319979675114155\n",
      "Test: epoch 165 batch 0 loss 0.006052763666957617\n",
      "epoch 165 finished - avarage train loss 0.009185174375708247  avarage test loss 0.01807205064687878\n",
      "Training: epoch 166 batch 0 loss 0.006537153851240873\n",
      "Training: epoch 166 batch 10 loss 0.00945108663290739\n",
      "Training: epoch 166 batch 20 loss 0.004719974938780069\n",
      "Test: epoch 166 batch 0 loss 0.0046117366291582584\n",
      "epoch 166 finished - avarage train loss 0.008213838120793989  avarage test loss 0.015409837593324482\n",
      "Training: epoch 167 batch 0 loss 0.009819777682423592\n",
      "Training: epoch 167 batch 10 loss 0.003524778876453638\n",
      "Training: epoch 167 batch 20 loss 0.004942086059600115\n",
      "Test: epoch 167 batch 0 loss 0.003944230731576681\n",
      "epoch 167 finished - avarage train loss 0.005356832150498341  avarage test loss 0.01523165067192167\n",
      "Training: epoch 168 batch 0 loss 0.005531736183911562\n",
      "Training: epoch 168 batch 10 loss 0.0036476170644164085\n",
      "Training: epoch 168 batch 20 loss 0.0047587002627551556\n",
      "Test: epoch 168 batch 0 loss 0.005342268850654364\n",
      "epoch 168 finished - avarage train loss 0.00795535142694054  avarage test loss 0.020154516911134124\n",
      "Training: epoch 169 batch 0 loss 0.0052678450010716915\n",
      "Training: epoch 169 batch 10 loss 0.00425537396222353\n",
      "Training: epoch 169 batch 20 loss 0.008304205723106861\n",
      "Test: epoch 169 batch 0 loss 0.007241169922053814\n",
      "epoch 169 finished - avarage train loss 0.007764648300884613  avarage test loss 0.02234242472331971\n",
      "Training: epoch 170 batch 0 loss 0.008590037003159523\n",
      "Training: epoch 170 batch 10 loss 0.00443432480096817\n",
      "Training: epoch 170 batch 20 loss 0.009032027795910835\n",
      "Test: epoch 170 batch 0 loss 0.00613018125295639\n",
      "epoch 170 finished - avarage train loss 0.006947447842886222  avarage test loss 0.021678594988770783\n",
      "Training: epoch 171 batch 0 loss 0.01029601227492094\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training: epoch 171 batch 10 loss 0.006626791786402464\n",
      "Training: epoch 171 batch 20 loss 0.005498209036886692\n",
      "Test: epoch 171 batch 0 loss 0.00658794678747654\n",
      "epoch 171 finished - avarage train loss 0.007941504763760444  avarage test loss 0.021724018617533147\n",
      "Training: epoch 172 batch 0 loss 0.0031700031831860542\n",
      "Training: epoch 172 batch 10 loss 0.012231520377099514\n",
      "Training: epoch 172 batch 20 loss 0.007462085224688053\n",
      "Test: epoch 172 batch 0 loss 0.006402624770998955\n",
      "epoch 172 finished - avarage train loss 0.008058214544093814  avarage test loss 0.021354667143896222\n",
      "Training: epoch 173 batch 0 loss 0.006348408758640289\n",
      "Training: epoch 173 batch 10 loss 0.008884704671800137\n",
      "Training: epoch 173 batch 20 loss 0.006852058693766594\n",
      "Test: epoch 173 batch 0 loss 0.010694185271859169\n",
      "epoch 173 finished - avarage train loss 0.006715813314477946  avarage test loss 0.02745603257790208\n",
      "Training: epoch 174 batch 0 loss 0.01154005341231823\n",
      "Training: epoch 174 batch 10 loss 0.010208839550614357\n",
      "Training: epoch 174 batch 20 loss 0.014284869655966759\n",
      "Test: epoch 174 batch 0 loss 0.012628771364688873\n",
      "epoch 174 finished - avarage train loss 0.014653058417526812  avarage test loss 0.026185529539361596\n",
      "Training: epoch 175 batch 0 loss 0.010347706265747547\n",
      "Training: epoch 175 batch 10 loss 0.005785034038126469\n",
      "Training: epoch 175 batch 20 loss 0.00582810677587986\n",
      "Test: epoch 175 batch 0 loss 0.01539812982082367\n",
      "epoch 175 finished - avarage train loss 0.009972994199729171  avarage test loss 0.02587832254357636\n",
      "Training: epoch 176 batch 0 loss 0.011696312576532364\n",
      "Training: epoch 176 batch 10 loss 0.008512288331985474\n",
      "Training: epoch 176 batch 20 loss 0.004857010208070278\n",
      "Test: epoch 176 batch 0 loss 0.012886867858469486\n",
      "epoch 176 finished - avarage train loss 0.009521277268128148  avarage test loss 0.024660569615662098\n",
      "Training: epoch 177 batch 0 loss 0.006446191109716892\n",
      "Training: epoch 177 batch 10 loss 0.004160286858677864\n",
      "Training: epoch 177 batch 20 loss 0.006882112007588148\n",
      "Test: epoch 177 batch 0 loss 0.008785493671894073\n",
      "epoch 177 finished - avarage train loss 0.00805124206918067  avarage test loss 0.022534032817929983\n",
      "Training: epoch 178 batch 0 loss 0.010960294865071774\n",
      "Training: epoch 178 batch 10 loss 0.009980200789868832\n",
      "Training: epoch 178 batch 20 loss 0.003084341296926141\n",
      "Test: epoch 178 batch 0 loss 0.005906765814870596\n",
      "epoch 178 finished - avarage train loss 0.008149051813986796  avarage test loss 0.021344122826121747\n",
      "Training: epoch 179 batch 0 loss 0.006513163913041353\n",
      "Training: epoch 179 batch 10 loss 0.004727616440504789\n",
      "Training: epoch 179 batch 20 loss 0.004674048162996769\n",
      "Test: epoch 179 batch 0 loss 0.005231925752013922\n",
      "epoch 179 finished - avarage train loss 0.007409125686912188  avarage test loss 0.02079476125072688\n",
      "Training: epoch 180 batch 0 loss 0.0042528086341917515\n",
      "Training: epoch 180 batch 10 loss 0.0059546660631895065\n",
      "Training: epoch 180 batch 20 loss 0.004511632025241852\n",
      "Test: epoch 180 batch 0 loss 0.005842204671353102\n",
      "epoch 180 finished - avarage train loss 0.007514713765603715  avarage test loss 0.021253898041322827\n",
      "Training: epoch 181 batch 0 loss 0.007009388413280249\n",
      "Training: epoch 181 batch 10 loss 0.0062068975530564785\n",
      "Training: epoch 181 batch 20 loss 0.00557389110326767\n",
      "Test: epoch 181 batch 0 loss 0.00486203096807003\n",
      "epoch 181 finished - avarage train loss 0.008191791876893619  avarage test loss 0.019586939248256385\n",
      "Training: epoch 182 batch 0 loss 0.007031724788248539\n",
      "Training: epoch 182 batch 10 loss 0.0036686204839497805\n",
      "Training: epoch 182 batch 20 loss 0.00859224982559681\n",
      "Test: epoch 182 batch 0 loss 0.0047822073101997375\n",
      "epoch 182 finished - avarage train loss 0.00768671325279464  avarage test loss 0.02004962181672454\n",
      "Training: epoch 183 batch 0 loss 0.004960146266967058\n",
      "Training: epoch 183 batch 10 loss 0.003989008720964193\n",
      "Training: epoch 183 batch 20 loss 0.005053115542978048\n",
      "Test: epoch 183 batch 0 loss 0.004922579042613506\n",
      "epoch 183 finished - avarage train loss 0.006734530160460493  avarage test loss 0.02015826420392841\n",
      "Training: epoch 184 batch 0 loss 0.005825753789395094\n",
      "Training: epoch 184 batch 10 loss 0.007969975471496582\n",
      "Training: epoch 184 batch 20 loss 0.00947137363255024\n",
      "Test: epoch 184 batch 0 loss 0.004817616660147905\n",
      "epoch 184 finished - avarage train loss 0.00890538760396684  avarage test loss 0.019738603150472045\n",
      "Training: epoch 185 batch 0 loss 0.0048289173282682896\n",
      "Training: epoch 185 batch 10 loss 0.007850618101656437\n",
      "Training: epoch 185 batch 20 loss 0.004667729139328003\n",
      "Test: epoch 185 batch 0 loss 0.0055253212340176105\n",
      "epoch 185 finished - avarage train loss 0.0074097288065943226  avarage test loss 0.01699190610088408\n",
      "Training: epoch 186 batch 0 loss 0.007695893291383982\n",
      "Training: epoch 186 batch 10 loss 0.013756813481450081\n",
      "Training: epoch 186 batch 20 loss 0.006036916747689247\n",
      "Test: epoch 186 batch 0 loss 0.004134876653552055\n",
      "epoch 186 finished - avarage train loss 0.009156112318280441  avarage test loss 0.01601230399683118\n",
      "Training: epoch 187 batch 0 loss 0.007717732340097427\n",
      "Training: epoch 187 batch 10 loss 0.007704378571361303\n",
      "Training: epoch 187 batch 20 loss 0.009368682280182838\n",
      "Test: epoch 187 batch 0 loss 0.0055187763646245\n",
      "epoch 187 finished - avarage train loss 0.007742203250206236  avarage test loss 0.02097422501537949\n",
      "Training: epoch 188 batch 0 loss 0.003209714312106371\n",
      "Training: epoch 188 batch 10 loss 0.006950054783374071\n",
      "Training: epoch 188 batch 20 loss 0.007167002186179161\n",
      "Test: epoch 188 batch 0 loss 0.004899237770587206\n",
      "epoch 188 finished - avarage train loss 0.007212549985932379  avarage test loss 0.020540055236779153\n",
      "Training: epoch 189 batch 0 loss 0.00476003997027874\n",
      "Training: epoch 189 batch 10 loss 0.005689130164682865\n",
      "Training: epoch 189 batch 20 loss 0.00420755660161376\n",
      "Test: epoch 189 batch 0 loss 0.005669246427714825\n",
      "epoch 189 finished - avarage train loss 0.007028393455427782  avarage test loss 0.014567916980013251\n",
      "Training: epoch 190 batch 0 loss 0.005916254594922066\n",
      "Training: epoch 190 batch 10 loss 0.005756639875471592\n",
      "Training: epoch 190 batch 20 loss 0.0064720576629042625\n",
      "Test: epoch 190 batch 0 loss 0.00408730935305357\n",
      "epoch 190 finished - avarage train loss 0.007897499707880718  avarage test loss 0.01396768435370177\n",
      "Training: epoch 191 batch 0 loss 0.006938603240996599\n",
      "Training: epoch 191 batch 10 loss 0.003976084757596254\n",
      "Training: epoch 191 batch 20 loss 0.005268204025924206\n",
      "Test: epoch 191 batch 0 loss 0.005691383965313435\n",
      "epoch 191 finished - avarage train loss 0.00822982267508733  avarage test loss 0.015446872916072607\n",
      "Training: epoch 192 batch 0 loss 0.0067596035078167915\n",
      "Training: epoch 192 batch 10 loss 0.006093020085245371\n",
      "Training: epoch 192 batch 20 loss 0.009009870700538158\n",
      "Test: epoch 192 batch 0 loss 0.005530203692615032\n",
      "epoch 192 finished - avarage train loss 0.0076258153622520385  avarage test loss 0.015231026685796678\n",
      "Training: epoch 193 batch 0 loss 0.006830875296145678\n",
      "Training: epoch 193 batch 10 loss 0.0031336655374616385\n",
      "Training: epoch 193 batch 20 loss 0.0035428842529654503\n",
      "Test: epoch 193 batch 0 loss 0.0055365231819450855\n",
      "epoch 193 finished - avarage train loss 0.0061814040025888846  avarage test loss 0.015291376737877727\n",
      "Training: epoch 194 batch 0 loss 0.007195780519396067\n",
      "Training: epoch 194 batch 10 loss 0.0038413640577346087\n",
      "Training: epoch 194 batch 20 loss 0.005537447519600391\n",
      "Test: epoch 194 batch 0 loss 0.006794244982302189\n",
      "epoch 194 finished - avarage train loss 0.006593430117736089  avarage test loss 0.021498529706150293\n",
      "Training: epoch 195 batch 0 loss 0.004596497863531113\n",
      "Training: epoch 195 batch 10 loss 0.009366831742227077\n",
      "Training: epoch 195 batch 20 loss 0.00889483280479908\n",
      "Test: epoch 195 batch 0 loss 0.006731770001351833\n",
      "epoch 195 finished - avarage train loss 0.006507903128733923  avarage test loss 0.02187060343567282\n",
      "Training: epoch 196 batch 0 loss 0.008041842840611935\n",
      "Training: epoch 196 batch 10 loss 0.0035464593674987555\n",
      "Training: epoch 196 batch 20 loss 0.013362635858356953\n",
      "Test: epoch 196 batch 0 loss 0.00586744025349617\n",
      "epoch 196 finished - avarage train loss 0.007994445915944103  avarage test loss 0.021102668717503548\n",
      "Training: epoch 197 batch 0 loss 0.006902821362018585\n",
      "Training: epoch 197 batch 10 loss 0.006311006844043732\n",
      "Training: epoch 197 batch 20 loss 0.0067637525498867035\n",
      "Test: epoch 197 batch 0 loss 0.007117826957255602\n",
      "epoch 197 finished - avarage train loss 0.006716296927810743  avarage test loss 0.021875902428291738\n",
      "Training: epoch 198 batch 0 loss 0.005787435919046402\n",
      "Training: epoch 198 batch 10 loss 0.00822594203054905\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training: epoch 198 batch 20 loss 0.006067714653909206\n",
      "Test: epoch 198 batch 0 loss 0.008131389506161213\n",
      "epoch 198 finished - avarage train loss 0.007084324466193031  avarage test loss 0.02251124766189605\n",
      "Training: epoch 199 batch 0 loss 0.0056133996695280075\n",
      "Training: epoch 199 batch 10 loss 0.006553974002599716\n",
      "Training: epoch 199 batch 20 loss 0.006098309997469187\n",
      "Test: epoch 199 batch 0 loss 0.0051850127056241035\n",
      "epoch 199 finished - avarage train loss 0.006938038008480236  avarage test loss 0.020769673865288496\n",
      "Training: epoch 0 batch 0 loss 0.553013026714325\n",
      "Training: epoch 0 batch 10 loss 0.5844072103500366\n",
      "Training: epoch 0 batch 20 loss 0.47610509395599365\n",
      "Test: epoch 0 batch 0 loss 0.4311726689338684\n",
      "epoch 0 finished - avarage train loss 0.5209992127171879  avarage test loss 0.5130997151136398\n",
      "Training: epoch 1 batch 0 loss 0.5859330296516418\n",
      "Training: epoch 1 batch 10 loss 0.4345068633556366\n",
      "Training: epoch 1 batch 20 loss 0.4141024649143219\n",
      "Test: epoch 1 batch 0 loss 0.42996981739997864\n",
      "epoch 1 finished - avarage train loss 0.5304769709192473  avarage test loss 0.512245200574398\n",
      "Training: epoch 2 batch 0 loss 0.5137137770652771\n",
      "Training: epoch 2 batch 10 loss 0.4656696319580078\n",
      "Training: epoch 2 batch 20 loss 0.5708826780319214\n",
      "Test: epoch 2 batch 0 loss 0.43239256739616394\n",
      "epoch 2 finished - avarage train loss 0.5089804478760424  avarage test loss 0.5124114528298378\n",
      "Training: epoch 3 batch 0 loss 0.5435987710952759\n",
      "Training: epoch 3 batch 10 loss 0.441915363073349\n",
      "Training: epoch 3 batch 20 loss 0.4236154556274414\n",
      "Test: epoch 3 batch 0 loss 0.42347949743270874\n",
      "epoch 3 finished - avarage train loss 0.5110339399041801  avarage test loss 0.5065616369247437\n",
      "Training: epoch 4 batch 0 loss 0.48370739817619324\n",
      "Training: epoch 4 batch 10 loss 0.3860812187194824\n",
      "Training: epoch 4 batch 20 loss 0.5129322409629822\n",
      "Test: epoch 4 batch 0 loss 0.4302724003791809\n",
      "epoch 4 finished - avarage train loss 0.4973017157151781  avarage test loss 0.5070415958762169\n",
      "Training: epoch 5 batch 0 loss 0.5702789425849915\n",
      "Training: epoch 5 batch 10 loss 0.4411125183105469\n",
      "Training: epoch 5 batch 20 loss 0.4557318687438965\n",
      "Test: epoch 5 batch 0 loss 0.43146538734436035\n",
      "epoch 5 finished - avarage train loss 0.5033676202954918  avarage test loss 0.5120979845523834\n",
      "Training: epoch 6 batch 0 loss 0.3790258765220642\n",
      "Training: epoch 6 batch 10 loss 0.4295026659965515\n",
      "Training: epoch 6 batch 20 loss 0.49978768825531006\n",
      "Test: epoch 6 batch 0 loss 0.4302383065223694\n",
      "epoch 6 finished - avarage train loss 0.5264485170101297  avarage test loss 0.515674039721489\n",
      "Training: epoch 7 batch 0 loss 0.4638386070728302\n",
      "Training: epoch 7 batch 10 loss 0.4933738112449646\n",
      "Training: epoch 7 batch 20 loss 0.256558358669281\n",
      "Test: epoch 7 batch 0 loss 0.13830605149269104\n",
      "epoch 7 finished - avarage train loss 0.3722013679557833  avarage test loss 0.13010410219430923\n",
      "Training: epoch 8 batch 0 loss 0.0986323356628418\n",
      "Training: epoch 8 batch 10 loss 0.10325409471988678\n",
      "Training: epoch 8 batch 20 loss 0.04757390916347504\n",
      "Test: epoch 8 batch 0 loss 0.03747991472482681\n",
      "epoch 8 finished - avarage train loss 0.0684097011027665  avarage test loss 0.04129290720447898\n",
      "Training: epoch 9 batch 0 loss 0.02036663144826889\n",
      "Training: epoch 9 batch 10 loss 0.03446444123983383\n",
      "Training: epoch 9 batch 20 loss 0.01741277612745762\n",
      "Test: epoch 9 batch 0 loss 0.030241593718528748\n",
      "epoch 9 finished - avarage train loss 0.024364519286258466  avarage test loss 0.03815487492829561\n",
      "Training: epoch 10 batch 0 loss 0.029714098200201988\n",
      "Training: epoch 10 batch 10 loss 0.012373922392725945\n",
      "Training: epoch 10 batch 20 loss 0.020613759756088257\n",
      "Test: epoch 10 batch 0 loss 0.02655385620892048\n",
      "epoch 10 finished - avarage train loss 0.022845684358015143  avarage test loss 0.03354533528909087\n",
      "Training: epoch 11 batch 0 loss 0.020631633698940277\n",
      "Training: epoch 11 batch 10 loss 0.028635019436478615\n",
      "Training: epoch 11 batch 20 loss 0.030836516991257668\n",
      "Test: epoch 11 batch 0 loss 0.02563461847603321\n",
      "epoch 11 finished - avarage train loss 0.022254688215666805  avarage test loss 0.033208202570676804\n",
      "Training: epoch 12 batch 0 loss 0.010473543778061867\n",
      "Training: epoch 12 batch 10 loss 0.01825796626508236\n",
      "Training: epoch 12 batch 20 loss 0.025340117514133453\n",
      "Test: epoch 12 batch 0 loss 0.024878589436411858\n",
      "epoch 12 finished - avarage train loss 0.022345808880596327  avarage test loss 0.033082849346101284\n",
      "Training: epoch 13 batch 0 loss 0.011526908725500107\n",
      "Training: epoch 13 batch 10 loss 0.011409149505198002\n",
      "Training: epoch 13 batch 20 loss 0.01198655553162098\n",
      "Test: epoch 13 batch 0 loss 0.025521915405988693\n",
      "epoch 13 finished - avarage train loss 0.02067254759885114  avarage test loss 0.03403827315196395\n",
      "Training: epoch 14 batch 0 loss 0.02368956059217453\n",
      "Training: epoch 14 batch 10 loss 0.028271343559026718\n",
      "Training: epoch 14 batch 20 loss 0.024495253339409828\n",
      "Test: epoch 14 batch 0 loss 0.025050634518265724\n",
      "epoch 14 finished - avarage train loss 0.020135684420579468  avarage test loss 0.0339992749504745\n",
      "Training: epoch 15 batch 0 loss 0.015850460156798363\n",
      "Training: epoch 15 batch 10 loss 0.027514014393091202\n",
      "Training: epoch 15 batch 20 loss 0.012191410176455975\n",
      "Test: epoch 15 batch 0 loss 0.02341768704354763\n",
      "epoch 15 finished - avarage train loss 0.020532138452961528  avarage test loss 0.032734998036175966\n",
      "Training: epoch 16 batch 0 loss 0.011419546790421009\n",
      "Training: epoch 16 batch 10 loss 0.010343638248741627\n",
      "Training: epoch 16 batch 20 loss 0.014507300220429897\n",
      "Test: epoch 16 batch 0 loss 0.021465471014380455\n",
      "epoch 16 finished - avarage train loss 0.01876403587260123  avarage test loss 0.03126551164314151\n",
      "Training: epoch 17 batch 0 loss 0.016545157879590988\n",
      "Training: epoch 17 batch 10 loss 0.01681794971227646\n",
      "Training: epoch 17 batch 20 loss 0.009873991832137108\n",
      "Test: epoch 17 batch 0 loss 0.01164999883621931\n",
      "epoch 17 finished - avarage train loss 0.012432000696145255  avarage test loss 0.02313964650966227\n",
      "Training: epoch 18 batch 0 loss 0.011696362867951393\n",
      "Training: epoch 18 batch 10 loss 0.009563770145177841\n",
      "Training: epoch 18 batch 20 loss 0.005681459791958332\n",
      "Test: epoch 18 batch 0 loss 0.007807629648596048\n",
      "epoch 18 finished - avarage train loss 0.008479824787455386  avarage test loss 0.020224190200679004\n",
      "Training: epoch 19 batch 0 loss 0.005508961621671915\n",
      "Training: epoch 19 batch 10 loss 0.011355102062225342\n",
      "Training: epoch 19 batch 20 loss 0.008059455081820488\n",
      "Test: epoch 19 batch 0 loss 0.006197117734700441\n",
      "epoch 19 finished - avarage train loss 0.008864875144228852  avarage test loss 0.01608994568232447\n",
      "Training: epoch 20 batch 0 loss 0.007869385182857513\n",
      "Training: epoch 20 batch 10 loss 0.010560495778918266\n",
      "Training: epoch 20 batch 20 loss 0.011410719715058804\n",
      "Test: epoch 20 batch 0 loss 0.005605341866612434\n",
      "epoch 20 finished - avarage train loss 0.01088459418829659  avarage test loss 0.015872788499109447\n",
      "Training: epoch 21 batch 0 loss 0.008225559256970882\n",
      "Training: epoch 21 batch 10 loss 0.008440898731350899\n",
      "Training: epoch 21 batch 20 loss 0.005604646168649197\n",
      "Test: epoch 21 batch 0 loss 0.0038845555391162634\n",
      "epoch 21 finished - avarage train loss 0.009000009584144271  avarage test loss 0.01348879252327606\n",
      "Training: epoch 22 batch 0 loss 0.008124354295432568\n",
      "Training: epoch 22 batch 10 loss 0.013237488456070423\n",
      "Training: epoch 22 batch 20 loss 0.006585722789168358\n",
      "Test: epoch 22 batch 0 loss 0.00442036846652627\n",
      "epoch 22 finished - avarage train loss 0.008323247287550876  avarage test loss 0.013418910908512771\n",
      "Training: epoch 23 batch 0 loss 0.005910096690058708\n",
      "Training: epoch 23 batch 10 loss 0.006789322942495346\n",
      "Training: epoch 23 batch 20 loss 0.004912835080176592\n",
      "Test: epoch 23 batch 0 loss 0.009078624658286572\n",
      "epoch 23 finished - avarage train loss 0.00943159360984533  avarage test loss 0.01953661034349352\n",
      "Training: epoch 24 batch 0 loss 0.011754930019378662\n",
      "Training: epoch 24 batch 10 loss 0.012398723512887955\n",
      "Training: epoch 24 batch 20 loss 0.003797857090830803\n",
      "Test: epoch 24 batch 0 loss 0.005035713780671358\n",
      "epoch 24 finished - avarage train loss 0.010750192586846393  avarage test loss 0.015439651673659682\n",
      "Training: epoch 25 batch 0 loss 0.010039055719971657\n",
      "Training: epoch 25 batch 10 loss 0.007377695757895708\n",
      "Training: epoch 25 batch 20 loss 0.011514431796967983\n",
      "Test: epoch 25 batch 0 loss 0.005982189904898405\n",
      "epoch 25 finished - avarage train loss 0.009713203052123046  avarage test loss 0.015475867548957467\n",
      "Training: epoch 26 batch 0 loss 0.009814509190618992\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training: epoch 26 batch 10 loss 0.01064370572566986\n",
      "Training: epoch 26 batch 20 loss 0.006380205973982811\n",
      "Test: epoch 26 batch 0 loss 0.0064986529760062695\n",
      "epoch 26 finished - avarage train loss 0.010622367166496557  avarage test loss 0.017027983558364213\n",
      "Training: epoch 27 batch 0 loss 0.0116109699010849\n",
      "Training: epoch 27 batch 10 loss 0.006507338955998421\n",
      "Training: epoch 27 batch 20 loss 0.0068613076582551\n",
      "Test: epoch 27 batch 0 loss 0.005089776590466499\n",
      "epoch 27 finished - avarage train loss 0.008944737965819138  avarage test loss 0.014950828510336578\n",
      "Training: epoch 28 batch 0 loss 0.007047944236546755\n",
      "Training: epoch 28 batch 10 loss 0.0128400307148695\n",
      "Training: epoch 28 batch 20 loss 0.005001280456781387\n",
      "Test: epoch 28 batch 0 loss 0.0045387702994048595\n",
      "epoch 28 finished - avarage train loss 0.009038936772287405  avarage test loss 0.0136213997611776\n",
      "Training: epoch 29 batch 0 loss 0.00795339047908783\n",
      "Training: epoch 29 batch 10 loss 0.006694325711578131\n",
      "Training: epoch 29 batch 20 loss 0.01215380523353815\n",
      "Test: epoch 29 batch 0 loss 0.0043188342824578285\n",
      "epoch 29 finished - avarage train loss 0.008161392319819024  avarage test loss 0.012986876827199012\n",
      "Training: epoch 30 batch 0 loss 0.003964889794588089\n",
      "Training: epoch 30 batch 10 loss 0.004671819508075714\n",
      "Training: epoch 30 batch 20 loss 0.007006985135376453\n",
      "Test: epoch 30 batch 0 loss 0.005704213865101337\n",
      "epoch 30 finished - avarage train loss 0.008473748260916307  avarage test loss 0.015833610552363098\n",
      "Training: epoch 31 batch 0 loss 0.007279528770595789\n",
      "Training: epoch 31 batch 10 loss 0.009021117351949215\n",
      "Training: epoch 31 batch 20 loss 0.007041605655103922\n",
      "Test: epoch 31 batch 0 loss 0.005061835050582886\n",
      "epoch 31 finished - avarage train loss 0.007915329252337587  avarage test loss 0.014341705478727818\n",
      "Training: epoch 32 batch 0 loss 0.005807074718177319\n",
      "Training: epoch 32 batch 10 loss 0.012232852168381214\n",
      "Training: epoch 32 batch 20 loss 0.009389033541083336\n",
      "Test: epoch 32 batch 0 loss 0.005824991501867771\n",
      "epoch 32 finished - avarage train loss 0.009437345706954085  avarage test loss 0.014489882392808795\n",
      "Training: epoch 33 batch 0 loss 0.008594805374741554\n",
      "Training: epoch 33 batch 10 loss 0.005030805245041847\n",
      "Training: epoch 33 batch 20 loss 0.009907416999340057\n",
      "Test: epoch 33 batch 0 loss 0.005926076322793961\n",
      "epoch 33 finished - avarage train loss 0.008840611204504967  avarage test loss 0.015416361391544342\n",
      "Training: epoch 34 batch 0 loss 0.0050001442432403564\n",
      "Training: epoch 34 batch 10 loss 0.003355038585141301\n",
      "Training: epoch 34 batch 20 loss 0.008045805618166924\n",
      "Test: epoch 34 batch 0 loss 0.004783193115144968\n",
      "epoch 34 finished - avarage train loss 0.008081809962810627  avarage test loss 0.014389406540431082\n",
      "Training: epoch 35 batch 0 loss 0.005824137944728136\n",
      "Training: epoch 35 batch 10 loss 0.008688235655426979\n",
      "Training: epoch 35 batch 20 loss 0.005548219662159681\n",
      "Test: epoch 35 batch 0 loss 0.004743172787129879\n",
      "epoch 35 finished - avarage train loss 0.007795372086673461  avarage test loss 0.015004511806182563\n",
      "Training: epoch 36 batch 0 loss 0.017002273350954056\n",
      "Training: epoch 36 batch 10 loss 0.007214733399450779\n",
      "Training: epoch 36 batch 20 loss 0.007809232454746962\n",
      "Test: epoch 36 batch 0 loss 0.0053115044720470905\n",
      "epoch 36 finished - avarage train loss 0.011148652961028033  avarage test loss 0.01830595964565873\n",
      "Training: epoch 37 batch 0 loss 0.006602967157959938\n",
      "Training: epoch 37 batch 10 loss 0.011175726540386677\n",
      "Training: epoch 37 batch 20 loss 0.008683383464813232\n",
      "Test: epoch 37 batch 0 loss 0.003965975716710091\n",
      "epoch 37 finished - avarage train loss 0.007431708589388892  avarage test loss 0.014124402892775834\n",
      "Training: epoch 38 batch 0 loss 0.004424198530614376\n",
      "Training: epoch 38 batch 10 loss 0.007535314187407494\n",
      "Training: epoch 38 batch 20 loss 0.007674019783735275\n",
      "Test: epoch 38 batch 0 loss 0.004970373585820198\n",
      "epoch 38 finished - avarage train loss 0.008646559387702367  avarage test loss 0.014379622065462172\n",
      "Training: epoch 39 batch 0 loss 0.0075914631597697735\n",
      "Training: epoch 39 batch 10 loss 0.005319454241544008\n",
      "Training: epoch 39 batch 20 loss 0.0074516781605780125\n",
      "Test: epoch 39 batch 0 loss 0.004195619840174913\n",
      "epoch 39 finished - avarage train loss 0.006734744581426012  avarage test loss 0.014424586202949286\n",
      "Training: epoch 40 batch 0 loss 0.007707700133323669\n",
      "Training: epoch 40 batch 10 loss 0.0062973215244710445\n",
      "Training: epoch 40 batch 20 loss 0.005723558831959963\n",
      "Test: epoch 40 batch 0 loss 0.004133000038564205\n",
      "epoch 40 finished - avarage train loss 0.0072690396296695384  avarage test loss 0.014707166817970574\n",
      "Training: epoch 41 batch 0 loss 0.005704623181372881\n",
      "Training: epoch 41 batch 10 loss 0.0049432008527219296\n",
      "Training: epoch 41 batch 20 loss 0.004882866516709328\n",
      "Test: epoch 41 batch 0 loss 0.004246844910085201\n",
      "epoch 41 finished - avarage train loss 0.007700168776550683  avarage test loss 0.015008508693426847\n",
      "Training: epoch 42 batch 0 loss 0.004611684009432793\n",
      "Training: epoch 42 batch 10 loss 0.009759525768458843\n",
      "Training: epoch 42 batch 20 loss 0.006214917171746492\n",
      "Test: epoch 42 batch 0 loss 0.004262490198016167\n",
      "epoch 42 finished - avarage train loss 0.008187416732182791  avarage test loss 0.013215477578341961\n",
      "Training: epoch 43 batch 0 loss 0.006646699272096157\n",
      "Training: epoch 43 batch 10 loss 0.005721075925976038\n",
      "Training: epoch 43 batch 20 loss 0.008217500522732735\n",
      "Test: epoch 43 batch 0 loss 0.004981641657650471\n",
      "epoch 43 finished - avarage train loss 0.007916615087667415  avarage test loss 0.01548097061458975\n",
      "Training: epoch 44 batch 0 loss 0.0068869260139763355\n",
      "Training: epoch 44 batch 10 loss 0.006915532983839512\n",
      "Training: epoch 44 batch 20 loss 0.007179540116339922\n",
      "Test: epoch 44 batch 0 loss 0.005013288930058479\n",
      "epoch 44 finished - avarage train loss 0.007758251319093437  avarage test loss 0.01472236285917461\n",
      "Training: epoch 45 batch 0 loss 0.007137087173759937\n",
      "Training: epoch 45 batch 10 loss 0.007982145994901657\n",
      "Training: epoch 45 batch 20 loss 0.004853141028434038\n",
      "Test: epoch 45 batch 0 loss 0.004444523248821497\n",
      "epoch 45 finished - avarage train loss 0.007612927013943936  avarage test loss 0.014401742140762508\n",
      "Training: epoch 46 batch 0 loss 0.011530307121574879\n",
      "Training: epoch 46 batch 10 loss 0.006091390270739794\n",
      "Training: epoch 46 batch 20 loss 0.004706212785094976\n",
      "Test: epoch 46 batch 0 loss 0.0040625520050525665\n",
      "epoch 46 finished - avarage train loss 0.00796172624164871  avarage test loss 0.014803249039687216\n",
      "Training: epoch 47 batch 0 loss 0.0058867051266133785\n",
      "Training: epoch 47 batch 10 loss 0.004161909222602844\n",
      "Training: epoch 47 batch 20 loss 0.011966205202043056\n",
      "Test: epoch 47 batch 0 loss 0.0038922016974538565\n",
      "epoch 47 finished - avarage train loss 0.00789424379613122  avarage test loss 0.01314652553992346\n",
      "Training: epoch 48 batch 0 loss 0.0033897715620696545\n",
      "Training: epoch 48 batch 10 loss 0.008717258460819721\n",
      "Training: epoch 48 batch 20 loss 0.008162850514054298\n",
      "Test: epoch 48 batch 0 loss 0.006463776342570782\n",
      "epoch 48 finished - avarage train loss 0.007235510547741734  avarage test loss 0.015222982736304402\n",
      "Training: epoch 49 batch 0 loss 0.0048659020103514194\n",
      "Training: epoch 49 batch 10 loss 0.006410703063011169\n",
      "Training: epoch 49 batch 20 loss 0.006727768108248711\n",
      "Test: epoch 49 batch 0 loss 0.004872855730354786\n",
      "epoch 49 finished - avarage train loss 0.008958214026843679  avarage test loss 0.012956091843079776\n",
      "Training: epoch 50 batch 0 loss 0.004484137054532766\n",
      "Training: epoch 50 batch 10 loss 0.010038810782134533\n",
      "Training: epoch 50 batch 20 loss 0.013294078409671783\n",
      "Test: epoch 50 batch 0 loss 0.004681607708334923\n",
      "epoch 50 finished - avarage train loss 0.009472344343646848  avarage test loss 0.013660514494404197\n",
      "Training: epoch 51 batch 0 loss 0.00785046722739935\n",
      "Training: epoch 51 batch 10 loss 0.006940891500562429\n",
      "Training: epoch 51 batch 20 loss 0.015685366466641426\n",
      "Test: epoch 51 batch 0 loss 0.0077725984156131744\n",
      "epoch 51 finished - avarage train loss 0.011938204105686525  avarage test loss 0.017681455239653587\n",
      "Training: epoch 52 batch 0 loss 0.007585256360471249\n",
      "Training: epoch 52 batch 10 loss 0.005040381103754044\n",
      "Training: epoch 52 batch 20 loss 0.007808464579284191\n",
      "Test: epoch 52 batch 0 loss 0.00406815716996789\n",
      "epoch 52 finished - avarage train loss 0.009692157169097456  avarage test loss 0.01285053021274507\n",
      "Training: epoch 53 batch 0 loss 0.008611577562987804\n",
      "Training: epoch 53 batch 10 loss 0.00785103254020214\n",
      "Training: epoch 53 batch 20 loss 0.009490946307778358\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test: epoch 53 batch 0 loss 0.004649700131267309\n",
      "epoch 53 finished - avarage train loss 0.009637628553499436  avarage test loss 0.01315527968108654\n",
      "Training: epoch 54 batch 0 loss 0.003811474423855543\n",
      "Training: epoch 54 batch 10 loss 0.008271519094705582\n",
      "Training: epoch 54 batch 20 loss 0.004834459163248539\n",
      "Test: epoch 54 batch 0 loss 0.005147268995642662\n",
      "epoch 54 finished - avarage train loss 0.007208540963394375  avarage test loss 0.014887213939800858\n",
      "Training: epoch 55 batch 0 loss 0.00481231277808547\n",
      "Training: epoch 55 batch 10 loss 0.00834779441356659\n",
      "Training: epoch 55 batch 20 loss 0.007004899904131889\n",
      "Test: epoch 55 batch 0 loss 0.005880068056285381\n",
      "epoch 55 finished - avarage train loss 0.008255244088198605  avarage test loss 0.019108910113573074\n",
      "Training: epoch 56 batch 0 loss 0.009172232821583748\n",
      "Training: epoch 56 batch 10 loss 0.0055137211456894875\n",
      "Training: epoch 56 batch 20 loss 0.008250012062489986\n",
      "Test: epoch 56 batch 0 loss 0.005819007754325867\n",
      "epoch 56 finished - avarage train loss 0.00969540351873328  avarage test loss 0.014382686815224588\n",
      "Training: epoch 57 batch 0 loss 0.011371972039341927\n",
      "Training: epoch 57 batch 10 loss 0.010440405458211899\n",
      "Training: epoch 57 batch 20 loss 0.008740975521504879\n",
      "Test: epoch 57 batch 0 loss 0.004490581341087818\n",
      "epoch 57 finished - avarage train loss 0.00811519636772573  avarage test loss 0.01400349976029247\n",
      "Training: epoch 58 batch 0 loss 0.0066591305658221245\n",
      "Training: epoch 58 batch 10 loss 0.007179059088230133\n",
      "Training: epoch 58 batch 20 loss 0.006176447961479425\n",
      "Test: epoch 58 batch 0 loss 0.004805424716323614\n",
      "epoch 58 finished - avarage train loss 0.006742943800857355  avarage test loss 0.01650391228031367\n",
      "Training: epoch 59 batch 0 loss 0.006693250499665737\n",
      "Training: epoch 59 batch 10 loss 0.005623582284897566\n",
      "Training: epoch 59 batch 20 loss 0.005986079573631287\n",
      "Test: epoch 59 batch 0 loss 0.004290795419365168\n",
      "epoch 59 finished - avarage train loss 0.007478229404459226  avarage test loss 0.015518863569013774\n",
      "Training: epoch 60 batch 0 loss 0.0036955380346626043\n",
      "Training: epoch 60 batch 10 loss 0.005561162251979113\n",
      "Training: epoch 60 batch 20 loss 0.007588554173707962\n",
      "Test: epoch 60 batch 0 loss 0.005206889472901821\n",
      "epoch 60 finished - avarage train loss 0.007774290981991538  avarage test loss 0.016134663368575275\n",
      "Training: epoch 61 batch 0 loss 0.004967105109244585\n",
      "Training: epoch 61 batch 10 loss 0.0065199751406908035\n",
      "Training: epoch 61 batch 20 loss 0.003175144549459219\n",
      "Test: epoch 61 batch 0 loss 0.0058038681745529175\n",
      "epoch 61 finished - avarage train loss 0.007629851666118564  avarage test loss 0.014444577507674694\n",
      "Training: epoch 62 batch 0 loss 0.012180307880043983\n",
      "Training: epoch 62 batch 10 loss 0.0048203784972429276\n",
      "Training: epoch 62 batch 20 loss 0.007689435034990311\n",
      "Test: epoch 62 batch 0 loss 0.005205786786973476\n",
      "epoch 62 finished - avarage train loss 0.008419858131172329  avarage test loss 0.013617442804388702\n",
      "Training: epoch 63 batch 0 loss 0.005315708462148905\n",
      "Training: epoch 63 batch 10 loss 0.008373077027499676\n",
      "Training: epoch 63 batch 20 loss 0.007156880106776953\n",
      "Test: epoch 63 batch 0 loss 0.0056654890067875385\n",
      "epoch 63 finished - avarage train loss 0.00823745516064609  avarage test loss 0.014405999216251075\n",
      "Training: epoch 64 batch 0 loss 0.012558075599372387\n",
      "Training: epoch 64 batch 10 loss 0.005992285907268524\n",
      "Training: epoch 64 batch 20 loss 0.0066778347827494144\n",
      "Test: epoch 64 batch 0 loss 0.004192485008388758\n",
      "epoch 64 finished - avarage train loss 0.007948530388289484  avarage test loss 0.013505337527021766\n",
      "Training: epoch 65 batch 0 loss 0.005003668367862701\n",
      "Training: epoch 65 batch 10 loss 0.005290305241942406\n",
      "Training: epoch 65 batch 20 loss 0.004909154959022999\n",
      "Test: epoch 65 batch 0 loss 0.00469666114076972\n",
      "epoch 65 finished - avarage train loss 0.007752130996307422  avarage test loss 0.014976866776123643\n",
      "Training: epoch 66 batch 0 loss 0.007733604870736599\n",
      "Training: epoch 66 batch 10 loss 0.0069089108146727085\n",
      "Training: epoch 66 batch 20 loss 0.006610437296330929\n",
      "Test: epoch 66 batch 0 loss 0.0036630616523325443\n",
      "epoch 66 finished - avarage train loss 0.007798128954038538  avarage test loss 0.012680116342380643\n",
      "Training: epoch 67 batch 0 loss 0.00620003929361701\n",
      "Training: epoch 67 batch 10 loss 0.006267840974032879\n",
      "Training: epoch 67 batch 20 loss 0.004539442248642445\n",
      "Test: epoch 67 batch 0 loss 0.009141966700553894\n",
      "epoch 67 finished - avarage train loss 0.00785206254282645  avarage test loss 0.018371846061199903\n",
      "Training: epoch 68 batch 0 loss 0.0086243636906147\n",
      "Training: epoch 68 batch 10 loss 0.010974821634590626\n",
      "Training: epoch 68 batch 20 loss 0.0038554100319743156\n",
      "Test: epoch 68 batch 0 loss 0.003930204082280397\n",
      "epoch 68 finished - avarage train loss 0.008548531022950494  avarage test loss 0.013641120749525726\n",
      "Training: epoch 69 batch 0 loss 0.0062871528789401054\n",
      "Training: epoch 69 batch 10 loss 0.007626985665410757\n",
      "Training: epoch 69 batch 20 loss 0.010264093987643719\n",
      "Test: epoch 69 batch 0 loss 0.01481175608932972\n",
      "epoch 69 finished - avarage train loss 0.011913908260135815  avarage test loss 0.027705221204087138\n",
      "Training: epoch 70 batch 0 loss 0.010079607367515564\n",
      "Training: epoch 70 batch 10 loss 0.009309814311563969\n",
      "Training: epoch 70 batch 20 loss 0.005320999771356583\n",
      "Test: epoch 70 batch 0 loss 0.006480669602751732\n",
      "epoch 70 finished - avarage train loss 0.010747025704717842  avarage test loss 0.021976686781272292\n",
      "Training: epoch 71 batch 0 loss 0.007039312273263931\n",
      "Training: epoch 71 batch 10 loss 0.006715204566717148\n",
      "Training: epoch 71 batch 20 loss 0.00650771101936698\n",
      "Test: epoch 71 batch 0 loss 0.007110172882676125\n",
      "epoch 71 finished - avarage train loss 0.008889784362038662  avarage test loss 0.02088719862513244\n",
      "Training: epoch 72 batch 0 loss 0.01039201021194458\n",
      "Training: epoch 72 batch 10 loss 0.006151976063847542\n",
      "Training: epoch 72 batch 20 loss 0.0065976716578006744\n",
      "Test: epoch 72 batch 0 loss 0.006663612090051174\n",
      "epoch 72 finished - avarage train loss 0.008092997065777409  avarage test loss 0.014021966373547912\n",
      "Training: epoch 73 batch 0 loss 0.00757255544885993\n",
      "Training: epoch 73 batch 10 loss 0.009104856289923191\n",
      "Training: epoch 73 batch 20 loss 0.006671482231467962\n",
      "Test: epoch 73 batch 0 loss 0.006984388455748558\n",
      "epoch 73 finished - avarage train loss 0.008125265546399972  avarage test loss 0.02001510700210929\n",
      "Training: epoch 74 batch 0 loss 0.005365676712244749\n",
      "Training: epoch 74 batch 10 loss 0.009520696476101875\n",
      "Training: epoch 74 batch 20 loss 0.00751873292028904\n",
      "Test: epoch 74 batch 0 loss 0.0053001828491687775\n",
      "epoch 74 finished - avarage train loss 0.009021580861560229  avarage test loss 0.01783869491191581\n",
      "Training: epoch 75 batch 0 loss 0.005710807163268328\n",
      "Training: epoch 75 batch 10 loss 0.004761616699397564\n",
      "Training: epoch 75 batch 20 loss 0.0063218120485544205\n",
      "Test: epoch 75 batch 0 loss 0.004683107603341341\n",
      "epoch 75 finished - avarage train loss 0.007156551084962898  avarage test loss 0.015067098254803568\n",
      "Training: epoch 76 batch 0 loss 0.004464002791792154\n",
      "Training: epoch 76 batch 10 loss 0.006161849480122328\n",
      "Training: epoch 76 batch 20 loss 0.004993551876395941\n",
      "Test: epoch 76 batch 0 loss 0.005364842712879181\n",
      "epoch 76 finished - avarage train loss 0.0075968787443406625  avarage test loss 0.016455348231829703\n",
      "Training: epoch 77 batch 0 loss 0.006670196074992418\n",
      "Training: epoch 77 batch 10 loss 0.006725533865392208\n",
      "Training: epoch 77 batch 20 loss 0.0073661827482283115\n",
      "Test: epoch 77 batch 0 loss 0.00432444317266345\n",
      "epoch 77 finished - avarage train loss 0.0072432945578776555  avarage test loss 0.014263954944908619\n",
      "Training: epoch 78 batch 0 loss 0.004721228964626789\n",
      "Training: epoch 78 batch 10 loss 0.005452747922390699\n",
      "Training: epoch 78 batch 20 loss 0.009250892326235771\n",
      "Test: epoch 78 batch 0 loss 0.0058931633830070496\n",
      "epoch 78 finished - avarage train loss 0.008360304861682755  avarage test loss 0.017559639643877745\n",
      "Training: epoch 79 batch 0 loss 0.004871973302215338\n",
      "Training: epoch 79 batch 10 loss 0.0150219751521945\n",
      "Training: epoch 79 batch 20 loss 0.011463380418717861\n",
      "Test: epoch 79 batch 0 loss 0.0044361562468111515\n",
      "epoch 79 finished - avarage train loss 0.007991954130281148  avarage test loss 0.013635509414598346\n",
      "Training: epoch 80 batch 0 loss 0.004931238945573568\n",
      "Training: epoch 80 batch 10 loss 0.006906122900545597\n",
      "Training: epoch 80 batch 20 loss 0.005017376504838467\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test: epoch 80 batch 0 loss 0.005410765763372183\n",
      "epoch 80 finished - avarage train loss 0.007367747635098881  avarage test loss 0.015993647393770516\n",
      "Training: epoch 81 batch 0 loss 0.007511272095143795\n",
      "Training: epoch 81 batch 10 loss 0.005855258088558912\n",
      "Training: epoch 81 batch 20 loss 0.006520736962556839\n",
      "Test: epoch 81 batch 0 loss 0.007199343293905258\n",
      "epoch 81 finished - avarage train loss 0.008407655637711287  avarage test loss 0.022032177192158997\n",
      "Training: epoch 82 batch 0 loss 0.008045582100749016\n",
      "Training: epoch 82 batch 10 loss 0.006533635314553976\n",
      "Training: epoch 82 batch 20 loss 0.007458351086825132\n",
      "Test: epoch 82 batch 0 loss 0.005528113339096308\n",
      "epoch 82 finished - avarage train loss 0.008356968888306412  avarage test loss 0.019363728817552328\n",
      "Training: epoch 83 batch 0 loss 0.005142875947058201\n",
      "Training: epoch 83 batch 10 loss 0.00429931003600359\n",
      "Training: epoch 83 batch 20 loss 0.0032845069654285908\n",
      "Test: epoch 83 batch 0 loss 0.005015222355723381\n",
      "epoch 83 finished - avarage train loss 0.006380544565939184  avarage test loss 0.020302455057390034\n",
      "Training: epoch 84 batch 0 loss 0.005878360942006111\n",
      "Training: epoch 84 batch 10 loss 0.00756479799747467\n",
      "Training: epoch 84 batch 20 loss 0.004278588108718395\n",
      "Test: epoch 84 batch 0 loss 0.005236354656517506\n",
      "epoch 84 finished - avarage train loss 0.007167865160916899  avarage test loss 0.01969902648124844\n",
      "Training: epoch 85 batch 0 loss 0.005399156827479601\n",
      "Training: epoch 85 batch 10 loss 0.007068679668009281\n",
      "Training: epoch 85 batch 20 loss 0.00824805535376072\n",
      "Test: epoch 85 batch 0 loss 0.004247256554663181\n",
      "epoch 85 finished - avarage train loss 0.007810694221728321  avarage test loss 0.015532821416854858\n",
      "Training: epoch 86 batch 0 loss 0.005819805432111025\n",
      "Training: epoch 86 batch 10 loss 0.00319449114613235\n",
      "Training: epoch 86 batch 20 loss 0.005491920281201601\n",
      "Test: epoch 86 batch 0 loss 0.004402871709316969\n",
      "epoch 86 finished - avarage train loss 0.006992868854311006  avarage test loss 0.01474298641551286\n",
      "Training: epoch 87 batch 0 loss 0.00440940773114562\n",
      "Training: epoch 87 batch 10 loss 0.01126023568212986\n",
      "Training: epoch 87 batch 20 loss 0.007078723516315222\n",
      "Test: epoch 87 batch 0 loss 0.007630130276083946\n",
      "epoch 87 finished - avarage train loss 0.00852396666746715  avarage test loss 0.015944599872455\n",
      "Training: epoch 88 batch 0 loss 0.010227715596556664\n",
      "Training: epoch 88 batch 10 loss 0.006194531451910734\n",
      "Training: epoch 88 batch 20 loss 0.0044960202649235725\n",
      "Test: epoch 88 batch 0 loss 0.005531106144189835\n",
      "epoch 88 finished - avarage train loss 0.007872623568079594  avarage test loss 0.015567695489153266\n",
      "Training: epoch 89 batch 0 loss 0.007450093515217304\n",
      "Training: epoch 89 batch 10 loss 0.004449516534805298\n",
      "Training: epoch 89 batch 20 loss 0.0022519188933074474\n",
      "Test: epoch 89 batch 0 loss 0.004342060070484877\n",
      "epoch 89 finished - avarage train loss 0.00833190202006492  avarage test loss 0.013275301549583673\n",
      "Training: epoch 90 batch 0 loss 0.0029223961755633354\n",
      "Training: epoch 90 batch 10 loss 0.0034349062480032444\n",
      "Training: epoch 90 batch 20 loss 0.010357423685491085\n",
      "Test: epoch 90 batch 0 loss 0.004477918613702059\n",
      "epoch 90 finished - avarage train loss 0.007897636933444903  avarage test loss 0.014599484158679843\n",
      "Training: epoch 91 batch 0 loss 0.007537147030234337\n",
      "Training: epoch 91 batch 10 loss 0.006883256137371063\n",
      "Training: epoch 91 batch 20 loss 0.00751818111166358\n",
      "Test: epoch 91 batch 0 loss 0.004563911352306604\n",
      "epoch 91 finished - avarage train loss 0.007883406894538423  avarage test loss 0.014299086411483586\n",
      "Training: epoch 92 batch 0 loss 0.004211612977087498\n",
      "Training: epoch 92 batch 10 loss 0.00543402461335063\n",
      "Training: epoch 92 batch 20 loss 0.005335214547812939\n",
      "Test: epoch 92 batch 0 loss 0.004067558329552412\n",
      "epoch 92 finished - avarage train loss 0.006760954720385629  avarage test loss 0.01455110707320273\n",
      "Training: epoch 93 batch 0 loss 0.0053534721955657005\n",
      "Training: epoch 93 batch 10 loss 0.005419038701802492\n",
      "Training: epoch 93 batch 20 loss 0.004349324852228165\n",
      "Test: epoch 93 batch 0 loss 0.004615183919668198\n",
      "epoch 93 finished - avarage train loss 0.007305317301431607  avarage test loss 0.01408860378433019\n",
      "Training: epoch 94 batch 0 loss 0.011000458151102066\n",
      "Training: epoch 94 batch 10 loss 0.004581594839692116\n",
      "Training: epoch 94 batch 20 loss 0.0040124161168932915\n",
      "Test: epoch 94 batch 0 loss 0.008138149976730347\n",
      "epoch 94 finished - avarage train loss 0.008414508293158022  avarage test loss 0.018026991980150342\n",
      "Training: epoch 95 batch 0 loss 0.0117870531976223\n",
      "Training: epoch 95 batch 10 loss 0.00530638312920928\n",
      "Training: epoch 95 batch 20 loss 0.009909548796713352\n",
      "Test: epoch 95 batch 0 loss 0.005530111957341433\n",
      "epoch 95 finished - avarage train loss 0.00988304083524593  avarage test loss 0.01576622959692031\n",
      "Training: epoch 96 batch 0 loss 0.006270183250308037\n",
      "Training: epoch 96 batch 10 loss 0.006960876286029816\n",
      "Training: epoch 96 batch 20 loss 0.01207549124956131\n",
      "Test: epoch 96 batch 0 loss 0.004893144592642784\n",
      "epoch 96 finished - avarage train loss 0.0077341533635325475  avarage test loss 0.014228461543098092\n",
      "Training: epoch 97 batch 0 loss 0.0062422179616987705\n",
      "Training: epoch 97 batch 10 loss 0.009925450198352337\n",
      "Training: epoch 97 batch 20 loss 0.008734514936804771\n",
      "Test: epoch 97 batch 0 loss 0.0046838722191751\n",
      "epoch 97 finished - avarage train loss 0.007272674661964692  avarage test loss 0.014644599752500653\n",
      "Training: epoch 98 batch 0 loss 0.004304938949644566\n",
      "Training: epoch 98 batch 10 loss 0.0038391558919101954\n",
      "Training: epoch 98 batch 20 loss 0.008064578287303448\n",
      "Test: epoch 98 batch 0 loss 0.004750247113406658\n",
      "epoch 98 finished - avarage train loss 0.008148134942969372  avarage test loss 0.014858871698379517\n",
      "Training: epoch 99 batch 0 loss 0.006077755708247423\n",
      "Training: epoch 99 batch 10 loss 0.011975741013884544\n",
      "Training: epoch 99 batch 20 loss 0.012756021693348885\n",
      "Test: epoch 99 batch 0 loss 0.007304198574274778\n",
      "epoch 99 finished - avarage train loss 0.013354177763364438  avarage test loss 0.022626854362897575\n",
      "Training: epoch 100 batch 0 loss 0.010065177455544472\n",
      "Training: epoch 100 batch 10 loss 0.008732796646654606\n",
      "Training: epoch 100 batch 20 loss 0.005976654589176178\n",
      "Test: epoch 100 batch 0 loss 0.006219774484634399\n",
      "epoch 100 finished - avarage train loss 0.008726608698609573  avarage test loss 0.01631693192757666\n",
      "Training: epoch 101 batch 0 loss 0.006466536317020655\n",
      "Training: epoch 101 batch 10 loss 0.004387747496366501\n",
      "Training: epoch 101 batch 20 loss 0.0063100652769207954\n",
      "Test: epoch 101 batch 0 loss 0.004321221727877855\n",
      "epoch 101 finished - avarage train loss 0.008466806142301908  avarage test loss 0.014531680382788181\n",
      "Training: epoch 102 batch 0 loss 0.0030470050405710936\n",
      "Training: epoch 102 batch 10 loss 0.003918261732906103\n",
      "Training: epoch 102 batch 20 loss 0.006148496642708778\n",
      "Test: epoch 102 batch 0 loss 0.004878165200352669\n",
      "epoch 102 finished - avarage train loss 0.007147506288862948  avarage test loss 0.014072989812120795\n",
      "Training: epoch 103 batch 0 loss 0.00888279639184475\n",
      "Training: epoch 103 batch 10 loss 0.009109045378863811\n",
      "Training: epoch 103 batch 20 loss 0.008506473153829575\n",
      "Test: epoch 103 batch 0 loss 0.004238598048686981\n",
      "epoch 103 finished - avarage train loss 0.007387586210950695  avarage test loss 0.014085488277487457\n",
      "Training: epoch 104 batch 0 loss 0.005396435037255287\n",
      "Training: epoch 104 batch 10 loss 0.011030684225261211\n",
      "Training: epoch 104 batch 20 loss 0.012811019085347652\n",
      "Test: epoch 104 batch 0 loss 0.00458199018612504\n",
      "epoch 104 finished - avarage train loss 0.007449968862507878  avarage test loss 0.014346755109727383\n",
      "Training: epoch 105 batch 0 loss 0.005399907473474741\n",
      "Training: epoch 105 batch 10 loss 0.013816439546644688\n",
      "Training: epoch 105 batch 20 loss 0.007390535436570644\n",
      "Test: epoch 105 batch 0 loss 0.004398099146783352\n",
      "epoch 105 finished - avarage train loss 0.008469133766303802  avarage test loss 0.013686459045857191\n",
      "Training: epoch 106 batch 0 loss 0.007958986796438694\n",
      "Training: epoch 106 batch 10 loss 0.00683742668479681\n",
      "Training: epoch 106 batch 20 loss 0.003379181958734989\n",
      "Test: epoch 106 batch 0 loss 0.0039018201641738415\n",
      "epoch 106 finished - avarage train loss 0.006559000109675629  avarage test loss 0.014068239950574934\n",
      "Training: epoch 107 batch 0 loss 0.008008791133761406\n",
      "Training: epoch 107 batch 10 loss 0.005139046348631382\n",
      "Training: epoch 107 batch 20 loss 0.0047982786782085896\n",
      "Test: epoch 107 batch 0 loss 0.004892253782600164\n",
      "epoch 107 finished - avarage train loss 0.00840155167729947  avarage test loss 0.01472269487567246\n",
      "Training: epoch 108 batch 0 loss 0.005764611065387726\n",
      "Training: epoch 108 batch 10 loss 0.012637317180633545\n",
      "Training: epoch 108 batch 20 loss 0.013954374939203262\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test: epoch 108 batch 0 loss 0.003949756734073162\n",
      "epoch 108 finished - avarage train loss 0.01033954622609348  avarage test loss 0.015546258422546089\n",
      "Training: epoch 109 batch 0 loss 0.007656918838620186\n",
      "Training: epoch 109 batch 10 loss 0.005743124522268772\n",
      "Training: epoch 109 batch 20 loss 0.005821532569825649\n",
      "Test: epoch 109 batch 0 loss 0.0035659954883158207\n",
      "epoch 109 finished - avarage train loss 0.008909170146517712  avarage test loss 0.015691686770878732\n",
      "Training: epoch 110 batch 0 loss 0.005459937267005444\n",
      "Training: epoch 110 batch 10 loss 0.00878260936588049\n",
      "Training: epoch 110 batch 20 loss 0.004416567739099264\n",
      "Test: epoch 110 batch 0 loss 0.0032387967221438885\n",
      "epoch 110 finished - avarage train loss 0.007602688125816399  avarage test loss 0.014694525627419353\n",
      "Training: epoch 111 batch 0 loss 0.005926384590566158\n",
      "Training: epoch 111 batch 10 loss 0.005636643618345261\n",
      "Training: epoch 111 batch 20 loss 0.008430656045675278\n",
      "Test: epoch 111 batch 0 loss 0.005907273851335049\n",
      "epoch 111 finished - avarage train loss 0.007042750708180769  avarage test loss 0.015942447003908455\n",
      "Training: epoch 112 batch 0 loss 0.0065211346372962\n",
      "Training: epoch 112 batch 10 loss 0.009387333877384663\n",
      "Training: epoch 112 batch 20 loss 0.005783918313682079\n",
      "Test: epoch 112 batch 0 loss 0.004118120297789574\n",
      "epoch 112 finished - avarage train loss 0.007470935695515624  avarage test loss 0.014341034577228129\n",
      "Training: epoch 113 batch 0 loss 0.004606253933161497\n",
      "Training: epoch 113 batch 10 loss 0.008748709224164486\n",
      "Training: epoch 113 batch 20 loss 0.004188485909253359\n",
      "Test: epoch 113 batch 0 loss 0.004611516371369362\n",
      "epoch 113 finished - avarage train loss 0.006468903610547041  avarage test loss 0.017571928910911083\n",
      "Training: epoch 114 batch 0 loss 0.014957171864807606\n",
      "Training: epoch 114 batch 10 loss 0.006030043121427298\n",
      "Training: epoch 114 batch 20 loss 0.008083458989858627\n",
      "Test: epoch 114 batch 0 loss 0.0040110270492732525\n",
      "epoch 114 finished - avarage train loss 0.008947303309908202  avarage test loss 0.015500809764489532\n",
      "Training: epoch 115 batch 0 loss 0.005469158291816711\n",
      "Training: epoch 115 batch 10 loss 0.003654042026028037\n",
      "Training: epoch 115 batch 20 loss 0.008361193351447582\n",
      "Test: epoch 115 batch 0 loss 0.005277868360280991\n",
      "epoch 115 finished - avarage train loss 0.007830967709165195  avarage test loss 0.01974371075630188\n",
      "Training: epoch 116 batch 0 loss 0.0048484643921256065\n",
      "Training: epoch 116 batch 10 loss 0.004438160918653011\n",
      "Training: epoch 116 batch 20 loss 0.006798018701374531\n",
      "Test: epoch 116 batch 0 loss 0.0062643010169267654\n",
      "epoch 116 finished - avarage train loss 0.006954940927118577  avarage test loss 0.021419725380837917\n",
      "Training: epoch 117 batch 0 loss 0.010887859389185905\n",
      "Training: epoch 117 batch 10 loss 0.008664189837872982\n",
      "Training: epoch 117 batch 20 loss 0.005956762004643679\n",
      "Test: epoch 117 batch 0 loss 0.005997501779347658\n",
      "epoch 117 finished - avarage train loss 0.00636784619940766  avarage test loss 0.02129664458334446\n",
      "Training: epoch 118 batch 0 loss 0.007052133325487375\n",
      "Training: epoch 118 batch 10 loss 0.004958771634846926\n",
      "Training: epoch 118 batch 20 loss 0.007024798542261124\n",
      "Test: epoch 118 batch 0 loss 0.006389006040990353\n",
      "epoch 118 finished - avarage train loss 0.007340195371994171  avarage test loss 0.02180276275612414\n",
      "Training: epoch 119 batch 0 loss 0.005176435690373182\n",
      "Training: epoch 119 batch 10 loss 0.012097783386707306\n",
      "Training: epoch 119 batch 20 loss 0.005655041895806789\n",
      "Test: epoch 119 batch 0 loss 0.012430626899003983\n",
      "epoch 119 finished - avarage train loss 0.008565498724708269  avarage test loss 0.024287937907502055\n",
      "Training: epoch 120 batch 0 loss 0.009685254655778408\n",
      "Training: epoch 120 batch 10 loss 0.009265918284654617\n",
      "Training: epoch 120 batch 20 loss 0.005446170922368765\n",
      "Test: epoch 120 batch 0 loss 0.008670754730701447\n",
      "epoch 120 finished - avarage train loss 0.008021101885443103  avarage test loss 0.022825779858976603\n",
      "Training: epoch 121 batch 0 loss 0.008528236299753189\n",
      "Training: epoch 121 batch 10 loss 0.008265099488198757\n",
      "Training: epoch 121 batch 20 loss 0.007640110328793526\n",
      "Test: epoch 121 batch 0 loss 0.020130585879087448\n",
      "epoch 121 finished - avarage train loss 0.009700021152545152  avarage test loss 0.0279344217851758\n",
      "Training: epoch 122 batch 0 loss 0.006846095900982618\n",
      "Training: epoch 122 batch 10 loss 0.008777273818850517\n",
      "Training: epoch 122 batch 20 loss 0.014076783321797848\n",
      "Test: epoch 122 batch 0 loss 0.009914923459291458\n",
      "epoch 122 finished - avarage train loss 0.009659640169863043  avarage test loss 0.02268491603899747\n",
      "Training: epoch 123 batch 0 loss 0.004400570876896381\n",
      "Training: epoch 123 batch 10 loss 0.007365189027041197\n",
      "Training: epoch 123 batch 20 loss 0.005783977452665567\n",
      "Test: epoch 123 batch 0 loss 0.006199078634381294\n",
      "epoch 123 finished - avarage train loss 0.00796974225547807  avarage test loss 0.02127413044217974\n",
      "Training: epoch 124 batch 0 loss 0.004600378684699535\n",
      "Training: epoch 124 batch 10 loss 0.00886074174195528\n",
      "Training: epoch 124 batch 20 loss 0.0075331032276153564\n",
      "Test: epoch 124 batch 0 loss 0.006353799719363451\n",
      "epoch 124 finished - avarage train loss 0.007356992842436865  avarage test loss 0.02116816712077707\n",
      "Training: epoch 125 batch 0 loss 0.007756081409752369\n",
      "Training: epoch 125 batch 10 loss 0.006831292994320393\n",
      "Training: epoch 125 batch 20 loss 0.009490691125392914\n",
      "Test: epoch 125 batch 0 loss 0.004692609887570143\n",
      "epoch 125 finished - avarage train loss 0.007087371793800387  avarage test loss 0.013037474476732314\n",
      "Training: epoch 126 batch 0 loss 0.007932904176414013\n",
      "Training: epoch 126 batch 10 loss 0.011838649399578571\n",
      "Training: epoch 126 batch 20 loss 0.003531059715896845\n",
      "Test: epoch 126 batch 0 loss 0.004613012541085482\n",
      "epoch 126 finished - avarage train loss 0.006825435264357205  avarage test loss 0.014075896120630205\n",
      "Training: epoch 127 batch 0 loss 0.005082134157419205\n",
      "Training: epoch 127 batch 10 loss 0.004366643726825714\n",
      "Training: epoch 127 batch 20 loss 0.004060717765241861\n",
      "Test: epoch 127 batch 0 loss 0.004286161623895168\n",
      "epoch 127 finished - avarage train loss 0.007312654758450286  avarage test loss 0.014686601236462593\n",
      "Training: epoch 128 batch 0 loss 0.008211210370063782\n",
      "Training: epoch 128 batch 10 loss 0.009008114226162434\n",
      "Training: epoch 128 batch 20 loss 0.007192855700850487\n",
      "Test: epoch 128 batch 0 loss 0.004431688226759434\n",
      "epoch 128 finished - avarage train loss 0.007500393958441143  avarage test loss 0.014270408311858773\n",
      "Training: epoch 129 batch 0 loss 0.010089145973324776\n",
      "Training: epoch 129 batch 10 loss 0.006823190022259951\n",
      "Training: epoch 129 batch 20 loss 0.008959596045315266\n",
      "Test: epoch 129 batch 0 loss 0.004479693248867989\n",
      "epoch 129 finished - avarage train loss 0.00796515969880696  avarage test loss 0.01405372063163668\n",
      "Training: epoch 130 batch 0 loss 0.010804392397403717\n",
      "Training: epoch 130 batch 10 loss 0.0027586568612605333\n",
      "Training: epoch 130 batch 20 loss 0.004611491691321135\n",
      "Test: epoch 130 batch 0 loss 0.005359337665140629\n",
      "epoch 130 finished - avarage train loss 0.007776371789453873  avarage test loss 0.014807163970544934\n",
      "Training: epoch 131 batch 0 loss 0.006989693269133568\n",
      "Training: epoch 131 batch 10 loss 0.007775772362947464\n",
      "Training: epoch 131 batch 20 loss 0.008585285395383835\n",
      "Test: epoch 131 batch 0 loss 0.0063385553658008575\n",
      "epoch 131 finished - avarage train loss 0.008011784492831292  avarage test loss 0.021861041081137955\n",
      "Training: epoch 132 batch 0 loss 0.004683433100581169\n",
      "Training: epoch 132 batch 10 loss 0.008452867157757282\n",
      "Training: epoch 132 batch 20 loss 0.00457041896879673\n",
      "Test: epoch 132 batch 0 loss 0.003555431729182601\n",
      "epoch 132 finished - avarage train loss 0.008154229389439369  avarage test loss 0.013588964298833162\n",
      "Training: epoch 133 batch 0 loss 0.006524658761918545\n",
      "Training: epoch 133 batch 10 loss 0.006749369204044342\n",
      "Training: epoch 133 batch 20 loss 0.009434899315237999\n",
      "Test: epoch 133 batch 0 loss 0.004240758251398802\n",
      "epoch 133 finished - avarage train loss 0.0076864140172456874  avarage test loss 0.013479157001711428\n",
      "Training: epoch 134 batch 0 loss 0.003658704925328493\n",
      "Training: epoch 134 batch 10 loss 0.005257511977106333\n",
      "Training: epoch 134 batch 20 loss 0.00540164764970541\n",
      "Test: epoch 134 batch 0 loss 0.003359441179782152\n",
      "epoch 134 finished - avarage train loss 0.008527244794471511  avarage test loss 0.013116366229951382\n",
      "Training: epoch 135 batch 0 loss 0.00967908464372158\n",
      "Training: epoch 135 batch 10 loss 0.0053716544061899185\n",
      "Training: epoch 135 batch 20 loss 0.0066664619371294975\n",
      "Test: epoch 135 batch 0 loss 0.005393927451223135\n",
      "epoch 135 finished - avarage train loss 0.0075469952285418225  avarage test loss 0.014442289364524186\n",
      "Training: epoch 136 batch 0 loss 0.0068430607207119465\n",
      "Training: epoch 136 batch 10 loss 0.009655044414103031\n",
      "Training: epoch 136 batch 20 loss 0.014727848581969738\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test: epoch 136 batch 0 loss 0.004672704264521599\n",
      "epoch 136 finished - avarage train loss 0.008628618871343547  avarage test loss 0.013566770008765161\n",
      "Training: epoch 137 batch 0 loss 0.005865768529474735\n",
      "Training: epoch 137 batch 10 loss 0.005643979646265507\n",
      "Training: epoch 137 batch 20 loss 0.007108412683010101\n",
      "Test: epoch 137 batch 0 loss 0.005895813927054405\n",
      "epoch 137 finished - avarage train loss 0.0089609711832399  avarage test loss 0.021171452943235636\n",
      "Training: epoch 138 batch 0 loss 0.00865048449486494\n",
      "Training: epoch 138 batch 10 loss 0.005772602278739214\n",
      "Training: epoch 138 batch 20 loss 0.004285574425011873\n",
      "Test: epoch 138 batch 0 loss 0.005042301956564188\n",
      "epoch 138 finished - avarage train loss 0.007863766112332714  avarage test loss 0.01879858272150159\n",
      "Training: epoch 139 batch 0 loss 0.009083379060029984\n",
      "Training: epoch 139 batch 10 loss 0.0037364789750427008\n",
      "Training: epoch 139 batch 20 loss 0.004776049870997667\n",
      "Test: epoch 139 batch 0 loss 0.005431466735899448\n",
      "epoch 139 finished - avarage train loss 0.007463726190978597  avarage test loss 0.015487160300835967\n",
      "Training: epoch 140 batch 0 loss 0.008375127799808979\n",
      "Training: epoch 140 batch 10 loss 0.007684468291699886\n",
      "Training: epoch 140 batch 20 loss 0.01610727235674858\n",
      "Test: epoch 140 batch 0 loss 0.011381279677152634\n",
      "epoch 140 finished - avarage train loss 0.010791245077190727  avarage test loss 0.028167036129161716\n",
      "Training: epoch 141 batch 0 loss 0.014467954635620117\n",
      "Training: epoch 141 batch 10 loss 0.011802297085523605\n",
      "Training: epoch 141 batch 20 loss 0.004846218973398209\n",
      "Test: epoch 141 batch 0 loss 0.007056081667542458\n",
      "epoch 141 finished - avarage train loss 0.009974509606073642  avarage test loss 0.021982300211675465\n",
      "Training: epoch 142 batch 0 loss 0.007592751178890467\n",
      "Training: epoch 142 batch 10 loss 0.007849106565117836\n",
      "Training: epoch 142 batch 20 loss 0.005960441660135984\n",
      "Test: epoch 142 batch 0 loss 0.01095473114401102\n",
      "epoch 142 finished - avarage train loss 0.009786356497427514  avarage test loss 0.025944872992113233\n",
      "Training: epoch 143 batch 0 loss 0.008868085220456123\n",
      "Training: epoch 143 batch 10 loss 0.008715219795703888\n",
      "Training: epoch 143 batch 20 loss 0.012231673114001751\n",
      "Test: epoch 143 batch 0 loss 0.006213711574673653\n",
      "epoch 143 finished - avarage train loss 0.009121861172326166  avarage test loss 0.021486795390956104\n",
      "Training: epoch 144 batch 0 loss 0.0075152162462472916\n",
      "Training: epoch 144 batch 10 loss 0.004392555449157953\n",
      "Training: epoch 144 batch 20 loss 0.010855927132070065\n",
      "Test: epoch 144 batch 0 loss 0.004747437778860331\n",
      "epoch 144 finished - avarage train loss 0.008247563486983037  avarage test loss 0.019105000072158873\n",
      "Training: epoch 145 batch 0 loss 0.005591586232185364\n",
      "Training: epoch 145 batch 10 loss 0.007459715008735657\n",
      "Training: epoch 145 batch 20 loss 0.011569945141673088\n",
      "Test: epoch 145 batch 0 loss 0.004818678367882967\n",
      "epoch 145 finished - avarage train loss 0.00780471894440466  avarage test loss 0.019179378054104745\n",
      "Training: epoch 146 batch 0 loss 0.00749275553971529\n",
      "Training: epoch 146 batch 10 loss 0.007993072271347046\n",
      "Training: epoch 146 batch 20 loss 0.004883532412350178\n",
      "Test: epoch 146 batch 0 loss 0.0051650842651724815\n",
      "epoch 146 finished - avarage train loss 0.006813160360565987  avarage test loss 0.015521429246291518\n",
      "Training: epoch 147 batch 0 loss 0.006473036482930183\n",
      "Training: epoch 147 batch 10 loss 0.009508613497018814\n",
      "Training: epoch 147 batch 20 loss 0.010861290618777275\n",
      "Test: epoch 147 batch 0 loss 0.004478743299841881\n",
      "epoch 147 finished - avarage train loss 0.008607837066439718  avarage test loss 0.014001964591443539\n",
      "Training: epoch 148 batch 0 loss 0.007561396341770887\n",
      "Training: epoch 148 batch 10 loss 0.004162570461630821\n",
      "Training: epoch 148 batch 20 loss 0.004774098750203848\n",
      "Test: epoch 148 batch 0 loss 0.0031254503410309553\n",
      "epoch 148 finished - avarage train loss 0.007771817581920788  avarage test loss 0.014710810442920774\n",
      "Training: epoch 149 batch 0 loss 0.00439408840611577\n",
      "Training: epoch 149 batch 10 loss 0.005754899233579636\n",
      "Training: epoch 149 batch 20 loss 0.005773476324975491\n",
      "Test: epoch 149 batch 0 loss 0.003306223778054118\n",
      "epoch 149 finished - avarage train loss 0.008015246596187353  avarage test loss 0.01418468466727063\n",
      "Training: epoch 150 batch 0 loss 0.005418258253484964\n",
      "Training: epoch 150 batch 10 loss 0.00399841507896781\n",
      "Training: epoch 150 batch 20 loss 0.005230288486927748\n",
      "Test: epoch 150 batch 0 loss 0.0030062294099479914\n",
      "epoch 150 finished - avarage train loss 0.007978710353952544  avarage test loss 0.014601138362195343\n",
      "Training: epoch 151 batch 0 loss 0.01271736714988947\n",
      "Training: epoch 151 batch 10 loss 0.004488691687583923\n",
      "Training: epoch 151 batch 20 loss 0.007181906141340733\n",
      "Test: epoch 151 batch 0 loss 0.004491434898227453\n",
      "epoch 151 finished - avarage train loss 0.008268475468302595  avarage test loss 0.01381709345150739\n",
      "Training: epoch 152 batch 0 loss 0.004709202330559492\n",
      "Training: epoch 152 batch 10 loss 0.009985484182834625\n",
      "Training: epoch 152 batch 20 loss 0.004445018246769905\n",
      "Test: epoch 152 batch 0 loss 0.00370858795940876\n",
      "epoch 152 finished - avarage train loss 0.008834545267745852  avarage test loss 0.014988352078944445\n",
      "Training: epoch 153 batch 0 loss 0.005920973140746355\n",
      "Training: epoch 153 batch 10 loss 0.007247561123222113\n",
      "Training: epoch 153 batch 20 loss 0.012793025001883507\n",
      "Test: epoch 153 batch 0 loss 0.0034831392113119364\n",
      "epoch 153 finished - avarage train loss 0.007954313002269843  avarage test loss 0.01394539576722309\n",
      "Training: epoch 154 batch 0 loss 0.008620910346508026\n",
      "Training: epoch 154 batch 10 loss 0.006917661055922508\n",
      "Training: epoch 154 batch 20 loss 0.00759192556142807\n",
      "Test: epoch 154 batch 0 loss 0.003522420534864068\n",
      "epoch 154 finished - avarage train loss 0.007150503742540705  avarage test loss 0.013746657583396882\n",
      "Training: epoch 155 batch 0 loss 0.0052320342510938644\n",
      "Training: epoch 155 batch 10 loss 0.004550074692815542\n",
      "Training: epoch 155 batch 20 loss 0.0062317922711372375\n",
      "Test: epoch 155 batch 0 loss 0.004359852522611618\n",
      "epoch 155 finished - avarage train loss 0.008350358537301934  avarage test loss 0.013651516055688262\n",
      "Training: epoch 156 batch 0 loss 0.011803080327808857\n",
      "Training: epoch 156 batch 10 loss 0.012862739153206348\n",
      "Training: epoch 156 batch 20 loss 0.004596860148012638\n",
      "Test: epoch 156 batch 0 loss 0.003887953469529748\n",
      "epoch 156 finished - avarage train loss 0.007885963615865028  avarage test loss 0.013662475917953998\n",
      "Training: epoch 157 batch 0 loss 0.009369310922920704\n",
      "Training: epoch 157 batch 10 loss 0.005553044378757477\n",
      "Training: epoch 157 batch 20 loss 0.006042842753231525\n",
      "Test: epoch 157 batch 0 loss 0.00366701977327466\n",
      "epoch 157 finished - avarage train loss 0.008209676754757249  avarage test loss 0.01312391145620495\n",
      "Training: epoch 158 batch 0 loss 0.0043557207100093365\n",
      "Training: epoch 158 batch 10 loss 0.005221270956099033\n",
      "Training: epoch 158 batch 20 loss 0.012192043475806713\n",
      "Test: epoch 158 batch 0 loss 0.0037142906803637743\n",
      "epoch 158 finished - avarage train loss 0.007614688811309892  avarage test loss 0.013910715526435524\n",
      "Training: epoch 159 batch 0 loss 0.006023736204952002\n",
      "Training: epoch 159 batch 10 loss 0.006861579604446888\n",
      "Training: epoch 159 batch 20 loss 0.005010084714740515\n",
      "Test: epoch 159 batch 0 loss 0.0050594862550497055\n",
      "epoch 159 finished - avarage train loss 0.008046434260904789  avarage test loss 0.01412700954824686\n",
      "Training: epoch 160 batch 0 loss 0.008806089870631695\n",
      "Training: epoch 160 batch 10 loss 0.0044404082000255585\n",
      "Training: epoch 160 batch 20 loss 0.005892099346965551\n",
      "Test: epoch 160 batch 0 loss 0.004068238195031881\n",
      "epoch 160 finished - avarage train loss 0.007912836888612345  avarage test loss 0.014761433121748269\n",
      "Training: epoch 161 batch 0 loss 0.005239472258836031\n",
      "Training: epoch 161 batch 10 loss 0.006676896475255489\n",
      "Training: epoch 161 batch 20 loss 0.007133980747312307\n",
      "Test: epoch 161 batch 0 loss 0.004901667591184378\n",
      "epoch 161 finished - avarage train loss 0.006616184110592665  avarage test loss 0.0199008866911754\n",
      "Training: epoch 162 batch 0 loss 0.006731001660227776\n",
      "Training: epoch 162 batch 10 loss 0.005922336596995592\n",
      "Training: epoch 162 batch 20 loss 0.005858652759343386\n",
      "Test: epoch 162 batch 0 loss 0.0042092218063771725\n",
      "epoch 162 finished - avarage train loss 0.00829942315688421  avarage test loss 0.013821959844790399\n",
      "Training: epoch 163 batch 0 loss 0.0035600101109594107\n",
      "Training: epoch 163 batch 10 loss 0.005942636635154486\n",
      "Training: epoch 163 batch 20 loss 0.007701126858592033\n",
      "Test: epoch 163 batch 0 loss 0.004450423177331686\n",
      "epoch 163 finished - avarage train loss 0.006758469470037982  avarage test loss 0.014277283451519907\n",
      "Training: epoch 164 batch 0 loss 0.003937991335988045\n",
      "Training: epoch 164 batch 10 loss 0.0053270901553332806\n",
      "Training: epoch 164 batch 20 loss 0.005635631270706654\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test: epoch 164 batch 0 loss 0.0039653480052948\n",
      "epoch 164 finished - avarage train loss 0.0072599689813395  avarage test loss 0.013168397941626608\n",
      "Training: epoch 165 batch 0 loss 0.004730153828859329\n",
      "Training: epoch 165 batch 10 loss 0.006723054684698582\n",
      "Training: epoch 165 batch 20 loss 0.006626846734434366\n",
      "Test: epoch 165 batch 0 loss 0.00511976471170783\n",
      "epoch 165 finished - avarage train loss 0.00704553809241745  avarage test loss 0.014285991666838527\n",
      "Training: epoch 166 batch 0 loss 0.007054763846099377\n",
      "Training: epoch 166 batch 10 loss 0.006860803812742233\n",
      "Training: epoch 166 batch 20 loss 0.0028324283193796873\n",
      "Test: epoch 166 batch 0 loss 0.004134592600166798\n",
      "epoch 166 finished - avarage train loss 0.007434479386449374  avarage test loss 0.014127237489446998\n",
      "Training: epoch 167 batch 0 loss 0.005157160572707653\n",
      "Training: epoch 167 batch 10 loss 0.007398514077067375\n",
      "Training: epoch 167 batch 20 loss 0.005529122427105904\n",
      "Test: epoch 167 batch 0 loss 0.0062094456516206264\n",
      "epoch 167 finished - avarage train loss 0.008409912328653294  avarage test loss 0.021417301846668124\n",
      "Training: epoch 168 batch 0 loss 0.006064764689654112\n",
      "Training: epoch 168 batch 10 loss 0.0027931679505854845\n",
      "Training: epoch 168 batch 20 loss 0.011299984529614449\n",
      "Test: epoch 168 batch 0 loss 0.005345194134861231\n",
      "epoch 168 finished - avarage train loss 0.006906671602086261  avarage test loss 0.01928978506475687\n",
      "Training: epoch 169 batch 0 loss 0.00723767327144742\n",
      "Training: epoch 169 batch 10 loss 0.0037916176952421665\n",
      "Training: epoch 169 batch 20 loss 0.00505446270108223\n",
      "Test: epoch 169 batch 0 loss 0.00568848242983222\n",
      "epoch 169 finished - avarage train loss 0.0075621923576269685  avarage test loss 0.021337586222216487\n",
      "Training: epoch 170 batch 0 loss 0.008212351240217686\n",
      "Training: epoch 170 batch 10 loss 0.0062571316957473755\n",
      "Training: epoch 170 batch 20 loss 0.003942284267395735\n",
      "Test: epoch 170 batch 0 loss 0.0062293848022818565\n",
      "epoch 170 finished - avarage train loss 0.00643117713003323  avarage test loss 0.02166945836506784\n",
      "Training: epoch 171 batch 0 loss 0.005672794301062822\n",
      "Training: epoch 171 batch 10 loss 0.007158835884183645\n",
      "Training: epoch 171 batch 20 loss 0.009607301093637943\n",
      "Test: epoch 171 batch 0 loss 0.006332044489681721\n",
      "epoch 171 finished - avarage train loss 0.007831509634531263  avarage test loss 0.021609503077343106\n",
      "Training: epoch 172 batch 0 loss 0.004544478375464678\n",
      "Training: epoch 172 batch 10 loss 0.002581170992925763\n",
      "Training: epoch 172 batch 20 loss 0.0051326570101082325\n",
      "Test: epoch 172 batch 0 loss 0.004821946378797293\n",
      "epoch 172 finished - avarage train loss 0.0067980121259160085  avarage test loss 0.021253751940093935\n",
      "Training: epoch 173 batch 0 loss 0.009361381642520428\n",
      "Training: epoch 173 batch 10 loss 0.004471117164939642\n",
      "Training: epoch 173 batch 20 loss 0.007054393645375967\n",
      "Test: epoch 173 batch 0 loss 0.005098517518490553\n",
      "epoch 173 finished - avarage train loss 0.008415648432705423  avarage test loss 0.019977208343334496\n",
      "Training: epoch 174 batch 0 loss 0.006239207461476326\n",
      "Training: epoch 174 batch 10 loss 0.006520751863718033\n",
      "Training: epoch 174 batch 20 loss 0.007543929386883974\n",
      "Test: epoch 174 batch 0 loss 0.00485968217253685\n",
      "epoch 174 finished - avarage train loss 0.008700511787600559  avarage test loss 0.019141975441016257\n",
      "Training: epoch 175 batch 0 loss 0.006931953597813845\n",
      "Training: epoch 175 batch 10 loss 0.005451323930174112\n",
      "Training: epoch 175 batch 20 loss 0.0049338811077177525\n",
      "Test: epoch 175 batch 0 loss 0.00385559955611825\n",
      "epoch 175 finished - avarage train loss 0.0070832227196159035  avarage test loss 0.01472157845273614\n",
      "Training: epoch 176 batch 0 loss 0.005112177692353725\n",
      "Training: epoch 176 batch 10 loss 0.004363060928881168\n",
      "Training: epoch 176 batch 20 loss 0.0060531883500516415\n",
      "Test: epoch 176 batch 0 loss 0.0043174284510314465\n",
      "epoch 176 finished - avarage train loss 0.008013417864025667  avarage test loss 0.014004839933477342\n",
      "Training: epoch 177 batch 0 loss 0.005901977885514498\n",
      "Training: epoch 177 batch 10 loss 0.004076343961060047\n",
      "Training: epoch 177 batch 20 loss 0.006728478241711855\n",
      "Test: epoch 177 batch 0 loss 0.004117817152291536\n",
      "epoch 177 finished - avarage train loss 0.00726435840900602  avarage test loss 0.013723451411351562\n",
      "Training: epoch 178 batch 0 loss 0.005536184646189213\n",
      "Training: epoch 178 batch 10 loss 0.00954211875796318\n",
      "Training: epoch 178 batch 20 loss 0.005126720294356346\n",
      "Test: epoch 178 batch 0 loss 0.004748699255287647\n",
      "epoch 178 finished - avarage train loss 0.007435113593036759  avarage test loss 0.013563152635470033\n",
      "Training: epoch 179 batch 0 loss 0.00476595014333725\n",
      "Training: epoch 179 batch 10 loss 0.005404313560575247\n",
      "Training: epoch 179 batch 20 loss 0.007063721772283316\n",
      "Test: epoch 179 batch 0 loss 0.0053737573325634\n",
      "epoch 179 finished - avarage train loss 0.007112122775087583  avarage test loss 0.020255970070138574\n",
      "Training: epoch 180 batch 0 loss 0.008574895560741425\n",
      "Training: epoch 180 batch 10 loss 0.006381130777299404\n",
      "Training: epoch 180 batch 20 loss 0.006070789415389299\n",
      "Test: epoch 180 batch 0 loss 0.0070916530676186085\n",
      "epoch 180 finished - avarage train loss 0.007935801191769284  avarage test loss 0.02273867535404861\n",
      "Training: epoch 181 batch 0 loss 0.0107672568410635\n",
      "Training: epoch 181 batch 10 loss 0.008863057941198349\n",
      "Training: epoch 181 batch 20 loss 0.0075278691947460175\n",
      "Test: epoch 181 batch 0 loss 0.006286943797022104\n",
      "epoch 181 finished - avarage train loss 0.008027268180238276  avarage test loss 0.021530960337258875\n",
      "Training: epoch 182 batch 0 loss 0.006546983495354652\n",
      "Training: epoch 182 batch 10 loss 0.01483482588082552\n",
      "Training: epoch 182 batch 20 loss 0.006681778933852911\n",
      "Test: epoch 182 batch 0 loss 0.006264412775635719\n",
      "epoch 182 finished - avarage train loss 0.0074428387933635504  avarage test loss 0.02148944849614054\n",
      "Training: epoch 183 batch 0 loss 0.004642624408006668\n",
      "Training: epoch 183 batch 10 loss 0.00519942119717598\n",
      "Training: epoch 183 batch 20 loss 0.0034821168519556522\n",
      "Test: epoch 183 batch 0 loss 0.006320500746369362\n",
      "epoch 183 finished - avarage train loss 0.008024117098478922  avarage test loss 0.021968481130898\n",
      "Training: epoch 184 batch 0 loss 0.0052146464586257935\n",
      "Training: epoch 184 batch 10 loss 0.003722456283867359\n",
      "Training: epoch 184 batch 20 loss 0.006810993887484074\n",
      "Test: epoch 184 batch 0 loss 0.004766788799315691\n",
      "epoch 184 finished - avarage train loss 0.007876903436885312  avarage test loss 0.020970634534023702\n",
      "Training: epoch 185 batch 0 loss 0.0057634045369923115\n",
      "Training: epoch 185 batch 10 loss 0.006935688201338053\n",
      "Training: epoch 185 batch 20 loss 0.0060075982473790646\n",
      "Test: epoch 185 batch 0 loss 0.007471372839063406\n",
      "epoch 185 finished - avarage train loss 0.00741565308597838  avarage test loss 0.02287136355880648\n",
      "Training: epoch 186 batch 0 loss 0.006700547877699137\n",
      "Training: epoch 186 batch 10 loss 0.006550606805831194\n",
      "Training: epoch 186 batch 20 loss 0.005220980383455753\n",
      "Test: epoch 186 batch 0 loss 0.0052037425339221954\n",
      "epoch 186 finished - avarage train loss 0.007476197515517987  avarage test loss 0.02119398640934378\n",
      "Training: epoch 187 batch 0 loss 0.0041349162347614765\n",
      "Training: epoch 187 batch 10 loss 0.005141436588019133\n",
      "Training: epoch 187 batch 20 loss 0.004281220957636833\n",
      "Test: epoch 187 batch 0 loss 0.004902753513306379\n",
      "epoch 187 finished - avarage train loss 0.007460294660694641  avarage test loss 0.020960346097126603\n",
      "Training: epoch 188 batch 0 loss 0.008214644156396389\n",
      "Training: epoch 188 batch 10 loss 0.004972780589014292\n",
      "Training: epoch 188 batch 20 loss 0.006394615396857262\n",
      "Test: epoch 188 batch 0 loss 0.005818305537104607\n",
      "epoch 188 finished - avarage train loss 0.008095581148718965  avarage test loss 0.014018386718817055\n",
      "Training: epoch 189 batch 0 loss 0.010438557714223862\n",
      "Training: epoch 189 batch 10 loss 0.007337396033108234\n",
      "Training: epoch 189 batch 20 loss 0.007272900082170963\n",
      "Test: epoch 189 batch 0 loss 0.003816379001364112\n",
      "epoch 189 finished - avarage train loss 0.009037059033289552  avarage test loss 0.013902206381317228\n",
      "Training: epoch 190 batch 0 loss 0.003915800713002682\n",
      "Training: epoch 190 batch 10 loss 0.008138335309922695\n",
      "Training: epoch 190 batch 20 loss 0.009044699370861053\n",
      "Test: epoch 190 batch 0 loss 0.0037146438844501972\n",
      "epoch 190 finished - avarage train loss 0.008359498496905997  avarage test loss 0.014307137113064528\n",
      "Training: epoch 191 batch 0 loss 0.008939133025705814\n",
      "Training: epoch 191 batch 10 loss 0.0049835024401545525\n",
      "Training: epoch 191 batch 20 loss 0.0047581978142261505\n",
      "Test: epoch 191 batch 0 loss 0.004928949289023876\n",
      "epoch 191 finished - avarage train loss 0.007627369514827071  avarage test loss 0.013810247881338\n",
      "Training: epoch 192 batch 0 loss 0.0057542165741324425\n",
      "Training: epoch 192 batch 10 loss 0.008591236546635628\n",
      "Training: epoch 192 batch 20 loss 0.0026907925494015217\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test: epoch 192 batch 0 loss 0.004161977209150791\n",
      "epoch 192 finished - avarage train loss 0.008016601686590704  avarage test loss 0.013955746893770993\n",
      "Training: epoch 193 batch 0 loss 0.004731319844722748\n",
      "Training: epoch 193 batch 10 loss 0.0028645924758166075\n",
      "Training: epoch 193 batch 20 loss 0.0070875342935323715\n",
      "Test: epoch 193 batch 0 loss 0.004525700118392706\n",
      "epoch 193 finished - avarage train loss 0.007125559881136849  avarage test loss 0.014149404945783317\n",
      "Training: epoch 194 batch 0 loss 0.009328283369541168\n",
      "Training: epoch 194 batch 10 loss 0.005582738667726517\n",
      "Training: epoch 194 batch 20 loss 0.004548042546957731\n",
      "Test: epoch 194 batch 0 loss 0.003780146362259984\n",
      "epoch 194 finished - avarage train loss 0.008507564053710165  avarage test loss 0.013583746214862913\n",
      "Training: epoch 195 batch 0 loss 0.009496049024164677\n",
      "Training: epoch 195 batch 10 loss 0.00608281884342432\n",
      "Training: epoch 195 batch 20 loss 0.005867067724466324\n",
      "Test: epoch 195 batch 0 loss 0.004292984493076801\n",
      "epoch 195 finished - avarage train loss 0.008176281076759613  avarage test loss 0.013555052224546671\n",
      "Training: epoch 196 batch 0 loss 0.007127281278371811\n",
      "Training: epoch 196 batch 10 loss 0.00390730332583189\n",
      "Training: epoch 196 batch 20 loss 0.0040503861382603645\n",
      "Test: epoch 196 batch 0 loss 0.003788137109950185\n",
      "epoch 196 finished - avarage train loss 0.007276234795050374  avarage test loss 0.013988415768835694\n",
      "Training: epoch 197 batch 0 loss 0.00818934291601181\n",
      "Training: epoch 197 batch 10 loss 0.005818914156407118\n",
      "Training: epoch 197 batch 20 loss 0.007610410917550325\n",
      "Test: epoch 197 batch 0 loss 0.007626127917319536\n",
      "epoch 197 finished - avarage train loss 0.0075978644208276064  avarage test loss 0.017922637634910643\n",
      "Training: epoch 198 batch 0 loss 0.013745647855103016\n",
      "Training: epoch 198 batch 10 loss 0.014551213942468166\n",
      "Training: epoch 198 batch 20 loss 0.008249348029494286\n",
      "Test: epoch 198 batch 0 loss 0.007102584466338158\n",
      "epoch 198 finished - avarage train loss 0.013211166502201352  avarage test loss 0.020586036378517747\n",
      "Training: epoch 199 batch 0 loss 0.006453249603509903\n",
      "Training: epoch 199 batch 10 loss 0.004941513761878014\n",
      "Training: epoch 199 batch 20 loss 0.008792738430202007\n",
      "Test: epoch 199 batch 0 loss 0.006405904423445463\n",
      "epoch 199 finished - avarage train loss 0.008699215370519408  avarage test loss 0.022195315337739885\n",
      "Training: epoch 0 batch 0 loss 0.7061007618904114\n",
      "Training: epoch 0 batch 10 loss 0.43104222416877747\n",
      "Training: epoch 0 batch 20 loss 0.4439558684825897\n",
      "Test: epoch 0 batch 0 loss 0.4436076879501343\n",
      "epoch 0 finished - avarage train loss 0.5071094909618641  avarage test loss 0.5142901986837387\n",
      "Training: epoch 1 batch 0 loss 0.48866555094718933\n",
      "Training: epoch 1 batch 10 loss 0.4674205183982849\n",
      "Training: epoch 1 batch 20 loss 0.5334308743476868\n",
      "Test: epoch 1 batch 0 loss 0.42085492610931396\n",
      "epoch 1 finished - avarage train loss 0.5106399490915495  avarage test loss 0.5143184512853622\n",
      "Training: epoch 2 batch 0 loss 0.5344055891036987\n",
      "Training: epoch 2 batch 10 loss 0.5072312355041504\n",
      "Training: epoch 2 batch 20 loss 0.5238558650016785\n",
      "Test: epoch 2 batch 0 loss 0.43302273750305176\n",
      "epoch 2 finished - avarage train loss 0.5200753653871601  avarage test loss 0.5082419365644455\n",
      "Training: epoch 3 batch 0 loss 0.5110967755317688\n",
      "Training: epoch 3 batch 10 loss 0.40877899527549744\n",
      "Training: epoch 3 batch 20 loss 0.5909259915351868\n",
      "Test: epoch 3 batch 0 loss 0.4100881516933441\n",
      "epoch 3 finished - avarage train loss 0.5141330114726362  avarage test loss 0.5038247182965279\n",
      "Training: epoch 4 batch 0 loss 0.49478206038475037\n",
      "Training: epoch 4 batch 10 loss 0.5966391563415527\n",
      "Training: epoch 4 batch 20 loss 0.4713878929615021\n",
      "Test: epoch 4 batch 0 loss 0.29435378313064575\n",
      "epoch 4 finished - avarage train loss 0.479824843077824  avarage test loss 0.3573753833770752\n",
      "Training: epoch 5 batch 0 loss 0.2957777678966522\n",
      "Training: epoch 5 batch 10 loss 0.11199046671390533\n",
      "Training: epoch 5 batch 20 loss 0.0329175665974617\n",
      "Test: epoch 5 batch 0 loss 0.03344755619764328\n",
      "epoch 5 finished - avarage train loss 0.10279432048314605  avarage test loss 0.042106615379452705\n",
      "Training: epoch 6 batch 0 loss 0.03675885498523712\n",
      "Training: epoch 6 batch 10 loss 0.04118693992495537\n",
      "Training: epoch 6 batch 20 loss 0.02008570358157158\n",
      "Test: epoch 6 batch 0 loss 0.013029715046286583\n",
      "epoch 6 finished - avarage train loss 0.02220650055798991  avarage test loss 0.02758511807769537\n",
      "Training: epoch 7 batch 0 loss 0.015683460980653763\n",
      "Training: epoch 7 batch 10 loss 0.015691181644797325\n",
      "Training: epoch 7 batch 20 loss 0.009200532920658588\n",
      "Test: epoch 7 batch 0 loss 0.008928943425416946\n",
      "epoch 7 finished - avarage train loss 0.011214949238788465  avarage test loss 0.023189428262412548\n",
      "Training: epoch 8 batch 0 loss 0.011346795596182346\n",
      "Training: epoch 8 batch 10 loss 0.014688968658447266\n",
      "Training: epoch 8 batch 20 loss 0.011635914444923401\n",
      "Test: epoch 8 batch 0 loss 0.008519894443452358\n",
      "epoch 8 finished - avarage train loss 0.01095992403811422  avarage test loss 0.022814907366409898\n",
      "Training: epoch 9 batch 0 loss 0.010211004875600338\n",
      "Training: epoch 9 batch 10 loss 0.006295133847743273\n",
      "Training: epoch 9 batch 20 loss 0.008239110931754112\n",
      "Test: epoch 9 batch 0 loss 0.008326442912220955\n",
      "epoch 9 finished - avarage train loss 0.010046766155624184  avarage test loss 0.022186823887750506\n",
      "Training: epoch 10 batch 0 loss 0.007163666188716888\n",
      "Training: epoch 10 batch 10 loss 0.008855126798152924\n",
      "Training: epoch 10 batch 20 loss 0.010370132513344288\n",
      "Test: epoch 10 batch 0 loss 0.005218452773988247\n",
      "epoch 10 finished - avarage train loss 0.009610100456609809  avarage test loss 0.017533166683278978\n",
      "Training: epoch 11 batch 0 loss 0.0054101161658763885\n",
      "Training: epoch 11 batch 10 loss 0.00799698568880558\n",
      "Training: epoch 11 batch 20 loss 0.004128014203161001\n",
      "Test: epoch 11 batch 0 loss 0.00510153966024518\n",
      "epoch 11 finished - avarage train loss 0.00803723085093601  avarage test loss 0.014563035918399692\n",
      "Training: epoch 12 batch 0 loss 0.011436119675636292\n",
      "Training: epoch 12 batch 10 loss 0.0031623253598809242\n",
      "Training: epoch 12 batch 20 loss 0.0078027863055467606\n",
      "Test: epoch 12 batch 0 loss 0.0046915230341255665\n",
      "epoch 12 finished - avarage train loss 0.008972989427375382  avarage test loss 0.017109254142269492\n",
      "Training: epoch 13 batch 0 loss 0.0048317136242985725\n",
      "Training: epoch 13 batch 10 loss 0.008289521560072899\n",
      "Training: epoch 13 batch 20 loss 0.006303459871560335\n",
      "Test: epoch 13 batch 0 loss 0.006676237564533949\n",
      "epoch 13 finished - avarage train loss 0.00926471394807871  avarage test loss 0.018905657925643027\n",
      "Training: epoch 14 batch 0 loss 0.00824738573282957\n",
      "Training: epoch 14 batch 10 loss 0.011021456681191921\n",
      "Training: epoch 14 batch 20 loss 0.00857083685696125\n",
      "Test: epoch 14 batch 0 loss 0.004881992936134338\n",
      "epoch 14 finished - avarage train loss 0.008767755838624876  avarage test loss 0.014091728604398668\n",
      "Training: epoch 15 batch 0 loss 0.008706449531018734\n",
      "Training: epoch 15 batch 10 loss 0.005033132154494524\n",
      "Training: epoch 15 batch 20 loss 0.007048753090202808\n",
      "Test: epoch 15 batch 0 loss 0.0034272479824721813\n",
      "epoch 15 finished - avarage train loss 0.008199756178234157  avarage test loss 0.013603428611531854\n",
      "Training: epoch 16 batch 0 loss 0.003165289992466569\n",
      "Training: epoch 16 batch 10 loss 0.007425540126860142\n",
      "Training: epoch 16 batch 20 loss 0.01211068220436573\n",
      "Test: epoch 16 batch 0 loss 0.007400872185826302\n",
      "epoch 16 finished - avarage train loss 0.00951915525767053  avarage test loss 0.0180587915237993\n",
      "Training: epoch 17 batch 0 loss 0.007020127959549427\n",
      "Training: epoch 17 batch 10 loss 0.007403324358165264\n",
      "Training: epoch 17 batch 20 loss 0.00947603490203619\n",
      "Test: epoch 17 batch 0 loss 0.00538914417847991\n",
      "epoch 17 finished - avarage train loss 0.00836102932627345  avarage test loss 0.01604801812209189\n",
      "Training: epoch 18 batch 0 loss 0.00841086357831955\n",
      "Training: epoch 18 batch 10 loss 0.008937356993556023\n",
      "Training: epoch 18 batch 20 loss 0.009621324017643929\n",
      "Test: epoch 18 batch 0 loss 0.005267309490591288\n",
      "epoch 18 finished - avarage train loss 0.00734451610659217  avarage test loss 0.015779783250764012\n",
      "Training: epoch 19 batch 0 loss 0.005228836089372635\n",
      "Training: epoch 19 batch 10 loss 0.0037367665208876133\n",
      "Training: epoch 19 batch 20 loss 0.0034973658621311188\n",
      "Test: epoch 19 batch 0 loss 0.004437558818608522\n",
      "epoch 19 finished - avarage train loss 0.007294530016851836  avarage test loss 0.01309148408472538\n",
      "Training: epoch 20 batch 0 loss 0.004590737167745829\n",
      "Training: epoch 20 batch 10 loss 0.006319473031908274\n",
      "Training: epoch 20 batch 20 loss 0.006438955664634705\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test: epoch 20 batch 0 loss 0.00572042353451252\n",
      "epoch 20 finished - avarage train loss 0.008180617772299668  avarage test loss 0.0142321438761428\n",
      "Training: epoch 21 batch 0 loss 0.007596344221383333\n",
      "Training: epoch 21 batch 10 loss 0.009670658968389034\n",
      "Training: epoch 21 batch 20 loss 0.005667961202561855\n",
      "Test: epoch 21 batch 0 loss 0.004468983504921198\n",
      "epoch 21 finished - avarage train loss 0.009745316226677648  avarage test loss 0.01627227000426501\n",
      "Training: epoch 22 batch 0 loss 0.005438001360744238\n",
      "Training: epoch 22 batch 10 loss 0.008795149624347687\n",
      "Training: epoch 22 batch 20 loss 0.006734447553753853\n",
      "Test: epoch 22 batch 0 loss 0.003645387478172779\n",
      "epoch 22 finished - avarage train loss 0.007197640402692145  avarage test loss 0.013245184207335114\n",
      "Training: epoch 23 batch 0 loss 0.004959177225828171\n",
      "Training: epoch 23 batch 10 loss 0.007138656452298164\n",
      "Training: epoch 23 batch 20 loss 0.004599877167493105\n",
      "Test: epoch 23 batch 0 loss 0.006165307015180588\n",
      "epoch 23 finished - avarage train loss 0.006563648420931964  avarage test loss 0.014232368674129248\n",
      "Training: epoch 24 batch 0 loss 0.006551352329552174\n",
      "Training: epoch 24 batch 10 loss 0.007332352921366692\n",
      "Training: epoch 24 batch 20 loss 0.006492298096418381\n",
      "Test: epoch 24 batch 0 loss 0.0036713918671011925\n",
      "epoch 24 finished - avarage train loss 0.008760697513433367  avarage test loss 0.012884520692750812\n",
      "Training: epoch 25 batch 0 loss 0.008711691945791245\n",
      "Training: epoch 25 batch 10 loss 0.00816661398857832\n",
      "Training: epoch 25 batch 20 loss 0.009537079371511936\n",
      "Test: epoch 25 batch 0 loss 0.00457215728238225\n",
      "epoch 25 finished - avarage train loss 0.008129243401746297  avarage test loss 0.014572635642252862\n",
      "Training: epoch 26 batch 0 loss 0.009189967066049576\n",
      "Training: epoch 26 batch 10 loss 0.007387954741716385\n",
      "Training: epoch 26 batch 20 loss 0.004747607745230198\n",
      "Test: epoch 26 batch 0 loss 0.004760518670082092\n",
      "epoch 26 finished - avarage train loss 0.008004065386244449  avarage test loss 0.015570446848869324\n",
      "Training: epoch 27 batch 0 loss 0.005946114659309387\n",
      "Training: epoch 27 batch 10 loss 0.005216290708631277\n",
      "Training: epoch 27 batch 20 loss 0.007840260863304138\n",
      "Test: epoch 27 batch 0 loss 0.004527975805103779\n",
      "epoch 27 finished - avarage train loss 0.008459808834796322  avarage test loss 0.015264692017808557\n",
      "Training: epoch 28 batch 0 loss 0.00644352613016963\n",
      "Training: epoch 28 batch 10 loss 0.005988436751067638\n",
      "Training: epoch 28 batch 20 loss 0.009283997118473053\n",
      "Test: epoch 28 batch 0 loss 0.007196975871920586\n",
      "epoch 28 finished - avarage train loss 0.010164682111092683  avarage test loss 0.018970783334225416\n",
      "Training: epoch 29 batch 0 loss 0.0059404498897492886\n",
      "Training: epoch 29 batch 10 loss 0.00989723950624466\n",
      "Training: epoch 29 batch 20 loss 0.007249794900417328\n",
      "Test: epoch 29 batch 0 loss 0.005514414981007576\n",
      "epoch 29 finished - avarage train loss 0.009886989353931156  avarage test loss 0.017109184758737683\n",
      "Training: epoch 30 batch 0 loss 0.01111447811126709\n",
      "Training: epoch 30 batch 10 loss 0.00506396172568202\n",
      "Training: epoch 30 batch 20 loss 0.007344496436417103\n",
      "Test: epoch 30 batch 0 loss 0.004241257905960083\n",
      "epoch 30 finished - avarage train loss 0.007259344874784864  avarage test loss 0.013414239627309144\n",
      "Training: epoch 31 batch 0 loss 0.003922716248780489\n",
      "Training: epoch 31 batch 10 loss 0.014366368763148785\n",
      "Training: epoch 31 batch 20 loss 0.007928242906928062\n",
      "Test: epoch 31 batch 0 loss 0.006091501098126173\n",
      "epoch 31 finished - avarage train loss 0.008140294186385542  avarage test loss 0.016566176316700876\n",
      "Training: epoch 32 batch 0 loss 0.01271267794072628\n",
      "Training: epoch 32 batch 10 loss 0.012091376818716526\n",
      "Training: epoch 32 batch 20 loss 0.00847660843282938\n",
      "Test: epoch 32 batch 0 loss 0.006973265204578638\n",
      "epoch 32 finished - avarage train loss 0.009979866814382118  avarage test loss 0.01892439171206206\n",
      "Training: epoch 33 batch 0 loss 0.006301258224993944\n",
      "Training: epoch 33 batch 10 loss 0.00709021370857954\n",
      "Training: epoch 33 batch 20 loss 0.00817076861858368\n",
      "Test: epoch 33 batch 0 loss 0.005362373776733875\n",
      "epoch 33 finished - avarage train loss 0.009024948949508112  avarage test loss 0.01428705989383161\n",
      "Training: epoch 34 batch 0 loss 0.007822123356163502\n",
      "Training: epoch 34 batch 10 loss 0.005014420486986637\n",
      "Training: epoch 34 batch 20 loss 0.010131566785275936\n",
      "Test: epoch 34 batch 0 loss 0.0036670651752501726\n",
      "epoch 34 finished - avarage train loss 0.0081616611187828  avarage test loss 0.013844840286765248\n",
      "Training: epoch 35 batch 0 loss 0.008373276330530643\n",
      "Training: epoch 35 batch 10 loss 0.007919777184724808\n",
      "Training: epoch 35 batch 20 loss 0.008116265758872032\n",
      "Test: epoch 35 batch 0 loss 0.0049838777631521225\n",
      "epoch 35 finished - avarage train loss 0.006809874734809172  avarage test loss 0.01339895254932344\n",
      "Training: epoch 36 batch 0 loss 0.009031590074300766\n",
      "Training: epoch 36 batch 10 loss 0.007799877785146236\n",
      "Training: epoch 36 batch 20 loss 0.014856153167784214\n",
      "Test: epoch 36 batch 0 loss 0.005041840486228466\n",
      "epoch 36 finished - avarage train loss 0.008764424040142832  avarage test loss 0.015047416440211236\n",
      "Training: epoch 37 batch 0 loss 0.005255230236798525\n",
      "Training: epoch 37 batch 10 loss 0.007188436575233936\n",
      "Training: epoch 37 batch 20 loss 0.0054819416254758835\n",
      "Test: epoch 37 batch 0 loss 0.005560475401580334\n",
      "epoch 37 finished - avarage train loss 0.007770784664899111  avarage test loss 0.014610136626288295\n",
      "Training: epoch 38 batch 0 loss 0.007321021519601345\n",
      "Training: epoch 38 batch 10 loss 0.00901516992598772\n",
      "Training: epoch 38 batch 20 loss 0.007478614337742329\n",
      "Test: epoch 38 batch 0 loss 0.0032381759956479073\n",
      "epoch 38 finished - avarage train loss 0.009033669990583741  avarage test loss 0.014114448567852378\n",
      "Training: epoch 39 batch 0 loss 0.006616984028369188\n",
      "Training: epoch 39 batch 10 loss 0.008599309250712395\n",
      "Training: epoch 39 batch 20 loss 0.0051489220932126045\n",
      "Test: epoch 39 batch 0 loss 0.00476923817768693\n",
      "epoch 39 finished - avarage train loss 0.008266884303683865  avarage test loss 0.013178725726902485\n",
      "Training: epoch 40 batch 0 loss 0.007128515746444464\n",
      "Training: epoch 40 batch 10 loss 0.007564648054540157\n",
      "Training: epoch 40 batch 20 loss 0.008666284382343292\n",
      "Test: epoch 40 batch 0 loss 0.004218133166432381\n",
      "epoch 40 finished - avarage train loss 0.008444092926922542  avarage test loss 0.01251514011528343\n",
      "Training: epoch 41 batch 0 loss 0.005679023452103138\n",
      "Training: epoch 41 batch 10 loss 0.005766557063907385\n",
      "Training: epoch 41 batch 20 loss 0.005550410598516464\n",
      "Test: epoch 41 batch 0 loss 0.004643403459340334\n",
      "epoch 41 finished - avarage train loss 0.007688906204340787  avarage test loss 0.013072501635178924\n",
      "Training: epoch 42 batch 0 loss 0.012933379970490932\n",
      "Training: epoch 42 batch 10 loss 0.006889564450830221\n",
      "Training: epoch 42 batch 20 loss 0.005345458630472422\n",
      "Test: epoch 42 batch 0 loss 0.00664799427613616\n",
      "epoch 42 finished - avarage train loss 0.010299562307973874  avarage test loss 0.015829485724680126\n",
      "Training: epoch 43 batch 0 loss 0.009532921016216278\n",
      "Training: epoch 43 batch 10 loss 0.007120886817574501\n",
      "Training: epoch 43 batch 20 loss 0.0094552431255579\n",
      "Test: epoch 43 batch 0 loss 0.006911724340170622\n",
      "epoch 43 finished - avarage train loss 0.010621706055926865  avarage test loss 0.014324766234494746\n",
      "Training: epoch 44 batch 0 loss 0.005885262042284012\n",
      "Training: epoch 44 batch 10 loss 0.005904137622565031\n",
      "Training: epoch 44 batch 20 loss 0.009225944988429546\n",
      "Test: epoch 44 batch 0 loss 0.006825259421020746\n",
      "epoch 44 finished - avarage train loss 0.010527349189566127  avarage test loss 0.016871065483428538\n",
      "Training: epoch 45 batch 0 loss 0.010856878943741322\n",
      "Training: epoch 45 batch 10 loss 0.006728092674165964\n",
      "Training: epoch 45 batch 20 loss 0.011504512280225754\n",
      "Test: epoch 45 batch 0 loss 0.007412648294121027\n",
      "epoch 45 finished - avarage train loss 0.010199592593286571  avarage test loss 0.016591617721132934\n",
      "Training: epoch 46 batch 0 loss 0.008879942819476128\n",
      "Training: epoch 46 batch 10 loss 0.013019165024161339\n",
      "Training: epoch 46 batch 20 loss 0.0059625315479934216\n",
      "Test: epoch 46 batch 0 loss 0.0066147297620773315\n",
      "epoch 46 finished - avarage train loss 0.00923975560300309  avarage test loss 0.014369922690093517\n",
      "Training: epoch 47 batch 0 loss 0.004487440921366215\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training: epoch 47 batch 10 loss 0.007702121511101723\n",
      "Training: epoch 47 batch 20 loss 0.008201521821320057\n",
      "Test: epoch 47 batch 0 loss 0.0047344788908958435\n",
      "epoch 47 finished - avarage train loss 0.007502771215513349  avarage test loss 0.01334321592003107\n",
      "Training: epoch 48 batch 0 loss 0.003274936228990555\n",
      "Training: epoch 48 batch 10 loss 0.0034830409567803144\n",
      "Training: epoch 48 batch 20 loss 0.005807716865092516\n",
      "Test: epoch 48 batch 0 loss 0.005864146631211042\n",
      "epoch 48 finished - avarage train loss 0.007849397755966618  avarage test loss 0.01755441923160106\n",
      "Training: epoch 49 batch 0 loss 0.009000740014016628\n",
      "Training: epoch 49 batch 10 loss 0.00517737353220582\n",
      "Training: epoch 49 batch 20 loss 0.004285041708499193\n",
      "Test: epoch 49 batch 0 loss 0.00391427194699645\n",
      "epoch 49 finished - avarage train loss 0.008690782629981124  avarage test loss 0.014960780157707632\n",
      "Training: epoch 50 batch 0 loss 0.002748857717961073\n",
      "Training: epoch 50 batch 10 loss 0.010364045388996601\n",
      "Training: epoch 50 batch 20 loss 0.010711085051298141\n",
      "Test: epoch 50 batch 0 loss 0.0031549769919365644\n",
      "epoch 50 finished - avarage train loss 0.008062048582360148  avarage test loss 0.01351910101948306\n",
      "Training: epoch 51 batch 0 loss 0.006762203760445118\n",
      "Training: epoch 51 batch 10 loss 0.006567778065800667\n",
      "Training: epoch 51 batch 20 loss 0.003960771486163139\n",
      "Test: epoch 51 batch 0 loss 0.005362895783036947\n",
      "epoch 51 finished - avarage train loss 0.0089664402385724  avarage test loss 0.017120438045822084\n",
      "Training: epoch 52 batch 0 loss 0.007716560736298561\n",
      "Training: epoch 52 batch 10 loss 0.0059693800285458565\n",
      "Training: epoch 52 batch 20 loss 0.008437533862888813\n",
      "Test: epoch 52 batch 0 loss 0.0033003678545355797\n",
      "epoch 52 finished - avarage train loss 0.0075852729797620195  avarage test loss 0.01443528279196471\n",
      "Training: epoch 53 batch 0 loss 0.008013082668185234\n",
      "Training: epoch 53 batch 10 loss 0.008046950213611126\n",
      "Training: epoch 53 batch 20 loss 0.004056559409946203\n",
      "Test: epoch 53 batch 0 loss 0.0038227105978876352\n",
      "epoch 53 finished - avarage train loss 0.008288387345663947  avarage test loss 0.013499197026249021\n",
      "Training: epoch 54 batch 0 loss 0.004317470360547304\n",
      "Training: epoch 54 batch 10 loss 0.0071072024293243885\n",
      "Training: epoch 54 batch 20 loss 0.007165904156863689\n",
      "Test: epoch 54 batch 0 loss 0.005267628002911806\n",
      "epoch 54 finished - avarage train loss 0.00757182322860021  avarage test loss 0.015430355560965836\n",
      "Training: epoch 55 batch 0 loss 0.00560449343174696\n",
      "Training: epoch 55 batch 10 loss 0.016871565952897072\n",
      "Training: epoch 55 batch 20 loss 0.00795817468315363\n",
      "Test: epoch 55 batch 0 loss 0.00530952587723732\n",
      "epoch 55 finished - avarage train loss 0.011985331308096647  avarage test loss 0.014728025998920202\n",
      "Training: epoch 56 batch 0 loss 0.015128177590668201\n",
      "Training: epoch 56 batch 10 loss 0.007382776588201523\n",
      "Training: epoch 56 batch 20 loss 0.005221941974014044\n",
      "Test: epoch 56 batch 0 loss 0.004036558326333761\n",
      "epoch 56 finished - avarage train loss 0.00797990531307356  avarage test loss 0.014296361710876226\n",
      "Training: epoch 57 batch 0 loss 0.0031406735070049763\n",
      "Training: epoch 57 batch 10 loss 0.008354231715202332\n",
      "Training: epoch 57 batch 20 loss 0.005830856971442699\n",
      "Test: epoch 57 batch 0 loss 0.004342827945947647\n",
      "epoch 57 finished - avarage train loss 0.007493541327080336  avarage test loss 0.014437692239880562\n",
      "Training: epoch 58 batch 0 loss 0.009268148802220821\n",
      "Training: epoch 58 batch 10 loss 0.0032860077917575836\n",
      "Training: epoch 58 batch 20 loss 0.004137546755373478\n",
      "Test: epoch 58 batch 0 loss 0.004995957016944885\n",
      "epoch 58 finished - avarage train loss 0.007485347783899513  avarage test loss 0.013541683438234031\n",
      "Training: epoch 59 batch 0 loss 0.005950358230620623\n",
      "Training: epoch 59 batch 10 loss 0.0043968381360173225\n",
      "Training: epoch 59 batch 20 loss 0.007728168275207281\n",
      "Test: epoch 59 batch 0 loss 0.005591552704572678\n",
      "epoch 59 finished - avarage train loss 0.0067241655116708115  avarage test loss 0.013809333788231015\n",
      "Training: epoch 60 batch 0 loss 0.006876318249851465\n",
      "Training: epoch 60 batch 10 loss 0.0052216192707419395\n",
      "Training: epoch 60 batch 20 loss 0.004510211758315563\n",
      "Test: epoch 60 batch 0 loss 0.005154171958565712\n",
      "epoch 60 finished - avarage train loss 0.008106498890716967  avarage test loss 0.014121741289272904\n",
      "Training: epoch 61 batch 0 loss 0.00702865282073617\n",
      "Training: epoch 61 batch 10 loss 0.003204002045094967\n",
      "Training: epoch 61 batch 20 loss 0.009107428602874279\n",
      "Test: epoch 61 batch 0 loss 0.004322776570916176\n",
      "epoch 61 finished - avarage train loss 0.007294783954797634  avarage test loss 0.013280416489578784\n",
      "Training: epoch 62 batch 0 loss 0.0053155855275690556\n",
      "Training: epoch 62 batch 10 loss 0.003760324325412512\n",
      "Training: epoch 62 batch 20 loss 0.003467678790912032\n",
      "Test: epoch 62 batch 0 loss 0.005406701471656561\n",
      "epoch 62 finished - avarage train loss 0.007854761049986399  avarage test loss 0.013942864374257624\n",
      "Training: epoch 63 batch 0 loss 0.0042432197369635105\n",
      "Training: epoch 63 batch 10 loss 0.005240453872829676\n",
      "Training: epoch 63 batch 20 loss 0.006203325930982828\n",
      "Test: epoch 63 batch 0 loss 0.0035977656953036785\n",
      "epoch 63 finished - avarage train loss 0.008765114134113336  avarage test loss 0.013392527238465846\n",
      "Training: epoch 64 batch 0 loss 0.0070833684876561165\n",
      "Training: epoch 64 batch 10 loss 0.003605661215260625\n",
      "Training: epoch 64 batch 20 loss 0.006200270727276802\n",
      "Test: epoch 64 batch 0 loss 0.005334159359335899\n",
      "epoch 64 finished - avarage train loss 0.008726079153410834  avarage test loss 0.015098829986527562\n",
      "Training: epoch 65 batch 0 loss 0.013279403559863567\n",
      "Training: epoch 65 batch 10 loss 0.005055658053606749\n",
      "Training: epoch 65 batch 20 loss 0.0070179905742406845\n",
      "Test: epoch 65 batch 0 loss 0.004164747428148985\n",
      "epoch 65 finished - avarage train loss 0.008948676079383185  avarage test loss 0.015362458769232035\n",
      "Training: epoch 66 batch 0 loss 0.009104866534471512\n",
      "Training: epoch 66 batch 10 loss 0.006505428347736597\n",
      "Training: epoch 66 batch 20 loss 0.009826503694057465\n",
      "Test: epoch 66 batch 0 loss 0.004061225801706314\n",
      "epoch 66 finished - avarage train loss 0.007561385848334637  avarage test loss 0.012897152337245643\n",
      "Training: epoch 67 batch 0 loss 0.0060144225135445595\n",
      "Training: epoch 67 batch 10 loss 0.005961196031421423\n",
      "Training: epoch 67 batch 20 loss 0.005251232534646988\n",
      "Test: epoch 67 batch 0 loss 0.007194796111434698\n",
      "epoch 67 finished - avarage train loss 0.008005997487183275  avarage test loss 0.018297448754310608\n",
      "Training: epoch 68 batch 0 loss 0.010349530726671219\n",
      "Training: epoch 68 batch 10 loss 0.006774816662073135\n",
      "Training: epoch 68 batch 20 loss 0.005787919741123915\n",
      "Test: epoch 68 batch 0 loss 0.005612976383417845\n",
      "epoch 68 finished - avarage train loss 0.008196758885129258  avarage test loss 0.014265958801843226\n",
      "Training: epoch 69 batch 0 loss 0.0029447998385876417\n",
      "Training: epoch 69 batch 10 loss 0.008593318052589893\n",
      "Training: epoch 69 batch 20 loss 0.004687376320362091\n",
      "Test: epoch 69 batch 0 loss 0.007802116684615612\n",
      "epoch 69 finished - avarage train loss 0.007813578728457978  avarage test loss 0.014800769742578268\n",
      "Training: epoch 70 batch 0 loss 0.0065019940957427025\n",
      "Training: epoch 70 batch 10 loss 0.004338025115430355\n",
      "Training: epoch 70 batch 20 loss 0.002888015005737543\n",
      "Test: epoch 70 batch 0 loss 0.004287512972950935\n",
      "epoch 70 finished - avarage train loss 0.007840136724427856  avarage test loss 0.012555085588246584\n",
      "Training: epoch 71 batch 0 loss 0.006560157518833876\n",
      "Training: epoch 71 batch 10 loss 0.00560113787651062\n",
      "Training: epoch 71 batch 20 loss 0.006161603145301342\n",
      "Test: epoch 71 batch 0 loss 0.00609382800757885\n",
      "epoch 71 finished - avarage train loss 0.008178191090901864  avarage test loss 0.01391082291956991\n",
      "Training: epoch 72 batch 0 loss 0.005821174941956997\n",
      "Training: epoch 72 batch 10 loss 0.006500376854091883\n",
      "Training: epoch 72 batch 20 loss 0.005060689058154821\n",
      "Test: epoch 72 batch 0 loss 0.005685842130333185\n",
      "epoch 72 finished - avarage train loss 0.007777480545036238  avarage test loss 0.015343041974119842\n",
      "Training: epoch 73 batch 0 loss 0.0059990668669342995\n",
      "Training: epoch 73 batch 10 loss 0.012463473714888096\n",
      "Training: epoch 73 batch 20 loss 0.007065293379127979\n",
      "Test: epoch 73 batch 0 loss 0.016178883612155914\n",
      "epoch 73 finished - avarage train loss 0.00806156187799984  avarage test loss 0.026344721671193838\n",
      "Training: epoch 74 batch 0 loss 0.019070416688919067\n",
      "Training: epoch 74 batch 10 loss 0.010134048759937286\n",
      "Training: epoch 74 batch 20 loss 0.009461435489356518\n",
      "Test: epoch 74 batch 0 loss 0.0036339687649160624\n",
      "epoch 74 finished - avarage train loss 0.010362321591197416  avarage test loss 0.01300366100622341\n",
      "Training: epoch 75 batch 0 loss 0.008123140782117844\n",
      "Training: epoch 75 batch 10 loss 0.004582600202411413\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training: epoch 75 batch 20 loss 0.0035351798869669437\n",
      "Test: epoch 75 batch 0 loss 0.003140491433441639\n",
      "epoch 75 finished - avarage train loss 0.007512952843359832  avarage test loss 0.01306620379909873\n",
      "Training: epoch 76 batch 0 loss 0.006304341368377209\n",
      "Training: epoch 76 batch 10 loss 0.008106481283903122\n",
      "Training: epoch 76 batch 20 loss 0.006705887150019407\n",
      "Test: epoch 76 batch 0 loss 0.004904524423182011\n",
      "epoch 76 finished - avarage train loss 0.007584699172655056  avarage test loss 0.013611086877062917\n",
      "Training: epoch 77 batch 0 loss 0.007070772349834442\n",
      "Training: epoch 77 batch 10 loss 0.008177924901247025\n",
      "Training: epoch 77 batch 20 loss 0.005519150290638208\n",
      "Test: epoch 77 batch 0 loss 0.006225774064660072\n",
      "epoch 77 finished - avarage train loss 0.009941432046993026  avarage test loss 0.015321861254051328\n",
      "Training: epoch 78 batch 0 loss 0.007042910438030958\n",
      "Training: epoch 78 batch 10 loss 0.01095570158213377\n",
      "Training: epoch 78 batch 20 loss 0.008178766816854477\n",
      "Test: epoch 78 batch 0 loss 0.004964576568454504\n",
      "epoch 78 finished - avarage train loss 0.009856665153698674  avarage test loss 0.0145839371252805\n",
      "Training: epoch 79 batch 0 loss 0.005409575998783112\n",
      "Training: epoch 79 batch 10 loss 0.0060226707719266415\n",
      "Training: epoch 79 batch 20 loss 0.005361382849514484\n",
      "Test: epoch 79 batch 0 loss 0.004817107226699591\n",
      "epoch 79 finished - avarage train loss 0.006717507085152741  avarage test loss 0.01516979478765279\n",
      "Training: epoch 80 batch 0 loss 0.004606050904840231\n",
      "Training: epoch 80 batch 10 loss 0.006912379991263151\n",
      "Training: epoch 80 batch 20 loss 0.010022448375821114\n",
      "Test: epoch 80 batch 0 loss 0.005238591227680445\n",
      "epoch 80 finished - avarage train loss 0.007404118407003838  avarage test loss 0.014606858720071614\n",
      "Training: epoch 81 batch 0 loss 0.007844129577279091\n",
      "Training: epoch 81 batch 10 loss 0.007217369507998228\n",
      "Training: epoch 81 batch 20 loss 0.007044812198728323\n",
      "Test: epoch 81 batch 0 loss 0.004084922838956118\n",
      "epoch 81 finished - avarage train loss 0.007923496750600893  avarage test loss 0.012856500572524965\n",
      "Training: epoch 82 batch 0 loss 0.007063296623528004\n",
      "Training: epoch 82 batch 10 loss 0.0031165010295808315\n",
      "Training: epoch 82 batch 20 loss 0.004229843616485596\n",
      "Test: epoch 82 batch 0 loss 0.0045658680610358715\n",
      "epoch 82 finished - avarage train loss 0.007774759461332498  avarage test loss 0.014235650654882193\n",
      "Training: epoch 83 batch 0 loss 0.010640823282301426\n",
      "Training: epoch 83 batch 10 loss 0.003579946467652917\n",
      "Training: epoch 83 batch 20 loss 0.008373269811272621\n",
      "Test: epoch 83 batch 0 loss 0.007815644145011902\n",
      "epoch 83 finished - avarage train loss 0.0073539876330900806  avarage test loss 0.015808305121026933\n",
      "Training: epoch 84 batch 0 loss 0.00867494847625494\n",
      "Training: epoch 84 batch 10 loss 0.007722130976617336\n",
      "Training: epoch 84 batch 20 loss 0.012530115433037281\n",
      "Test: epoch 84 batch 0 loss 0.005018272902816534\n",
      "epoch 84 finished - avarage train loss 0.011278052637674686  avarage test loss 0.015535769402049482\n",
      "Training: epoch 85 batch 0 loss 0.0065923044458031654\n",
      "Training: epoch 85 batch 10 loss 0.004641145467758179\n",
      "Training: epoch 85 batch 20 loss 0.004789082333445549\n",
      "Test: epoch 85 batch 0 loss 0.0042363968677818775\n",
      "epoch 85 finished - avarage train loss 0.007491439468516358  avarage test loss 0.013989166589453816\n",
      "Training: epoch 86 batch 0 loss 0.007618275471031666\n",
      "Training: epoch 86 batch 10 loss 0.006753889843821526\n",
      "Training: epoch 86 batch 20 loss 0.002646045759320259\n",
      "Test: epoch 86 batch 0 loss 0.004017005208879709\n",
      "epoch 86 finished - avarage train loss 0.006951004981288108  avarage test loss 0.012618274835404009\n",
      "Training: epoch 87 batch 0 loss 0.0036261635832488537\n",
      "Training: epoch 87 batch 10 loss 0.007522198837250471\n",
      "Training: epoch 87 batch 20 loss 0.0080989059060812\n",
      "Test: epoch 87 batch 0 loss 0.004341220483183861\n",
      "epoch 87 finished - avarage train loss 0.006981945692979056  avarage test loss 0.01295108487829566\n",
      "Training: epoch 88 batch 0 loss 0.006132045760750771\n",
      "Training: epoch 88 batch 10 loss 0.008256673812866211\n",
      "Training: epoch 88 batch 20 loss 0.005015274044126272\n",
      "Test: epoch 88 batch 0 loss 0.005271197762340307\n",
      "epoch 88 finished - avarage train loss 0.0077476105813322396  avarage test loss 0.014785116421990097\n",
      "Training: epoch 89 batch 0 loss 0.0056588295847177505\n",
      "Training: epoch 89 batch 10 loss 0.00505993515253067\n",
      "Training: epoch 89 batch 20 loss 0.007218428421765566\n",
      "Test: epoch 89 batch 0 loss 0.006433557253330946\n",
      "epoch 89 finished - avarage train loss 0.008082468985933167  avarage test loss 0.015017970465123653\n",
      "Training: epoch 90 batch 0 loss 0.009394355118274689\n",
      "Training: epoch 90 batch 10 loss 0.009729872457683086\n",
      "Training: epoch 90 batch 20 loss 0.005700887646526098\n",
      "Test: epoch 90 batch 0 loss 0.0037176450714468956\n",
      "epoch 90 finished - avarage train loss 0.007806150860891774  avarage test loss 0.012903647613711655\n",
      "Training: epoch 91 batch 0 loss 0.007421967573463917\n",
      "Training: epoch 91 batch 10 loss 0.005119022913277149\n",
      "Training: epoch 91 batch 20 loss 0.00865841843187809\n",
      "Test: epoch 91 batch 0 loss 0.006351635791361332\n",
      "epoch 91 finished - avarage train loss 0.007817644882818749  avarage test loss 0.0159650124842301\n",
      "Training: epoch 92 batch 0 loss 0.009046993218362331\n",
      "Training: epoch 92 batch 10 loss 0.014995919540524483\n",
      "Training: epoch 92 batch 20 loss 0.012246943078935146\n",
      "Test: epoch 92 batch 0 loss 0.008248889818787575\n",
      "epoch 92 finished - avarage train loss 0.014921569907716635  avarage test loss 0.01963237556628883\n",
      "Training: epoch 93 batch 0 loss 0.014480468817055225\n",
      "Training: epoch 93 batch 10 loss 0.009422628208994865\n",
      "Training: epoch 93 batch 20 loss 0.005589785985648632\n",
      "Test: epoch 93 batch 0 loss 0.0052206614054739475\n",
      "epoch 93 finished - avarage train loss 0.007969126747599963  avarage test loss 0.013062275247648358\n",
      "Training: epoch 94 batch 0 loss 0.011559383943676949\n",
      "Training: epoch 94 batch 10 loss 0.007903587073087692\n",
      "Training: epoch 94 batch 20 loss 0.006177065894007683\n",
      "Test: epoch 94 batch 0 loss 0.004583330824971199\n",
      "epoch 94 finished - avarage train loss 0.00804100074837434  avarage test loss 0.012712002266198397\n",
      "Training: epoch 95 batch 0 loss 0.006223748438060284\n",
      "Training: epoch 95 batch 10 loss 0.005832033231854439\n",
      "Training: epoch 95 batch 20 loss 0.005533316638320684\n",
      "Test: epoch 95 batch 0 loss 0.0033424454741179943\n",
      "epoch 95 finished - avarage train loss 0.006505571618868873  avarage test loss 0.012551887892186642\n",
      "Training: epoch 96 batch 0 loss 0.005191424395889044\n",
      "Training: epoch 96 batch 10 loss 0.0068493993021547794\n",
      "Training: epoch 96 batch 20 loss 0.00468845758587122\n",
      "Test: epoch 96 batch 0 loss 0.0045365444384515285\n",
      "epoch 96 finished - avarage train loss 0.0075053511877897485  avarage test loss 0.012890799494925886\n",
      "Training: epoch 97 batch 0 loss 0.008855381980538368\n",
      "Training: epoch 97 batch 10 loss 0.0066882590763270855\n",
      "Training: epoch 97 batch 20 loss 0.006806936115026474\n",
      "Test: epoch 97 batch 0 loss 0.004417014308273792\n",
      "epoch 97 finished - avarage train loss 0.0075321531260450336  avarage test loss 0.012993645388633013\n",
      "Training: epoch 98 batch 0 loss 0.007009252440184355\n",
      "Training: epoch 98 batch 10 loss 0.005489702336490154\n",
      "Training: epoch 98 batch 20 loss 0.004881422501057386\n",
      "Test: epoch 98 batch 0 loss 0.003683391259983182\n",
      "epoch 98 finished - avarage train loss 0.007704563567350651  avarage test loss 0.012488130189012736\n",
      "Training: epoch 99 batch 0 loss 0.01017437782138586\n",
      "Training: epoch 99 batch 10 loss 0.005215066485106945\n",
      "Training: epoch 99 batch 20 loss 0.006471838802099228\n",
      "Test: epoch 99 batch 0 loss 0.0041815596632659435\n",
      "epoch 99 finished - avarage train loss 0.008462823542027638  avarage test loss 0.014112977543845773\n",
      "Training: epoch 100 batch 0 loss 0.005206522531807423\n",
      "Training: epoch 100 batch 10 loss 0.0052759298123419285\n",
      "Training: epoch 100 batch 20 loss 0.006475243717432022\n",
      "Test: epoch 100 batch 0 loss 0.0038588859606534243\n",
      "epoch 100 finished - avarage train loss 0.008410403440738547  avarage test loss 0.01409352762857452\n",
      "Training: epoch 101 batch 0 loss 0.01174788549542427\n",
      "Training: epoch 101 batch 10 loss 0.00523805245757103\n",
      "Training: epoch 101 batch 20 loss 0.008611144497990608\n",
      "Test: epoch 101 batch 0 loss 0.00404265196993947\n",
      "epoch 101 finished - avarage train loss 0.009613243268866992  avarage test loss 0.01288353861309588\n",
      "Training: epoch 102 batch 0 loss 0.004845881834626198\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training: epoch 102 batch 10 loss 0.003861485980451107\n",
      "Training: epoch 102 batch 20 loss 0.002955604810267687\n",
      "Test: epoch 102 batch 0 loss 0.003973101731389761\n",
      "epoch 102 finished - avarage train loss 0.0072800409418113275  avarage test loss 0.012719892081804574\n",
      "Training: epoch 103 batch 0 loss 0.007182719185948372\n",
      "Training: epoch 103 batch 10 loss 0.004975166171789169\n",
      "Training: epoch 103 batch 20 loss 0.009377690963447094\n",
      "Test: epoch 103 batch 0 loss 0.004297958686947823\n",
      "epoch 103 finished - avarage train loss 0.008571220465518278  avarage test loss 0.013129825121723115\n",
      "Training: epoch 104 batch 0 loss 0.006599194835871458\n",
      "Training: epoch 104 batch 10 loss 0.004344012588262558\n",
      "Training: epoch 104 batch 20 loss 0.01023837924003601\n",
      "Test: epoch 104 batch 0 loss 0.0035551944747567177\n",
      "epoch 104 finished - avarage train loss 0.007704352767303072  avarage test loss 0.013614908210001886\n",
      "Training: epoch 105 batch 0 loss 0.008460464887320995\n",
      "Training: epoch 105 batch 10 loss 0.003976818174123764\n",
      "Training: epoch 105 batch 20 loss 0.007161588873714209\n",
      "Test: epoch 105 batch 0 loss 0.003184947418048978\n",
      "epoch 105 finished - avarage train loss 0.007314121278388233  avarage test loss 0.01311739400262013\n",
      "Training: epoch 106 batch 0 loss 0.007283869665116072\n",
      "Training: epoch 106 batch 10 loss 0.005312659777700901\n",
      "Training: epoch 106 batch 20 loss 0.005990149453282356\n",
      "Test: epoch 106 batch 0 loss 0.0033610472455620766\n",
      "epoch 106 finished - avarage train loss 0.0077493984958735005  avarage test loss 0.013029947527684271\n",
      "Training: epoch 107 batch 0 loss 0.012659374624490738\n",
      "Training: epoch 107 batch 10 loss 0.005157554522156715\n",
      "Training: epoch 107 batch 20 loss 0.006366102024912834\n",
      "Test: epoch 107 batch 0 loss 0.004154723137617111\n",
      "epoch 107 finished - avarage train loss 0.007970582870445374  avarage test loss 0.013463307986967266\n",
      "Training: epoch 108 batch 0 loss 0.010451112873852253\n",
      "Training: epoch 108 batch 10 loss 0.005596962757408619\n",
      "Training: epoch 108 batch 20 loss 0.00729336217045784\n",
      "Test: epoch 108 batch 0 loss 0.0035882582888007164\n",
      "epoch 108 finished - avarage train loss 0.00869322852392135  avarage test loss 0.01297317142598331\n",
      "Training: epoch 109 batch 0 loss 0.005511526949703693\n",
      "Training: epoch 109 batch 10 loss 0.007534525822848082\n",
      "Training: epoch 109 batch 20 loss 0.016804443672299385\n",
      "Test: epoch 109 batch 0 loss 0.0032272501848638058\n",
      "epoch 109 finished - avarage train loss 0.009912736922630975  avarage test loss 0.012703525950200856\n",
      "Training: epoch 110 batch 0 loss 0.005391968414187431\n",
      "Training: epoch 110 batch 10 loss 0.010222101584076881\n",
      "Training: epoch 110 batch 20 loss 0.006975927855819464\n",
      "Test: epoch 110 batch 0 loss 0.006614469923079014\n",
      "epoch 110 finished - avarage train loss 0.007831226745299224  avarage test loss 0.014258275041356683\n",
      "Training: epoch 111 batch 0 loss 0.009450322948396206\n",
      "Training: epoch 111 batch 10 loss 0.005975537002086639\n",
      "Training: epoch 111 batch 20 loss 0.011959258466959\n",
      "Test: epoch 111 batch 0 loss 0.008301165886223316\n",
      "epoch 111 finished - avarage train loss 0.011310381028030453  avarage test loss 0.01844608993269503\n",
      "Training: epoch 112 batch 0 loss 0.011030793190002441\n",
      "Training: epoch 112 batch 10 loss 0.007185282651335001\n",
      "Training: epoch 112 batch 20 loss 0.005763145163655281\n",
      "Test: epoch 112 batch 0 loss 0.005824479274451733\n",
      "epoch 112 finished - avarage train loss 0.009222356995953054  avarage test loss 0.017176006454974413\n",
      "Training: epoch 113 batch 0 loss 0.007040662225335836\n",
      "Training: epoch 113 batch 10 loss 0.005183428060263395\n",
      "Training: epoch 113 batch 20 loss 0.0050409589894115925\n",
      "Test: epoch 113 batch 0 loss 0.004289187025278807\n",
      "epoch 113 finished - avarage train loss 0.00814076978713274  avarage test loss 0.012914704042486846\n",
      "Training: epoch 114 batch 0 loss 0.005387790501117706\n",
      "Training: epoch 114 batch 10 loss 0.01235339604318142\n",
      "Training: epoch 114 batch 20 loss 0.004690706729888916\n",
      "Test: epoch 114 batch 0 loss 0.0057021500542759895\n",
      "epoch 114 finished - avarage train loss 0.008224873289722821  avarage test loss 0.015454535954631865\n",
      "Training: epoch 115 batch 0 loss 0.0063569205813109875\n",
      "Training: epoch 115 batch 10 loss 0.005397286731749773\n",
      "Training: epoch 115 batch 20 loss 0.004728698171675205\n",
      "Test: epoch 115 batch 0 loss 0.005359991453588009\n",
      "epoch 115 finished - avarage train loss 0.009361545185975987  avarage test loss 0.013622825732454658\n",
      "Training: epoch 116 batch 0 loss 0.00620008772239089\n",
      "Training: epoch 116 batch 10 loss 0.022211605682969093\n",
      "Training: epoch 116 batch 20 loss 0.011068475432693958\n",
      "Test: epoch 116 batch 0 loss 0.0048164865002036095\n",
      "epoch 116 finished - avarage train loss 0.009233715141127849  avarage test loss 0.014360921806655824\n",
      "Training: epoch 117 batch 0 loss 0.006628985516726971\n",
      "Training: epoch 117 batch 10 loss 0.0036599754821509123\n",
      "Training: epoch 117 batch 20 loss 0.011351155117154121\n",
      "Test: epoch 117 batch 0 loss 0.00834186002612114\n",
      "epoch 117 finished - avarage train loss 0.007947036635194873  avarage test loss 0.015969098079949617\n",
      "Training: epoch 118 batch 0 loss 0.005372487939894199\n",
      "Training: epoch 118 batch 10 loss 0.0034455691929906607\n",
      "Training: epoch 118 batch 20 loss 0.012449718080461025\n",
      "Test: epoch 118 batch 0 loss 0.0042412374168634415\n",
      "epoch 118 finished - avarage train loss 0.008919596358941033  avarage test loss 0.013182067428715527\n",
      "Training: epoch 119 batch 0 loss 0.006129124201834202\n",
      "Training: epoch 119 batch 10 loss 0.00437577348202467\n",
      "Training: epoch 119 batch 20 loss 0.0036198922898620367\n",
      "Test: epoch 119 batch 0 loss 0.006130227353423834\n",
      "epoch 119 finished - avarage train loss 0.008177566510657298  avarage test loss 0.014085100847296417\n",
      "Training: epoch 120 batch 0 loss 0.0057906582951545715\n",
      "Training: epoch 120 batch 10 loss 0.010733294300734997\n",
      "Training: epoch 120 batch 20 loss 0.009263134561479092\n",
      "Test: epoch 120 batch 0 loss 0.004525725729763508\n",
      "epoch 120 finished - avarage train loss 0.009902516685040861  avarage test loss 0.013121677446179092\n",
      "Training: epoch 121 batch 0 loss 0.0048129381611943245\n",
      "Training: epoch 121 batch 10 loss 0.003606237703934312\n",
      "Training: epoch 121 batch 20 loss 0.005258978810161352\n",
      "Test: epoch 121 batch 0 loss 0.004986834246665239\n",
      "epoch 121 finished - avarage train loss 0.008024334554271451  avarage test loss 0.01346048800041899\n",
      "Training: epoch 122 batch 0 loss 0.004021621309220791\n",
      "Training: epoch 122 batch 10 loss 0.010801577940583229\n",
      "Training: epoch 122 batch 20 loss 0.004541744012385607\n",
      "Test: epoch 122 batch 0 loss 0.0042699361220002174\n",
      "epoch 122 finished - avarage train loss 0.007734102269249229  avarage test loss 0.012953839963302016\n",
      "Training: epoch 123 batch 0 loss 0.005138302221894264\n",
      "Training: epoch 123 batch 10 loss 0.006188469938933849\n",
      "Training: epoch 123 batch 20 loss 0.008235513232648373\n",
      "Test: epoch 123 batch 0 loss 0.005115197971463203\n",
      "epoch 123 finished - avarage train loss 0.007061443387948233  avarage test loss 0.013396078487858176\n",
      "Training: epoch 124 batch 0 loss 0.006833896040916443\n",
      "Training: epoch 124 batch 10 loss 0.004588962998241186\n",
      "Training: epoch 124 batch 20 loss 0.008622418157756329\n",
      "Test: epoch 124 batch 0 loss 0.004971457179635763\n",
      "epoch 124 finished - avarage train loss 0.007134542792457445  avarage test loss 0.013322730083018541\n",
      "Training: epoch 125 batch 0 loss 0.006658373400568962\n",
      "Training: epoch 125 batch 10 loss 0.004876953549683094\n",
      "Training: epoch 125 batch 20 loss 0.004958807025104761\n",
      "Test: epoch 125 batch 0 loss 0.0049756611697375774\n",
      "epoch 125 finished - avarage train loss 0.00793523848827543  avarage test loss 0.013786137336865067\n",
      "Training: epoch 126 batch 0 loss 0.00783609226346016\n",
      "Training: epoch 126 batch 10 loss 0.004240789916366339\n",
      "Training: epoch 126 batch 20 loss 0.004671531263738871\n",
      "Test: epoch 126 batch 0 loss 0.004101028200238943\n",
      "epoch 126 finished - avarage train loss 0.007827055271201092  avarage test loss 0.012575211934745312\n",
      "Training: epoch 127 batch 0 loss 0.0084230350330472\n",
      "Training: epoch 127 batch 10 loss 0.008166399784386158\n",
      "Training: epoch 127 batch 20 loss 0.005249738693237305\n",
      "Test: epoch 127 batch 0 loss 0.004376693163067102\n",
      "epoch 127 finished - avarage train loss 0.0085706924859049  avarage test loss 0.01298658101586625\n",
      "Training: epoch 128 batch 0 loss 0.0037231582682579756\n",
      "Training: epoch 128 batch 10 loss 0.0076771080493927\n",
      "Training: epoch 128 batch 20 loss 0.005074537359178066\n",
      "Test: epoch 128 batch 0 loss 0.0041570053435862064\n",
      "epoch 128 finished - avarage train loss 0.0071945690533852785  avarage test loss 0.013016438635531813\n",
      "Training: epoch 129 batch 0 loss 0.010553921572864056\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training: epoch 129 batch 10 loss 0.01043405756354332\n",
      "Training: epoch 129 batch 20 loss 0.004504246171563864\n",
      "Test: epoch 129 batch 0 loss 0.004328819457441568\n",
      "epoch 129 finished - avarage train loss 0.006870887442973667  avarage test loss 0.013099135947413743\n",
      "Training: epoch 130 batch 0 loss 0.0054823532700538635\n",
      "Training: epoch 130 batch 10 loss 0.002589880023151636\n",
      "Training: epoch 130 batch 20 loss 0.00730195501819253\n",
      "Test: epoch 130 batch 0 loss 0.005007254891097546\n",
      "epoch 130 finished - avarage train loss 0.006763527715771362  avarage test loss 0.013562806183472276\n",
      "Training: epoch 131 batch 0 loss 0.00660532433539629\n",
      "Training: epoch 131 batch 10 loss 0.010269436053931713\n",
      "Training: epoch 131 batch 20 loss 0.008569849655032158\n",
      "Test: epoch 131 batch 0 loss 0.005264153704047203\n",
      "epoch 131 finished - avarage train loss 0.008685983813785273  avarage test loss 0.014057188527658582\n",
      "Training: epoch 132 batch 0 loss 0.007122996263206005\n",
      "Training: epoch 132 batch 10 loss 0.005729747470468283\n",
      "Training: epoch 132 batch 20 loss 0.006962265819311142\n",
      "Test: epoch 132 batch 0 loss 0.007089031860232353\n",
      "epoch 132 finished - avarage train loss 0.007603888929789436  avarage test loss 0.015904898522421718\n",
      "Training: epoch 133 batch 0 loss 0.0061226654797792435\n",
      "Training: epoch 133 batch 10 loss 0.004381692502647638\n",
      "Training: epoch 133 batch 20 loss 0.01210831943899393\n",
      "Test: epoch 133 batch 0 loss 0.006471957080066204\n",
      "epoch 133 finished - avarage train loss 0.008291675131125697  avarage test loss 0.020026977290399373\n",
      "Training: epoch 134 batch 0 loss 0.009261656552553177\n",
      "Training: epoch 134 batch 10 loss 0.008167843334376812\n",
      "Training: epoch 134 batch 20 loss 0.005866348743438721\n",
      "Test: epoch 134 batch 0 loss 0.005244586616754532\n",
      "epoch 134 finished - avarage train loss 0.00740317961779134  avarage test loss 0.015627597691491246\n",
      "Training: epoch 135 batch 0 loss 0.007057273760437965\n",
      "Training: epoch 135 batch 10 loss 0.010338700376451015\n",
      "Training: epoch 135 batch 20 loss 0.004740262869745493\n",
      "Test: epoch 135 batch 0 loss 0.004904469009488821\n",
      "epoch 135 finished - avarage train loss 0.008333497399721166  avarage test loss 0.016310099745169282\n",
      "Training: epoch 136 batch 0 loss 0.004884851165115833\n",
      "Training: epoch 136 batch 10 loss 0.005183866247534752\n",
      "Training: epoch 136 batch 20 loss 0.005005232524126768\n",
      "Test: epoch 136 batch 0 loss 0.005755354650318623\n",
      "epoch 136 finished - avarage train loss 0.007317431303190774  avarage test loss 0.015720795723609626\n",
      "Training: epoch 137 batch 0 loss 0.006428574211895466\n",
      "Training: epoch 137 batch 10 loss 0.004980144090950489\n",
      "Training: epoch 137 batch 20 loss 0.006627227179706097\n",
      "Test: epoch 137 batch 0 loss 0.0055777872912585735\n",
      "epoch 137 finished - avarage train loss 0.006897772357253165  avarage test loss 0.014157366706058383\n",
      "Training: epoch 138 batch 0 loss 0.01375403068959713\n",
      "Training: epoch 138 batch 10 loss 0.008478188887238503\n",
      "Training: epoch 138 batch 20 loss 0.0031649786978960037\n",
      "Test: epoch 138 batch 0 loss 0.004206543788313866\n",
      "epoch 138 finished - avarage train loss 0.006964181044042624  avarage test loss 0.012932270066812634\n",
      "Training: epoch 139 batch 0 loss 0.012416472658514977\n",
      "Training: epoch 139 batch 10 loss 0.006303858011960983\n",
      "Training: epoch 139 batch 20 loss 0.008440796285867691\n",
      "Test: epoch 139 batch 0 loss 0.004617421887814999\n",
      "epoch 139 finished - avarage train loss 0.008246695381942493  avarage test loss 0.013245546258985996\n",
      "Training: epoch 140 batch 0 loss 0.0030120564624667168\n",
      "Training: epoch 140 batch 10 loss 0.007751364726573229\n",
      "Training: epoch 140 batch 20 loss 0.006898088846355677\n",
      "Test: epoch 140 batch 0 loss 0.004202378913760185\n",
      "epoch 140 finished - avarage train loss 0.00898728333413601  avarage test loss 0.012604283285327256\n",
      "Training: epoch 141 batch 0 loss 0.004647920373827219\n",
      "Training: epoch 141 batch 10 loss 0.00924268551170826\n",
      "Training: epoch 141 batch 20 loss 0.004864290356636047\n",
      "Test: epoch 141 batch 0 loss 0.0032452151644974947\n",
      "epoch 141 finished - avarage train loss 0.007096213995125787  avarage test loss 0.013278315600473434\n",
      "Training: epoch 142 batch 0 loss 0.004860293585807085\n",
      "Training: epoch 142 batch 10 loss 0.005550611764192581\n",
      "Training: epoch 142 batch 20 loss 0.0060214633122086525\n",
      "Test: epoch 142 batch 0 loss 0.0035534650087356567\n",
      "epoch 142 finished - avarage train loss 0.008078909575425345  avarage test loss 0.01257572218310088\n",
      "Training: epoch 143 batch 0 loss 0.005025923252105713\n",
      "Training: epoch 143 batch 10 loss 0.00994791928678751\n",
      "Training: epoch 143 batch 20 loss 0.005566329229623079\n",
      "Test: epoch 143 batch 0 loss 0.003994363360106945\n",
      "epoch 143 finished - avarage train loss 0.00798481974172695  avarage test loss 0.013143061543814838\n",
      "Training: epoch 144 batch 0 loss 0.0027151498943567276\n",
      "Training: epoch 144 batch 10 loss 0.006878208369016647\n",
      "Training: epoch 144 batch 20 loss 0.004797695204615593\n",
      "Test: epoch 144 batch 0 loss 0.0041679502464830875\n",
      "epoch 144 finished - avarage train loss 0.007503758891132371  avarage test loss 0.012857049121521413\n",
      "Training: epoch 145 batch 0 loss 0.008292515762150288\n",
      "Training: epoch 145 batch 10 loss 0.00403214106336236\n",
      "Training: epoch 145 batch 20 loss 0.006039764266461134\n",
      "Test: epoch 145 batch 0 loss 0.003311873646453023\n",
      "epoch 145 finished - avarage train loss 0.008304169401526451  avarage test loss 0.01298898010281846\n",
      "Training: epoch 146 batch 0 loss 0.005365035030990839\n",
      "Training: epoch 146 batch 10 loss 0.0038380357436835766\n",
      "Training: epoch 146 batch 20 loss 0.006244133226573467\n",
      "Test: epoch 146 batch 0 loss 0.0034949500113725662\n",
      "epoch 146 finished - avarage train loss 0.008331103041639617  avarage test loss 0.012505981605499983\n",
      "Training: epoch 147 batch 0 loss 0.004557272884994745\n",
      "Training: epoch 147 batch 10 loss 0.006058755796402693\n",
      "Training: epoch 147 batch 20 loss 0.006764144636690617\n",
      "Test: epoch 147 batch 0 loss 0.005799743812531233\n",
      "epoch 147 finished - avarage train loss 0.007041308009226261  avarage test loss 0.014025125303305686\n",
      "Training: epoch 148 batch 0 loss 0.007180791813880205\n",
      "Training: epoch 148 batch 10 loss 0.007700860500335693\n",
      "Training: epoch 148 batch 20 loss 0.0052542975172400475\n",
      "Test: epoch 148 batch 0 loss 0.006139820441603661\n",
      "epoch 148 finished - avarage train loss 0.00981103130295102  avarage test loss 0.016294218599796295\n",
      "Training: epoch 149 batch 0 loss 0.008448830805718899\n",
      "Training: epoch 149 batch 10 loss 0.005202394910156727\n",
      "Training: epoch 149 batch 20 loss 0.009055674076080322\n",
      "Test: epoch 149 batch 0 loss 0.00673644058406353\n",
      "epoch 149 finished - avarage train loss 0.008483463175723264  avarage test loss 0.013781553017906845\n",
      "Training: epoch 150 batch 0 loss 0.006574675906449556\n",
      "Training: epoch 150 batch 10 loss 0.0060682231560349464\n",
      "Training: epoch 150 batch 20 loss 0.006098251789808273\n",
      "Test: epoch 150 batch 0 loss 0.004484313540160656\n",
      "epoch 150 finished - avarage train loss 0.007010064529383491  avarage test loss 0.01372441602870822\n",
      "Training: epoch 151 batch 0 loss 0.007473009172827005\n",
      "Training: epoch 151 batch 10 loss 0.0055508422665297985\n",
      "Training: epoch 151 batch 20 loss 0.007977677509188652\n",
      "Test: epoch 151 batch 0 loss 0.006294446066021919\n",
      "epoch 151 finished - avarage train loss 0.007565136316457185  avarage test loss 0.014614152256399393\n",
      "Training: epoch 152 batch 0 loss 0.006777648348361254\n",
      "Training: epoch 152 batch 10 loss 0.013950705528259277\n",
      "Training: epoch 152 batch 20 loss 0.008096293546259403\n",
      "Test: epoch 152 batch 0 loss 0.005667613819241524\n",
      "epoch 152 finished - avarage train loss 0.009875898281561917  avarage test loss 0.015403187251649797\n",
      "Training: epoch 153 batch 0 loss 0.008594728074967861\n",
      "Training: epoch 153 batch 10 loss 0.00925255287438631\n",
      "Training: epoch 153 batch 20 loss 0.005461086984723806\n",
      "Test: epoch 153 batch 0 loss 0.005181494168937206\n",
      "epoch 153 finished - avarage train loss 0.008954520899288613  avarage test loss 0.013363883248530328\n",
      "Training: epoch 154 batch 0 loss 0.003710518591105938\n",
      "Training: epoch 154 batch 10 loss 0.009080896154046059\n",
      "Training: epoch 154 batch 20 loss 0.009714943356812\n",
      "Test: epoch 154 batch 0 loss 0.00508055230602622\n",
      "epoch 154 finished - avarage train loss 0.006224141094897841  avarage test loss 0.01280587154906243\n",
      "Training: epoch 155 batch 0 loss 0.005030390340834856\n",
      "Training: epoch 155 batch 10 loss 0.004787907004356384\n",
      "Training: epoch 155 batch 20 loss 0.005658063106238842\n",
      "Test: epoch 155 batch 0 loss 0.005196104757487774\n",
      "epoch 155 finished - avarage train loss 0.008304038984251434  avarage test loss 0.014599583460949361\n",
      "Training: epoch 156 batch 0 loss 0.00489266961812973\n",
      "Training: epoch 156 batch 10 loss 0.006119293626397848\n",
      "Training: epoch 156 batch 20 loss 0.005969562567770481\n",
      "Test: epoch 156 batch 0 loss 0.005033949390053749\n",
      "epoch 156 finished - avarage train loss 0.007933848810478532  avarage test loss 0.016172614064998925\n",
      "Training: epoch 157 batch 0 loss 0.0053545283153653145\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training: epoch 157 batch 10 loss 0.0074908132664859295\n",
      "Training: epoch 157 batch 20 loss 0.003370709950104356\n",
      "Test: epoch 157 batch 0 loss 0.004719554912298918\n",
      "epoch 157 finished - avarage train loss 0.006646823435058367  avarage test loss 0.013390986248850822\n",
      "Training: epoch 158 batch 0 loss 0.0029887408018112183\n",
      "Training: epoch 158 batch 10 loss 0.006767537444829941\n",
      "Training: epoch 158 batch 20 loss 0.004733756650239229\n",
      "Test: epoch 158 batch 0 loss 0.004729883745312691\n",
      "epoch 158 finished - avarage train loss 0.006993083909539313  avarage test loss 0.013608635519631207\n",
      "Training: epoch 159 batch 0 loss 0.011319171637296677\n",
      "Training: epoch 159 batch 10 loss 0.007297378499060869\n",
      "Training: epoch 159 batch 20 loss 0.006768970284610987\n",
      "Test: epoch 159 batch 0 loss 0.004997356794774532\n",
      "epoch 159 finished - avarage train loss 0.007946234485455629  avarage test loss 0.01420317031443119\n",
      "Training: epoch 160 batch 0 loss 0.007939806208014488\n",
      "Training: epoch 160 batch 10 loss 0.009789840318262577\n",
      "Training: epoch 160 batch 20 loss 0.006661586929112673\n",
      "Test: epoch 160 batch 0 loss 0.005755615420639515\n",
      "epoch 160 finished - avarage train loss 0.008563124638950002  avarage test loss 0.01603373559191823\n",
      "Training: epoch 161 batch 0 loss 0.007684073876589537\n",
      "Training: epoch 161 batch 10 loss 0.005414471961557865\n",
      "Training: epoch 161 batch 20 loss 0.011469234712421894\n",
      "Test: epoch 161 batch 0 loss 0.005361764691770077\n",
      "epoch 161 finished - avarage train loss 0.00778544115320105  avarage test loss 0.014622661634348333\n",
      "Training: epoch 162 batch 0 loss 0.00750447902828455\n",
      "Training: epoch 162 batch 10 loss 0.006812356412410736\n",
      "Training: epoch 162 batch 20 loss 0.012632063589990139\n",
      "Test: epoch 162 batch 0 loss 0.004348723217844963\n",
      "epoch 162 finished - avarage train loss 0.00781828158631407  avarage test loss 0.013341593206860125\n",
      "Training: epoch 163 batch 0 loss 0.0038513923063874245\n",
      "Training: epoch 163 batch 10 loss 0.012471377849578857\n",
      "Training: epoch 163 batch 20 loss 0.003135891165584326\n",
      "Test: epoch 163 batch 0 loss 0.004608049988746643\n",
      "epoch 163 finished - avarage train loss 0.007070973290708558  avarage test loss 0.015410458203405142\n",
      "Training: epoch 164 batch 0 loss 0.008400988765060902\n",
      "Training: epoch 164 batch 10 loss 0.006679346784949303\n",
      "Training: epoch 164 batch 20 loss 0.010728523135185242\n",
      "Test: epoch 164 batch 0 loss 0.004851858597248793\n",
      "epoch 164 finished - avarage train loss 0.007599574778678602  avarage test loss 0.014839129522442818\n",
      "Training: epoch 165 batch 0 loss 0.006441151723265648\n",
      "Training: epoch 165 batch 10 loss 0.011684938333928585\n",
      "Training: epoch 165 batch 20 loss 0.00678192637860775\n",
      "Test: epoch 165 batch 0 loss 0.004444845952093601\n",
      "epoch 165 finished - avarage train loss 0.008286288088380262  avarage test loss 0.014158977079205215\n",
      "Training: epoch 166 batch 0 loss 0.005470100324600935\n",
      "Training: epoch 166 batch 10 loss 0.0064528025686740875\n",
      "Training: epoch 166 batch 20 loss 0.004987807013094425\n",
      "Test: epoch 166 batch 0 loss 0.0058048563078045845\n",
      "epoch 166 finished - avarage train loss 0.008077377228644388  avarage test loss 0.014767252490855753\n",
      "Training: epoch 167 batch 0 loss 0.010816491208970547\n",
      "Training: epoch 167 batch 10 loss 0.004632825497537851\n",
      "Training: epoch 167 batch 20 loss 0.003176320344209671\n",
      "Test: epoch 167 batch 0 loss 0.005045960191637278\n",
      "epoch 167 finished - avarage train loss 0.0075490129629856555  avarage test loss 0.013844848028384149\n",
      "Training: epoch 168 batch 0 loss 0.0052716536447405815\n",
      "Training: epoch 168 batch 10 loss 0.00846833735704422\n",
      "Training: epoch 168 batch 20 loss 0.007121490314602852\n",
      "Test: epoch 168 batch 0 loss 0.004623361397534609\n",
      "epoch 168 finished - avarage train loss 0.009067234631370881  avarage test loss 0.013350631459616125\n",
      "Training: epoch 169 batch 0 loss 0.009432662278413773\n",
      "Training: epoch 169 batch 10 loss 0.00576253654435277\n",
      "Training: epoch 169 batch 20 loss 0.00513825099915266\n",
      "Test: epoch 169 batch 0 loss 0.005958431400358677\n",
      "epoch 169 finished - avarage train loss 0.00708279826132388  avarage test loss 0.015184756368398666\n",
      "Training: epoch 170 batch 0 loss 0.007636046502739191\n",
      "Training: epoch 170 batch 10 loss 0.009684212505817413\n",
      "Training: epoch 170 batch 20 loss 0.009146235883235931\n",
      "Test: epoch 170 batch 0 loss 0.015569399110972881\n",
      "epoch 170 finished - avarage train loss 0.011620160946943637  avarage test loss 0.026193120051175356\n",
      "Training: epoch 171 batch 0 loss 0.012728814966976643\n",
      "Training: epoch 171 batch 10 loss 0.010300791822373867\n",
      "Training: epoch 171 batch 20 loss 0.006650873925536871\n",
      "Test: epoch 171 batch 0 loss 0.006912663578987122\n",
      "epoch 171 finished - avarage train loss 0.010185502694341642  avarage test loss 0.01869418239220977\n",
      "Training: epoch 172 batch 0 loss 0.007149163167923689\n",
      "Training: epoch 172 batch 10 loss 0.007441399153321981\n",
      "Training: epoch 172 batch 20 loss 0.008280739188194275\n",
      "Test: epoch 172 batch 0 loss 0.004421836696565151\n",
      "epoch 172 finished - avarage train loss 0.008629789848908269  avarage test loss 0.013901981059461832\n",
      "Training: epoch 173 batch 0 loss 0.0019994096364825964\n",
      "Training: epoch 173 batch 10 loss 0.00671569025143981\n",
      "Training: epoch 173 batch 20 loss 0.007838916033506393\n",
      "Test: epoch 173 batch 0 loss 0.004380363039672375\n",
      "epoch 173 finished - avarage train loss 0.007144157381342916  avarage test loss 0.013012063573114574\n",
      "Training: epoch 174 batch 0 loss 0.007958605885505676\n",
      "Training: epoch 174 batch 10 loss 0.006021919660270214\n",
      "Training: epoch 174 batch 20 loss 0.007391214370727539\n",
      "Test: epoch 174 batch 0 loss 0.004822741728276014\n",
      "epoch 174 finished - avarage train loss 0.008071995894264045  avarage test loss 0.013729255413636565\n",
      "Training: epoch 175 batch 0 loss 0.006093828473240137\n",
      "Training: epoch 175 batch 10 loss 0.007818641141057014\n",
      "Training: epoch 175 batch 20 loss 0.007645945064723492\n",
      "Test: epoch 175 batch 0 loss 0.0044051166623830795\n",
      "epoch 175 finished - avarage train loss 0.007808807554879579  avarage test loss 0.01552002492826432\n",
      "Training: epoch 176 batch 0 loss 0.0054534743539988995\n",
      "Training: epoch 176 batch 10 loss 0.005996135529130697\n",
      "Training: epoch 176 batch 20 loss 0.005738410633057356\n",
      "Test: epoch 176 batch 0 loss 0.006012740544974804\n",
      "epoch 176 finished - avarage train loss 0.0076911395399606435  avarage test loss 0.014496861258521676\n",
      "Training: epoch 177 batch 0 loss 0.008568575605750084\n",
      "Training: epoch 177 batch 10 loss 0.007435237523168325\n",
      "Training: epoch 177 batch 20 loss 0.006714878138154745\n",
      "Test: epoch 177 batch 0 loss 0.005206040106713772\n",
      "epoch 177 finished - avarage train loss 0.00755079752155419  avarage test loss 0.013850384973920882\n",
      "Training: epoch 178 batch 0 loss 0.005881455726921558\n",
      "Training: epoch 178 batch 10 loss 0.0031490337569266558\n",
      "Training: epoch 178 batch 20 loss 0.0049119372852146626\n",
      "Test: epoch 178 batch 0 loss 0.003474547527730465\n",
      "epoch 178 finished - avarage train loss 0.006668633363884071  avarage test loss 0.01282683468889445\n",
      "Training: epoch 179 batch 0 loss 0.006669082213193178\n",
      "Training: epoch 179 batch 10 loss 0.006753384135663509\n",
      "Training: epoch 179 batch 20 loss 0.005603281781077385\n",
      "Test: epoch 179 batch 0 loss 0.004230250138789415\n",
      "epoch 179 finished - avarage train loss 0.00787966595641498  avarage test loss 0.012844501878134906\n",
      "Training: epoch 180 batch 0 loss 0.004512735642492771\n",
      "Training: epoch 180 batch 10 loss 0.0057268994860351086\n",
      "Training: epoch 180 batch 20 loss 0.005071442108601332\n",
      "Test: epoch 180 batch 0 loss 0.004192003048956394\n",
      "epoch 180 finished - avarage train loss 0.006592184562107612  avarage test loss 0.01379656360950321\n",
      "Training: epoch 181 batch 0 loss 0.006023096852004528\n",
      "Training: epoch 181 batch 10 loss 0.004345732741057873\n",
      "Training: epoch 181 batch 20 loss 0.00800562184303999\n",
      "Test: epoch 181 batch 0 loss 0.0033923201262950897\n",
      "epoch 181 finished - avarage train loss 0.007775600309130447  avarage test loss 0.01246920065023005\n",
      "Training: epoch 182 batch 0 loss 0.00716304499655962\n",
      "Training: epoch 182 batch 10 loss 0.0039012806955724955\n",
      "Training: epoch 182 batch 20 loss 0.005891919136047363\n",
      "Test: epoch 182 batch 0 loss 0.004604277666658163\n",
      "epoch 182 finished - avarage train loss 0.007336087613205971  avarage test loss 0.01436627795919776\n",
      "Training: epoch 183 batch 0 loss 0.008876723237335682\n",
      "Training: epoch 183 batch 10 loss 0.010252108797430992\n",
      "Training: epoch 183 batch 20 loss 0.0044511230662465096\n",
      "Test: epoch 183 batch 0 loss 0.0037169991992413998\n",
      "epoch 183 finished - avarage train loss 0.0075319305881215584  avarage test loss 0.01344277651514858\n",
      "Training: epoch 184 batch 0 loss 0.005818320903927088\n",
      "Training: epoch 184 batch 10 loss 0.005577377043664455\n",
      "Training: epoch 184 batch 20 loss 0.007547039072960615\n",
      "Test: epoch 184 batch 0 loss 0.004228862933814526\n",
      "epoch 184 finished - avarage train loss 0.00792346816060358  avarage test loss 0.01285300706513226\n",
      "Training: epoch 185 batch 0 loss 0.004982460755854845\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training: epoch 185 batch 10 loss 0.00396390538662672\n",
      "Training: epoch 185 batch 20 loss 0.004556138999760151\n",
      "Test: epoch 185 batch 0 loss 0.004301552660763264\n",
      "epoch 185 finished - avarage train loss 0.006962082324678014  avarage test loss 0.01358151389285922\n",
      "Training: epoch 186 batch 0 loss 0.006830771919339895\n",
      "Training: epoch 186 batch 10 loss 0.005270439200103283\n",
      "Training: epoch 186 batch 20 loss 0.0055268085561692715\n",
      "Test: epoch 186 batch 0 loss 0.004159920383244753\n",
      "epoch 186 finished - avarage train loss 0.006479899543498097  avarage test loss 0.012903891503810883\n",
      "Training: epoch 187 batch 0 loss 0.0055495938286185265\n",
      "Training: epoch 187 batch 10 loss 0.006965856999158859\n",
      "Training: epoch 187 batch 20 loss 0.00787727814167738\n",
      "Test: epoch 187 batch 0 loss 0.004963526502251625\n",
      "epoch 187 finished - avarage train loss 0.007137358549918081  avarage test loss 0.015651590889319777\n",
      "Training: epoch 188 batch 0 loss 0.007923878729343414\n",
      "Training: epoch 188 batch 10 loss 0.008361205458641052\n",
      "Training: epoch 188 batch 20 loss 0.006639113649725914\n",
      "Test: epoch 188 batch 0 loss 0.010786963626742363\n",
      "epoch 188 finished - avarage train loss 0.007462336151892769  avarage test loss 0.01975927921012044\n",
      "Training: epoch 189 batch 0 loss 0.00877380557358265\n",
      "Training: epoch 189 batch 10 loss 0.011229470372200012\n",
      "Training: epoch 189 batch 20 loss 0.013185367919504642\n",
      "Test: epoch 189 batch 0 loss 0.009799955412745476\n",
      "epoch 189 finished - avarage train loss 0.014368996762766921  avarage test loss 0.01883427519351244\n",
      "Training: epoch 190 batch 0 loss 0.009880056604743004\n",
      "Training: epoch 190 batch 10 loss 0.01262671872973442\n",
      "Training: epoch 190 batch 20 loss 0.00957091897726059\n",
      "Test: epoch 190 batch 0 loss 0.005775489844381809\n",
      "epoch 190 finished - avarage train loss 0.009347374920315784  avarage test loss 0.014135751756839454\n",
      "Training: epoch 191 batch 0 loss 0.009309818036854267\n",
      "Training: epoch 191 batch 10 loss 0.009020796045660973\n",
      "Training: epoch 191 batch 20 loss 0.007660964038223028\n",
      "Test: epoch 191 batch 0 loss 0.005011545494198799\n",
      "epoch 191 finished - avarage train loss 0.0077329287626619995  avarage test loss 0.013552622520364821\n",
      "Training: epoch 192 batch 0 loss 0.0060351695865392685\n",
      "Training: epoch 192 batch 10 loss 0.004216040950268507\n",
      "Training: epoch 192 batch 20 loss 0.004500796552747488\n",
      "Test: epoch 192 batch 0 loss 0.003948302939534187\n",
      "epoch 192 finished - avarage train loss 0.007951084411992082  avarage test loss 0.012925939983688295\n",
      "Training: epoch 193 batch 0 loss 0.005408665630966425\n",
      "Training: epoch 193 batch 10 loss 0.008549648337066174\n",
      "Training: epoch 193 batch 20 loss 0.009425056166946888\n",
      "Test: epoch 193 batch 0 loss 0.00691047590225935\n",
      "epoch 193 finished - avarage train loss 0.007690116409854642  avarage test loss 0.01460600784048438\n",
      "Training: epoch 194 batch 0 loss 0.0069432733580470085\n",
      "Training: epoch 194 batch 10 loss 0.00780519749969244\n",
      "Training: epoch 194 batch 20 loss 0.010427365079522133\n",
      "Test: epoch 194 batch 0 loss 0.0033634852152317762\n",
      "epoch 194 finished - avarage train loss 0.009209532451269955  avarage test loss 0.012383790162857622\n",
      "Training: epoch 195 batch 0 loss 0.005972117651253939\n",
      "Training: epoch 195 batch 10 loss 0.003255580086261034\n",
      "Training: epoch 195 batch 20 loss 0.01039801724255085\n",
      "Test: epoch 195 batch 0 loss 0.004016919061541557\n",
      "epoch 195 finished - avarage train loss 0.00823107189177696  avarage test loss 0.013024625019170344\n",
      "Training: epoch 196 batch 0 loss 0.00619299104437232\n",
      "Training: epoch 196 batch 10 loss 0.0051530152559280396\n",
      "Training: epoch 196 batch 20 loss 0.005386962089687586\n",
      "Test: epoch 196 batch 0 loss 0.005070631857961416\n",
      "epoch 196 finished - avarage train loss 0.008653842868154933  avarage test loss 0.01311383571010083\n",
      "Training: epoch 197 batch 0 loss 0.009600571356713772\n",
      "Training: epoch 197 batch 10 loss 0.004569167271256447\n",
      "Training: epoch 197 batch 20 loss 0.006297918036580086\n",
      "Test: epoch 197 batch 0 loss 0.003705895971506834\n",
      "epoch 197 finished - avarage train loss 0.008543204355599552  avarage test loss 0.012653286452405155\n",
      "Training: epoch 198 batch 0 loss 0.003945375792682171\n",
      "Training: epoch 198 batch 10 loss 0.0030880155973136425\n",
      "Training: epoch 198 batch 20 loss 0.00677415169775486\n",
      "Test: epoch 198 batch 0 loss 0.0038734397385269403\n",
      "epoch 198 finished - avarage train loss 0.007296065189715089  avarage test loss 0.012518380710389465\n",
      "Training: epoch 199 batch 0 loss 0.011694775894284248\n",
      "Training: epoch 199 batch 10 loss 0.0071100215427577496\n",
      "Training: epoch 199 batch 20 loss 0.006550387479364872\n",
      "Test: epoch 199 batch 0 loss 0.0038317451253533363\n",
      "epoch 199 finished - avarage train loss 0.007625043038921109  avarage test loss 0.01266759994905442\n",
      "Training: epoch 0 batch 0 loss 0.443581759929657\n",
      "Training: epoch 0 batch 10 loss 0.6728060841560364\n",
      "Training: epoch 0 batch 20 loss 0.5446589589118958\n",
      "Test: epoch 0 batch 0 loss 0.432021826505661\n",
      "epoch 0 finished - avarage train loss 0.5264381889639229  avarage test loss 0.5193865448236465\n",
      "Training: epoch 1 batch 0 loss 0.5802620649337769\n",
      "Training: epoch 1 batch 10 loss 0.5028669834136963\n",
      "Training: epoch 1 batch 20 loss 0.4175295829772949\n",
      "Test: epoch 1 batch 0 loss 0.4389995336532593\n",
      "epoch 1 finished - avarage train loss 0.499266366506445  avarage test loss 0.5173261910676956\n",
      "Training: epoch 2 batch 0 loss 0.4615388810634613\n",
      "Training: epoch 2 batch 10 loss 0.5439199805259705\n",
      "Training: epoch 2 batch 20 loss 0.5148763060569763\n",
      "Test: epoch 2 batch 0 loss 0.4237179756164551\n",
      "epoch 2 finished - avarage train loss 0.5144841999843203  avarage test loss 0.515541210770607\n",
      "Training: epoch 3 batch 0 loss 0.6376575827598572\n",
      "Training: epoch 3 batch 10 loss 0.5079304575920105\n",
      "Training: epoch 3 batch 20 loss 0.4957737922668457\n",
      "Test: epoch 3 batch 0 loss 0.42462971806526184\n",
      "epoch 3 finished - avarage train loss 0.5145915828902146  avarage test loss 0.5157377049326897\n",
      "Training: epoch 4 batch 0 loss 0.5996498465538025\n",
      "Training: epoch 4 batch 10 loss 0.4108663499355316\n",
      "Training: epoch 4 batch 20 loss 0.6127564311027527\n",
      "Test: epoch 4 batch 0 loss 0.43254101276397705\n",
      "epoch 4 finished - avarage train loss 0.524260046153233  avarage test loss 0.515431322157383\n",
      "Training: epoch 5 batch 0 loss 0.5987510085105896\n",
      "Training: epoch 5 batch 10 loss 0.438572496175766\n",
      "Training: epoch 5 batch 20 loss 0.4920233190059662\n",
      "Test: epoch 5 batch 0 loss 0.43480101227760315\n",
      "epoch 5 finished - avarage train loss 0.5178429967370527  avarage test loss 0.5142950937151909\n",
      "Training: epoch 6 batch 0 loss 0.5106106996536255\n",
      "Training: epoch 6 batch 10 loss 0.41729405522346497\n",
      "Training: epoch 6 batch 20 loss 0.4315073788166046\n",
      "Test: epoch 6 batch 0 loss 0.4314289093017578\n",
      "epoch 6 finished - avarage train loss 0.5046167425040541  avarage test loss 0.5135590359568596\n",
      "Training: epoch 7 batch 0 loss 0.43077945709228516\n",
      "Training: epoch 7 batch 10 loss 0.5431270003318787\n",
      "Training: epoch 7 batch 20 loss 0.49173957109451294\n",
      "Test: epoch 7 batch 0 loss 0.4299018681049347\n",
      "epoch 7 finished - avarage train loss 0.5144089984482733  avarage test loss 0.5154244527220726\n",
      "Training: epoch 8 batch 0 loss 0.4980822205543518\n",
      "Training: epoch 8 batch 10 loss 0.430724173784256\n",
      "Training: epoch 8 batch 20 loss 0.44423454999923706\n",
      "Test: epoch 8 batch 0 loss 0.428043395280838\n",
      "epoch 8 finished - avarage train loss 0.5071618217846443  avarage test loss 0.5152269750833511\n",
      "Training: epoch 9 batch 0 loss 0.3943881094455719\n",
      "Training: epoch 9 batch 10 loss 0.44335871934890747\n",
      "Training: epoch 9 batch 20 loss 0.47505879402160645\n",
      "Test: epoch 9 batch 0 loss 0.42052075266838074\n",
      "epoch 9 finished - avarage train loss 0.5029780448510729  avarage test loss 0.5176047831773758\n",
      "Training: epoch 10 batch 0 loss 0.6273893713951111\n",
      "Training: epoch 10 batch 10 loss 0.4754756987094879\n",
      "Training: epoch 10 batch 20 loss 0.3778851628303528\n",
      "Test: epoch 10 batch 0 loss 0.17788837850093842\n",
      "epoch 10 finished - avarage train loss 0.4748884519112521  avarage test loss 0.2984016053378582\n",
      "Training: epoch 11 batch 0 loss 0.27651479840278625\n",
      "Training: epoch 11 batch 10 loss 0.33135372400283813\n",
      "Training: epoch 11 batch 20 loss 0.16125670075416565\n",
      "Test: epoch 11 batch 0 loss 0.12015274167060852\n",
      "epoch 11 finished - avarage train loss 0.22704717722432366  avarage test loss 0.14431891217827797\n",
      "Training: epoch 12 batch 0 loss 0.1770976334810257\n",
      "Training: epoch 12 batch 10 loss 0.1137281134724617\n",
      "Training: epoch 12 batch 20 loss 0.0775732696056366\n",
      "Test: epoch 12 batch 0 loss 0.03539289906620979\n",
      "epoch 12 finished - avarage train loss 0.08844775334000587  avarage test loss 0.04318173881620169\n",
      "Training: epoch 13 batch 0 loss 0.029871582984924316\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training: epoch 13 batch 10 loss 0.03472469747066498\n",
      "Training: epoch 13 batch 20 loss 0.04194117709994316\n",
      "Test: epoch 13 batch 0 loss 0.033642955124378204\n",
      "epoch 13 finished - avarage train loss 0.029182206453948187  avarage test loss 0.041253252886235714\n",
      "Training: epoch 14 batch 0 loss 0.01816076971590519\n",
      "Training: epoch 14 batch 10 loss 0.016532694920897484\n",
      "Training: epoch 14 batch 20 loss 0.01340158749371767\n",
      "Test: epoch 14 batch 0 loss 0.031116630882024765\n",
      "epoch 14 finished - avarage train loss 0.020780608411236055  avarage test loss 0.03666944522410631\n",
      "Training: epoch 15 batch 0 loss 0.009367316961288452\n",
      "Training: epoch 15 batch 10 loss 0.020151017233729362\n",
      "Training: epoch 15 batch 20 loss 0.023379381746053696\n",
      "Test: epoch 15 batch 0 loss 0.0313933901488781\n",
      "epoch 15 finished - avarage train loss 0.020037009541330666  avarage test loss 0.03650623932480812\n",
      "Training: epoch 16 batch 0 loss 0.020610051229596138\n",
      "Training: epoch 16 batch 10 loss 0.023488765582442284\n",
      "Training: epoch 16 batch 20 loss 0.008936304599046707\n",
      "Test: epoch 16 batch 0 loss 0.03145914152264595\n",
      "epoch 16 finished - avarage train loss 0.01847910603251437  avarage test loss 0.03633067989721894\n",
      "Training: epoch 17 batch 0 loss 0.03610895201563835\n",
      "Training: epoch 17 batch 10 loss 0.0228005088865757\n",
      "Training: epoch 17 batch 20 loss 0.023420019075274467\n",
      "Test: epoch 17 batch 0 loss 0.031212463974952698\n",
      "epoch 17 finished - avarage train loss 0.018979280994370067  avarage test loss 0.03612585924565792\n",
      "Training: epoch 18 batch 0 loss 0.026836421340703964\n",
      "Training: epoch 18 batch 10 loss 0.011908963322639465\n",
      "Training: epoch 18 batch 20 loss 0.01252681016921997\n",
      "Test: epoch 18 batch 0 loss 0.030645906925201416\n",
      "epoch 18 finished - avarage train loss 0.018595422168487107  avarage test loss 0.03537273779511452\n",
      "Training: epoch 19 batch 0 loss 0.019268041476607323\n",
      "Training: epoch 19 batch 10 loss 0.008757281117141247\n",
      "Training: epoch 19 batch 20 loss 0.01148269884288311\n",
      "Test: epoch 19 batch 0 loss 0.012493688613176346\n",
      "epoch 19 finished - avarage train loss 0.014937400015006805  avarage test loss 0.017498346976935863\n",
      "Training: epoch 20 batch 0 loss 0.012825817801058292\n",
      "Training: epoch 20 batch 10 loss 0.0158388651907444\n",
      "Training: epoch 20 batch 20 loss 0.00790507160127163\n",
      "Test: epoch 20 batch 0 loss 0.02142851985991001\n",
      "epoch 20 finished - avarage train loss 0.012907441737579888  avarage test loss 0.029507711296901107\n",
      "Training: epoch 21 batch 0 loss 0.019981563091278076\n",
      "Training: epoch 21 batch 10 loss 0.014001846313476562\n",
      "Training: epoch 21 batch 20 loss 0.012800261378288269\n",
      "Test: epoch 21 batch 0 loss 0.019395951181650162\n",
      "epoch 21 finished - avarage train loss 0.013261225475961792  avarage test loss 0.029861833434551954\n",
      "Training: epoch 22 batch 0 loss 0.01686418615281582\n",
      "Training: epoch 22 batch 10 loss 0.018568789586424828\n",
      "Training: epoch 22 batch 20 loss 0.015616195276379585\n",
      "Test: epoch 22 batch 0 loss 0.010740473866462708\n",
      "epoch 22 finished - avarage train loss 0.013989725664001086  avarage test loss 0.018095495994202793\n",
      "Training: epoch 23 batch 0 loss 0.008446660824120045\n",
      "Training: epoch 23 batch 10 loss 0.01547995675355196\n",
      "Training: epoch 23 batch 20 loss 0.00868250709027052\n",
      "Test: epoch 23 batch 0 loss 0.014696428552269936\n",
      "epoch 23 finished - avarage train loss 0.01373264349290523  avarage test loss 0.025309199234470725\n",
      "Training: epoch 24 batch 0 loss 0.014498521573841572\n",
      "Training: epoch 24 batch 10 loss 0.011911331675946712\n",
      "Training: epoch 24 batch 20 loss 0.004782968200743198\n",
      "Test: epoch 24 batch 0 loss 0.008455537259578705\n",
      "epoch 24 finished - avarage train loss 0.011154434756086818  avarage test loss 0.014949469245038927\n",
      "Training: epoch 25 batch 0 loss 0.011250526644289494\n",
      "Training: epoch 25 batch 10 loss 0.0059020863845944405\n",
      "Training: epoch 25 batch 20 loss 0.005737374536693096\n",
      "Test: epoch 25 batch 0 loss 0.006316520273685455\n",
      "epoch 25 finished - avarage train loss 0.009790244223228816  avarage test loss 0.014042906695976853\n",
      "Training: epoch 26 batch 0 loss 0.005331232212483883\n",
      "Training: epoch 26 batch 10 loss 0.005223199725151062\n",
      "Training: epoch 26 batch 20 loss 0.007408414501696825\n",
      "Test: epoch 26 batch 0 loss 0.00541101535782218\n",
      "epoch 26 finished - avarage train loss 0.007948368579020789  avarage test loss 0.01309035939630121\n",
      "Training: epoch 27 batch 0 loss 0.0057083978317677975\n",
      "Training: epoch 27 batch 10 loss 0.009577326476573944\n",
      "Training: epoch 27 batch 20 loss 0.0061350478790700436\n",
      "Test: epoch 27 batch 0 loss 0.0063294717110693455\n",
      "epoch 27 finished - avarage train loss 0.008906962494526443  avarage test loss 0.01939248840790242\n",
      "Training: epoch 28 batch 0 loss 0.0038820267654955387\n",
      "Training: epoch 28 batch 10 loss 0.006257419008761644\n",
      "Training: epoch 28 batch 20 loss 0.003797427285462618\n",
      "Test: epoch 28 batch 0 loss 0.008686556480824947\n",
      "epoch 28 finished - avarage train loss 0.00930531845620737  avarage test loss 0.018937668297439814\n",
      "Training: epoch 29 batch 0 loss 0.00856139324605465\n",
      "Training: epoch 29 batch 10 loss 0.008718280121684074\n",
      "Training: epoch 29 batch 20 loss 0.007633038330823183\n",
      "Test: epoch 29 batch 0 loss 0.004208678845316172\n",
      "epoch 29 finished - avarage train loss 0.009586008956077797  avarage test loss 0.012858265545219183\n",
      "Training: epoch 30 batch 0 loss 0.006737918127328157\n",
      "Training: epoch 30 batch 10 loss 0.00594884529709816\n",
      "Training: epoch 30 batch 20 loss 0.006666992790997028\n",
      "Test: epoch 30 batch 0 loss 0.004222148098051548\n",
      "epoch 30 finished - avarage train loss 0.007640333581266218  avarage test loss 0.014745845226570964\n",
      "Training: epoch 31 batch 0 loss 0.0060918075032532215\n",
      "Training: epoch 31 batch 10 loss 0.005729530472308397\n",
      "Training: epoch 31 batch 20 loss 0.006940886378288269\n",
      "Test: epoch 31 batch 0 loss 0.003268567845225334\n",
      "epoch 31 finished - avarage train loss 0.00850538769736886  avarage test loss 0.013735434506088495\n",
      "Training: epoch 32 batch 0 loss 0.004556721076369286\n",
      "Training: epoch 32 batch 10 loss 0.007459679152816534\n",
      "Training: epoch 32 batch 20 loss 0.0037026272621005774\n",
      "Test: epoch 32 batch 0 loss 0.003953661769628525\n",
      "epoch 32 finished - avarage train loss 0.00879998004128193  avarage test loss 0.013502579648047686\n",
      "Training: epoch 33 batch 0 loss 0.006216882262378931\n",
      "Training: epoch 33 batch 10 loss 0.007351691834628582\n",
      "Training: epoch 33 batch 20 loss 0.0044956817291677\n",
      "Test: epoch 33 batch 0 loss 0.003189590061083436\n",
      "epoch 33 finished - avarage train loss 0.007251721959368422  avarage test loss 0.012395995610859245\n",
      "Training: epoch 34 batch 0 loss 0.011225705966353416\n",
      "Training: epoch 34 batch 10 loss 0.005261005833745003\n",
      "Training: epoch 34 batch 20 loss 0.005323026794940233\n",
      "Test: epoch 34 batch 0 loss 0.0047721583396196365\n",
      "epoch 34 finished - avarage train loss 0.007863710819069168  avarage test loss 0.013679019641131163\n",
      "Training: epoch 35 batch 0 loss 0.005058279726654291\n",
      "Training: epoch 35 batch 10 loss 0.00716413464397192\n",
      "Training: epoch 35 batch 20 loss 0.00787421502172947\n",
      "Test: epoch 35 batch 0 loss 0.0031046904623508453\n",
      "epoch 35 finished - avarage train loss 0.008172910628390723  avarage test loss 0.012629219330847263\n",
      "Training: epoch 36 batch 0 loss 0.005940916482359171\n",
      "Training: epoch 36 batch 10 loss 0.004616223741322756\n",
      "Training: epoch 36 batch 20 loss 0.01671091839671135\n",
      "Test: epoch 36 batch 0 loss 0.004256085958331823\n",
      "epoch 36 finished - avarage train loss 0.008408794902136614  avarage test loss 0.019000466680154204\n",
      "Training: epoch 37 batch 0 loss 0.005034488160163164\n",
      "Training: epoch 37 batch 10 loss 0.0139236468821764\n",
      "Training: epoch 37 batch 20 loss 0.00848984345793724\n",
      "Test: epoch 37 batch 0 loss 0.005778435152024031\n",
      "epoch 37 finished - avarage train loss 0.011552163351584098  avarage test loss 0.018991939607076347\n",
      "Training: epoch 38 batch 0 loss 0.009381815791130066\n",
      "Training: epoch 38 batch 10 loss 0.011930830776691437\n",
      "Training: epoch 38 batch 20 loss 0.009244585409760475\n",
      "Test: epoch 38 batch 0 loss 0.0031975694000720978\n",
      "epoch 38 finished - avarage train loss 0.009206077838637706  avarage test loss 0.013507901108823717\n",
      "Training: epoch 39 batch 0 loss 0.004297989886254072\n",
      "Training: epoch 39 batch 10 loss 0.012827728874981403\n",
      "Training: epoch 39 batch 20 loss 0.005596660077571869\n",
      "Test: epoch 39 batch 0 loss 0.009611365385353565\n",
      "epoch 39 finished - avarage train loss 0.00802587107594671  avarage test loss 0.020870859967544675\n",
      "Training: epoch 40 batch 0 loss 0.009846349246799946\n",
      "Training: epoch 40 batch 10 loss 0.014420763589441776\n",
      "Training: epoch 40 batch 20 loss 0.005967528559267521\n",
      "Test: epoch 40 batch 0 loss 0.005023610778152943\n",
      "epoch 40 finished - avarage train loss 0.008807296768344682  avarage test loss 0.015163130941800773\n",
      "Training: epoch 41 batch 0 loss 0.005960717797279358\n",
      "Training: epoch 41 batch 10 loss 0.006724458187818527\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training: epoch 41 batch 20 loss 0.005140954162925482\n",
      "Test: epoch 41 batch 0 loss 0.006429367698729038\n",
      "epoch 41 finished - avarage train loss 0.007512144037875636  avarage test loss 0.01451103133149445\n",
      "Training: epoch 42 batch 0 loss 0.007869346998631954\n",
      "Training: epoch 42 batch 10 loss 0.004729506094008684\n",
      "Training: epoch 42 batch 20 loss 0.006575610488653183\n",
      "Test: epoch 42 batch 0 loss 0.0047474997118115425\n",
      "epoch 42 finished - avarage train loss 0.007864623507163647  avarage test loss 0.014541313634254038\n",
      "Training: epoch 43 batch 0 loss 0.007978389970958233\n",
      "Training: epoch 43 batch 10 loss 0.006259505171328783\n",
      "Training: epoch 43 batch 20 loss 0.005370107479393482\n",
      "Test: epoch 43 batch 0 loss 0.0028257446829229593\n",
      "epoch 43 finished - avarage train loss 0.007132827143730788  avarage test loss 0.013495567429345101\n",
      "Training: epoch 44 batch 0 loss 0.00664810324087739\n",
      "Training: epoch 44 batch 10 loss 0.004297555889934301\n",
      "Training: epoch 44 batch 20 loss 0.0031474544666707516\n",
      "Test: epoch 44 batch 0 loss 0.0038760516326874495\n",
      "epoch 44 finished - avarage train loss 0.00743595266650463  avarage test loss 0.01448780600912869\n",
      "Training: epoch 45 batch 0 loss 0.005749494768679142\n",
      "Training: epoch 45 batch 10 loss 0.00840797834098339\n",
      "Training: epoch 45 batch 20 loss 0.0032350695692002773\n",
      "Test: epoch 45 batch 0 loss 0.005544138606637716\n",
      "epoch 45 finished - avarage train loss 0.007523377594570148  avarage test loss 0.013085921935271472\n",
      "Training: epoch 46 batch 0 loss 0.008044523186981678\n",
      "Training: epoch 46 batch 10 loss 0.008844470605254173\n",
      "Training: epoch 46 batch 20 loss 0.0074904500506818295\n",
      "Test: epoch 46 batch 0 loss 0.005477643106132746\n",
      "epoch 46 finished - avarage train loss 0.007526057997140391  avarage test loss 0.013422068441286683\n",
      "Training: epoch 47 batch 0 loss 0.005596124101430178\n",
      "Training: epoch 47 batch 10 loss 0.005289663095027208\n",
      "Training: epoch 47 batch 20 loss 0.005994084291160107\n",
      "Test: epoch 47 batch 0 loss 0.006814062595367432\n",
      "epoch 47 finished - avarage train loss 0.008854799887871948  avarage test loss 0.014123523840680718\n",
      "Training: epoch 48 batch 0 loss 0.008600541390478611\n",
      "Training: epoch 48 batch 10 loss 0.007816370576620102\n",
      "Training: epoch 48 batch 20 loss 0.007916254922747612\n",
      "Test: epoch 48 batch 0 loss 0.005422361195087433\n",
      "epoch 48 finished - avarage train loss 0.008790073860115532  avarage test loss 0.019882893189787865\n",
      "Training: epoch 49 batch 0 loss 0.007616220973432064\n",
      "Training: epoch 49 batch 10 loss 0.0159456804394722\n",
      "Training: epoch 49 batch 20 loss 0.008438361808657646\n",
      "Test: epoch 49 batch 0 loss 0.006323520094156265\n",
      "epoch 49 finished - avarage train loss 0.00888201750108394  avarage test loss 0.02010611759033054\n",
      "Training: epoch 50 batch 0 loss 0.008654317818582058\n",
      "Training: epoch 50 batch 10 loss 0.008905838243663311\n",
      "Training: epoch 50 batch 20 loss 0.006591206416487694\n",
      "Test: epoch 50 batch 0 loss 0.004468585830181837\n",
      "epoch 50 finished - avarage train loss 0.008074661082556021  avarage test loss 0.017128251842223108\n",
      "Training: epoch 51 batch 0 loss 0.007228696718811989\n",
      "Training: epoch 51 batch 10 loss 0.0037152052391320467\n",
      "Training: epoch 51 batch 20 loss 0.003934082575142384\n",
      "Test: epoch 51 batch 0 loss 0.00408921018242836\n",
      "epoch 51 finished - avarage train loss 0.008366992006656426  avarage test loss 0.01580240484327078\n",
      "Training: epoch 52 batch 0 loss 0.00699963141232729\n",
      "Training: epoch 52 batch 10 loss 0.0038970927707850933\n",
      "Training: epoch 52 batch 20 loss 0.005271467845886946\n",
      "Test: epoch 52 batch 0 loss 0.004863740410655737\n",
      "epoch 52 finished - avarage train loss 0.006894177556487507  avarage test loss 0.013563042040914297\n",
      "Training: epoch 53 batch 0 loss 0.004575495142489672\n",
      "Training: epoch 53 batch 10 loss 0.0051111383363604546\n",
      "Training: epoch 53 batch 20 loss 0.003259047167375684\n",
      "Test: epoch 53 batch 0 loss 0.005440926179289818\n",
      "epoch 53 finished - avarage train loss 0.006853196224005058  avarage test loss 0.014673414290882647\n",
      "Training: epoch 54 batch 0 loss 0.007975615561008453\n",
      "Training: epoch 54 batch 10 loss 0.006529929582029581\n",
      "Training: epoch 54 batch 20 loss 0.009442335925996304\n",
      "Test: epoch 54 batch 0 loss 0.005588149651885033\n",
      "epoch 54 finished - avarage train loss 0.008155172642577311  avarage test loss 0.0155471155885607\n",
      "Training: epoch 55 batch 0 loss 0.008171436376869678\n",
      "Training: epoch 55 batch 10 loss 0.0034017718862742186\n",
      "Training: epoch 55 batch 20 loss 0.008072172291576862\n",
      "Test: epoch 55 batch 0 loss 0.006885225884616375\n",
      "epoch 55 finished - avarage train loss 0.006811590801025259  avarage test loss 0.01565211347769946\n",
      "Training: epoch 56 batch 0 loss 0.014343898743391037\n",
      "Training: epoch 56 batch 10 loss 0.012533453293144703\n",
      "Training: epoch 56 batch 20 loss 0.006565493531525135\n",
      "Test: epoch 56 batch 0 loss 0.0030458164401352406\n",
      "epoch 56 finished - avarage train loss 0.01088021628173261  avarage test loss 0.01397985266521573\n",
      "Training: epoch 57 batch 0 loss 0.007769429124891758\n",
      "Training: epoch 57 batch 10 loss 0.010184373706579208\n",
      "Training: epoch 57 batch 20 loss 0.00510404072701931\n",
      "Test: epoch 57 batch 0 loss 0.003832072950899601\n",
      "epoch 57 finished - avarage train loss 0.007767872716268075  avarage test loss 0.013602025574073195\n",
      "Training: epoch 58 batch 0 loss 0.006654045544564724\n",
      "Training: epoch 58 batch 10 loss 0.006047094706445932\n",
      "Training: epoch 58 batch 20 loss 0.008873004466295242\n",
      "Test: epoch 58 batch 0 loss 0.007487988565117121\n",
      "epoch 58 finished - avarage train loss 0.009259094781210196  avarage test loss 0.014536619069986045\n",
      "Training: epoch 59 batch 0 loss 0.011151785030961037\n",
      "Training: epoch 59 batch 10 loss 0.004962869919836521\n",
      "Training: epoch 59 batch 20 loss 0.011756125837564468\n",
      "Test: epoch 59 batch 0 loss 0.00599686661735177\n",
      "epoch 59 finished - avarage train loss 0.009446754682295281  avarage test loss 0.013143412885256112\n",
      "Training: epoch 60 batch 0 loss 0.007081345189362764\n",
      "Training: epoch 60 batch 10 loss 0.003902861848473549\n",
      "Training: epoch 60 batch 20 loss 0.007124088238924742\n",
      "Test: epoch 60 batch 0 loss 0.006122987251728773\n",
      "epoch 60 finished - avarage train loss 0.008092355472838571  avarage test loss 0.01470909419003874\n",
      "Training: epoch 61 batch 0 loss 0.003924289718270302\n",
      "Training: epoch 61 batch 10 loss 0.010885771363973618\n",
      "Training: epoch 61 batch 20 loss 0.01053287461400032\n",
      "Test: epoch 61 batch 0 loss 0.005914304871112108\n",
      "epoch 61 finished - avarage train loss 0.007771769450206695  avarage test loss 0.014807116473093629\n",
      "Training: epoch 62 batch 0 loss 0.005704426672309637\n",
      "Training: epoch 62 batch 10 loss 0.0033802189864218235\n",
      "Training: epoch 62 batch 20 loss 0.006618688348680735\n",
      "Test: epoch 62 batch 0 loss 0.005910316947847605\n",
      "epoch 62 finished - avarage train loss 0.007554030824645326  avarage test loss 0.015221410198137164\n",
      "Training: epoch 63 batch 0 loss 0.013167543336749077\n",
      "Training: epoch 63 batch 10 loss 0.006918369792401791\n",
      "Training: epoch 63 batch 20 loss 0.003520192112773657\n",
      "Test: epoch 63 batch 0 loss 0.0046294149942696095\n",
      "epoch 63 finished - avarage train loss 0.008080989098304818  avarage test loss 0.013140674214810133\n",
      "Training: epoch 64 batch 0 loss 0.008438183926045895\n",
      "Training: epoch 64 batch 10 loss 0.010394302196800709\n",
      "Training: epoch 64 batch 20 loss 0.005289971828460693\n",
      "Test: epoch 64 batch 0 loss 0.006008065305650234\n",
      "epoch 64 finished - avarage train loss 0.009056756401370311  avarage test loss 0.014539549127221107\n",
      "Training: epoch 65 batch 0 loss 0.004946026485413313\n",
      "Training: epoch 65 batch 10 loss 0.010547913610935211\n",
      "Training: epoch 65 batch 20 loss 0.006320297252386808\n",
      "Test: epoch 65 batch 0 loss 0.005774122662842274\n",
      "epoch 65 finished - avarage train loss 0.008958966952973399  avarage test loss 0.013458245783112943\n",
      "Training: epoch 66 batch 0 loss 0.007547448389232159\n",
      "Training: epoch 66 batch 10 loss 0.005747370421886444\n",
      "Training: epoch 66 batch 20 loss 0.007743231952190399\n",
      "Test: epoch 66 batch 0 loss 0.004853756632655859\n",
      "epoch 66 finished - avarage train loss 0.007111275247458754  avarage test loss 0.013165200070943683\n",
      "Training: epoch 67 batch 0 loss 0.00855600368231535\n",
      "Training: epoch 67 batch 10 loss 0.006182536948472261\n",
      "Training: epoch 67 batch 20 loss 0.006524633150547743\n",
      "Test: epoch 67 batch 0 loss 0.0060506886802613735\n",
      "epoch 67 finished - avarage train loss 0.008417882715705139  avarage test loss 0.014813952846452594\n",
      "Training: epoch 68 batch 0 loss 0.008418301120400429\n",
      "Training: epoch 68 batch 10 loss 0.009652414359152317\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training: epoch 68 batch 20 loss 0.008574672043323517\n",
      "Test: epoch 68 batch 0 loss 0.00606080237776041\n",
      "epoch 68 finished - avarage train loss 0.0073442857005986675  avarage test loss 0.01425744069274515\n",
      "Training: epoch 69 batch 0 loss 0.005175001919269562\n",
      "Training: epoch 69 batch 10 loss 0.008949174545705318\n",
      "Training: epoch 69 batch 20 loss 0.0064074876718223095\n",
      "Test: epoch 69 batch 0 loss 0.008210157975554466\n",
      "epoch 69 finished - avarage train loss 0.00906173971577965  avarage test loss 0.01634944067336619\n",
      "Training: epoch 70 batch 0 loss 0.013325749896466732\n",
      "Training: epoch 70 batch 10 loss 0.00922340713441372\n",
      "Training: epoch 70 batch 20 loss 0.004787461832165718\n",
      "Test: epoch 70 batch 0 loss 0.005383563693612814\n",
      "epoch 70 finished - avarage train loss 0.008017252616841218  avarage test loss 0.014324527815915644\n",
      "Training: epoch 71 batch 0 loss 0.01246236078441143\n",
      "Training: epoch 71 batch 10 loss 0.00566706620156765\n",
      "Training: epoch 71 batch 20 loss 0.004929624497890472\n",
      "Test: epoch 71 batch 0 loss 0.006696201395243406\n",
      "epoch 71 finished - avarage train loss 0.006929390871062361  avarage test loss 0.014471148257143795\n",
      "Training: epoch 72 batch 0 loss 0.01007829885929823\n",
      "Training: epoch 72 batch 10 loss 0.008082091808319092\n",
      "Training: epoch 72 batch 20 loss 0.006514900829643011\n",
      "Test: epoch 72 batch 0 loss 0.004724037833511829\n",
      "epoch 72 finished - avarage train loss 0.008866925418762297  avarage test loss 0.0144189567072317\n",
      "Training: epoch 73 batch 0 loss 0.005332750268280506\n",
      "Training: epoch 73 batch 10 loss 0.0066907056607306\n",
      "Training: epoch 73 batch 20 loss 0.008595507591962814\n",
      "Test: epoch 73 batch 0 loss 0.005582768004387617\n",
      "epoch 73 finished - avarage train loss 0.008741147824209827  avarage test loss 0.014614484854973853\n",
      "Training: epoch 74 batch 0 loss 0.01157943345606327\n",
      "Training: epoch 74 batch 10 loss 0.005471113137900829\n",
      "Training: epoch 74 batch 20 loss 0.004958192817866802\n",
      "Test: epoch 74 batch 0 loss 0.006150008644908667\n",
      "epoch 74 finished - avarage train loss 0.008976572193205357  avarage test loss 0.014712232281453907\n",
      "Training: epoch 75 batch 0 loss 0.006877208594232798\n",
      "Training: epoch 75 batch 10 loss 0.0058418442495167255\n",
      "Training: epoch 75 batch 20 loss 0.005662089213728905\n",
      "Test: epoch 75 batch 0 loss 0.005423780996352434\n",
      "epoch 75 finished - avarage train loss 0.009299250150998604  avarage test loss 0.015428026090376079\n",
      "Training: epoch 76 batch 0 loss 0.005605165380984545\n",
      "Training: epoch 76 batch 10 loss 0.00379050406627357\n",
      "Training: epoch 76 batch 20 loss 0.006243123207241297\n",
      "Test: epoch 76 batch 0 loss 0.008364805020391941\n",
      "epoch 76 finished - avarage train loss 0.008099354341915199  avarage test loss 0.018027638318017125\n",
      "Training: epoch 77 batch 0 loss 0.012196121737360954\n",
      "Training: epoch 77 batch 10 loss 0.009064631536602974\n",
      "Training: epoch 77 batch 20 loss 0.007000750862061977\n",
      "Test: epoch 77 batch 0 loss 0.0064440383575856686\n",
      "epoch 77 finished - avarage train loss 0.009696397384435966  avarage test loss 0.017197104054503143\n",
      "Training: epoch 78 batch 0 loss 0.006470008287578821\n",
      "Training: epoch 78 batch 10 loss 0.004723456222563982\n",
      "Training: epoch 78 batch 20 loss 0.005473743658512831\n",
      "Test: epoch 78 batch 0 loss 0.005824392195791006\n",
      "epoch 78 finished - avarage train loss 0.008524356321592269  avarage test loss 0.01351314946077764\n",
      "Training: epoch 79 batch 0 loss 0.006060206796973944\n",
      "Training: epoch 79 batch 10 loss 0.004435854963958263\n",
      "Training: epoch 79 batch 20 loss 0.005471743643283844\n",
      "Test: epoch 79 batch 0 loss 0.004119956865906715\n",
      "epoch 79 finished - avarage train loss 0.008147543095501846  avarage test loss 0.012752028414979577\n",
      "Training: epoch 80 batch 0 loss 0.006189416628330946\n",
      "Training: epoch 80 batch 10 loss 0.00473162904381752\n",
      "Training: epoch 80 batch 20 loss 0.017147943377494812\n",
      "Test: epoch 80 batch 0 loss 0.005211293697357178\n",
      "epoch 80 finished - avarage train loss 0.00829692512493709  avarage test loss 0.01373278209939599\n",
      "Training: epoch 81 batch 0 loss 0.005191591568291187\n",
      "Training: epoch 81 batch 10 loss 0.0048730745911598206\n",
      "Training: epoch 81 batch 20 loss 0.013797232881188393\n",
      "Test: epoch 81 batch 0 loss 0.004989488050341606\n",
      "epoch 81 finished - avarage train loss 0.007437057744968554  avarage test loss 0.01443285378627479\n",
      "Training: epoch 82 batch 0 loss 0.006167160347104073\n",
      "Training: epoch 82 batch 10 loss 0.014173684641718864\n",
      "Training: epoch 82 batch 20 loss 0.0077466522343456745\n",
      "Test: epoch 82 batch 0 loss 0.005861981771886349\n",
      "epoch 82 finished - avarage train loss 0.009594589327301445  avarage test loss 0.014753537834621966\n",
      "Training: epoch 83 batch 0 loss 0.005027268547564745\n",
      "Training: epoch 83 batch 10 loss 0.007322036195546389\n",
      "Training: epoch 83 batch 20 loss 0.006383746862411499\n",
      "Test: epoch 83 batch 0 loss 0.005541414953768253\n",
      "epoch 83 finished - avarage train loss 0.008236925165843347  avarage test loss 0.012980601051822305\n",
      "Training: epoch 84 batch 0 loss 0.008910559117794037\n",
      "Training: epoch 84 batch 10 loss 0.005168458912521601\n",
      "Training: epoch 84 batch 20 loss 0.0053876047022640705\n",
      "Test: epoch 84 batch 0 loss 0.005017515271902084\n",
      "epoch 84 finished - avarage train loss 0.008063406193757365  avarage test loss 0.013260678853839636\n",
      "Training: epoch 85 batch 0 loss 0.012771524488925934\n",
      "Training: epoch 85 batch 10 loss 0.010935475118458271\n",
      "Training: epoch 85 batch 20 loss 0.005552510730922222\n",
      "Test: epoch 85 batch 0 loss 0.009365295059978962\n",
      "epoch 85 finished - avarage train loss 0.010047942640837925  avarage test loss 0.02101208781823516\n",
      "Training: epoch 86 batch 0 loss 0.011217661201953888\n",
      "Training: epoch 86 batch 10 loss 0.01402809377759695\n",
      "Training: epoch 86 batch 20 loss 0.009692546911537647\n",
      "Test: epoch 86 batch 0 loss 0.009121210314333439\n",
      "epoch 86 finished - avarage train loss 0.01739813801671924  avarage test loss 0.016836385359056294\n",
      "Training: epoch 87 batch 0 loss 0.014846524223685265\n",
      "Training: epoch 87 batch 10 loss 0.006757398135960102\n",
      "Training: epoch 87 batch 20 loss 0.0057634697295725346\n",
      "Test: epoch 87 batch 0 loss 0.0067018186673521996\n",
      "epoch 87 finished - avarage train loss 0.009832092705343303  avarage test loss 0.01846797752659768\n",
      "Training: epoch 88 batch 0 loss 0.0042099738493561745\n",
      "Training: epoch 88 batch 10 loss 0.008301029913127422\n",
      "Training: epoch 88 batch 20 loss 0.00762260751798749\n",
      "Test: epoch 88 batch 0 loss 0.011561435647308826\n",
      "epoch 88 finished - avarage train loss 0.006899728768922645  avarage test loss 0.015478691319003701\n",
      "Training: epoch 89 batch 0 loss 0.01665753312408924\n",
      "Training: epoch 89 batch 10 loss 0.008869444020092487\n",
      "Training: epoch 89 batch 20 loss 0.013006800785660744\n",
      "Test: epoch 89 batch 0 loss 0.00609201705083251\n",
      "epoch 89 finished - avarage train loss 0.012122770514467666  avarage test loss 0.013252830598503351\n",
      "Training: epoch 90 batch 0 loss 0.01020636036992073\n",
      "Training: epoch 90 batch 10 loss 0.0068449839018285275\n",
      "Training: epoch 90 batch 20 loss 0.002414075890555978\n",
      "Test: epoch 90 batch 0 loss 0.005716454237699509\n",
      "epoch 90 finished - avarage train loss 0.00718743458454465  avarage test loss 0.014133153017610312\n",
      "Training: epoch 91 batch 0 loss 0.004948681220412254\n",
      "Training: epoch 91 batch 10 loss 0.006033019628375769\n",
      "Training: epoch 91 batch 20 loss 0.008872073143720627\n",
      "Test: epoch 91 batch 0 loss 0.0058515844866633415\n",
      "epoch 91 finished - avarage train loss 0.008724381818018597  avarage test loss 0.013430743128992617\n",
      "Training: epoch 92 batch 0 loss 0.007904578931629658\n",
      "Training: epoch 92 batch 10 loss 0.012009815312922001\n",
      "Training: epoch 92 batch 20 loss 0.005944196134805679\n",
      "Test: epoch 92 batch 0 loss 0.006100034341216087\n",
      "epoch 92 finished - avarage train loss 0.0096370459148853  avarage test loss 0.015390878892503679\n",
      "Training: epoch 93 batch 0 loss 0.008950654417276382\n",
      "Training: epoch 93 batch 10 loss 0.006877518724650145\n",
      "Training: epoch 93 batch 20 loss 0.009373635984957218\n",
      "Test: epoch 93 batch 0 loss 0.006115978583693504\n",
      "epoch 93 finished - avarage train loss 0.007943499897574556  avarage test loss 0.01531075278762728\n",
      "Training: epoch 94 batch 0 loss 0.005296609364449978\n",
      "Training: epoch 94 batch 10 loss 0.006649097893387079\n",
      "Training: epoch 94 batch 20 loss 0.0103391632437706\n",
      "Test: epoch 94 batch 0 loss 0.01173140574246645\n",
      "epoch 94 finished - avarage train loss 0.0076503925073634965  avarage test loss 0.024951735278591514\n",
      "Training: epoch 95 batch 0 loss 0.012554755434393883\n",
      "Training: epoch 95 batch 10 loss 0.016175298020243645\n",
      "Training: epoch 95 batch 20 loss 0.019591398537158966\n",
      "Test: epoch 95 batch 0 loss 0.02135576494038105\n",
      "epoch 95 finished - avarage train loss 0.02504998082616206  avarage test loss 0.03190331533551216\n",
      "Training: epoch 96 batch 0 loss 0.0233013816177845\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training: epoch 96 batch 10 loss 0.012439724989235401\n",
      "Training: epoch 96 batch 20 loss 0.016619518399238586\n",
      "Test: epoch 96 batch 0 loss 0.010181780904531479\n",
      "epoch 96 finished - avarage train loss 0.016867463653586035  avarage test loss 0.021783454343676567\n",
      "Training: epoch 97 batch 0 loss 0.008573740720748901\n",
      "Training: epoch 97 batch 10 loss 0.008631546050310135\n",
      "Training: epoch 97 batch 20 loss 0.006475560832768679\n",
      "Test: epoch 97 batch 0 loss 0.006466406397521496\n",
      "epoch 97 finished - avarage train loss 0.009327190077124998  avarage test loss 0.01377128402236849\n",
      "Training: epoch 98 batch 0 loss 0.011976944282650948\n",
      "Training: epoch 98 batch 10 loss 0.0056805480271577835\n",
      "Training: epoch 98 batch 20 loss 0.003975893370807171\n",
      "Test: epoch 98 batch 0 loss 0.005199573468416929\n",
      "epoch 98 finished - avarage train loss 0.008673900184382138  avarage test loss 0.014579153503291309\n",
      "Training: epoch 99 batch 0 loss 0.004292955156415701\n",
      "Training: epoch 99 batch 10 loss 0.01022953912615776\n",
      "Training: epoch 99 batch 20 loss 0.006687433458864689\n",
      "Test: epoch 99 batch 0 loss 0.006545874290168285\n",
      "epoch 99 finished - avarage train loss 0.009913092920685122  avarage test loss 0.01440864079631865\n",
      "Training: epoch 100 batch 0 loss 0.012279520742595196\n",
      "Training: epoch 100 batch 10 loss 0.011716840788722038\n",
      "Training: epoch 100 batch 20 loss 0.011034258641302586\n",
      "Test: epoch 100 batch 0 loss 0.004669498652219772\n",
      "epoch 100 finished - avarage train loss 0.008553517067098412  avarage test loss 0.01337728404905647\n",
      "Training: epoch 101 batch 0 loss 0.007960053160786629\n",
      "Training: epoch 101 batch 10 loss 0.0062788622453808784\n",
      "Training: epoch 101 batch 20 loss 0.006607672665268183\n",
      "Test: epoch 101 batch 0 loss 0.004529024474322796\n",
      "epoch 101 finished - avarage train loss 0.007986572466726446  avarage test loss 0.012622548034414649\n",
      "Training: epoch 102 batch 0 loss 0.003633493557572365\n",
      "Training: epoch 102 batch 10 loss 0.008789388462901115\n",
      "Training: epoch 102 batch 20 loss 0.007494458928704262\n",
      "Test: epoch 102 batch 0 loss 0.004611806944012642\n",
      "epoch 102 finished - avarage train loss 0.007930082766788787  avarage test loss 0.013755259802564979\n",
      "Training: epoch 103 batch 0 loss 0.01251532044261694\n",
      "Training: epoch 103 batch 10 loss 0.012904396280646324\n",
      "Training: epoch 103 batch 20 loss 0.0182991623878479\n",
      "Test: epoch 103 batch 0 loss 0.004924264270812273\n",
      "epoch 103 finished - avarage train loss 0.009581159109827774  avarage test loss 0.014429594506509602\n",
      "Training: epoch 104 batch 0 loss 0.005979486741125584\n",
      "Training: epoch 104 batch 10 loss 0.008881201036274433\n",
      "Training: epoch 104 batch 20 loss 0.007299167104065418\n",
      "Test: epoch 104 batch 0 loss 0.004627377260476351\n",
      "epoch 104 finished - avarage train loss 0.007703552552466762  avarage test loss 0.014180950005538762\n",
      "Training: epoch 105 batch 0 loss 0.005496559198945761\n",
      "Training: epoch 105 batch 10 loss 0.021679645404219627\n",
      "Training: epoch 105 batch 20 loss 0.0033073141239583492\n",
      "Test: epoch 105 batch 0 loss 0.004998805467039347\n",
      "epoch 105 finished - avarage train loss 0.007551689198690242  avarage test loss 0.014566192170605063\n",
      "Training: epoch 106 batch 0 loss 0.006212841719388962\n",
      "Training: epoch 106 batch 10 loss 0.005148349795490503\n",
      "Training: epoch 106 batch 20 loss 0.0038827043026685715\n",
      "Test: epoch 106 batch 0 loss 0.00450413953512907\n",
      "epoch 106 finished - avarage train loss 0.010028173247801846  avarage test loss 0.013496645377017558\n",
      "Training: epoch 107 batch 0 loss 0.004664610605686903\n",
      "Training: epoch 107 batch 10 loss 0.0037278318777680397\n",
      "Training: epoch 107 batch 20 loss 0.006564042065292597\n",
      "Test: epoch 107 batch 0 loss 0.004929200746119022\n",
      "epoch 107 finished - avarage train loss 0.0077843186124388515  avarage test loss 0.012855300505179912\n",
      "Training: epoch 108 batch 0 loss 0.004160192795097828\n",
      "Training: epoch 108 batch 10 loss 0.010211605578660965\n",
      "Training: epoch 108 batch 20 loss 0.006545746233314276\n",
      "Test: epoch 108 batch 0 loss 0.004315217956900597\n",
      "epoch 108 finished - avarage train loss 0.008741808982951373  avarage test loss 0.01316429377766326\n",
      "Training: epoch 109 batch 0 loss 0.005148767493665218\n",
      "Training: epoch 109 batch 10 loss 0.006308100651949644\n",
      "Training: epoch 109 batch 20 loss 0.009691922925412655\n",
      "Test: epoch 109 batch 0 loss 0.0056429049000144005\n",
      "epoch 109 finished - avarage train loss 0.0075793713812941105  avarage test loss 0.0158313944702968\n",
      "Training: epoch 110 batch 0 loss 0.006779196206480265\n",
      "Training: epoch 110 batch 10 loss 0.010932943783700466\n",
      "Training: epoch 110 batch 20 loss 0.005595683585852385\n",
      "Test: epoch 110 batch 0 loss 0.004682175349444151\n",
      "epoch 110 finished - avarage train loss 0.009552567298042363  avarage test loss 0.014278783462941647\n",
      "Training: epoch 111 batch 0 loss 0.004173356108367443\n",
      "Training: epoch 111 batch 10 loss 0.004834366962313652\n",
      "Training: epoch 111 batch 20 loss 0.00390440272167325\n",
      "Test: epoch 111 batch 0 loss 0.0050765154883265495\n",
      "epoch 111 finished - avarage train loss 0.0069201406139623505  avarage test loss 0.013311625341884792\n",
      "Training: epoch 112 batch 0 loss 0.0034682361874729395\n",
      "Training: epoch 112 batch 10 loss 0.012114067561924458\n",
      "Training: epoch 112 batch 20 loss 0.008546898141503334\n",
      "Test: epoch 112 batch 0 loss 0.004393298178911209\n",
      "epoch 112 finished - avarage train loss 0.0071175832237149106  avarage test loss 0.014318407163955271\n",
      "Training: epoch 113 batch 0 loss 0.00967331137508154\n",
      "Training: epoch 113 batch 10 loss 0.009477086365222931\n",
      "Training: epoch 113 batch 20 loss 0.007686405908316374\n",
      "Test: epoch 113 batch 0 loss 0.004716822411864996\n",
      "epoch 113 finished - avarage train loss 0.009123085684881642  avarage test loss 0.012877514935098588\n",
      "Training: epoch 114 batch 0 loss 0.007221403531730175\n",
      "Training: epoch 114 batch 10 loss 0.007847352884709835\n",
      "Training: epoch 114 batch 20 loss 0.005933418869972229\n",
      "Test: epoch 114 batch 0 loss 0.0040427567437291145\n",
      "epoch 114 finished - avarage train loss 0.008726596784103533  avarage test loss 0.013234184007160366\n",
      "Training: epoch 115 batch 0 loss 0.006742337718605995\n",
      "Training: epoch 115 batch 10 loss 0.006761632859706879\n",
      "Training: epoch 115 batch 20 loss 0.007558849640190601\n",
      "Test: epoch 115 batch 0 loss 0.004818276036530733\n",
      "epoch 115 finished - avarage train loss 0.007458192039409588  avarage test loss 0.015030742739327252\n",
      "Training: epoch 116 batch 0 loss 0.005653146654367447\n",
      "Training: epoch 116 batch 10 loss 0.004997811745852232\n",
      "Training: epoch 116 batch 20 loss 0.006918833591043949\n",
      "Test: epoch 116 batch 0 loss 0.0045812008902430534\n",
      "epoch 116 finished - avarage train loss 0.008655785639160153  avarage test loss 0.013785382499918342\n",
      "Training: epoch 117 batch 0 loss 0.005078398156911135\n",
      "Training: epoch 117 batch 10 loss 0.006888350937515497\n",
      "Training: epoch 117 batch 20 loss 0.01384230237454176\n",
      "Test: epoch 117 batch 0 loss 0.0042852177284657955\n",
      "epoch 117 finished - avarage train loss 0.007600690398750634  avarage test loss 0.013181769521906972\n",
      "Training: epoch 118 batch 0 loss 0.0049659209325909615\n",
      "Training: epoch 118 batch 10 loss 0.010662803426384926\n",
      "Training: epoch 118 batch 20 loss 0.0077643864788115025\n",
      "Test: epoch 118 batch 0 loss 0.005233201663941145\n",
      "epoch 118 finished - avarage train loss 0.007715884018047103  avarage test loss 0.014087254763580859\n",
      "Training: epoch 119 batch 0 loss 0.011767793446779251\n",
      "Training: epoch 119 batch 10 loss 0.0028332415968179703\n",
      "Training: epoch 119 batch 20 loss 0.007018894888460636\n",
      "Test: epoch 119 batch 0 loss 0.004220508970320225\n",
      "epoch 119 finished - avarage train loss 0.008043844421426284  avarage test loss 0.012617399450391531\n",
      "Training: epoch 120 batch 0 loss 0.005665730684995651\n",
      "Training: epoch 120 batch 10 loss 0.006223834119737148\n",
      "Training: epoch 120 batch 20 loss 0.003909930121153593\n",
      "Test: epoch 120 batch 0 loss 0.004077798221260309\n",
      "epoch 120 finished - avarage train loss 0.007767557993466998  avarage test loss 0.013091718661598861\n",
      "Training: epoch 121 batch 0 loss 0.0047098686918616295\n",
      "Training: epoch 121 batch 10 loss 0.006731861270964146\n",
      "Training: epoch 121 batch 20 loss 0.004426926840096712\n",
      "Test: epoch 121 batch 0 loss 0.00435630464926362\n",
      "epoch 121 finished - avarage train loss 0.006715622096290362  avarage test loss 0.012593452935107052\n",
      "Training: epoch 122 batch 0 loss 0.003563138423487544\n",
      "Training: epoch 122 batch 10 loss 0.004287600051611662\n",
      "Training: epoch 122 batch 20 loss 0.005802263971418142\n",
      "Test: epoch 122 batch 0 loss 0.004738913383334875\n",
      "epoch 122 finished - avarage train loss 0.0080651278479089  avarage test loss 0.014159644022583961\n",
      "Training: epoch 123 batch 0 loss 0.005009931046515703\n",
      "Training: epoch 123 batch 10 loss 0.01352667436003685\n",
      "Training: epoch 123 batch 20 loss 0.011564759537577629\n",
      "Test: epoch 123 batch 0 loss 0.0076854173094034195\n",
      "epoch 123 finished - avarage train loss 0.01009059077578372  avarage test loss 0.018853554036468267\n",
      "Training: epoch 124 batch 0 loss 0.008962970227003098\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training: epoch 124 batch 10 loss 0.007297891657799482\n",
      "Training: epoch 124 batch 20 loss 0.006718828808516264\n",
      "Test: epoch 124 batch 0 loss 0.004629632458090782\n",
      "epoch 124 finished - avarage train loss 0.008370140294448054  avarage test loss 0.013104996411129832\n",
      "Training: epoch 125 batch 0 loss 0.008448583073914051\n",
      "Training: epoch 125 batch 10 loss 0.005625570192933083\n",
      "Training: epoch 125 batch 20 loss 0.005412234924733639\n",
      "Test: epoch 125 batch 0 loss 0.0043458519503474236\n",
      "epoch 125 finished - avarage train loss 0.007632129745365217  avarage test loss 0.013958885800093412\n",
      "Training: epoch 126 batch 0 loss 0.006117269862443209\n",
      "Training: epoch 126 batch 10 loss 0.004851188510656357\n",
      "Training: epoch 126 batch 20 loss 0.004810229875147343\n",
      "Test: epoch 126 batch 0 loss 0.004068107344210148\n",
      "epoch 126 finished - avarage train loss 0.007450392285104969  avarage test loss 0.012434344331268221\n",
      "Training: epoch 127 batch 0 loss 0.0034991323482245207\n",
      "Training: epoch 127 batch 10 loss 0.008728302083909512\n",
      "Training: epoch 127 batch 20 loss 0.0020960692781955004\n",
      "Test: epoch 127 batch 0 loss 0.004610234871506691\n",
      "epoch 127 finished - avarage train loss 0.007361102712758142  avarage test loss 0.013522036257199943\n",
      "Training: epoch 128 batch 0 loss 0.008395198732614517\n",
      "Training: epoch 128 batch 10 loss 0.009543616324663162\n",
      "Training: epoch 128 batch 20 loss 0.006807113066315651\n",
      "Test: epoch 128 batch 0 loss 0.004574744030833244\n",
      "epoch 128 finished - avarage train loss 0.0076259961110892996  avarage test loss 0.01327418361324817\n",
      "Training: epoch 129 batch 0 loss 0.011299827136099339\n",
      "Training: epoch 129 batch 10 loss 0.00518933217972517\n",
      "Training: epoch 129 batch 20 loss 0.013673662208020687\n",
      "Test: epoch 129 batch 0 loss 0.005232086405158043\n",
      "epoch 129 finished - avarage train loss 0.008615760052383974  avarage test loss 0.014554711640812457\n",
      "Training: epoch 130 batch 0 loss 0.004734387155622244\n",
      "Training: epoch 130 batch 10 loss 0.004897317849099636\n",
      "Training: epoch 130 batch 20 loss 0.005757952108979225\n",
      "Test: epoch 130 batch 0 loss 0.004588152281939983\n",
      "epoch 130 finished - avarage train loss 0.008119575045039427  avarage test loss 0.014184817904606462\n",
      "Training: epoch 131 batch 0 loss 0.0075915502384305\n",
      "Training: epoch 131 batch 10 loss 0.00469930050894618\n",
      "Training: epoch 131 batch 20 loss 0.009047789499163628\n",
      "Test: epoch 131 batch 0 loss 0.006204731296747923\n",
      "epoch 131 finished - avarage train loss 0.0076814143645480785  avarage test loss 0.01398152275942266\n",
      "Training: epoch 132 batch 0 loss 0.0034984219819307327\n",
      "Training: epoch 132 batch 10 loss 0.006562911905348301\n",
      "Training: epoch 132 batch 20 loss 0.0072460295632481575\n",
      "Test: epoch 132 batch 0 loss 0.004515327513217926\n",
      "epoch 132 finished - avarage train loss 0.007283495353727505  avarage test loss 0.014516093768179417\n",
      "Training: epoch 133 batch 0 loss 0.006598346401005983\n",
      "Training: epoch 133 batch 10 loss 0.011797487735748291\n",
      "Training: epoch 133 batch 20 loss 0.00645867083221674\n",
      "Test: epoch 133 batch 0 loss 0.004710883367806673\n",
      "epoch 133 finished - avarage train loss 0.008229233292413169  avarage test loss 0.014914502389729023\n",
      "Training: epoch 134 batch 0 loss 0.01060311496257782\n",
      "Training: epoch 134 batch 10 loss 0.005201523192226887\n",
      "Training: epoch 134 batch 20 loss 0.005240204278379679\n",
      "Test: epoch 134 batch 0 loss 0.0041745565831661224\n",
      "epoch 134 finished - avarage train loss 0.008555723434506819  avarage test loss 0.013702569645829499\n",
      "Training: epoch 135 batch 0 loss 0.005511716939508915\n",
      "Training: epoch 135 batch 10 loss 0.011239164508879185\n",
      "Training: epoch 135 batch 20 loss 0.008317667990922928\n",
      "Test: epoch 135 batch 0 loss 0.004779121372848749\n",
      "epoch 135 finished - avarage train loss 0.007579370273341392  avarage test loss 0.012940319662448019\n",
      "Training: epoch 136 batch 0 loss 0.00470686936751008\n",
      "Training: epoch 136 batch 10 loss 0.0025421965401619673\n",
      "Training: epoch 136 batch 20 loss 0.010740161873400211\n",
      "Test: epoch 136 batch 0 loss 0.005852216854691505\n",
      "epoch 136 finished - avarage train loss 0.008065160532514083  avarage test loss 0.01651344879064709\n",
      "Training: epoch 137 batch 0 loss 0.005966365337371826\n",
      "Training: epoch 137 batch 10 loss 0.006690895184874535\n",
      "Training: epoch 137 batch 20 loss 0.003877598326653242\n",
      "Test: epoch 137 batch 0 loss 0.006189308129251003\n",
      "epoch 137 finished - avarage train loss 0.008276135813252166  avarage test loss 0.013702921103686094\n",
      "Training: epoch 138 batch 0 loss 0.004490723833441734\n",
      "Training: epoch 138 batch 10 loss 0.011190577410161495\n",
      "Training: epoch 138 batch 20 loss 0.004279700573533773\n",
      "Test: epoch 138 batch 0 loss 0.004914299584925175\n",
      "epoch 138 finished - avarage train loss 0.008079581315918216  avarage test loss 0.014057075837627053\n",
      "Training: epoch 139 batch 0 loss 0.006959956604987383\n",
      "Training: epoch 139 batch 10 loss 0.00508368294686079\n",
      "Training: epoch 139 batch 20 loss 0.007706581614911556\n",
      "Test: epoch 139 batch 0 loss 0.004837607033550739\n",
      "epoch 139 finished - avarage train loss 0.007345091061795066  avarage test loss 0.013268516166135669\n",
      "Training: epoch 140 batch 0 loss 0.005755217745900154\n",
      "Training: epoch 140 batch 10 loss 0.011280300095677376\n",
      "Training: epoch 140 batch 20 loss 0.003870905376970768\n",
      "Test: epoch 140 batch 0 loss 0.00922310072928667\n",
      "epoch 140 finished - avarage train loss 0.008080592306716174  avarage test loss 0.018551636254414916\n",
      "Training: epoch 141 batch 0 loss 0.0054593696258962154\n",
      "Training: epoch 141 batch 10 loss 0.018269330263137817\n",
      "Training: epoch 141 batch 20 loss 0.0055656819604337215\n",
      "Test: epoch 141 batch 0 loss 0.007891669869422913\n",
      "epoch 141 finished - avarage train loss 0.015498136667984313  avarage test loss 0.019842150388285518\n",
      "Training: epoch 142 batch 0 loss 0.012736589647829533\n",
      "Training: epoch 142 batch 10 loss 0.007809127680957317\n",
      "Training: epoch 142 batch 20 loss 0.008891166187822819\n",
      "Test: epoch 142 batch 0 loss 0.0054493676871061325\n",
      "epoch 142 finished - avarage train loss 0.008906967472285032  avarage test loss 0.015216438681818545\n",
      "Training: epoch 143 batch 0 loss 0.004071762785315514\n",
      "Training: epoch 143 batch 10 loss 0.008684991858899593\n",
      "Training: epoch 143 batch 20 loss 0.007460616994649172\n",
      "Test: epoch 143 batch 0 loss 0.005363226402550936\n",
      "epoch 143 finished - avarage train loss 0.008197681954258988  avarage test loss 0.0158348799450323\n",
      "Training: epoch 144 batch 0 loss 0.007384208962321281\n",
      "Training: epoch 144 batch 10 loss 0.010930899530649185\n",
      "Training: epoch 144 batch 20 loss 0.016011016443371773\n",
      "Test: epoch 144 batch 0 loss 0.0050136330537498\n",
      "epoch 144 finished - avarage train loss 0.008620757200145003  avarage test loss 0.013082466030027717\n",
      "Training: epoch 145 batch 0 loss 0.004977306816726923\n",
      "Training: epoch 145 batch 10 loss 0.009502189233899117\n",
      "Training: epoch 145 batch 20 loss 0.009922266006469727\n",
      "Test: epoch 145 batch 0 loss 0.004257352091372013\n",
      "epoch 145 finished - avarage train loss 0.009304116022419828  avarage test loss 0.013436436420306563\n",
      "Training: epoch 146 batch 0 loss 0.00616344204172492\n",
      "Training: epoch 146 batch 10 loss 0.003291928907856345\n",
      "Training: epoch 146 batch 20 loss 0.004632377065718174\n",
      "Test: epoch 146 batch 0 loss 0.004881072789430618\n",
      "epoch 146 finished - avarage train loss 0.007923678061443156  avarage test loss 0.014764077262952924\n",
      "Training: epoch 147 batch 0 loss 0.005195552948862314\n",
      "Training: epoch 147 batch 10 loss 0.007013638503849506\n",
      "Training: epoch 147 batch 20 loss 0.0059923939406871796\n",
      "Test: epoch 147 batch 0 loss 0.005905675236135721\n",
      "epoch 147 finished - avarage train loss 0.00785734233093159  avarage test loss 0.016080363420769572\n",
      "Training: epoch 148 batch 0 loss 0.004634496755897999\n",
      "Training: epoch 148 batch 10 loss 0.0061874049715697765\n",
      "Training: epoch 148 batch 20 loss 0.0071667819283902645\n",
      "Test: epoch 148 batch 0 loss 0.0049950494430959225\n",
      "epoch 148 finished - avarage train loss 0.008310002579899698  avarage test loss 0.013917611096985638\n",
      "Training: epoch 149 batch 0 loss 0.005418581888079643\n",
      "Training: epoch 149 batch 10 loss 0.00452651409432292\n",
      "Training: epoch 149 batch 20 loss 0.006455852650105953\n",
      "Test: epoch 149 batch 0 loss 0.004075662232935429\n",
      "epoch 149 finished - avarage train loss 0.007178911810805057  avarage test loss 0.013039045850746334\n",
      "Training: epoch 150 batch 0 loss 0.010525676421821117\n",
      "Training: epoch 150 batch 10 loss 0.0058119092136621475\n",
      "Training: epoch 150 batch 20 loss 0.006704521365463734\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test: epoch 150 batch 0 loss 0.003892612410709262\n",
      "epoch 150 finished - avarage train loss 0.007770355549994214  avarage test loss 0.012916132749523968\n",
      "Training: epoch 151 batch 0 loss 0.004506428260356188\n",
      "Training: epoch 151 batch 10 loss 0.010389996692538261\n",
      "Training: epoch 151 batch 20 loss 0.009602386504411697\n",
      "Test: epoch 151 batch 0 loss 0.005298128351569176\n",
      "epoch 151 finished - avarage train loss 0.008539266081462646  avarage test loss 0.013244941364973783\n",
      "Training: epoch 152 batch 0 loss 0.007583403028547764\n",
      "Training: epoch 152 batch 10 loss 0.0056442865170538425\n",
      "Training: epoch 152 batch 20 loss 0.007076977752149105\n",
      "Test: epoch 152 batch 0 loss 0.004882802255451679\n",
      "epoch 152 finished - avarage train loss 0.008195101532378587  avarage test loss 0.014254766050726175\n",
      "Training: epoch 153 batch 0 loss 0.005465946160256863\n",
      "Training: epoch 153 batch 10 loss 0.007803813088685274\n",
      "Training: epoch 153 batch 20 loss 0.005669774487614632\n",
      "Test: epoch 153 batch 0 loss 0.005342465825378895\n",
      "epoch 153 finished - avarage train loss 0.009161747574549297  avarage test loss 0.014797841082327068\n",
      "Training: epoch 154 batch 0 loss 0.008104927837848663\n",
      "Training: epoch 154 batch 10 loss 0.011099728755652905\n",
      "Training: epoch 154 batch 20 loss 0.003025255398824811\n",
      "Test: epoch 154 batch 0 loss 0.0054817646741867065\n",
      "epoch 154 finished - avarage train loss 0.00831947926078634  avarage test loss 0.013644473860040307\n",
      "Training: epoch 155 batch 0 loss 0.007204858120530844\n",
      "Training: epoch 155 batch 10 loss 0.01024432573467493\n",
      "Training: epoch 155 batch 20 loss 0.0068433573469519615\n",
      "Test: epoch 155 batch 0 loss 0.004252547863870859\n",
      "epoch 155 finished - avarage train loss 0.008267201820452666  avarage test loss 0.012691509036812931\n",
      "Training: epoch 156 batch 0 loss 0.005581962876021862\n",
      "Training: epoch 156 batch 10 loss 0.007178127765655518\n",
      "Training: epoch 156 batch 20 loss 0.007771786767989397\n",
      "Test: epoch 156 batch 0 loss 0.004759420175105333\n",
      "epoch 156 finished - avarage train loss 0.008326193341856887  avarage test loss 0.013666786951944232\n",
      "Training: epoch 157 batch 0 loss 0.0076607693918049335\n",
      "Training: epoch 157 batch 10 loss 0.004210508428514004\n",
      "Training: epoch 157 batch 20 loss 0.007375294808298349\n",
      "Test: epoch 157 batch 0 loss 0.005995729472488165\n",
      "epoch 157 finished - avarage train loss 0.008008327214689604  avarage test loss 0.013962591299787164\n",
      "Training: epoch 158 batch 0 loss 0.005772827658802271\n",
      "Training: epoch 158 batch 10 loss 0.003845823463052511\n",
      "Training: epoch 158 batch 20 loss 0.0057254573330283165\n",
      "Test: epoch 158 batch 0 loss 0.005209086928516626\n",
      "epoch 158 finished - avarage train loss 0.007085856275054915  avarage test loss 0.014543009456247091\n",
      "Training: epoch 159 batch 0 loss 0.006802478805184364\n",
      "Training: epoch 159 batch 10 loss 0.004660828039050102\n",
      "Training: epoch 159 batch 20 loss 0.007671175058931112\n",
      "Test: epoch 159 batch 0 loss 0.004394147079437971\n",
      "epoch 159 finished - avarage train loss 0.007742917687021966  avarage test loss 0.01246681867633015\n",
      "Training: epoch 160 batch 0 loss 0.006534183397889137\n",
      "Training: epoch 160 batch 10 loss 0.007310706190764904\n",
      "Training: epoch 160 batch 20 loss 0.003540890756994486\n",
      "Test: epoch 160 batch 0 loss 0.004326827824115753\n",
      "epoch 160 finished - avarage train loss 0.00882964085081014  avarage test loss 0.012416843208484352\n",
      "Training: epoch 161 batch 0 loss 0.004174374975264072\n",
      "Training: epoch 161 batch 10 loss 0.004336921498179436\n",
      "Training: epoch 161 batch 20 loss 0.005951861385256052\n",
      "Test: epoch 161 batch 0 loss 0.005586656276136637\n",
      "epoch 161 finished - avarage train loss 0.0077534846977555544  avarage test loss 0.013424449600279331\n",
      "Training: epoch 162 batch 0 loss 0.005261106416583061\n",
      "Training: epoch 162 batch 10 loss 0.007454501930624247\n",
      "Training: epoch 162 batch 20 loss 0.00585201196372509\n",
      "Test: epoch 162 batch 0 loss 0.005750824231654406\n",
      "epoch 162 finished - avarage train loss 0.008928757048501023  avarage test loss 0.013477090513333678\n",
      "Training: epoch 163 batch 0 loss 0.005806093104183674\n",
      "Training: epoch 163 batch 10 loss 0.01170740183442831\n",
      "Training: epoch 163 batch 20 loss 0.007237146608531475\n",
      "Test: epoch 163 batch 0 loss 0.003859100164845586\n",
      "epoch 163 finished - avarage train loss 0.007772440355720705  avarage test loss 0.012234172376338392\n",
      "Training: epoch 164 batch 0 loss 0.004109715577214956\n",
      "Training: epoch 164 batch 10 loss 0.007267957087606192\n",
      "Training: epoch 164 batch 20 loss 0.007992031052708626\n",
      "Test: epoch 164 batch 0 loss 0.004421152640134096\n",
      "epoch 164 finished - avarage train loss 0.007021614687581514  avarage test loss 0.013970492756925523\n",
      "Training: epoch 165 batch 0 loss 0.008599642664194107\n",
      "Training: epoch 165 batch 10 loss 0.006805405020713806\n",
      "Training: epoch 165 batch 20 loss 0.004300975706428289\n",
      "Test: epoch 165 batch 0 loss 0.0059794653207063675\n",
      "epoch 165 finished - avarage train loss 0.00789665785650241  avarage test loss 0.013815301470458508\n",
      "Training: epoch 166 batch 0 loss 0.013787518255412579\n",
      "Training: epoch 166 batch 10 loss 0.005867505446076393\n",
      "Training: epoch 166 batch 20 loss 0.007818377576768398\n",
      "Test: epoch 166 batch 0 loss 0.006465618498623371\n",
      "epoch 166 finished - avarage train loss 0.007663932460328114  avarage test loss 0.014766679611057043\n",
      "Training: epoch 167 batch 0 loss 0.010187240317463875\n",
      "Training: epoch 167 batch 10 loss 0.0070943511091172695\n",
      "Training: epoch 167 batch 20 loss 0.0033692438155412674\n",
      "Test: epoch 167 batch 0 loss 0.004856284242123365\n",
      "epoch 167 finished - avarage train loss 0.008729309373503101  avarage test loss 0.012928736628964543\n",
      "Training: epoch 168 batch 0 loss 0.006870605982840061\n",
      "Training: epoch 168 batch 10 loss 0.0046468498185276985\n",
      "Training: epoch 168 batch 20 loss 0.0060809338465332985\n",
      "Test: epoch 168 batch 0 loss 0.005239306483417749\n",
      "epoch 168 finished - avarage train loss 0.007605451504410854  avarage test loss 0.013627574313431978\n",
      "Training: epoch 169 batch 0 loss 0.010223101824522018\n",
      "Training: epoch 169 batch 10 loss 0.01032146718353033\n",
      "Training: epoch 169 batch 20 loss 0.006602684035897255\n",
      "Test: epoch 169 batch 0 loss 0.006620782893151045\n",
      "epoch 169 finished - avarage train loss 0.009383038938816252  avarage test loss 0.017401813180185854\n",
      "Training: epoch 170 batch 0 loss 0.009312105365097523\n",
      "Training: epoch 170 batch 10 loss 0.008464958518743515\n",
      "Training: epoch 170 batch 20 loss 0.012887733057141304\n",
      "Test: epoch 170 batch 0 loss 0.004378491546958685\n",
      "epoch 170 finished - avarage train loss 0.008996202080542672  avarage test loss 0.012981571431737393\n",
      "Training: epoch 171 batch 0 loss 0.004398856312036514\n",
      "Training: epoch 171 batch 10 loss 0.0032519970554858446\n",
      "Training: epoch 171 batch 20 loss 0.008702258579432964\n",
      "Test: epoch 171 batch 0 loss 0.004483988508582115\n",
      "epoch 171 finished - avarage train loss 0.008555506765521291  avarage test loss 0.012655157595872879\n",
      "Training: epoch 172 batch 0 loss 0.005956576205790043\n",
      "Training: epoch 172 batch 10 loss 0.00713074766099453\n",
      "Training: epoch 172 batch 20 loss 0.007006462663412094\n",
      "Test: epoch 172 batch 0 loss 0.004812633153051138\n",
      "epoch 172 finished - avarage train loss 0.008440446437753994  avarage test loss 0.014342651702463627\n",
      "Training: epoch 173 batch 0 loss 0.008084168657660484\n",
      "Training: epoch 173 batch 10 loss 0.004883207380771637\n",
      "Training: epoch 173 batch 20 loss 0.00418351124972105\n",
      "Test: epoch 173 batch 0 loss 0.0053252908401191235\n",
      "epoch 173 finished - avarage train loss 0.008491284117616456  avarage test loss 0.013905825675465167\n",
      "Training: epoch 174 batch 0 loss 0.007717677857726812\n",
      "Training: epoch 174 batch 10 loss 0.006839565467089415\n",
      "Training: epoch 174 batch 20 loss 0.007538084406405687\n",
      "Test: epoch 174 batch 0 loss 0.006888433359563351\n",
      "epoch 174 finished - avarage train loss 0.008102252124005864  avarage test loss 0.018639770220033824\n",
      "Training: epoch 175 batch 0 loss 0.006281128618866205\n",
      "Training: epoch 175 batch 10 loss 0.00902307964861393\n",
      "Training: epoch 175 batch 20 loss 0.005815497133880854\n",
      "Test: epoch 175 batch 0 loss 0.003993953578174114\n",
      "epoch 175 finished - avarage train loss 0.007613086045302194  avarage test loss 0.013504248927347362\n",
      "Training: epoch 176 batch 0 loss 0.006924326065927744\n",
      "Training: epoch 176 batch 10 loss 0.004089174792170525\n",
      "Training: epoch 176 batch 20 loss 0.00479989405721426\n",
      "Test: epoch 176 batch 0 loss 0.004733403213322163\n",
      "epoch 176 finished - avarage train loss 0.007894296022453185  avarage test loss 0.013170808670111\n",
      "Training: epoch 177 batch 0 loss 0.010729244910180569\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training: epoch 177 batch 10 loss 0.010914241895079613\n",
      "Training: epoch 177 batch 20 loss 0.009116903878748417\n",
      "Test: epoch 177 batch 0 loss 0.004238590598106384\n",
      "epoch 177 finished - avarage train loss 0.008288584264187977  avarage test loss 0.014140516403131187\n",
      "Training: epoch 178 batch 0 loss 0.004679541103541851\n",
      "Training: epoch 178 batch 10 loss 0.005258483346551657\n",
      "Training: epoch 178 batch 20 loss 0.007925079204142094\n",
      "Test: epoch 178 batch 0 loss 0.005447354167699814\n",
      "epoch 178 finished - avarage train loss 0.008254754742414787  avarage test loss 0.015663803089410067\n",
      "Training: epoch 179 batch 0 loss 0.005274649243801832\n",
      "Training: epoch 179 batch 10 loss 0.008425398729741573\n",
      "Training: epoch 179 batch 20 loss 0.006642261520028114\n",
      "Test: epoch 179 batch 0 loss 0.004168882966041565\n",
      "epoch 179 finished - avarage train loss 0.007204674513496715  avarage test loss 0.013666176702827215\n",
      "Training: epoch 180 batch 0 loss 0.005847252439707518\n",
      "Training: epoch 180 batch 10 loss 0.0111483009532094\n",
      "Training: epoch 180 batch 20 loss 0.012984813190996647\n",
      "Test: epoch 180 batch 0 loss 0.005118840374052525\n",
      "epoch 180 finished - avarage train loss 0.007051294227933575  avarage test loss 0.015578783815726638\n",
      "Training: epoch 181 batch 0 loss 0.009697210043668747\n",
      "Training: epoch 181 batch 10 loss 0.0058642541989684105\n",
      "Training: epoch 181 batch 20 loss 0.006210732273757458\n",
      "Test: epoch 181 batch 0 loss 0.007559964433312416\n",
      "epoch 181 finished - avarage train loss 0.008484765678515723  avarage test loss 0.01647552044596523\n",
      "Training: epoch 182 batch 0 loss 0.006026502698659897\n",
      "Training: epoch 182 batch 10 loss 0.012534249573946\n",
      "Training: epoch 182 batch 20 loss 0.011388454586267471\n",
      "Test: epoch 182 batch 0 loss 0.005671228747814894\n",
      "epoch 182 finished - avarage train loss 0.011712475727986673  avarage test loss 0.01594901701901108\n",
      "Training: epoch 183 batch 0 loss 0.0102156987413764\n",
      "Training: epoch 183 batch 10 loss 0.007471233606338501\n",
      "Training: epoch 183 batch 20 loss 0.01282777264714241\n",
      "Test: epoch 183 batch 0 loss 0.004093647934496403\n",
      "epoch 183 finished - avarage train loss 0.007918544442810375  avarage test loss 0.01378735841717571\n",
      "Training: epoch 184 batch 0 loss 0.0051124547608196735\n",
      "Training: epoch 184 batch 10 loss 0.005845191422849894\n",
      "Training: epoch 184 batch 20 loss 0.006905108690261841\n",
      "Test: epoch 184 batch 0 loss 0.004736168775707483\n",
      "epoch 184 finished - avarage train loss 0.007695440363524289  avarage test loss 0.01293357671238482\n",
      "Training: epoch 185 batch 0 loss 0.005015885923057795\n",
      "Training: epoch 185 batch 10 loss 0.014239281415939331\n",
      "Training: epoch 185 batch 20 loss 0.008085818961262703\n",
      "Test: epoch 185 batch 0 loss 0.004075067583471537\n",
      "epoch 185 finished - avarage train loss 0.007874637312287915  avarage test loss 0.012475591211114079\n",
      "Training: epoch 186 batch 0 loss 0.006598995998501778\n",
      "Training: epoch 186 batch 10 loss 0.006397790741175413\n",
      "Training: epoch 186 batch 20 loss 0.006444179452955723\n",
      "Test: epoch 186 batch 0 loss 0.003978522028774023\n",
      "epoch 186 finished - avarage train loss 0.006855497578287433  avarage test loss 0.012737529643345624\n",
      "Training: epoch 187 batch 0 loss 0.004727302584797144\n",
      "Training: epoch 187 batch 10 loss 0.0046296147629618645\n",
      "Training: epoch 187 batch 20 loss 0.005832564551383257\n",
      "Test: epoch 187 batch 0 loss 0.003952569793909788\n",
      "epoch 187 finished - avarage train loss 0.007262393414717296  avarage test loss 0.013419972383417189\n",
      "Training: epoch 188 batch 0 loss 0.004755896981805563\n",
      "Training: epoch 188 batch 10 loss 0.003561880439519882\n",
      "Training: epoch 188 batch 20 loss 0.005864547099918127\n",
      "Test: epoch 188 batch 0 loss 0.004074974916875362\n",
      "epoch 188 finished - avarage train loss 0.00748074907359892  avarage test loss 0.012479949393309653\n",
      "Training: epoch 189 batch 0 loss 0.014407224021852016\n",
      "Training: epoch 189 batch 10 loss 0.008954914286732674\n",
      "Training: epoch 189 batch 20 loss 0.0034202109090983868\n",
      "Test: epoch 189 batch 0 loss 0.005660964176058769\n",
      "epoch 189 finished - avarage train loss 0.009259933140128851  avarage test loss 0.014053905149921775\n",
      "Training: epoch 190 batch 0 loss 0.004738951567560434\n",
      "Training: epoch 190 batch 10 loss 0.0065637798979878426\n",
      "Training: epoch 190 batch 20 loss 0.008024262264370918\n",
      "Test: epoch 190 batch 0 loss 0.0060224030166864395\n",
      "epoch 190 finished - avarage train loss 0.00901450077071786  avarage test loss 0.016973104909993708\n",
      "Training: epoch 191 batch 0 loss 0.005761447362601757\n",
      "Training: epoch 191 batch 10 loss 0.0077545857056975365\n",
      "Training: epoch 191 batch 20 loss 0.003229251829907298\n",
      "Test: epoch 191 batch 0 loss 0.004588305484503508\n",
      "epoch 191 finished - avarage train loss 0.006473533857356885  avarage test loss 0.01299196423497051\n",
      "Training: epoch 192 batch 0 loss 0.010383588261902332\n",
      "Training: epoch 192 batch 10 loss 0.005083091557025909\n",
      "Training: epoch 192 batch 20 loss 0.006893927231431007\n",
      "Test: epoch 192 batch 0 loss 0.004557418636977673\n",
      "epoch 192 finished - avarage train loss 0.0073605727500699715  avarage test loss 0.014573964988812804\n",
      "Training: epoch 193 batch 0 loss 0.00808340311050415\n",
      "Training: epoch 193 batch 10 loss 0.00869061704725027\n",
      "Training: epoch 193 batch 20 loss 0.00678640604019165\n",
      "Test: epoch 193 batch 0 loss 0.004009026102721691\n",
      "epoch 193 finished - avarage train loss 0.007153176140553993  avarage test loss 0.012734554591588676\n",
      "Training: epoch 194 batch 0 loss 0.00479635177180171\n",
      "Training: epoch 194 batch 10 loss 0.009560026228427887\n",
      "Training: epoch 194 batch 20 loss 0.009362169541418552\n",
      "Test: epoch 194 batch 0 loss 0.004675000905990601\n",
      "epoch 194 finished - avarage train loss 0.008348359621997023  avarage test loss 0.01367155509069562\n",
      "Training: epoch 195 batch 0 loss 0.008565657772123814\n",
      "Training: epoch 195 batch 10 loss 0.0070030842907726765\n",
      "Training: epoch 195 batch 20 loss 0.006474840454757214\n",
      "Test: epoch 195 batch 0 loss 0.004335458390414715\n",
      "epoch 195 finished - avarage train loss 0.008319851854045329  avarage test loss 0.013539400650188327\n",
      "Training: epoch 196 batch 0 loss 0.008103787899017334\n",
      "Training: epoch 196 batch 10 loss 0.0072748735547065735\n",
      "Training: epoch 196 batch 20 loss 0.006589971482753754\n",
      "Test: epoch 196 batch 0 loss 0.004182716365903616\n",
      "epoch 196 finished - avarage train loss 0.008071938674126205  avarage test loss 0.01394389453344047\n",
      "Training: epoch 197 batch 0 loss 0.005133166443556547\n",
      "Training: epoch 197 batch 10 loss 0.007507015485316515\n",
      "Training: epoch 197 batch 20 loss 0.009791646152734756\n",
      "Test: epoch 197 batch 0 loss 0.004551567602902651\n",
      "epoch 197 finished - avarage train loss 0.007605162120006722  avarage test loss 0.014675237820483744\n",
      "Training: epoch 198 batch 0 loss 0.006245930213481188\n",
      "Training: epoch 198 batch 10 loss 0.0056549785658717155\n",
      "Training: epoch 198 batch 20 loss 0.008421563543379307\n",
      "Test: epoch 198 batch 0 loss 0.005575684364885092\n",
      "epoch 198 finished - avarage train loss 0.00799837837734356  avarage test loss 0.01395358971785754\n",
      "Training: epoch 199 batch 0 loss 0.005462112836539745\n",
      "Training: epoch 199 batch 10 loss 0.011533613316714764\n",
      "Training: epoch 199 batch 20 loss 0.0024197346065193415\n",
      "Test: epoch 199 batch 0 loss 0.004045786336064339\n",
      "epoch 199 finished - avarage train loss 0.008198157779808188  avarage test loss 0.013201229507103562\n",
      "Training: epoch 0 batch 0 loss 0.5088120102882385\n",
      "Training: epoch 0 batch 10 loss 0.5839547514915466\n",
      "Training: epoch 0 batch 20 loss 0.5627740025520325\n",
      "Test: epoch 0 batch 0 loss 0.43194761872291565\n",
      "epoch 0 finished - avarage train loss 0.5281217879262464  avarage test loss 0.5176458433270454\n",
      "Training: epoch 1 batch 0 loss 0.40330997109413147\n",
      "Training: epoch 1 batch 10 loss 0.46838998794555664\n",
      "Training: epoch 1 batch 20 loss 0.5386569499969482\n",
      "Test: epoch 1 batch 0 loss 0.43366438150405884\n",
      "epoch 1 finished - avarage train loss 0.5030768981267666  avarage test loss 0.5173152163624763\n",
      "Training: epoch 2 batch 0 loss 0.6188812255859375\n",
      "Training: epoch 2 batch 10 loss 0.4841649532318115\n",
      "Training: epoch 2 batch 20 loss 0.4729798436164856\n",
      "Test: epoch 2 batch 0 loss 0.43799418210983276\n",
      "epoch 2 finished - avarage train loss 0.5198010596735724  avarage test loss 0.5138139203190804\n",
      "Training: epoch 3 batch 0 loss 0.4148562550544739\n",
      "Training: epoch 3 batch 10 loss 0.4423794150352478\n",
      "Training: epoch 3 batch 20 loss 0.4589611887931824\n",
      "Test: epoch 3 batch 0 loss 0.43358415365219116\n",
      "epoch 3 finished - avarage train loss 0.5193924420866473  avarage test loss 0.5074341371655464\n",
      "Training: epoch 4 batch 0 loss 0.5865216851234436\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training: epoch 4 batch 10 loss 0.4416084289550781\n",
      "Training: epoch 4 batch 20 loss 0.6207058429718018\n",
      "Test: epoch 4 batch 0 loss 0.42690509557724\n",
      "epoch 4 finished - avarage train loss 0.5074877307332796  avarage test loss 0.5111056119203568\n",
      "Training: epoch 5 batch 0 loss 0.5839543342590332\n",
      "Training: epoch 5 batch 10 loss 0.5732029676437378\n",
      "Training: epoch 5 batch 20 loss 0.48621243238449097\n",
      "Test: epoch 5 batch 0 loss 0.43000179529190063\n",
      "epoch 5 finished - avarage train loss 0.5094116730936642  avarage test loss 0.5090158507227898\n",
      "Training: epoch 6 batch 0 loss 0.5771918892860413\n",
      "Training: epoch 6 batch 10 loss 0.4687737822532654\n",
      "Training: epoch 6 batch 20 loss 0.5036252737045288\n",
      "Test: epoch 6 batch 0 loss 0.42801526188850403\n",
      "epoch 6 finished - avarage train loss 0.513835942950742  avarage test loss 0.5149641633033752\n",
      "Training: epoch 7 batch 0 loss 0.670939028263092\n",
      "Training: epoch 7 batch 10 loss 0.572179913520813\n",
      "Training: epoch 7 batch 20 loss 0.4782266318798065\n",
      "Test: epoch 7 batch 0 loss 0.427658349275589\n",
      "epoch 7 finished - avarage train loss 0.5173889110828268  avarage test loss 0.5079960078001022\n",
      "Training: epoch 8 batch 0 loss 0.5221055746078491\n",
      "Training: epoch 8 batch 10 loss 0.5158330798149109\n",
      "Training: epoch 8 batch 20 loss 0.4155399203300476\n",
      "Test: epoch 8 batch 0 loss 0.4284464418888092\n",
      "epoch 8 finished - avarage train loss 0.4993983979882865  avarage test loss 0.5082624405622482\n",
      "Training: epoch 9 batch 0 loss 0.4051664173603058\n",
      "Training: epoch 9 batch 10 loss 0.46568015217781067\n",
      "Training: epoch 9 batch 20 loss 0.4708034098148346\n",
      "Test: epoch 9 batch 0 loss 0.4382059872150421\n",
      "epoch 9 finished - avarage train loss 0.5146659962062178  avarage test loss 0.5139115229249\n",
      "Training: epoch 10 batch 0 loss 0.37948691844940186\n",
      "Training: epoch 10 batch 10 loss 0.415033757686615\n",
      "Training: epoch 10 batch 20 loss 0.48487532138824463\n",
      "Test: epoch 10 batch 0 loss 0.43869748711586\n",
      "epoch 10 finished - avarage train loss 0.5230146194326466  avarage test loss 0.5195235833525658\n",
      "Training: epoch 11 batch 0 loss 0.7548781037330627\n",
      "Training: epoch 11 batch 10 loss 0.4506801962852478\n",
      "Training: epoch 11 batch 20 loss 0.33879977464675903\n",
      "Test: epoch 11 batch 0 loss 0.30416882038116455\n",
      "epoch 11 finished - avarage train loss 0.4995351341264001  avarage test loss 0.3829881548881531\n",
      "Training: epoch 12 batch 0 loss 0.4176150858402252\n",
      "Training: epoch 12 batch 10 loss 0.19585394859313965\n",
      "Training: epoch 12 batch 20 loss 0.16675397753715515\n",
      "Test: epoch 12 batch 0 loss 0.09919323772192001\n",
      "epoch 12 finished - avarage train loss 0.21886180880768547  avarage test loss 0.09828087501227856\n",
      "Training: epoch 13 batch 0 loss 0.08811751008033752\n",
      "Training: epoch 13 batch 10 loss 0.14230021834373474\n",
      "Training: epoch 13 batch 20 loss 0.10661035776138306\n",
      "Test: epoch 13 batch 0 loss 0.0889057070016861\n",
      "epoch 13 finished - avarage train loss 0.09020397914894696  avarage test loss 0.09012371674180031\n",
      "Training: epoch 14 batch 0 loss 0.07770579308271408\n",
      "Training: epoch 14 batch 10 loss 0.074945367872715\n",
      "Training: epoch 14 batch 20 loss 0.08089430630207062\n",
      "Test: epoch 14 batch 0 loss 0.08075587451457977\n",
      "epoch 14 finished - avarage train loss 0.07580773044249108  avarage test loss 0.08795136399567127\n",
      "Training: epoch 15 batch 0 loss 0.04956676810979843\n",
      "Training: epoch 15 batch 10 loss 0.08719395101070404\n",
      "Training: epoch 15 batch 20 loss 0.0722443088889122\n",
      "Test: epoch 15 batch 0 loss 0.07180394977331161\n",
      "epoch 15 finished - avarage train loss 0.06858248314980803  avarage test loss 0.07320026122033596\n",
      "Training: epoch 16 batch 0 loss 0.08112604171037674\n",
      "Training: epoch 16 batch 10 loss 0.07287780195474625\n",
      "Training: epoch 16 batch 20 loss 0.06096755713224411\n",
      "Test: epoch 16 batch 0 loss 0.06696648895740509\n",
      "epoch 16 finished - avarage train loss 0.06636553899995212  avarage test loss 0.06783895939588547\n",
      "Training: epoch 17 batch 0 loss 0.0659312903881073\n",
      "Training: epoch 17 batch 10 loss 0.045034557580947876\n",
      "Training: epoch 17 batch 20 loss 0.034257132560014725\n",
      "Test: epoch 17 batch 0 loss 0.042277876287698746\n",
      "epoch 17 finished - avarage train loss 0.04820737037165412  avarage test loss 0.05098536517471075\n",
      "Training: epoch 18 batch 0 loss 0.040833376348018646\n",
      "Training: epoch 18 batch 10 loss 0.027852166444063187\n",
      "Training: epoch 18 batch 20 loss 0.048024971038103104\n",
      "Test: epoch 18 batch 0 loss 0.03866160660982132\n",
      "epoch 18 finished - avarage train loss 0.03652641273521144  avarage test loss 0.046868802048265934\n",
      "Training: epoch 19 batch 0 loss 0.022148264572024345\n",
      "Training: epoch 19 batch 10 loss 0.019320949912071228\n",
      "Training: epoch 19 batch 20 loss 0.031016014516353607\n",
      "Test: epoch 19 batch 0 loss 0.03658401221036911\n",
      "epoch 19 finished - avarage train loss 0.030122617966142195  avarage test loss 0.0467206509783864\n",
      "Training: epoch 20 batch 0 loss 0.03196551650762558\n",
      "Training: epoch 20 batch 10 loss 0.017435641959309578\n",
      "Training: epoch 20 batch 20 loss 0.0249521154910326\n",
      "Test: epoch 20 batch 0 loss 0.032729193568229675\n",
      "epoch 20 finished - avarage train loss 0.023465617270819073  avarage test loss 0.03862045053392649\n",
      "Training: epoch 21 batch 0 loss 0.022514743730425835\n",
      "Training: epoch 21 batch 10 loss 0.013104095123708248\n",
      "Training: epoch 21 batch 20 loss 0.012457421980798244\n",
      "Test: epoch 21 batch 0 loss 0.026612579822540283\n",
      "epoch 21 finished - avarage train loss 0.019636150064139532  avarage test loss 0.03489338746294379\n",
      "Training: epoch 22 batch 0 loss 0.011499673128128052\n",
      "Training: epoch 22 batch 10 loss 0.010697684250772\n",
      "Training: epoch 22 batch 20 loss 0.012303064577281475\n",
      "Test: epoch 22 batch 0 loss 0.02403140999376774\n",
      "epoch 22 finished - avarage train loss 0.015294031288217881  avarage test loss 0.03347376175224781\n",
      "Training: epoch 23 batch 0 loss 0.01200888678431511\n",
      "Training: epoch 23 batch 10 loss 0.015113918110728264\n",
      "Training: epoch 23 batch 20 loss 0.015174567699432373\n",
      "Test: epoch 23 batch 0 loss 0.027485664933919907\n",
      "epoch 23 finished - avarage train loss 0.015468236861814713  avarage test loss 0.03389350697398186\n",
      "Training: epoch 24 batch 0 loss 0.014447701163589954\n",
      "Training: epoch 24 batch 10 loss 0.01742587424814701\n",
      "Training: epoch 24 batch 20 loss 0.011066196486353874\n",
      "Test: epoch 24 batch 0 loss 0.015900930389761925\n",
      "epoch 24 finished - avarage train loss 0.015344186477234652  avarage test loss 0.027225639671087265\n",
      "Training: epoch 25 batch 0 loss 0.010223114863038063\n",
      "Training: epoch 25 batch 10 loss 0.016882985830307007\n",
      "Training: epoch 25 batch 20 loss 0.017805559560656548\n",
      "Test: epoch 25 batch 0 loss 0.020103543996810913\n",
      "epoch 25 finished - avarage train loss 0.014149001253576115  avarage test loss 0.02921558520756662\n",
      "Training: epoch 26 batch 0 loss 0.009214070625603199\n",
      "Training: epoch 26 batch 10 loss 0.011496847495436668\n",
      "Training: epoch 26 batch 20 loss 0.016052676364779472\n",
      "Test: epoch 26 batch 0 loss 0.012942680157721043\n",
      "epoch 26 finished - avarage train loss 0.014468922713321859  avarage test loss 0.020271381828933954\n",
      "Training: epoch 27 batch 0 loss 0.017885573208332062\n",
      "Training: epoch 27 batch 10 loss 0.01492287963628769\n",
      "Training: epoch 27 batch 20 loss 0.010915013961493969\n",
      "Test: epoch 27 batch 0 loss 0.018112819641828537\n",
      "epoch 27 finished - avarage train loss 0.01398343867461743  avarage test loss 0.028536304831504822\n",
      "Training: epoch 28 batch 0 loss 0.008791874162852764\n",
      "Training: epoch 28 batch 10 loss 0.01869993284344673\n",
      "Training: epoch 28 batch 20 loss 0.01539316400885582\n",
      "Test: epoch 28 batch 0 loss 0.015548180788755417\n",
      "epoch 28 finished - avarage train loss 0.014973123686323905  avarage test loss 0.02582023898139596\n",
      "Training: epoch 29 batch 0 loss 0.016643576323986053\n",
      "Training: epoch 29 batch 10 loss 0.013125103898346424\n",
      "Training: epoch 29 batch 20 loss 0.013217325322329998\n",
      "Test: epoch 29 batch 0 loss 0.01339134480804205\n",
      "epoch 29 finished - avarage train loss 0.014402483397259795  avarage test loss 0.02426292421296239\n",
      "Training: epoch 30 batch 0 loss 0.005913286469876766\n",
      "Training: epoch 30 batch 10 loss 0.007092493586242199\n",
      "Training: epoch 30 batch 20 loss 0.009391313418745995\n",
      "Test: epoch 30 batch 0 loss 0.01596924476325512\n",
      "epoch 30 finished - avarage train loss 0.013352893967309902  avarage test loss 0.02601597341708839\n",
      "Training: epoch 31 batch 0 loss 0.01012556254863739\n",
      "Training: epoch 31 batch 10 loss 0.021979430690407753\n",
      "Training: epoch 31 batch 20 loss 0.010463718324899673\n",
      "Test: epoch 31 batch 0 loss 0.01476799976080656\n",
      "epoch 31 finished - avarage train loss 0.01448196834274407  avarage test loss 0.02604054124094546\n",
      "Training: epoch 32 batch 0 loss 0.010781541466712952\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training: epoch 32 batch 10 loss 0.00876816175878048\n",
      "Training: epoch 32 batch 20 loss 0.011997046880424023\n",
      "Test: epoch 32 batch 0 loss 0.012899313122034073\n",
      "epoch 32 finished - avarage train loss 0.013449061934932553  avarage test loss 0.02091364236548543\n",
      "Training: epoch 33 batch 0 loss 0.014435457065701485\n",
      "Training: epoch 33 batch 10 loss 0.010801115073263645\n",
      "Training: epoch 33 batch 20 loss 0.00979068037122488\n",
      "Test: epoch 33 batch 0 loss 0.014872278086841106\n",
      "epoch 33 finished - avarage train loss 0.012320584456982284  avarage test loss 0.025457632029429078\n",
      "Training: epoch 34 batch 0 loss 0.008628192357718945\n",
      "Training: epoch 34 batch 10 loss 0.008606025949120522\n",
      "Training: epoch 34 batch 20 loss 0.010965699329972267\n",
      "Test: epoch 34 batch 0 loss 0.013151698745787144\n",
      "epoch 34 finished - avarage train loss 0.012909993208174047  avarage test loss 0.023555072490125895\n",
      "Training: epoch 35 batch 0 loss 0.01163430791348219\n",
      "Training: epoch 35 batch 10 loss 0.00820604432374239\n",
      "Training: epoch 35 batch 20 loss 0.009078842587769032\n",
      "Test: epoch 35 batch 0 loss 0.011933314613997936\n",
      "epoch 35 finished - avarage train loss 0.011632404225910532  avarage test loss 0.020463584922254086\n",
      "Training: epoch 36 batch 0 loss 0.014714912511408329\n",
      "Training: epoch 36 batch 10 loss 0.010110491886734962\n",
      "Training: epoch 36 batch 20 loss 0.005154433194547892\n",
      "Test: epoch 36 batch 0 loss 0.011471264064311981\n",
      "epoch 36 finished - avarage train loss 0.013428747445216467  avarage test loss 0.018707892391830683\n",
      "Training: epoch 37 batch 0 loss 0.01624557003378868\n",
      "Training: epoch 37 batch 10 loss 0.014531546272337437\n",
      "Training: epoch 37 batch 20 loss 0.007310771849006414\n",
      "Test: epoch 37 batch 0 loss 0.01413350272923708\n",
      "epoch 37 finished - avarage train loss 0.013689706374987447  avarage test loss 0.02387540810741484\n",
      "Training: epoch 38 batch 0 loss 0.011366474442183971\n",
      "Training: epoch 38 batch 10 loss 0.016765454784035683\n",
      "Training: epoch 38 batch 20 loss 0.02768983133137226\n",
      "Test: epoch 38 batch 0 loss 0.010761218145489693\n",
      "epoch 38 finished - avarage train loss 0.013575553059064108  avarage test loss 0.018374185310676694\n",
      "Training: epoch 39 batch 0 loss 0.012214768677949905\n",
      "Training: epoch 39 batch 10 loss 0.012764726765453815\n",
      "Training: epoch 39 batch 20 loss 0.004825727082788944\n",
      "Test: epoch 39 batch 0 loss 0.010636935010552406\n",
      "epoch 39 finished - avarage train loss 0.011503591443058746  avarage test loss 0.018395613646134734\n",
      "Training: epoch 40 batch 0 loss 0.017620379105210304\n",
      "Training: epoch 40 batch 10 loss 0.009657799266278744\n",
      "Training: epoch 40 batch 20 loss 0.013165974989533424\n",
      "Test: epoch 40 batch 0 loss 0.010560011491179466\n",
      "epoch 40 finished - avarage train loss 0.010853499190175328  avarage test loss 0.01944569416809827\n",
      "Training: epoch 41 batch 0 loss 0.012641381472349167\n",
      "Training: epoch 41 batch 10 loss 0.00483681121841073\n",
      "Training: epoch 41 batch 20 loss 0.01634128950536251\n",
      "Test: epoch 41 batch 0 loss 0.010189782828092575\n",
      "epoch 41 finished - avarage train loss 0.010786665956778773  avarage test loss 0.018384070601314306\n",
      "Training: epoch 42 batch 0 loss 0.007332814857363701\n",
      "Training: epoch 42 batch 10 loss 0.01659911870956421\n",
      "Training: epoch 42 batch 20 loss 0.006556617096066475\n",
      "Test: epoch 42 batch 0 loss 0.00912125501781702\n",
      "epoch 42 finished - avarage train loss 0.010451388922680554  avarage test loss 0.016607950208708644\n",
      "Training: epoch 43 batch 0 loss 0.009617341682314873\n",
      "Training: epoch 43 batch 10 loss 0.005722647998481989\n",
      "Training: epoch 43 batch 20 loss 0.012503094971179962\n",
      "Test: epoch 43 batch 0 loss 0.009214715100824833\n",
      "epoch 43 finished - avarage train loss 0.010906868373397095  avarage test loss 0.016761290258727968\n",
      "Training: epoch 44 batch 0 loss 0.007989614270627499\n",
      "Training: epoch 44 batch 10 loss 0.01201761607080698\n",
      "Training: epoch 44 batch 20 loss 0.006499037146568298\n",
      "Test: epoch 44 batch 0 loss 0.010687315836548805\n",
      "epoch 44 finished - avarage train loss 0.010561696109201374  avarage test loss 0.019274611491709948\n",
      "Training: epoch 45 batch 0 loss 0.009981371462345123\n",
      "Training: epoch 45 batch 10 loss 0.013265389949083328\n",
      "Training: epoch 45 batch 20 loss 0.010133837349712849\n",
      "Test: epoch 45 batch 0 loss 0.0072917393408715725\n",
      "epoch 45 finished - avarage train loss 0.009861087322170878  avarage test loss 0.014262737007811666\n",
      "Training: epoch 46 batch 0 loss 0.014382727444171906\n",
      "Training: epoch 46 batch 10 loss 0.004004897084087133\n",
      "Training: epoch 46 batch 20 loss 0.01135411486029625\n",
      "Test: epoch 46 batch 0 loss 0.00831475481390953\n",
      "epoch 46 finished - avarage train loss 0.009118283986402997  avarage test loss 0.015813650097697973\n",
      "Training: epoch 47 batch 0 loss 0.014699770137667656\n",
      "Training: epoch 47 batch 10 loss 0.0039092968218028545\n",
      "Training: epoch 47 batch 20 loss 0.009107732214033604\n",
      "Test: epoch 47 batch 0 loss 0.006867579650133848\n",
      "epoch 47 finished - avarage train loss 0.009456153461260015  avarage test loss 0.01434037578292191\n",
      "Training: epoch 48 batch 0 loss 0.008065941743552685\n",
      "Training: epoch 48 batch 10 loss 0.005976961459964514\n",
      "Training: epoch 48 batch 20 loss 0.00783189944922924\n",
      "Test: epoch 48 batch 0 loss 0.008604519069194794\n",
      "epoch 48 finished - avarage train loss 0.009123971131790814  avarage test loss 0.016816618270240724\n",
      "Training: epoch 49 batch 0 loss 0.008350587449967861\n",
      "Training: epoch 49 batch 10 loss 0.00497647188603878\n",
      "Training: epoch 49 batch 20 loss 0.004802966024726629\n",
      "Test: epoch 49 batch 0 loss 0.01224823109805584\n",
      "epoch 49 finished - avarage train loss 0.00987867453809956  avarage test loss 0.018564300378784537\n",
      "Training: epoch 50 batch 0 loss 0.010464554652571678\n",
      "Training: epoch 50 batch 10 loss 0.020282825455069542\n",
      "Training: epoch 50 batch 20 loss 0.009586349129676819\n",
      "Test: epoch 50 batch 0 loss 0.008620125241577625\n",
      "epoch 50 finished - avarage train loss 0.013840892593023079  avarage test loss 0.01831746520474553\n",
      "Training: epoch 51 batch 0 loss 0.008765372447669506\n",
      "Training: epoch 51 batch 10 loss 0.007425693329423666\n",
      "Training: epoch 51 batch 20 loss 0.009377839975059032\n",
      "Test: epoch 51 batch 0 loss 0.005662646144628525\n",
      "epoch 51 finished - avarage train loss 0.008852075078877909  avarage test loss 0.013620787533000112\n",
      "Training: epoch 52 batch 0 loss 0.005016411188989878\n",
      "Training: epoch 52 batch 10 loss 0.006913039367645979\n",
      "Training: epoch 52 batch 20 loss 0.011012589558959007\n",
      "Test: epoch 52 batch 0 loss 0.009183109737932682\n",
      "epoch 52 finished - avarage train loss 0.00931584196775381  avarage test loss 0.018303236225619912\n",
      "Training: epoch 53 batch 0 loss 0.009667589329183102\n",
      "Training: epoch 53 batch 10 loss 0.006500256713479757\n",
      "Training: epoch 53 batch 20 loss 0.009257680736482143\n",
      "Test: epoch 53 batch 0 loss 0.005410578101873398\n",
      "epoch 53 finished - avarage train loss 0.010042432543082997  avarage test loss 0.014461909886449575\n",
      "Training: epoch 54 batch 0 loss 0.006178266368806362\n",
      "Training: epoch 54 batch 10 loss 0.010637775994837284\n",
      "Training: epoch 54 batch 20 loss 0.006858662236481905\n",
      "Test: epoch 54 batch 0 loss 0.005782273598015308\n",
      "epoch 54 finished - avarage train loss 0.0086165150017317  avarage test loss 0.013627556909341365\n",
      "Training: epoch 55 batch 0 loss 0.00734753767028451\n",
      "Training: epoch 55 batch 10 loss 0.013627049513161182\n",
      "Training: epoch 55 batch 20 loss 0.006345429457724094\n",
      "Test: epoch 55 batch 0 loss 0.00700657581910491\n",
      "epoch 55 finished - avarage train loss 0.010704941654192477  avarage test loss 0.016348659875802696\n",
      "Training: epoch 56 batch 0 loss 0.008696152828633785\n",
      "Training: epoch 56 batch 10 loss 0.0062921661883592606\n",
      "Training: epoch 56 batch 20 loss 0.008613037876784801\n",
      "Test: epoch 56 batch 0 loss 0.005704541690647602\n",
      "epoch 56 finished - avarage train loss 0.00861639487717686  avarage test loss 0.014158651581965387\n",
      "Training: epoch 57 batch 0 loss 0.0044627138413488865\n",
      "Training: epoch 57 batch 10 loss 0.0077257598750293255\n",
      "Training: epoch 57 batch 20 loss 0.00847997423261404\n",
      "Test: epoch 57 batch 0 loss 0.005778898950666189\n",
      "epoch 57 finished - avarage train loss 0.009591804512230486  avarage test loss 0.014651553297881037\n",
      "Training: epoch 58 batch 0 loss 0.005024665035307407\n",
      "Training: epoch 58 batch 10 loss 0.0048256078734993935\n",
      "Training: epoch 58 batch 20 loss 0.008121824823319912\n",
      "Test: epoch 58 batch 0 loss 0.005854728166013956\n",
      "epoch 58 finished - avarage train loss 0.00878663825128099  avarage test loss 0.014716404490172863\n",
      "Training: epoch 59 batch 0 loss 0.010425487533211708\n",
      "Training: epoch 59 batch 10 loss 0.008016673848032951\n",
      "Training: epoch 59 batch 20 loss 0.006010241806507111\n",
      "Test: epoch 59 batch 0 loss 0.006299872417002916\n",
      "epoch 59 finished - avarage train loss 0.008853466755806887  avarage test loss 0.013931769761256874\n",
      "Training: epoch 60 batch 0 loss 0.00850286241620779\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training: epoch 60 batch 10 loss 0.00880602840334177\n",
      "Training: epoch 60 batch 20 loss 0.007348141632974148\n",
      "Test: epoch 60 batch 0 loss 0.00961236096918583\n",
      "epoch 60 finished - avarage train loss 0.010612014199382272  avarage test loss 0.01807018695399165\n",
      "Training: epoch 61 batch 0 loss 0.006876882631331682\n",
      "Training: epoch 61 batch 10 loss 0.007274618372321129\n",
      "Training: epoch 61 batch 20 loss 0.0076599265448749065\n",
      "Test: epoch 61 batch 0 loss 0.007901075296103954\n",
      "epoch 61 finished - avarage train loss 0.01005411751825234  avarage test loss 0.01620358065702021\n",
      "Training: epoch 62 batch 0 loss 0.01000460609793663\n",
      "Training: epoch 62 batch 10 loss 0.009966392070055008\n",
      "Training: epoch 62 batch 20 loss 0.01037856750190258\n",
      "Test: epoch 62 batch 0 loss 0.005381246563047171\n",
      "epoch 62 finished - avarage train loss 0.010430873355603424  avarage test loss 0.013568829745054245\n",
      "Training: epoch 63 batch 0 loss 0.003987302538007498\n",
      "Training: epoch 63 batch 10 loss 0.012262914329767227\n",
      "Training: epoch 63 batch 20 loss 0.005178958177566528\n",
      "Test: epoch 63 batch 0 loss 0.00498470151796937\n",
      "epoch 63 finished - avarage train loss 0.008298533502966166  avarage test loss 0.013661309785675257\n",
      "Training: epoch 64 batch 0 loss 0.00743041792884469\n",
      "Training: epoch 64 batch 10 loss 0.01003500446677208\n",
      "Training: epoch 64 batch 20 loss 0.010191948153078556\n",
      "Test: epoch 64 batch 0 loss 0.005431922152638435\n",
      "epoch 64 finished - avarage train loss 0.007765296172222187  avarage test loss 0.013780246372334659\n",
      "Training: epoch 65 batch 0 loss 0.010981947183609009\n",
      "Training: epoch 65 batch 10 loss 0.005135095678269863\n",
      "Training: epoch 65 batch 20 loss 0.004747498780488968\n",
      "Test: epoch 65 batch 0 loss 0.005464788526296616\n",
      "epoch 65 finished - avarage train loss 0.008634228644699886  avarage test loss 0.013122864766046405\n",
      "Training: epoch 66 batch 0 loss 0.007185119669884443\n",
      "Training: epoch 66 batch 10 loss 0.00485031446442008\n",
      "Training: epoch 66 batch 20 loss 0.0075187343172729015\n",
      "Test: epoch 66 batch 0 loss 0.005289963446557522\n",
      "epoch 66 finished - avarage train loss 0.00813860204552525  avarage test loss 0.013754580984823406\n",
      "Training: epoch 67 batch 0 loss 0.01634204015135765\n",
      "Training: epoch 67 batch 10 loss 0.004805212374776602\n",
      "Training: epoch 67 batch 20 loss 0.006073622964322567\n",
      "Test: epoch 67 batch 0 loss 0.00525279063731432\n",
      "epoch 67 finished - avarage train loss 0.009199156289406377  avarage test loss 0.013124251330737025\n",
      "Training: epoch 68 batch 0 loss 0.004214601591229439\n",
      "Training: epoch 68 batch 10 loss 0.005395534448325634\n",
      "Training: epoch 68 batch 20 loss 0.009664108976721764\n",
      "Test: epoch 68 batch 0 loss 0.00528539065271616\n",
      "epoch 68 finished - avarage train loss 0.008367497522127012  avarage test loss 0.0131775758927688\n",
      "Training: epoch 69 batch 0 loss 0.007217139936983585\n",
      "Training: epoch 69 batch 10 loss 0.005724905990064144\n",
      "Training: epoch 69 batch 20 loss 0.0041227685287594795\n",
      "Test: epoch 69 batch 0 loss 0.005115688778460026\n",
      "epoch 69 finished - avarage train loss 0.007689156296566643  avarage test loss 0.013205579831264913\n",
      "Training: epoch 70 batch 0 loss 0.005555527750402689\n",
      "Training: epoch 70 batch 10 loss 0.003986705094575882\n",
      "Training: epoch 70 batch 20 loss 0.005829979665577412\n",
      "Test: epoch 70 batch 0 loss 0.006285971496254206\n",
      "epoch 70 finished - avarage train loss 0.007006370205560635  avarage test loss 0.014081662287935615\n",
      "Training: epoch 71 batch 0 loss 0.012232367880642414\n",
      "Training: epoch 71 batch 10 loss 0.011102729476988316\n",
      "Training: epoch 71 batch 20 loss 0.004199679475277662\n",
      "Test: epoch 71 batch 0 loss 0.005166793242096901\n",
      "epoch 71 finished - avarage train loss 0.009543871884394822  avarage test loss 0.01334439089987427\n",
      "Training: epoch 72 batch 0 loss 0.010966404341161251\n",
      "Training: epoch 72 batch 10 loss 0.0040887403301894665\n",
      "Training: epoch 72 batch 20 loss 0.006931097712367773\n",
      "Test: epoch 72 batch 0 loss 0.005625403951853514\n",
      "epoch 72 finished - avarage train loss 0.008017238638973955  avarage test loss 0.013325646053999662\n",
      "Training: epoch 73 batch 0 loss 0.004398919176310301\n",
      "Training: epoch 73 batch 10 loss 0.006042227149009705\n",
      "Training: epoch 73 batch 20 loss 0.004522758070379496\n",
      "Test: epoch 73 batch 0 loss 0.005637400317937136\n",
      "epoch 73 finished - avarage train loss 0.008671594727463249  avarage test loss 0.01299576583551243\n",
      "Training: epoch 74 batch 0 loss 0.0064760539680719376\n",
      "Training: epoch 74 batch 10 loss 0.008014869876205921\n",
      "Training: epoch 74 batch 20 loss 0.006847066339105368\n",
      "Test: epoch 74 batch 0 loss 0.0062645599246025085\n",
      "epoch 74 finished - avarage train loss 0.008264335097166998  avarage test loss 0.013875331089366227\n",
      "Training: epoch 75 batch 0 loss 0.011533273383975029\n",
      "Training: epoch 75 batch 10 loss 0.005685963202267885\n",
      "Training: epoch 75 batch 20 loss 0.005026238039135933\n",
      "Test: epoch 75 batch 0 loss 0.007135243155062199\n",
      "epoch 75 finished - avarage train loss 0.008668687780676731  avarage test loss 0.01560786901973188\n",
      "Training: epoch 76 batch 0 loss 0.007841195911169052\n",
      "Training: epoch 76 batch 10 loss 0.010793328285217285\n",
      "Training: epoch 76 batch 20 loss 0.0064594498835504055\n",
      "Test: epoch 76 batch 0 loss 0.008476536720991135\n",
      "epoch 76 finished - avarage train loss 0.011406145187030578  avarage test loss 0.01742603606544435\n",
      "Training: epoch 77 batch 0 loss 0.0038855508901178837\n",
      "Training: epoch 77 batch 10 loss 0.006247340235859156\n",
      "Training: epoch 77 batch 20 loss 0.008370542898774147\n",
      "Test: epoch 77 batch 0 loss 0.006576193496584892\n",
      "epoch 77 finished - avarage train loss 0.009958347562572053  avarage test loss 0.01625637151300907\n",
      "Training: epoch 78 batch 0 loss 0.006012044847011566\n",
      "Training: epoch 78 batch 10 loss 0.005372868850827217\n",
      "Training: epoch 78 batch 20 loss 0.008467369712889194\n",
      "Test: epoch 78 batch 0 loss 0.005151272751390934\n",
      "epoch 78 finished - avarage train loss 0.01000567219733935  avarage test loss 0.013425262761302292\n",
      "Training: epoch 79 batch 0 loss 0.006635028403252363\n",
      "Training: epoch 79 batch 10 loss 0.01024792529642582\n",
      "Training: epoch 79 batch 20 loss 0.00522339902818203\n",
      "Test: epoch 79 batch 0 loss 0.006389060523360968\n",
      "epoch 79 finished - avarage train loss 0.009008204805311459  avarage test loss 0.01413511869031936\n",
      "Training: epoch 80 batch 0 loss 0.009610630571842194\n",
      "Training: epoch 80 batch 10 loss 0.006890771444886923\n",
      "Training: epoch 80 batch 20 loss 0.012454506941139698\n",
      "Test: epoch 80 batch 0 loss 0.005971367005258799\n",
      "epoch 80 finished - avarage train loss 0.009007059920836112  avarage test loss 0.013541828433517367\n",
      "Training: epoch 81 batch 0 loss 0.008202445693314075\n",
      "Training: epoch 81 batch 10 loss 0.005037448834627867\n",
      "Training: epoch 81 batch 20 loss 0.006174637470394373\n",
      "Test: epoch 81 batch 0 loss 0.005286186467856169\n",
      "epoch 81 finished - avarage train loss 0.007788752438500524  avarage test loss 0.014138868427835405\n",
      "Training: epoch 82 batch 0 loss 0.006579258944839239\n",
      "Training: epoch 82 batch 10 loss 0.004722724203020334\n",
      "Training: epoch 82 batch 20 loss 0.007548369467258453\n",
      "Test: epoch 82 batch 0 loss 0.0057353004813194275\n",
      "epoch 82 finished - avarage train loss 0.009181954865825587  avarage test loss 0.015742761665023863\n",
      "Training: epoch 83 batch 0 loss 0.005580467637628317\n",
      "Training: epoch 83 batch 10 loss 0.006193390116095543\n",
      "Training: epoch 83 batch 20 loss 0.00472855381667614\n",
      "Test: epoch 83 batch 0 loss 0.005931215360760689\n",
      "epoch 83 finished - avarage train loss 0.007627691688208744  avarage test loss 0.013752166763879359\n",
      "Training: epoch 84 batch 0 loss 0.006327546201646328\n",
      "Training: epoch 84 batch 10 loss 0.0073989154770970345\n",
      "Training: epoch 84 batch 20 loss 0.00488545186817646\n",
      "Test: epoch 84 batch 0 loss 0.007909540086984634\n",
      "epoch 84 finished - avarage train loss 0.008335524110187745  avarage test loss 0.015047147287987173\n",
      "Training: epoch 85 batch 0 loss 0.009886650368571281\n",
      "Training: epoch 85 batch 10 loss 0.009302917867898941\n",
      "Training: epoch 85 batch 20 loss 0.0025791116058826447\n",
      "Test: epoch 85 batch 0 loss 0.006180962081998587\n",
      "epoch 85 finished - avarage train loss 0.01036217614430292  avarage test loss 0.015338625758886337\n",
      "Training: epoch 86 batch 0 loss 0.006660600658506155\n",
      "Training: epoch 86 batch 10 loss 0.007803203072398901\n",
      "Training: epoch 86 batch 20 loss 0.005202203523367643\n",
      "Test: epoch 86 batch 0 loss 0.006150461733341217\n",
      "epoch 86 finished - avarage train loss 0.008507569954762685  avarage test loss 0.013889486668631434\n",
      "Training: epoch 87 batch 0 loss 0.010202508419752121\n",
      "Training: epoch 87 batch 10 loss 0.006242642644792795\n",
      "Training: epoch 87 batch 20 loss 0.010878672823309898\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test: epoch 87 batch 0 loss 0.005659697111696005\n",
      "epoch 87 finished - avarage train loss 0.008864105309777218  avarage test loss 0.013345380953978747\n",
      "Training: epoch 88 batch 0 loss 0.0035362732596695423\n",
      "Training: epoch 88 batch 10 loss 0.007491412106901407\n",
      "Training: epoch 88 batch 20 loss 0.010883157141506672\n",
      "Test: epoch 88 batch 0 loss 0.006728276144713163\n",
      "epoch 88 finished - avarage train loss 0.00818753668813613  avarage test loss 0.014874721295200288\n",
      "Training: epoch 89 batch 0 loss 0.007442062254995108\n",
      "Training: epoch 89 batch 10 loss 0.010874868370592594\n",
      "Training: epoch 89 batch 20 loss 0.004844368901103735\n",
      "Test: epoch 89 batch 0 loss 0.008763700723648071\n",
      "epoch 89 finished - avarage train loss 0.00897439315529733  avarage test loss 0.01607813755981624\n",
      "Training: epoch 90 batch 0 loss 0.007913116365671158\n",
      "Training: epoch 90 batch 10 loss 0.006114011164754629\n",
      "Training: epoch 90 batch 20 loss 0.011531553231179714\n",
      "Test: epoch 90 batch 0 loss 0.0059090834110975266\n",
      "epoch 90 finished - avarage train loss 0.009120600819908854  avarage test loss 0.013758499000687152\n",
      "Training: epoch 91 batch 0 loss 0.0039635999128222466\n",
      "Training: epoch 91 batch 10 loss 0.004230246879160404\n",
      "Training: epoch 91 batch 20 loss 0.005409654229879379\n",
      "Test: epoch 91 batch 0 loss 0.005836822558194399\n",
      "epoch 91 finished - avarage train loss 0.00902105087478613  avarage test loss 0.013769114622846246\n",
      "Training: epoch 92 batch 0 loss 0.00620253523811698\n",
      "Training: epoch 92 batch 10 loss 0.008190623484551907\n",
      "Training: epoch 92 batch 20 loss 0.004024456720799208\n",
      "Test: epoch 92 batch 0 loss 0.008443537168204784\n",
      "epoch 92 finished - avarage train loss 0.007749579533741906  avarage test loss 0.0160475701559335\n",
      "Training: epoch 93 batch 0 loss 0.006102649960666895\n",
      "Training: epoch 93 batch 10 loss 0.006252325605601072\n",
      "Training: epoch 93 batch 20 loss 0.004633451346307993\n",
      "Test: epoch 93 batch 0 loss 0.005660115275532007\n",
      "epoch 93 finished - avarage train loss 0.009498645960963491  avarage test loss 0.014276591944508255\n",
      "Training: epoch 94 batch 0 loss 0.005239682737737894\n",
      "Training: epoch 94 batch 10 loss 0.008143124170601368\n",
      "Training: epoch 94 batch 20 loss 0.003424588590860367\n",
      "Test: epoch 94 batch 0 loss 0.00675218366086483\n",
      "epoch 94 finished - avarage train loss 0.008686098510977524  avarage test loss 0.014584789285436273\n",
      "Training: epoch 95 batch 0 loss 0.005484153516590595\n",
      "Training: epoch 95 batch 10 loss 0.0048289671540260315\n",
      "Training: epoch 95 batch 20 loss 0.007312678266316652\n",
      "Test: epoch 95 batch 0 loss 0.006108150351792574\n",
      "epoch 95 finished - avarage train loss 0.007703850680062997  avarage test loss 0.013512515637557954\n",
      "Training: epoch 96 batch 0 loss 0.0050824666395783424\n",
      "Training: epoch 96 batch 10 loss 0.010979264043271542\n",
      "Training: epoch 96 batch 20 loss 0.005537107586860657\n",
      "Test: epoch 96 batch 0 loss 0.005364926066249609\n",
      "epoch 96 finished - avarage train loss 0.009805795800840032  avarage test loss 0.013369654829148203\n",
      "Training: epoch 97 batch 0 loss 0.00481833703815937\n",
      "Training: epoch 97 batch 10 loss 0.010086978785693645\n",
      "Training: epoch 97 batch 20 loss 0.00526907853782177\n",
      "Test: epoch 97 batch 0 loss 0.0051429723389446735\n",
      "epoch 97 finished - avarage train loss 0.008934621506466948  avarage test loss 0.013418088667094707\n",
      "Training: epoch 98 batch 0 loss 0.006620596628636122\n",
      "Training: epoch 98 batch 10 loss 0.013226683251559734\n",
      "Training: epoch 98 batch 20 loss 0.007124819327145815\n",
      "Test: epoch 98 batch 0 loss 0.00521711865440011\n",
      "epoch 98 finished - avarage train loss 0.007467833588477866  avarage test loss 0.013059782912023365\n",
      "Training: epoch 99 batch 0 loss 0.009277001954615116\n",
      "Training: epoch 99 batch 10 loss 0.004203512333333492\n",
      "Training: epoch 99 batch 20 loss 0.007029589731246233\n",
      "Test: epoch 99 batch 0 loss 0.005345263052731752\n",
      "epoch 99 finished - avarage train loss 0.007437641009816836  avarage test loss 0.013033214549068362\n",
      "Training: epoch 100 batch 0 loss 0.006979797501116991\n",
      "Training: epoch 100 batch 10 loss 0.006679206620901823\n",
      "Training: epoch 100 batch 20 loss 0.006311564240604639\n",
      "Test: epoch 100 batch 0 loss 0.007297281175851822\n",
      "epoch 100 finished - avarage train loss 0.008431251377573815  avarage test loss 0.014259003335610032\n",
      "Training: epoch 101 batch 0 loss 0.010157146491110325\n",
      "Training: epoch 101 batch 10 loss 0.012271125800907612\n",
      "Training: epoch 101 batch 20 loss 0.008846630342304707\n",
      "Test: epoch 101 batch 0 loss 0.00870763324201107\n",
      "epoch 101 finished - avarage train loss 0.009736815547763273  avarage test loss 0.01631299324799329\n",
      "Training: epoch 102 batch 0 loss 0.008055884391069412\n",
      "Training: epoch 102 batch 10 loss 0.006841864436864853\n",
      "Training: epoch 102 batch 20 loss 0.005303327459841967\n",
      "Test: epoch 102 batch 0 loss 0.005564992316067219\n",
      "epoch 102 finished - avarage train loss 0.008620982893325132  avarage test loss 0.013535208534449339\n",
      "Training: epoch 103 batch 0 loss 0.00732846325263381\n",
      "Training: epoch 103 batch 10 loss 0.004169814754277468\n",
      "Training: epoch 103 batch 20 loss 0.0057138726115226746\n",
      "Test: epoch 103 batch 0 loss 0.005870230030268431\n",
      "epoch 103 finished - avarage train loss 0.009014223011788624  avarage test loss 0.014132286130916327\n",
      "Training: epoch 104 batch 0 loss 0.0074155209586024284\n",
      "Training: epoch 104 batch 10 loss 0.004407781176269054\n",
      "Training: epoch 104 batch 20 loss 0.004428104031831026\n",
      "Test: epoch 104 batch 0 loss 0.006140135694295168\n",
      "epoch 104 finished - avarage train loss 0.00796740615322929  avarage test loss 0.014678945299237967\n",
      "Training: epoch 105 batch 0 loss 0.004504477139562368\n",
      "Training: epoch 105 batch 10 loss 0.005675940308719873\n",
      "Training: epoch 105 batch 20 loss 0.006776041351258755\n",
      "Test: epoch 105 batch 0 loss 0.005583556834608316\n",
      "epoch 105 finished - avarage train loss 0.008351964233764287  avarage test loss 0.014474089723080397\n",
      "Training: epoch 106 batch 0 loss 0.00619706604629755\n",
      "Training: epoch 106 batch 10 loss 0.007699739187955856\n",
      "Training: epoch 106 batch 20 loss 0.008944838307797909\n",
      "Test: epoch 106 batch 0 loss 0.006461924407631159\n",
      "epoch 106 finished - avarage train loss 0.008809762051457474  avarage test loss 0.013899138139095157\n",
      "Training: epoch 107 batch 0 loss 0.007772661745548248\n",
      "Training: epoch 107 batch 10 loss 0.0061124530620872974\n",
      "Training: epoch 107 batch 20 loss 0.012467265129089355\n",
      "Test: epoch 107 batch 0 loss 0.005528378766030073\n",
      "epoch 107 finished - avarage train loss 0.008353012381121516  avarage test loss 0.013851779978722334\n",
      "Training: epoch 108 batch 0 loss 0.008415056392550468\n",
      "Training: epoch 108 batch 10 loss 0.006086201872676611\n",
      "Training: epoch 108 batch 20 loss 0.006319865584373474\n",
      "Test: epoch 108 batch 0 loss 0.0065458002500236034\n",
      "epoch 108 finished - avarage train loss 0.007500793841057296  avarage test loss 0.014450397167820483\n",
      "Training: epoch 109 batch 0 loss 0.007032545283436775\n",
      "Training: epoch 109 batch 10 loss 0.006353197619318962\n",
      "Training: epoch 109 batch 20 loss 0.01018617209047079\n",
      "Test: epoch 109 batch 0 loss 0.007918289862573147\n",
      "epoch 109 finished - avarage train loss 0.008218589254879746  avarage test loss 0.01588848838582635\n",
      "Training: epoch 110 batch 0 loss 0.006129889748990536\n",
      "Training: epoch 110 batch 10 loss 0.007404663600027561\n",
      "Training: epoch 110 batch 20 loss 0.0058803558349609375\n",
      "Test: epoch 110 batch 0 loss 0.006087675224989653\n",
      "epoch 110 finished - avarage train loss 0.009033326236209992  avarage test loss 0.014238177274819463\n",
      "Training: epoch 111 batch 0 loss 0.006119434721767902\n",
      "Training: epoch 111 batch 10 loss 0.009096420370042324\n",
      "Training: epoch 111 batch 20 loss 0.0061429706402122974\n",
      "Test: epoch 111 batch 0 loss 0.006130153313279152\n",
      "epoch 111 finished - avarage train loss 0.008236691146960547  avarage test loss 0.013847233320120722\n",
      "Training: epoch 112 batch 0 loss 0.008307808078825474\n",
      "Training: epoch 112 batch 10 loss 0.012633740901947021\n",
      "Training: epoch 112 batch 20 loss 0.004845158662647009\n",
      "Test: epoch 112 batch 0 loss 0.005291684530675411\n",
      "epoch 112 finished - avarage train loss 0.007984067674662018  avarage test loss 0.013186371652409434\n",
      "Training: epoch 113 batch 0 loss 0.008081872947514057\n",
      "Training: epoch 113 batch 10 loss 0.004802955314517021\n",
      "Training: epoch 113 batch 20 loss 0.00599676975980401\n",
      "Test: epoch 113 batch 0 loss 0.047149840742349625\n",
      "epoch 113 finished - avarage train loss 0.0079872764422205  avarage test loss 0.04701767768710852\n",
      "Training: epoch 114 batch 0 loss 0.04550030454993248\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training: epoch 114 batch 10 loss 0.040883779525756836\n",
      "Training: epoch 114 batch 20 loss 0.017855824902653694\n",
      "Test: epoch 114 batch 0 loss 0.008723972365260124\n",
      "epoch 114 finished - avarage train loss 0.028170880090830654  avarage test loss 0.01629897637758404\n",
      "Training: epoch 115 batch 0 loss 0.009296417236328125\n",
      "Training: epoch 115 batch 10 loss 0.007104599382728338\n",
      "Training: epoch 115 batch 20 loss 0.006220731418579817\n",
      "Test: epoch 115 batch 0 loss 0.008389731869101524\n",
      "epoch 115 finished - avarage train loss 0.012118731802008275  avarage test loss 0.01539703638991341\n",
      "Training: epoch 116 batch 0 loss 0.008203944191336632\n",
      "Training: epoch 116 batch 10 loss 0.009349903091788292\n",
      "Training: epoch 116 batch 20 loss 0.009864326566457748\n",
      "Test: epoch 116 batch 0 loss 0.00684807263314724\n",
      "epoch 116 finished - avarage train loss 0.008873119083201063  avarage test loss 0.013915590126998723\n",
      "Training: epoch 117 batch 0 loss 0.009178723208606243\n",
      "Training: epoch 117 batch 10 loss 0.009937805123627186\n",
      "Training: epoch 117 batch 20 loss 0.0069506727159023285\n",
      "Test: epoch 117 batch 0 loss 0.006391524337232113\n",
      "epoch 117 finished - avarage train loss 0.009282035648758555  avarage test loss 0.014221567602362484\n",
      "Training: epoch 118 batch 0 loss 0.007742041256278753\n",
      "Training: epoch 118 batch 10 loss 0.014203904196619987\n",
      "Training: epoch 118 batch 20 loss 0.0056694913655519485\n",
      "Test: epoch 118 batch 0 loss 0.005900557152926922\n",
      "epoch 118 finished - avarage train loss 0.008323425234391772  avarage test loss 0.01389746624045074\n",
      "Training: epoch 119 batch 0 loss 0.017500974237918854\n",
      "Training: epoch 119 batch 10 loss 0.006238211877644062\n",
      "Training: epoch 119 batch 20 loss 0.008211608044803143\n",
      "Test: epoch 119 batch 0 loss 0.005433900747448206\n",
      "epoch 119 finished - avarage train loss 0.00789618940927602  avarage test loss 0.014205600251443684\n",
      "Training: epoch 120 batch 0 loss 0.005606666672974825\n",
      "Training: epoch 120 batch 10 loss 0.004091658163815737\n",
      "Training: epoch 120 batch 20 loss 0.015004327520728111\n",
      "Test: epoch 120 batch 0 loss 0.005117969587445259\n",
      "epoch 120 finished - avarage train loss 0.00883262553091707  avarage test loss 0.01348869880894199\n",
      "Training: epoch 121 batch 0 loss 0.006940709892660379\n",
      "Training: epoch 121 batch 10 loss 0.005256264936178923\n",
      "Training: epoch 121 batch 20 loss 0.006485262885689735\n",
      "Test: epoch 121 batch 0 loss 0.004692533984780312\n",
      "epoch 121 finished - avarage train loss 0.008625013737714496  avarage test loss 0.013349482556805015\n",
      "Training: epoch 122 batch 0 loss 0.013243908993899822\n",
      "Training: epoch 122 batch 10 loss 0.009596526622772217\n",
      "Training: epoch 122 batch 20 loss 0.014702805317938328\n",
      "Test: epoch 122 batch 0 loss 0.005565205588936806\n",
      "epoch 122 finished - avarage train loss 0.009559685685511294  avarage test loss 0.013557327794842422\n",
      "Training: epoch 123 batch 0 loss 0.004372225608676672\n",
      "Training: epoch 123 batch 10 loss 0.006912389770150185\n",
      "Training: epoch 123 batch 20 loss 0.0062783267349004745\n",
      "Test: epoch 123 batch 0 loss 0.005639663897454739\n",
      "epoch 123 finished - avarage train loss 0.007577526126185368  avarage test loss 0.015302305459044874\n",
      "Training: epoch 124 batch 0 loss 0.00633779214695096\n",
      "Training: epoch 124 batch 10 loss 0.009024834260344505\n",
      "Training: epoch 124 batch 20 loss 0.008840327151119709\n",
      "Test: epoch 124 batch 0 loss 0.006669311318546534\n",
      "epoch 124 finished - avarage train loss 0.008687639914067655  avarage test loss 0.01444978517247364\n",
      "Training: epoch 125 batch 0 loss 0.007519599981606007\n",
      "Training: epoch 125 batch 10 loss 0.007132640108466148\n",
      "Training: epoch 125 batch 20 loss 0.010797063820064068\n",
      "Test: epoch 125 batch 0 loss 0.006639826577156782\n",
      "epoch 125 finished - avarage train loss 0.007591527308626422  avarage test loss 0.013487698859535158\n",
      "Training: epoch 126 batch 0 loss 0.004758766852319241\n",
      "Training: epoch 126 batch 10 loss 0.01404140517115593\n",
      "Training: epoch 126 batch 20 loss 0.005233593285083771\n",
      "Test: epoch 126 batch 0 loss 0.0055879103019833565\n",
      "epoch 126 finished - avarage train loss 0.010350292468250826  avarage test loss 0.014181759324856102\n",
      "Training: epoch 127 batch 0 loss 0.00974759366363287\n",
      "Training: epoch 127 batch 10 loss 0.011335736140608788\n",
      "Training: epoch 127 batch 20 loss 0.0096435546875\n",
      "Test: epoch 127 batch 0 loss 0.0069910320453345776\n",
      "epoch 127 finished - avarage train loss 0.009163222612877345  avarage test loss 0.015173602849245071\n",
      "Training: epoch 128 batch 0 loss 0.008547679521143436\n",
      "Training: epoch 128 batch 10 loss 0.006570181343704462\n",
      "Training: epoch 128 batch 20 loss 0.0050023640505969524\n",
      "Test: epoch 128 batch 0 loss 0.005881514400243759\n",
      "epoch 128 finished - avarage train loss 0.009183524537767315  avarage test loss 0.013581170176621526\n",
      "Training: epoch 129 batch 0 loss 0.005880140699446201\n",
      "Training: epoch 129 batch 10 loss 0.0054857018403708935\n",
      "Training: epoch 129 batch 20 loss 0.004162430297583342\n",
      "Test: epoch 129 batch 0 loss 0.006119769997894764\n",
      "epoch 129 finished - avarage train loss 0.007800056245820275  avarage test loss 0.01326182612683624\n",
      "Training: epoch 130 batch 0 loss 0.004720992874354124\n",
      "Training: epoch 130 batch 10 loss 0.007303293794393539\n",
      "Training: epoch 130 batch 20 loss 0.005263140425086021\n",
      "Test: epoch 130 batch 0 loss 0.005821533035486937\n",
      "epoch 130 finished - avarage train loss 0.009615110337798452  avarage test loss 0.014650266733951867\n",
      "Training: epoch 131 batch 0 loss 0.0033764648251235485\n",
      "Training: epoch 131 batch 10 loss 0.005994408391416073\n",
      "Training: epoch 131 batch 20 loss 0.005180812906473875\n",
      "Test: epoch 131 batch 0 loss 0.007343744859099388\n",
      "epoch 131 finished - avarage train loss 0.008882486807375118  avarage test loss 0.016108575859107077\n",
      "Training: epoch 132 batch 0 loss 0.013916108757257462\n",
      "Training: epoch 132 batch 10 loss 0.012404228560626507\n",
      "Training: epoch 132 batch 20 loss 0.008127843029797077\n",
      "Test: epoch 132 batch 0 loss 0.006187342572957277\n",
      "epoch 132 finished - avarage train loss 0.010163594727757675  avarage test loss 0.013895659940317273\n",
      "Training: epoch 133 batch 0 loss 0.007112252060323954\n",
      "Training: epoch 133 batch 10 loss 0.0068952832370996475\n",
      "Training: epoch 133 batch 20 loss 0.005336175672709942\n",
      "Test: epoch 133 batch 0 loss 0.006140298210084438\n",
      "epoch 133 finished - avarage train loss 0.007180739876589385  avarage test loss 0.014875159366056323\n",
      "Training: epoch 134 batch 0 loss 0.004626620095223188\n",
      "Training: epoch 134 batch 10 loss 0.010224775411188602\n",
      "Training: epoch 134 batch 20 loss 0.005927486345171928\n",
      "Test: epoch 134 batch 0 loss 0.005694325547665358\n",
      "epoch 134 finished - avarage train loss 0.008345122384870875  avarage test loss 0.013207015348598361\n",
      "Training: epoch 135 batch 0 loss 0.004035064950585365\n",
      "Training: epoch 135 batch 10 loss 0.007571588270366192\n",
      "Training: epoch 135 batch 20 loss 0.008017621003091335\n",
      "Test: epoch 135 batch 0 loss 0.00567314587533474\n",
      "epoch 135 finished - avarage train loss 0.009260850637380419  avarage test loss 0.014086987357586622\n",
      "Training: epoch 136 batch 0 loss 0.006744439713656902\n",
      "Training: epoch 136 batch 10 loss 0.00814257562160492\n",
      "Training: epoch 136 batch 20 loss 0.008850363083183765\n",
      "Test: epoch 136 batch 0 loss 0.005734636913985014\n",
      "epoch 136 finished - avarage train loss 0.007899580651829982  avarage test loss 0.013361066929064691\n",
      "Training: epoch 137 batch 0 loss 0.0064076269045472145\n",
      "Training: epoch 137 batch 10 loss 0.012506619095802307\n",
      "Training: epoch 137 batch 20 loss 0.005379103124141693\n",
      "Test: epoch 137 batch 0 loss 0.005909276660531759\n",
      "epoch 137 finished - avarage train loss 0.008915865087303621  avarage test loss 0.013871390896383673\n",
      "Training: epoch 138 batch 0 loss 0.011846175417304039\n",
      "Training: epoch 138 batch 10 loss 0.00708752078935504\n",
      "Training: epoch 138 batch 20 loss 0.002865478163585067\n",
      "Test: epoch 138 batch 0 loss 0.005432518664747477\n",
      "epoch 138 finished - avarage train loss 0.009397516982501438  avarage test loss 0.013655440823640674\n",
      "Training: epoch 139 batch 0 loss 0.004794483073055744\n",
      "Training: epoch 139 batch 10 loss 0.006281416863203049\n",
      "Training: epoch 139 batch 20 loss 0.011347746476531029\n",
      "Test: epoch 139 batch 0 loss 0.005090736318379641\n",
      "epoch 139 finished - avarage train loss 0.008464833721518517  avarage test loss 0.013308031193446368\n",
      "Training: epoch 140 batch 0 loss 0.007607246749103069\n",
      "Training: epoch 140 batch 10 loss 0.007088334299623966\n",
      "Training: epoch 140 batch 20 loss 0.010921659879386425\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test: epoch 140 batch 0 loss 0.005292502231895924\n",
      "epoch 140 finished - avarage train loss 0.00793789920461332  avarage test loss 0.014141501393169165\n",
      "Training: epoch 141 batch 0 loss 0.010332588106393814\n",
      "Training: epoch 141 batch 10 loss 0.009473663754761219\n",
      "Training: epoch 141 batch 20 loss 0.0052050030790269375\n",
      "Test: epoch 141 batch 0 loss 0.005331863649189472\n",
      "epoch 141 finished - avarage train loss 0.008577550408141366  avarage test loss 0.01334036432672292\n",
      "Training: epoch 142 batch 0 loss 0.0071444278582930565\n",
      "Training: epoch 142 batch 10 loss 0.01202478352934122\n",
      "Training: epoch 142 batch 20 loss 0.003682676935568452\n",
      "Test: epoch 142 batch 0 loss 0.005127351731061935\n",
      "epoch 142 finished - avarage train loss 0.00823732949632766  avarage test loss 0.013458933564834297\n",
      "Training: epoch 143 batch 0 loss 0.0051927464082837105\n",
      "Training: epoch 143 batch 10 loss 0.01378181017935276\n",
      "Training: epoch 143 batch 20 loss 0.009236862882971764\n",
      "Test: epoch 143 batch 0 loss 0.005264541134238243\n",
      "epoch 143 finished - avarage train loss 0.008948544192866519  avarage test loss 0.014207262545824051\n",
      "Training: epoch 144 batch 0 loss 0.011139786802232265\n",
      "Training: epoch 144 batch 10 loss 0.01621815748512745\n",
      "Training: epoch 144 batch 20 loss 0.003638039343059063\n",
      "Test: epoch 144 batch 0 loss 0.004907915834337473\n",
      "epoch 144 finished - avarage train loss 0.008712549340622178  avarage test loss 0.013202323752921075\n",
      "Training: epoch 145 batch 0 loss 0.007993072271347046\n",
      "Training: epoch 145 batch 10 loss 0.007593002635985613\n",
      "Training: epoch 145 batch 20 loss 0.0036955492105334997\n",
      "Test: epoch 145 batch 0 loss 0.005673440173268318\n",
      "epoch 145 finished - avarage train loss 0.009064967816310197  avarage test loss 0.014561471063643694\n",
      "Training: epoch 146 batch 0 loss 0.003839635755866766\n",
      "Training: epoch 146 batch 10 loss 0.005639359354972839\n",
      "Training: epoch 146 batch 20 loss 0.013599277473986149\n",
      "Test: epoch 146 batch 0 loss 0.005188162438571453\n",
      "epoch 146 finished - avarage train loss 0.008473186601146028  avarage test loss 0.01385206263512373\n",
      "Training: epoch 147 batch 0 loss 0.008531800471246243\n",
      "Training: epoch 147 batch 10 loss 0.01918271742761135\n",
      "Training: epoch 147 batch 20 loss 0.007853404618799686\n",
      "Test: epoch 147 batch 0 loss 0.00541660375893116\n",
      "epoch 147 finished - avarage train loss 0.009560124113641936  avarage test loss 0.014482801198028028\n",
      "Training: epoch 148 batch 0 loss 0.007398224901407957\n",
      "Training: epoch 148 batch 10 loss 0.0031685945577919483\n",
      "Training: epoch 148 batch 20 loss 0.004967831075191498\n",
      "Test: epoch 148 batch 0 loss 0.004889072850346565\n",
      "epoch 148 finished - avarage train loss 0.007826315736847705  avarage test loss 0.013182072900235653\n",
      "Training: epoch 149 batch 0 loss 0.006277445703744888\n",
      "Training: epoch 149 batch 10 loss 0.005204366520047188\n",
      "Training: epoch 149 batch 20 loss 0.009303584694862366\n",
      "Test: epoch 149 batch 0 loss 0.004881919361650944\n",
      "epoch 149 finished - avarage train loss 0.007592282089372647  avarage test loss 0.013253849348984659\n",
      "Training: epoch 150 batch 0 loss 0.007215472403913736\n",
      "Training: epoch 150 batch 10 loss 0.007027123123407364\n",
      "Training: epoch 150 batch 20 loss 0.008337263017892838\n",
      "Test: epoch 150 batch 0 loss 0.005366805475205183\n",
      "epoch 150 finished - avarage train loss 0.007672452143040197  avarage test loss 0.013423049182165414\n",
      "Training: epoch 151 batch 0 loss 0.008755704388022423\n",
      "Training: epoch 151 batch 10 loss 0.012140914797782898\n",
      "Training: epoch 151 batch 20 loss 0.008379681967198849\n",
      "Test: epoch 151 batch 0 loss 0.005161338485777378\n",
      "epoch 151 finished - avarage train loss 0.009412338090097082  avarage test loss 0.016410143696703017\n",
      "Training: epoch 152 batch 0 loss 0.008691906929016113\n",
      "Training: epoch 152 batch 10 loss 0.006820885930210352\n",
      "Training: epoch 152 batch 20 loss 0.0058383094146847725\n",
      "Test: epoch 152 batch 0 loss 0.004592008888721466\n",
      "epoch 152 finished - avarage train loss 0.008663126604695773  avarage test loss 0.013842510583344847\n",
      "Training: epoch 153 batch 0 loss 0.004126498941332102\n",
      "Training: epoch 153 batch 10 loss 0.00434825848788023\n",
      "Training: epoch 153 batch 20 loss 0.011997286230325699\n",
      "Test: epoch 153 batch 0 loss 0.004834340885281563\n",
      "epoch 153 finished - avarage train loss 0.008333685872112882  avarage test loss 0.013374476286116987\n",
      "Training: epoch 154 batch 0 loss 0.005784731823951006\n",
      "Training: epoch 154 batch 10 loss 0.00808919407427311\n",
      "Training: epoch 154 batch 20 loss 0.007927757687866688\n",
      "Test: epoch 154 batch 0 loss 0.006505322176963091\n",
      "epoch 154 finished - avarage train loss 0.008108431866777868  avarage test loss 0.014397740364074707\n",
      "Training: epoch 155 batch 0 loss 0.010068787261843681\n",
      "Training: epoch 155 batch 10 loss 0.008260337635874748\n",
      "Training: epoch 155 batch 20 loss 0.011470871977508068\n",
      "Test: epoch 155 batch 0 loss 0.006635954137891531\n",
      "epoch 155 finished - avarage train loss 0.009467130928332436  avarage test loss 0.016550974105484784\n",
      "Training: epoch 156 batch 0 loss 0.01815783604979515\n",
      "Training: epoch 156 batch 10 loss 0.004285051021724939\n",
      "Training: epoch 156 batch 20 loss 0.0061358921229839325\n",
      "Test: epoch 156 batch 0 loss 0.00623167073354125\n",
      "epoch 156 finished - avarage train loss 0.008838803057783636  avarage test loss 0.015612988732755184\n",
      "Training: epoch 157 batch 0 loss 0.008194966241717339\n",
      "Training: epoch 157 batch 10 loss 0.006532093044370413\n",
      "Training: epoch 157 batch 20 loss 0.012146079912781715\n",
      "Test: epoch 157 batch 0 loss 0.005213144700974226\n",
      "epoch 157 finished - avarage train loss 0.008618348117532402  avarage test loss 0.013779791072010994\n",
      "Training: epoch 158 batch 0 loss 0.005327680613845587\n",
      "Training: epoch 158 batch 10 loss 0.011171326972544193\n",
      "Training: epoch 158 batch 20 loss 0.01212195586413145\n",
      "Test: epoch 158 batch 0 loss 0.005311928223818541\n",
      "epoch 158 finished - avarage train loss 0.008537960864988894  avarage test loss 0.013010516355279833\n",
      "Training: epoch 159 batch 0 loss 0.0029599484987556934\n",
      "Training: epoch 159 batch 10 loss 0.007935196161270142\n",
      "Training: epoch 159 batch 20 loss 0.005449221935123205\n",
      "Test: epoch 159 batch 0 loss 0.008689101785421371\n",
      "epoch 159 finished - avarage train loss 0.007645843574648787  avarage test loss 0.01706490048673004\n",
      "Training: epoch 160 batch 0 loss 0.009487491101026535\n",
      "Training: epoch 160 batch 10 loss 0.021482333540916443\n",
      "Training: epoch 160 batch 20 loss 0.007062780670821667\n",
      "Test: epoch 160 batch 0 loss 0.00711359828710556\n",
      "epoch 160 finished - avarage train loss 0.01266970596244109  avarage test loss 0.017515302053652704\n",
      "Training: epoch 161 batch 0 loss 0.007048186380416155\n",
      "Training: epoch 161 batch 10 loss 0.008687714114785194\n",
      "Training: epoch 161 batch 20 loss 0.011619620025157928\n",
      "Test: epoch 161 batch 0 loss 0.005169320851564407\n",
      "epoch 161 finished - avarage train loss 0.0072507367726286935  avarage test loss 0.014427762886043638\n",
      "Training: epoch 162 batch 0 loss 0.004101097118109465\n",
      "Training: epoch 162 batch 10 loss 0.005752918776124716\n",
      "Training: epoch 162 batch 20 loss 0.010097495280206203\n",
      "Test: epoch 162 batch 0 loss 0.004851944278925657\n",
      "epoch 162 finished - avarage train loss 0.007952069325757951  avarage test loss 0.013589339214377105\n",
      "Training: epoch 163 batch 0 loss 0.004838005173951387\n",
      "Training: epoch 163 batch 10 loss 0.0056009613908827305\n",
      "Training: epoch 163 batch 20 loss 0.007413674145936966\n",
      "Test: epoch 163 batch 0 loss 0.004846859723329544\n",
      "epoch 163 finished - avarage train loss 0.008183872006062803  avarage test loss 0.013537145336158574\n",
      "Training: epoch 164 batch 0 loss 0.008065411821007729\n",
      "Training: epoch 164 batch 10 loss 0.008889430202543736\n",
      "Training: epoch 164 batch 20 loss 0.007704477291554213\n",
      "Test: epoch 164 batch 0 loss 0.005119637120515108\n",
      "epoch 164 finished - avarage train loss 0.008402792488000002  avarage test loss 0.013503850670531392\n",
      "Training: epoch 165 batch 0 loss 0.006801712792366743\n",
      "Training: epoch 165 batch 10 loss 0.005650273524224758\n",
      "Training: epoch 165 batch 20 loss 0.004562307149171829\n",
      "Test: epoch 165 batch 0 loss 0.004855817649513483\n",
      "epoch 165 finished - avarage train loss 0.009017126217227557  avarage test loss 0.013011917821131647\n",
      "Training: epoch 166 batch 0 loss 0.006400905549526215\n",
      "Training: epoch 166 batch 10 loss 0.0036827607546001673\n",
      "Training: epoch 166 batch 20 loss 0.0027579814195632935\n",
      "Test: epoch 166 batch 0 loss 0.004909373354166746\n",
      "epoch 166 finished - avarage train loss 0.008602517978127661  avarage test loss 0.01318001264007762\n",
      "Training: epoch 167 batch 0 loss 0.0074109951965510845\n",
      "Training: epoch 167 batch 10 loss 0.008080053143203259\n",
      "Training: epoch 167 batch 20 loss 0.009531203657388687\n",
      "Test: epoch 167 batch 0 loss 0.005149707663804293\n",
      "epoch 167 finished - avarage train loss 0.008673982734888279  avarage test loss 0.013626314117573202\n",
      "Training: epoch 168 batch 0 loss 0.010504139587283134\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training: epoch 168 batch 10 loss 0.002908050548285246\n",
      "Training: epoch 168 batch 20 loss 0.0051676565781235695\n",
      "Test: epoch 168 batch 0 loss 0.005128965247422457\n",
      "epoch 168 finished - avarage train loss 0.0083363878556752  avarage test loss 0.013230921817012131\n",
      "Training: epoch 169 batch 0 loss 0.005944510456174612\n",
      "Training: epoch 169 batch 10 loss 0.005683461669832468\n",
      "Training: epoch 169 batch 20 loss 0.009049564599990845\n",
      "Test: epoch 169 batch 0 loss 0.004458829760551453\n",
      "epoch 169 finished - avarage train loss 0.008526891694757444  avarage test loss 0.01269086159300059\n",
      "Training: epoch 170 batch 0 loss 0.0051976763643324375\n",
      "Training: epoch 170 batch 10 loss 0.00884744618088007\n",
      "Training: epoch 170 batch 20 loss 0.0036263701040297747\n",
      "Test: epoch 170 batch 0 loss 0.004475265741348267\n",
      "epoch 170 finished - avarage train loss 0.009106821846216917  avarage test loss 0.012616099382285029\n",
      "Training: epoch 171 batch 0 loss 0.0060493978671729565\n",
      "Training: epoch 171 batch 10 loss 0.0057897502556443214\n",
      "Training: epoch 171 batch 20 loss 0.011832754127681255\n",
      "Test: epoch 171 batch 0 loss 0.004603048320859671\n",
      "epoch 171 finished - avarage train loss 0.008572494554943565  avarage test loss 0.013066147628705949\n",
      "Training: epoch 172 batch 0 loss 0.00739207724109292\n",
      "Training: epoch 172 batch 10 loss 0.004720996133983135\n",
      "Training: epoch 172 batch 20 loss 0.01036110520362854\n",
      "Test: epoch 172 batch 0 loss 0.005128864198923111\n",
      "epoch 172 finished - avarage train loss 0.007887265371993697  avarage test loss 0.013243342691566795\n",
      "Training: epoch 173 batch 0 loss 0.0049052769318223\n",
      "Training: epoch 173 batch 10 loss 0.00911741890013218\n",
      "Training: epoch 173 batch 20 loss 0.011927579529583454\n",
      "Test: epoch 173 batch 0 loss 0.004837428219616413\n",
      "epoch 173 finished - avarage train loss 0.007348763435308276  avarage test loss 0.013344062550459057\n",
      "Training: epoch 174 batch 0 loss 0.0052119651809334755\n",
      "Training: epoch 174 batch 10 loss 0.005985653959214687\n",
      "Training: epoch 174 batch 20 loss 0.003758423263207078\n",
      "Test: epoch 174 batch 0 loss 0.004721072968095541\n",
      "epoch 174 finished - avarage train loss 0.008456207603087714  avarage test loss 0.013440918235573918\n",
      "Training: epoch 175 batch 0 loss 0.004816800821572542\n",
      "Training: epoch 175 batch 10 loss 0.009083084762096405\n",
      "Training: epoch 175 batch 20 loss 0.007572788279503584\n",
      "Test: epoch 175 batch 0 loss 0.005102735012769699\n",
      "epoch 175 finished - avarage train loss 0.008903860275087685  avarage test loss 0.014095043181441724\n",
      "Training: epoch 176 batch 0 loss 0.00880423840135336\n",
      "Training: epoch 176 batch 10 loss 0.00610971637070179\n",
      "Training: epoch 176 batch 20 loss 0.007376925088465214\n",
      "Test: epoch 176 batch 0 loss 0.004845741204917431\n",
      "epoch 176 finished - avarage train loss 0.008245226830757898  avarage test loss 0.013269462157040834\n",
      "Training: epoch 177 batch 0 loss 0.00405987910926342\n",
      "Training: epoch 177 batch 10 loss 0.006110004615038633\n",
      "Training: epoch 177 batch 20 loss 0.006442543119192123\n",
      "Test: epoch 177 batch 0 loss 0.005104885436594486\n",
      "epoch 177 finished - avarage train loss 0.007232746093309131  avarage test loss 0.013723154552280903\n",
      "Training: epoch 178 batch 0 loss 0.004793196450918913\n",
      "Training: epoch 178 batch 10 loss 0.006926223635673523\n",
      "Training: epoch 178 batch 20 loss 0.007854979485273361\n",
      "Test: epoch 178 batch 0 loss 0.004877428058534861\n",
      "epoch 178 finished - avarage train loss 0.008256446819045934  avarage test loss 0.012858779751695693\n",
      "Training: epoch 179 batch 0 loss 0.004946820437908173\n",
      "Training: epoch 179 batch 10 loss 0.0033016889356076717\n",
      "Training: epoch 179 batch 20 loss 0.00534668518230319\n",
      "Test: epoch 179 batch 0 loss 0.005067647900432348\n",
      "epoch 179 finished - avarage train loss 0.007459557237874331  avarage test loss 0.01343169406754896\n",
      "Training: epoch 180 batch 0 loss 0.006781204137951136\n",
      "Training: epoch 180 batch 10 loss 0.005399747751653194\n",
      "Training: epoch 180 batch 20 loss 0.008089655078947544\n",
      "Test: epoch 180 batch 0 loss 0.004575423430651426\n",
      "epoch 180 finished - avarage train loss 0.00773354639813047  avarage test loss 0.013086932769510895\n",
      "Training: epoch 181 batch 0 loss 0.006543018855154514\n",
      "Training: epoch 181 batch 10 loss 0.0031296461820602417\n",
      "Training: epoch 181 batch 20 loss 0.007158495485782623\n",
      "Test: epoch 181 batch 0 loss 0.004868172109127045\n",
      "epoch 181 finished - avarage train loss 0.008151759979604134  avarage test loss 0.012850214901845902\n",
      "Training: epoch 182 batch 0 loss 0.00711376778781414\n",
      "Training: epoch 182 batch 10 loss 0.009652400389313698\n",
      "Training: epoch 182 batch 20 loss 0.017457501962780952\n",
      "Test: epoch 182 batch 0 loss 0.005478520877659321\n",
      "epoch 182 finished - avarage train loss 0.008550472461200994  avarage test loss 0.013619921752251685\n",
      "Training: epoch 183 batch 0 loss 0.004879831336438656\n",
      "Training: epoch 183 batch 10 loss 0.00733450660482049\n",
      "Training: epoch 183 batch 20 loss 0.011404481716454029\n",
      "Test: epoch 183 batch 0 loss 0.004963255487382412\n",
      "epoch 183 finished - avarage train loss 0.008731734023654255  avarage test loss 0.0136368777602911\n",
      "Training: epoch 184 batch 0 loss 0.0094589339569211\n",
      "Training: epoch 184 batch 10 loss 0.011812116950750351\n",
      "Training: epoch 184 batch 20 loss 0.01095914002507925\n",
      "Test: epoch 184 batch 0 loss 0.005201274529099464\n",
      "epoch 184 finished - avarage train loss 0.008719410474315799  avarage test loss 0.013037778961006552\n",
      "Training: epoch 185 batch 0 loss 0.005642234347760677\n",
      "Training: epoch 185 batch 10 loss 0.006800998467952013\n",
      "Training: epoch 185 batch 20 loss 0.006170288659632206\n",
      "Test: epoch 185 batch 0 loss 0.004892056807875633\n",
      "epoch 185 finished - avarage train loss 0.007371207113088718  avarage test loss 0.012742418795824051\n",
      "Training: epoch 186 batch 0 loss 0.008626684546470642\n",
      "Training: epoch 186 batch 10 loss 0.005386443808674812\n",
      "Training: epoch 186 batch 20 loss 0.005972679704427719\n",
      "Test: epoch 186 batch 0 loss 0.006148312706500292\n",
      "epoch 186 finished - avarage train loss 0.009145082777430272  avarage test loss 0.013779891887679696\n",
      "Training: epoch 187 batch 0 loss 0.009548163041472435\n",
      "Training: epoch 187 batch 10 loss 0.005858602002263069\n",
      "Training: epoch 187 batch 20 loss 0.005017171613872051\n",
      "Test: epoch 187 batch 0 loss 0.005233194679021835\n",
      "epoch 187 finished - avarage train loss 0.01013526333303287  avarage test loss 0.01308911177329719\n",
      "Training: epoch 188 batch 0 loss 0.006454812828451395\n",
      "Training: epoch 188 batch 10 loss 0.007898600772023201\n",
      "Training: epoch 188 batch 20 loss 0.004764961544424295\n",
      "Test: epoch 188 batch 0 loss 0.005557030905038118\n",
      "epoch 188 finished - avarage train loss 0.009271599898307488  avarage test loss 0.013620928977616131\n",
      "Training: epoch 189 batch 0 loss 0.012465315870940685\n",
      "Training: epoch 189 batch 10 loss 0.00633121095597744\n",
      "Training: epoch 189 batch 20 loss 0.005396466236561537\n",
      "Test: epoch 189 batch 0 loss 0.00890100933611393\n",
      "epoch 189 finished - avarage train loss 0.009359008231168163  avarage test loss 0.016700227046385407\n",
      "Training: epoch 190 batch 0 loss 0.014262686483561993\n",
      "Training: epoch 190 batch 10 loss 0.01316418219357729\n",
      "Training: epoch 190 batch 20 loss 0.007798789069056511\n",
      "Test: epoch 190 batch 0 loss 0.006437874399125576\n",
      "epoch 190 finished - avarage train loss 0.009721857913095376  avarage test loss 0.01381418900564313\n",
      "Training: epoch 191 batch 0 loss 0.00533022778108716\n",
      "Training: epoch 191 batch 10 loss 0.008767918683588505\n",
      "Training: epoch 191 batch 20 loss 0.009059982374310493\n",
      "Test: epoch 191 batch 0 loss 0.006084341090172529\n",
      "epoch 191 finished - avarage train loss 0.007874966711448184  avarage test loss 0.014335664920508862\n",
      "Training: epoch 192 batch 0 loss 0.011677026748657227\n",
      "Training: epoch 192 batch 10 loss 0.0064516919665038586\n",
      "Training: epoch 192 batch 20 loss 0.012456637807190418\n",
      "Test: epoch 192 batch 0 loss 0.00572162214666605\n",
      "epoch 192 finished - avarage train loss 0.008419328577944944  avarage test loss 0.01364841591566801\n",
      "Training: epoch 193 batch 0 loss 0.007255158852785826\n",
      "Training: epoch 193 batch 10 loss 0.00394084258005023\n",
      "Training: epoch 193 batch 20 loss 0.011112265288829803\n",
      "Test: epoch 193 batch 0 loss 0.008890755474567413\n",
      "epoch 193 finished - avarage train loss 0.007995699596559179  avarage test loss 0.016369763179682195\n",
      "Training: epoch 194 batch 0 loss 0.01291029341518879\n",
      "Training: epoch 194 batch 10 loss 0.008880478329956532\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training: epoch 194 batch 20 loss 0.0168827585875988\n",
      "Test: epoch 194 batch 0 loss 0.00636599725112319\n",
      "epoch 194 finished - avarage train loss 0.012387618294049954  avarage test loss 0.014816504495684057\n",
      "Training: epoch 195 batch 0 loss 0.008868008852005005\n",
      "Training: epoch 195 batch 10 loss 0.0059172622859478\n",
      "Training: epoch 195 batch 20 loss 0.0022696061059832573\n",
      "Test: epoch 195 batch 0 loss 0.00548400916159153\n",
      "epoch 195 finished - avarage train loss 0.0073461155567703575  avarage test loss 0.01343308121431619\n",
      "Training: epoch 196 batch 0 loss 0.008235706016421318\n",
      "Training: epoch 196 batch 10 loss 0.005406972020864487\n",
      "Training: epoch 196 batch 20 loss 0.006581658031791449\n",
      "Test: epoch 196 batch 0 loss 0.006558716297149658\n",
      "epoch 196 finished - avarage train loss 0.008846886777158442  avarage test loss 0.014142756001092494\n",
      "Training: epoch 197 batch 0 loss 0.005312116350978613\n",
      "Training: epoch 197 batch 10 loss 0.010874009691178799\n",
      "Training: epoch 197 batch 20 loss 0.007239155005663633\n",
      "Test: epoch 197 batch 0 loss 0.005883568897843361\n",
      "epoch 197 finished - avarage train loss 0.010388599002155764  avarage test loss 0.01387896970845759\n",
      "Training: epoch 198 batch 0 loss 0.010674916207790375\n",
      "Training: epoch 198 batch 10 loss 0.007869306020438671\n",
      "Training: epoch 198 batch 20 loss 0.006281908601522446\n",
      "Test: epoch 198 batch 0 loss 0.007298832759261131\n",
      "epoch 198 finished - avarage train loss 0.00921559789025321  avarage test loss 0.01612764596939087\n",
      "Training: epoch 199 batch 0 loss 0.009471096098423004\n",
      "Training: epoch 199 batch 10 loss 0.007973818108439445\n",
      "Training: epoch 199 batch 20 loss 0.011225325986742973\n",
      "Test: epoch 199 batch 0 loss 0.005980564281344414\n",
      "epoch 199 finished - avarage train loss 0.010698111751919677  avarage test loss 0.013444890268146992\n",
      "Training: epoch 0 batch 0 loss 0.5484119653701782\n",
      "Training: epoch 0 batch 10 loss 0.4828590154647827\n",
      "Training: epoch 0 batch 20 loss 0.4573521018028259\n",
      "Test: epoch 0 batch 0 loss 0.4271133840084076\n",
      "epoch 0 finished - avarage train loss 0.5243023500360292  avarage test loss 0.5110637322068214\n",
      "Training: epoch 1 batch 0 loss 0.6365458369255066\n",
      "Training: epoch 1 batch 10 loss 0.449159175157547\n",
      "Training: epoch 1 batch 20 loss 0.3708091974258423\n",
      "Test: epoch 1 batch 0 loss 0.42962098121643066\n",
      "epoch 1 finished - avarage train loss 0.5029549423990578  avarage test loss 0.5172507166862488\n",
      "Training: epoch 2 batch 0 loss 0.5324897766113281\n",
      "Training: epoch 2 batch 10 loss 0.6747112274169922\n",
      "Training: epoch 2 batch 20 loss 0.5934843420982361\n",
      "Test: epoch 2 batch 0 loss 0.43913981318473816\n",
      "epoch 2 finished - avarage train loss 0.49946116887289904  avarage test loss 0.5126694664359093\n",
      "Training: epoch 3 batch 0 loss 0.43397414684295654\n",
      "Training: epoch 3 batch 10 loss 0.5046988129615784\n",
      "Training: epoch 3 batch 20 loss 0.5173524618148804\n",
      "Test: epoch 3 batch 0 loss 0.4304742217063904\n",
      "epoch 3 finished - avarage train loss 0.5006951802763445  avarage test loss 0.5104185491800308\n",
      "Training: epoch 4 batch 0 loss 0.3489748239517212\n",
      "Training: epoch 4 batch 10 loss 0.3373483121395111\n",
      "Training: epoch 4 batch 20 loss 0.23530055582523346\n",
      "Test: epoch 4 batch 0 loss 0.15027891099452972\n",
      "epoch 4 finished - avarage train loss 0.36711759312913333  avarage test loss 0.1358887441456318\n",
      "Training: epoch 5 batch 0 loss 0.12414546310901642\n",
      "Training: epoch 5 batch 10 loss 0.06338053941726685\n",
      "Training: epoch 5 batch 20 loss 0.025785362347960472\n",
      "Test: epoch 5 batch 0 loss 0.03329379856586456\n",
      "epoch 5 finished - avarage train loss 0.05793255066563343  avarage test loss 0.04081033682450652\n",
      "Training: epoch 6 batch 0 loss 0.02879316732287407\n",
      "Training: epoch 6 batch 10 loss 0.026919856667518616\n",
      "Training: epoch 6 batch 20 loss 0.019046738743782043\n",
      "Test: epoch 6 batch 0 loss 0.028345655649900436\n",
      "epoch 6 finished - avarage train loss 0.026241615864223446  avarage test loss 0.03646809095516801\n",
      "Training: epoch 7 batch 0 loss 0.022648712620139122\n",
      "Training: epoch 7 batch 10 loss 0.021382136270403862\n",
      "Training: epoch 7 batch 20 loss 0.016890637576580048\n",
      "Test: epoch 7 batch 0 loss 0.027530085295438766\n",
      "epoch 7 finished - avarage train loss 0.020348969750620168  avarage test loss 0.03580856742337346\n",
      "Training: epoch 8 batch 0 loss 0.01947115734219551\n",
      "Training: epoch 8 batch 10 loss 0.02182193100452423\n",
      "Training: epoch 8 batch 20 loss 0.01584664359688759\n",
      "Test: epoch 8 batch 0 loss 0.025650259107351303\n",
      "epoch 8 finished - avarage train loss 0.01983182507984597  avarage test loss 0.03392508625984192\n",
      "Training: epoch 9 batch 0 loss 0.009093274362385273\n",
      "Training: epoch 9 batch 10 loss 0.010215209797024727\n",
      "Training: epoch 9 batch 20 loss 0.019484279677271843\n",
      "Test: epoch 9 batch 0 loss 0.025831766426563263\n",
      "epoch 9 finished - avarage train loss 0.017745435591144807  avarage test loss 0.034219877794384956\n",
      "Training: epoch 10 batch 0 loss 0.024507243186235428\n",
      "Training: epoch 10 batch 10 loss 0.027157267555594444\n",
      "Training: epoch 10 batch 20 loss 0.012675769627094269\n",
      "Test: epoch 10 batch 0 loss 0.02422090619802475\n",
      "epoch 10 finished - avarage train loss 0.019017359643275368  avarage test loss 0.03294409578666091\n",
      "Training: epoch 11 batch 0 loss 0.011764331720769405\n",
      "Training: epoch 11 batch 10 loss 0.013168618083000183\n",
      "Training: epoch 11 batch 20 loss 0.02765144407749176\n",
      "Test: epoch 11 batch 0 loss 0.023298032581806183\n",
      "epoch 11 finished - avarage train loss 0.016695950148177558  avarage test loss 0.03223889600485563\n",
      "Training: epoch 12 batch 0 loss 0.011287335306406021\n",
      "Training: epoch 12 batch 10 loss 0.016464918851852417\n",
      "Training: epoch 12 batch 20 loss 0.01028987392783165\n",
      "Test: epoch 12 batch 0 loss 0.011043726466596127\n",
      "epoch 12 finished - avarage train loss 0.01347484845860765  avarage test loss 0.022271064342930913\n",
      "Training: epoch 13 batch 0 loss 0.004412409383803606\n",
      "Training: epoch 13 batch 10 loss 0.015331018716096878\n",
      "Training: epoch 13 batch 20 loss 0.00828056875616312\n",
      "Test: epoch 13 batch 0 loss 0.011948416009545326\n",
      "epoch 13 finished - avarage train loss 0.01076728002778415  avarage test loss 0.024413231760263443\n",
      "Training: epoch 14 batch 0 loss 0.008486728183925152\n",
      "Training: epoch 14 batch 10 loss 0.009486877359449863\n",
      "Training: epoch 14 batch 20 loss 0.007572629023343325\n",
      "Test: epoch 14 batch 0 loss 0.014024178497493267\n",
      "epoch 14 finished - avarage train loss 0.0111393502046322  avarage test loss 0.024744310649111867\n",
      "Training: epoch 15 batch 0 loss 0.013241899199783802\n",
      "Training: epoch 15 batch 10 loss 0.006709206849336624\n",
      "Training: epoch 15 batch 20 loss 0.006590496748685837\n",
      "Test: epoch 15 batch 0 loss 0.016030002385377884\n",
      "epoch 15 finished - avarage train loss 0.010593855753540993  avarage test loss 0.02780187875032425\n",
      "Training: epoch 16 batch 0 loss 0.014826138503849506\n",
      "Training: epoch 16 batch 10 loss 0.008120040409266949\n",
      "Training: epoch 16 batch 20 loss 0.01219162717461586\n",
      "Test: epoch 16 batch 0 loss 0.010578126646578312\n",
      "epoch 16 finished - avarage train loss 0.010344626414107865  avarage test loss 0.0232098747510463\n",
      "Training: epoch 17 batch 0 loss 0.013896750286221504\n",
      "Training: epoch 17 batch 10 loss 0.0064560361206531525\n",
      "Training: epoch 17 batch 20 loss 0.006976757198572159\n",
      "Test: epoch 17 batch 0 loss 0.007989858277142048\n",
      "epoch 17 finished - avarage train loss 0.010058152457249576  avarage test loss 0.021038506412878633\n",
      "Training: epoch 18 batch 0 loss 0.010451815091073513\n",
      "Training: epoch 18 batch 10 loss 0.014589106664061546\n",
      "Training: epoch 18 batch 20 loss 0.010198749601840973\n",
      "Test: epoch 18 batch 0 loss 0.005876570008695126\n",
      "epoch 18 finished - avarage train loss 0.008571443420931184  avarage test loss 0.017340623307973146\n",
      "Training: epoch 19 batch 0 loss 0.005988059565424919\n",
      "Training: epoch 19 batch 10 loss 0.007448442280292511\n",
      "Training: epoch 19 batch 20 loss 0.010865302756428719\n",
      "Test: epoch 19 batch 0 loss 0.005436391104012728\n",
      "epoch 19 finished - avarage train loss 0.009449895599792743  avarage test loss 0.016479673678986728\n",
      "Training: epoch 20 batch 0 loss 0.007441402412950993\n",
      "Training: epoch 20 batch 10 loss 0.005398050881922245\n",
      "Training: epoch 20 batch 20 loss 0.005393930710852146\n",
      "Test: epoch 20 batch 0 loss 0.004584098234772682\n",
      "epoch 20 finished - avarage train loss 0.010106704743771717  avarage test loss 0.016079411492682993\n",
      "Training: epoch 21 batch 0 loss 0.0067362054251134396\n",
      "Training: epoch 21 batch 10 loss 0.00620867358520627\n",
      "Training: epoch 21 batch 20 loss 0.01054858323186636\n",
      "Test: epoch 21 batch 0 loss 0.0046294392086565495\n",
      "epoch 21 finished - avarage train loss 0.008596151609551805  avarage test loss 0.014699714491143823\n",
      "Training: epoch 22 batch 0 loss 0.004813326522707939\n",
      "Training: epoch 22 batch 10 loss 0.009672390297055244\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training: epoch 22 batch 20 loss 0.007235212251543999\n",
      "Test: epoch 22 batch 0 loss 0.00500311516225338\n",
      "epoch 22 finished - avarage train loss 0.009364243131130934  avarage test loss 0.014803301426582038\n",
      "Training: epoch 23 batch 0 loss 0.009817039594054222\n",
      "Training: epoch 23 batch 10 loss 0.004660626407712698\n",
      "Training: epoch 23 batch 20 loss 0.00785223301500082\n",
      "Test: epoch 23 batch 0 loss 0.004826891701668501\n",
      "epoch 23 finished - avarage train loss 0.007379982893451534  avarage test loss 0.013702860451303422\n",
      "Training: epoch 24 batch 0 loss 0.0038212407380342484\n",
      "Training: epoch 24 batch 10 loss 0.010493703186511993\n",
      "Training: epoch 24 batch 20 loss 0.005855809897184372\n",
      "Test: epoch 24 batch 0 loss 0.006304218899458647\n",
      "epoch 24 finished - avarage train loss 0.008521415485904134  avarage test loss 0.013247679802589118\n",
      "Training: epoch 25 batch 0 loss 0.005318762268871069\n",
      "Training: epoch 25 batch 10 loss 0.009660129435360432\n",
      "Training: epoch 25 batch 20 loss 0.005876685958355665\n",
      "Test: epoch 25 batch 0 loss 0.00794336199760437\n",
      "epoch 25 finished - avarage train loss 0.007189991659131543  avarage test loss 0.015760979033075273\n",
      "Training: epoch 26 batch 0 loss 0.00753824133425951\n",
      "Training: epoch 26 batch 10 loss 0.0031182714737951756\n",
      "Training: epoch 26 batch 20 loss 0.006764532066881657\n",
      "Test: epoch 26 batch 0 loss 0.005640807095915079\n",
      "epoch 26 finished - avarage train loss 0.00995886468746025  avarage test loss 0.014487626613117754\n",
      "Training: epoch 27 batch 0 loss 0.005466351751238108\n",
      "Training: epoch 27 batch 10 loss 0.004958967212587595\n",
      "Training: epoch 27 batch 20 loss 0.005848548840731382\n",
      "Test: epoch 27 batch 0 loss 0.006950462237000465\n",
      "epoch 27 finished - avarage train loss 0.007872101537690595  avarage test loss 0.017864427412860096\n",
      "Training: epoch 28 batch 0 loss 0.009841110557317734\n",
      "Training: epoch 28 batch 10 loss 0.0064454833045601845\n",
      "Training: epoch 28 batch 20 loss 0.005076988600194454\n",
      "Test: epoch 28 batch 0 loss 0.008945774286985397\n",
      "epoch 28 finished - avarage train loss 0.007478297977098103  avarage test loss 0.016072408528998494\n",
      "Training: epoch 29 batch 0 loss 0.005667334422469139\n",
      "Training: epoch 29 batch 10 loss 0.012793347239494324\n",
      "Training: epoch 29 batch 20 loss 0.007779978681355715\n",
      "Test: epoch 29 batch 0 loss 0.006124832667410374\n",
      "epoch 29 finished - avarage train loss 0.00940169867706196  avarage test loss 0.017686858773231506\n",
      "Training: epoch 30 batch 0 loss 0.005967785604298115\n",
      "Training: epoch 30 batch 10 loss 0.003716591978445649\n",
      "Training: epoch 30 batch 20 loss 0.009528658352792263\n",
      "Test: epoch 30 batch 0 loss 0.00628449534997344\n",
      "epoch 30 finished - avarage train loss 0.009312943113839319  avarage test loss 0.01682354649528861\n",
      "Training: epoch 31 batch 0 loss 0.006444614380598068\n",
      "Training: epoch 31 batch 10 loss 0.010704782791435719\n",
      "Training: epoch 31 batch 20 loss 0.005523286759853363\n",
      "Test: epoch 31 batch 0 loss 0.005037758499383926\n",
      "epoch 31 finished - avarage train loss 0.008396592472904715  avarage test loss 0.016040700604207814\n",
      "Training: epoch 32 batch 0 loss 0.013321888633072376\n",
      "Training: epoch 32 batch 10 loss 0.0049184528179466724\n",
      "Training: epoch 32 batch 20 loss 0.011866653338074684\n",
      "Test: epoch 32 batch 0 loss 0.005542305298149586\n",
      "epoch 32 finished - avarage train loss 0.009268306428566575  avarage test loss 0.0152145407628268\n",
      "Training: epoch 33 batch 0 loss 0.008592983707785606\n",
      "Training: epoch 33 batch 10 loss 0.004175451584160328\n",
      "Training: epoch 33 batch 20 loss 0.010914449580013752\n",
      "Test: epoch 33 batch 0 loss 0.005363193806260824\n",
      "epoch 33 finished - avarage train loss 0.008346926493184834  avarage test loss 0.01655559940263629\n",
      "Training: epoch 34 batch 0 loss 0.005878627300262451\n",
      "Training: epoch 34 batch 10 loss 0.00890298094600439\n",
      "Training: epoch 34 batch 20 loss 0.007884676568210125\n",
      "Test: epoch 34 batch 0 loss 0.00482861278578639\n",
      "epoch 34 finished - avarage train loss 0.00812486945330326  avarage test loss 0.014417891507036984\n",
      "Training: epoch 35 batch 0 loss 0.005644048564136028\n",
      "Training: epoch 35 batch 10 loss 0.004323432222008705\n",
      "Training: epoch 35 batch 20 loss 0.00471595861017704\n",
      "Test: epoch 35 batch 0 loss 0.006000488065183163\n",
      "epoch 35 finished - avarage train loss 0.008363504059098918  avarage test loss 0.014003104530274868\n",
      "Training: epoch 36 batch 0 loss 0.0069871507585048676\n",
      "Training: epoch 36 batch 10 loss 0.0076604364439845085\n",
      "Training: epoch 36 batch 20 loss 0.011349997483193874\n",
      "Test: epoch 36 batch 0 loss 0.0062104482203722\n",
      "epoch 36 finished - avarage train loss 0.009344040335894659  avarage test loss 0.016986753325909376\n",
      "Training: epoch 37 batch 0 loss 0.009414291940629482\n",
      "Training: epoch 37 batch 10 loss 0.0040818206034600735\n",
      "Training: epoch 37 batch 20 loss 0.006973250303417444\n",
      "Test: epoch 37 batch 0 loss 0.006960493046790361\n",
      "epoch 37 finished - avarage train loss 0.008085338832360917  avarage test loss 0.015313848969526589\n",
      "Training: epoch 38 batch 0 loss 0.007804764900356531\n",
      "Training: epoch 38 batch 10 loss 0.002694422611966729\n",
      "Training: epoch 38 batch 20 loss 0.013421985320746899\n",
      "Test: epoch 38 batch 0 loss 0.0071341246366500854\n",
      "epoch 38 finished - avarage train loss 0.008481026667266571  avarage test loss 0.014271322404965758\n",
      "Training: epoch 39 batch 0 loss 0.006621313281357288\n",
      "Training: epoch 39 batch 10 loss 0.012736194767057896\n",
      "Training: epoch 39 batch 20 loss 0.004004240036010742\n",
      "Test: epoch 39 batch 0 loss 0.010132826864719391\n",
      "epoch 39 finished - avarage train loss 0.009408616573260776  avarage test loss 0.019540111534297466\n",
      "Training: epoch 40 batch 0 loss 0.016971249133348465\n",
      "Training: epoch 40 batch 10 loss 0.010474177077412605\n",
      "Training: epoch 40 batch 20 loss 0.007405557669699192\n",
      "Test: epoch 40 batch 0 loss 0.0054740700870752335\n",
      "epoch 40 finished - avarage train loss 0.009299358152302688  avarage test loss 0.01685549528338015\n",
      "Training: epoch 41 batch 0 loss 0.008719182573258877\n",
      "Training: epoch 41 batch 10 loss 0.007099766284227371\n",
      "Training: epoch 41 batch 20 loss 0.010986936278641224\n",
      "Test: epoch 41 batch 0 loss 0.005300310906022787\n",
      "epoch 41 finished - avarage train loss 0.007526756408784924  avarage test loss 0.013965278281830251\n",
      "Training: epoch 42 batch 0 loss 0.0028732800856232643\n",
      "Training: epoch 42 batch 10 loss 0.005617637187242508\n",
      "Training: epoch 42 batch 20 loss 0.009246138855814934\n",
      "Test: epoch 42 batch 0 loss 0.004167406354099512\n",
      "epoch 42 finished - avarage train loss 0.009437148603771267  avarage test loss 0.016141254571266472\n",
      "Training: epoch 43 batch 0 loss 0.00393641646951437\n",
      "Training: epoch 43 batch 10 loss 0.0067915963008999825\n",
      "Training: epoch 43 batch 20 loss 0.009448588825762272\n",
      "Test: epoch 43 batch 0 loss 0.005558244418352842\n",
      "epoch 43 finished - avarage train loss 0.008349906364134673  avarage test loss 0.017767248675227165\n",
      "Training: epoch 44 batch 0 loss 0.0052347201853990555\n",
      "Training: epoch 44 batch 10 loss 0.00679886806756258\n",
      "Training: epoch 44 batch 20 loss 0.006582091096788645\n",
      "Test: epoch 44 batch 0 loss 0.005219925660640001\n",
      "epoch 44 finished - avarage train loss 0.00775699897123308  avarage test loss 0.014894622028805315\n",
      "Training: epoch 45 batch 0 loss 0.00980154424905777\n",
      "Training: epoch 45 batch 10 loss 0.011080731637775898\n",
      "Training: epoch 45 batch 20 loss 0.008029925636947155\n",
      "Test: epoch 45 batch 0 loss 0.013928101398050785\n",
      "epoch 45 finished - avarage train loss 0.010024848329866755  avarage test loss 0.02450757077895105\n",
      "Training: epoch 46 batch 0 loss 0.006136877462267876\n",
      "Training: epoch 46 batch 10 loss 0.006868136581033468\n",
      "Training: epoch 46 batch 20 loss 0.008266149088740349\n",
      "Test: epoch 46 batch 0 loss 0.00846915878355503\n",
      "epoch 46 finished - avarage train loss 0.01048600590177651  avarage test loss 0.01791557134129107\n",
      "Training: epoch 47 batch 0 loss 0.013717819936573505\n",
      "Training: epoch 47 batch 10 loss 0.007055784575641155\n",
      "Training: epoch 47 batch 20 loss 0.009024023078382015\n",
      "Test: epoch 47 batch 0 loss 0.005153648555278778\n",
      "epoch 47 finished - avarage train loss 0.00910982195737547  avarage test loss 0.013323517167009413\n",
      "Training: epoch 48 batch 0 loss 0.01006145030260086\n",
      "Training: epoch 48 batch 10 loss 0.010166597552597523\n",
      "Training: epoch 48 batch 20 loss 0.01025458239018917\n",
      "Test: epoch 48 batch 0 loss 0.004289029166102409\n",
      "epoch 48 finished - avarage train loss 0.008326643788865927  avarage test loss 0.015492034843191504\n",
      "Training: epoch 49 batch 0 loss 0.006324038840830326\n",
      "Training: epoch 49 batch 10 loss 0.005816595163196325\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training: epoch 49 batch 20 loss 0.002620114479213953\n",
      "Test: epoch 49 batch 0 loss 0.004624922759830952\n",
      "epoch 49 finished - avarage train loss 0.007271181149729367  avarage test loss 0.013996713445521891\n",
      "Training: epoch 50 batch 0 loss 0.0049851639196276665\n",
      "Training: epoch 50 batch 10 loss 0.013299131765961647\n",
      "Training: epoch 50 batch 20 loss 0.004317193757742643\n",
      "Test: epoch 50 batch 0 loss 0.004517320543527603\n",
      "epoch 50 finished - avarage train loss 0.008423069154779458  avarage test loss 0.01260368776274845\n",
      "Training: epoch 51 batch 0 loss 0.007406417280435562\n",
      "Training: epoch 51 batch 10 loss 0.005716079380363226\n",
      "Training: epoch 51 batch 20 loss 0.00466884346678853\n",
      "Test: epoch 51 batch 0 loss 0.00632074847817421\n",
      "epoch 51 finished - avarage train loss 0.007629908163678543  avarage test loss 0.014474833849817514\n",
      "Training: epoch 52 batch 0 loss 0.005524255335330963\n",
      "Training: epoch 52 batch 10 loss 0.006962398998439312\n",
      "Training: epoch 52 batch 20 loss 0.005630032625049353\n",
      "Test: epoch 52 batch 0 loss 0.004636100959032774\n",
      "epoch 52 finished - avarage train loss 0.009486281682319683  avarage test loss 0.013100783340632915\n",
      "Training: epoch 53 batch 0 loss 0.00917294155806303\n",
      "Training: epoch 53 batch 10 loss 0.0070433118380606174\n",
      "Training: epoch 53 batch 20 loss 0.0040552690625190735\n",
      "Test: epoch 53 batch 0 loss 0.005548757966607809\n",
      "epoch 53 finished - avarage train loss 0.00799984833354066  avarage test loss 0.013323712046258152\n",
      "Training: epoch 54 batch 0 loss 0.009392046369612217\n",
      "Training: epoch 54 batch 10 loss 0.01069069467484951\n",
      "Training: epoch 54 batch 20 loss 0.006057878024876118\n",
      "Test: epoch 54 batch 0 loss 0.005898329429328442\n",
      "epoch 54 finished - avarage train loss 0.008354176317566428  avarage test loss 0.01362174772657454\n",
      "Training: epoch 55 batch 0 loss 0.004850293509662151\n",
      "Training: epoch 55 batch 10 loss 0.006304172333329916\n",
      "Training: epoch 55 batch 20 loss 0.014773384667932987\n",
      "Test: epoch 55 batch 0 loss 0.005332371219992638\n",
      "epoch 55 finished - avarage train loss 0.009550999031113139  avarage test loss 0.017977035837247968\n",
      "Training: epoch 56 batch 0 loss 0.008397243916988373\n",
      "Training: epoch 56 batch 10 loss 0.010184024460613728\n",
      "Training: epoch 56 batch 20 loss 0.005132898688316345\n",
      "Test: epoch 56 batch 0 loss 0.003689536591991782\n",
      "epoch 56 finished - avarage train loss 0.008233867320713812  avarage test loss 0.012521103082690388\n",
      "Training: epoch 57 batch 0 loss 0.004014684818685055\n",
      "Training: epoch 57 batch 10 loss 0.0029472103342413902\n",
      "Training: epoch 57 batch 20 loss 0.00703531876206398\n",
      "Test: epoch 57 batch 0 loss 0.005490553565323353\n",
      "epoch 57 finished - avarage train loss 0.007162360169378848  avarage test loss 0.013623215141706169\n",
      "Training: epoch 58 batch 0 loss 0.011920219287276268\n",
      "Training: epoch 58 batch 10 loss 0.005253469105809927\n",
      "Training: epoch 58 batch 20 loss 0.007987734861671925\n",
      "Test: epoch 58 batch 0 loss 0.004390710033476353\n",
      "epoch 58 finished - avarage train loss 0.008160785707677233  avarage test loss 0.014226468512788415\n",
      "Training: epoch 59 batch 0 loss 0.01127835363149643\n",
      "Training: epoch 59 batch 10 loss 0.004947340115904808\n",
      "Training: epoch 59 batch 20 loss 0.007680980022996664\n",
      "Test: epoch 59 batch 0 loss 0.004277197178453207\n",
      "epoch 59 finished - avarage train loss 0.007470847131555964  avarage test loss 0.014865262201055884\n",
      "Training: epoch 60 batch 0 loss 0.010269303806126118\n",
      "Training: epoch 60 batch 10 loss 0.015062783844769001\n",
      "Training: epoch 60 batch 20 loss 0.007188948802649975\n",
      "Test: epoch 60 batch 0 loss 0.003944200463593006\n",
      "epoch 60 finished - avarage train loss 0.008302523971310464  avarage test loss 0.013590400689281523\n",
      "Training: epoch 61 batch 0 loss 0.006054515950381756\n",
      "Training: epoch 61 batch 10 loss 0.004130791872739792\n",
      "Training: epoch 61 batch 20 loss 0.007897526025772095\n",
      "Test: epoch 61 batch 0 loss 0.004682249389588833\n",
      "epoch 61 finished - avarage train loss 0.007923791634625402  avarage test loss 0.013730373699218035\n",
      "Training: epoch 62 batch 0 loss 0.006833650171756744\n",
      "Training: epoch 62 batch 10 loss 0.007784143555909395\n",
      "Training: epoch 62 batch 20 loss 0.0049422443844377995\n",
      "Test: epoch 62 batch 0 loss 0.00723625160753727\n",
      "epoch 62 finished - avarage train loss 0.008544004618607718  avarage test loss 0.015359849203377962\n",
      "Training: epoch 63 batch 0 loss 0.009836733341217041\n",
      "Training: epoch 63 batch 10 loss 0.009051899425685406\n",
      "Training: epoch 63 batch 20 loss 0.004597310442477465\n",
      "Test: epoch 63 batch 0 loss 0.005952563602477312\n",
      "epoch 63 finished - avarage train loss 0.009360015167501467  avarage test loss 0.018851707922294736\n",
      "Training: epoch 64 batch 0 loss 0.004346637986600399\n",
      "Training: epoch 64 batch 10 loss 0.0063301208429038525\n",
      "Training: epoch 64 batch 20 loss 0.009654756635427475\n",
      "Test: epoch 64 batch 0 loss 0.006328764837235212\n",
      "epoch 64 finished - avarage train loss 0.00821509540626972  avarage test loss 0.014718227903358638\n",
      "Training: epoch 65 batch 0 loss 0.007647496182471514\n",
      "Training: epoch 65 batch 10 loss 0.010679272934794426\n",
      "Training: epoch 65 batch 20 loss 0.007289276458323002\n",
      "Test: epoch 65 batch 0 loss 0.0038153626956045628\n",
      "epoch 65 finished - avarage train loss 0.010541899105662415  avarage test loss 0.01279158634133637\n",
      "Training: epoch 66 batch 0 loss 0.004584746900945902\n",
      "Training: epoch 66 batch 10 loss 0.012630339711904526\n",
      "Training: epoch 66 batch 20 loss 0.005386967211961746\n",
      "Test: epoch 66 batch 0 loss 0.0035446789115667343\n",
      "epoch 66 finished - avarage train loss 0.007690044882675183  avarage test loss 0.013721456052735448\n",
      "Training: epoch 67 batch 0 loss 0.008721334859728813\n",
      "Training: epoch 67 batch 10 loss 0.006753530818969011\n",
      "Training: epoch 67 batch 20 loss 0.007210432551801205\n",
      "Test: epoch 67 batch 0 loss 0.0036852750927209854\n",
      "epoch 67 finished - avarage train loss 0.0090202613460735  avarage test loss 0.013463014853186905\n",
      "Training: epoch 68 batch 0 loss 0.005805700086057186\n",
      "Training: epoch 68 batch 10 loss 0.009134127758443356\n",
      "Training: epoch 68 batch 20 loss 0.008557731285691261\n",
      "Test: epoch 68 batch 0 loss 0.005349214654415846\n",
      "epoch 68 finished - avarage train loss 0.007865136714074118  avarage test loss 0.01309889298863709\n",
      "Training: epoch 69 batch 0 loss 0.0042661395855247974\n",
      "Training: epoch 69 batch 10 loss 0.004780587740242481\n",
      "Training: epoch 69 batch 20 loss 0.006194007117301226\n",
      "Test: epoch 69 batch 0 loss 0.0031539963092654943\n",
      "epoch 69 finished - avarage train loss 0.0078197724410686  avarage test loss 0.013416410249192268\n",
      "Training: epoch 70 batch 0 loss 0.004112257156521082\n",
      "Training: epoch 70 batch 10 loss 0.009446077980101109\n",
      "Training: epoch 70 batch 20 loss 0.008947283960878849\n",
      "Test: epoch 70 batch 0 loss 0.003098602406680584\n",
      "epoch 70 finished - avarage train loss 0.007387142002197175  avarage test loss 0.01355060818605125\n",
      "Training: epoch 71 batch 0 loss 0.007256189361214638\n",
      "Training: epoch 71 batch 10 loss 0.005786349065601826\n",
      "Training: epoch 71 batch 20 loss 0.00452648289501667\n",
      "Test: epoch 71 batch 0 loss 0.003489656839519739\n",
      "epoch 71 finished - avarage train loss 0.007703139318217491  avarage test loss 0.012445918517187238\n",
      "Training: epoch 72 batch 0 loss 0.006739635020494461\n",
      "Training: epoch 72 batch 10 loss 0.005088584031909704\n",
      "Training: epoch 72 batch 20 loss 0.010626046918332577\n",
      "Test: epoch 72 batch 0 loss 0.0034298792015761137\n",
      "epoch 72 finished - avarage train loss 0.00964254589655019  avarage test loss 0.0144443089957349\n",
      "Training: epoch 73 batch 0 loss 0.00574957812204957\n",
      "Training: epoch 73 batch 10 loss 0.004894975572824478\n",
      "Training: epoch 73 batch 20 loss 0.003963600378483534\n",
      "Test: epoch 73 batch 0 loss 0.0041289301589131355\n",
      "epoch 73 finished - avarage train loss 0.006616075177966007  avarage test loss 0.012656534439884126\n",
      "Training: epoch 74 batch 0 loss 0.006188323721289635\n",
      "Training: epoch 74 batch 10 loss 0.0029489081352949142\n",
      "Training: epoch 74 batch 20 loss 0.007415847387164831\n",
      "Test: epoch 74 batch 0 loss 0.003707475960254669\n",
      "epoch 74 finished - avarage train loss 0.006866286267879708  avarage test loss 0.012577269808389246\n",
      "Training: epoch 75 batch 0 loss 0.01116172969341278\n",
      "Training: epoch 75 batch 10 loss 0.006620360072702169\n",
      "Training: epoch 75 batch 20 loss 0.004358209669589996\n",
      "Test: epoch 75 batch 0 loss 0.004139244556427002\n",
      "epoch 75 finished - avarage train loss 0.008879913450700456  avarage test loss 0.013028963468968868\n",
      "Training: epoch 76 batch 0 loss 0.013819819316267967\n",
      "Training: epoch 76 batch 10 loss 0.005198756232857704\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training: epoch 76 batch 20 loss 0.008235563524067402\n",
      "Test: epoch 76 batch 0 loss 0.0035013651940971613\n",
      "epoch 76 finished - avarage train loss 0.008592486959593049  avarage test loss 0.012480013479944319\n",
      "Training: epoch 77 batch 0 loss 0.013118477538228035\n",
      "Training: epoch 77 batch 10 loss 0.008538328111171722\n",
      "Training: epoch 77 batch 20 loss 0.006070572882890701\n",
      "Test: epoch 77 batch 0 loss 0.006418849341571331\n",
      "epoch 77 finished - avarage train loss 0.008922135682198507  avarage test loss 0.01624973735306412\n",
      "Training: epoch 78 batch 0 loss 0.009564991109073162\n",
      "Training: epoch 78 batch 10 loss 0.0070710680447518826\n",
      "Training: epoch 78 batch 20 loss 0.005441196728497744\n",
      "Test: epoch 78 batch 0 loss 0.004791238810867071\n",
      "epoch 78 finished - avarage train loss 0.00976690162647644  avarage test loss 0.0155641152523458\n",
      "Training: epoch 79 batch 0 loss 0.003996080718934536\n",
      "Training: epoch 79 batch 10 loss 0.008298620581626892\n",
      "Training: epoch 79 batch 20 loss 0.004635264631360769\n",
      "Test: epoch 79 batch 0 loss 0.004114208742976189\n",
      "epoch 79 finished - avarage train loss 0.007598554731571469  avarage test loss 0.012815618421882391\n",
      "Training: epoch 80 batch 0 loss 0.0075440602377057076\n",
      "Training: epoch 80 batch 10 loss 0.005921577103435993\n",
      "Training: epoch 80 batch 20 loss 0.005964387208223343\n",
      "Test: epoch 80 batch 0 loss 0.004532518330961466\n",
      "epoch 80 finished - avarage train loss 0.008705481485431564  avarage test loss 0.016347647411748767\n",
      "Training: epoch 81 batch 0 loss 0.0036638944875448942\n",
      "Training: epoch 81 batch 10 loss 0.004649958107620478\n",
      "Training: epoch 81 batch 20 loss 0.008411300368607044\n",
      "Test: epoch 81 batch 0 loss 0.004067259840667248\n",
      "epoch 81 finished - avarage train loss 0.007000108016654849  avarage test loss 0.012679108185693622\n",
      "Training: epoch 82 batch 0 loss 0.004704735241830349\n",
      "Training: epoch 82 batch 10 loss 0.009419316425919533\n",
      "Training: epoch 82 batch 20 loss 0.009614693932235241\n",
      "Test: epoch 82 batch 0 loss 0.0038308091461658478\n",
      "epoch 82 finished - avarage train loss 0.008713247663951639  avarage test loss 0.013298475998453796\n",
      "Training: epoch 83 batch 0 loss 0.012363772839307785\n",
      "Training: epoch 83 batch 10 loss 0.008064868859946728\n",
      "Training: epoch 83 batch 20 loss 0.005388579796999693\n",
      "Test: epoch 83 batch 0 loss 0.0031035644933581352\n",
      "epoch 83 finished - avarage train loss 0.0080954075132593  avarage test loss 0.014003815245814621\n",
      "Training: epoch 84 batch 0 loss 0.0057825143449008465\n",
      "Training: epoch 84 batch 10 loss 0.007181839551776648\n",
      "Training: epoch 84 batch 20 loss 0.011850829236209393\n",
      "Test: epoch 84 batch 0 loss 0.004931203555315733\n",
      "epoch 84 finished - avarage train loss 0.0075121190928821935  avarage test loss 0.01775213109795004\n",
      "Training: epoch 85 batch 0 loss 0.0062635429203510284\n",
      "Training: epoch 85 batch 10 loss 0.004102407954633236\n",
      "Training: epoch 85 batch 20 loss 0.005147701129317284\n",
      "Test: epoch 85 batch 0 loss 0.0029574146028608084\n",
      "epoch 85 finished - avarage train loss 0.007184620055466376  avarage test loss 0.013012865150813013\n",
      "Training: epoch 86 batch 0 loss 0.006447561550885439\n",
      "Training: epoch 86 batch 10 loss 0.007268983405083418\n",
      "Training: epoch 86 batch 20 loss 0.0076143485493958\n",
      "Test: epoch 86 batch 0 loss 0.0031566668767482042\n",
      "epoch 86 finished - avarage train loss 0.0072169668876148505  avarage test loss 0.013070241606328636\n",
      "Training: epoch 87 batch 0 loss 0.01118847168982029\n",
      "Training: epoch 87 batch 10 loss 0.008722727186977863\n",
      "Training: epoch 87 batch 20 loss 0.006620552856475115\n",
      "Test: epoch 87 batch 0 loss 0.0037791309878230095\n",
      "epoch 87 finished - avarage train loss 0.007700349750189945  avarage test loss 0.014793999143876135\n",
      "Training: epoch 88 batch 0 loss 0.0053009833209216595\n",
      "Training: epoch 88 batch 10 loss 0.00363068375736475\n",
      "Training: epoch 88 batch 20 loss 0.006376567762345076\n",
      "Test: epoch 88 batch 0 loss 0.0033965385518968105\n",
      "epoch 88 finished - avarage train loss 0.007746552277741761  avarage test loss 0.01429386727977544\n",
      "Training: epoch 89 batch 0 loss 0.006656715180724859\n",
      "Training: epoch 89 batch 10 loss 0.01205059140920639\n",
      "Training: epoch 89 batch 20 loss 0.006937016267329454\n",
      "Test: epoch 89 batch 0 loss 0.0076848845928907394\n",
      "epoch 89 finished - avarage train loss 0.008686131508699778  avarage test loss 0.01645172550342977\n",
      "Training: epoch 90 batch 0 loss 0.009402740746736526\n",
      "Training: epoch 90 batch 10 loss 0.004995367489755154\n",
      "Training: epoch 90 batch 20 loss 0.00987361278384924\n",
      "Test: epoch 90 batch 0 loss 0.005565205588936806\n",
      "epoch 90 finished - avarage train loss 0.00908642340901083  avarage test loss 0.014200163539499044\n",
      "Training: epoch 91 batch 0 loss 0.006492723245173693\n",
      "Training: epoch 91 batch 10 loss 0.008384080603718758\n",
      "Training: epoch 91 batch 20 loss 0.005220273043960333\n",
      "Test: epoch 91 batch 0 loss 0.005591542925685644\n",
      "epoch 91 finished - avarage train loss 0.00787705375716604  avarage test loss 0.014207451953552663\n",
      "Training: epoch 92 batch 0 loss 0.0056290337815880775\n",
      "Training: epoch 92 batch 10 loss 0.01718415692448616\n",
      "Training: epoch 92 batch 20 loss 0.009217829443514347\n",
      "Test: epoch 92 batch 0 loss 0.004008983727544546\n",
      "epoch 92 finished - avarage train loss 0.010212795495794251  avarage test loss 0.015543502639047801\n",
      "Training: epoch 93 batch 0 loss 0.012272370979189873\n",
      "Training: epoch 93 batch 10 loss 0.010708878748118877\n",
      "Training: epoch 93 batch 20 loss 0.009465375915169716\n",
      "Test: epoch 93 batch 0 loss 0.003309538122266531\n",
      "epoch 93 finished - avarage train loss 0.009340523404936338  avarage test loss 0.013705724733881652\n",
      "Training: epoch 94 batch 0 loss 0.006639787461608648\n",
      "Training: epoch 94 batch 10 loss 0.008882683701813221\n",
      "Training: epoch 94 batch 20 loss 0.003806452499702573\n",
      "Test: epoch 94 batch 0 loss 0.0030536141712218523\n",
      "epoch 94 finished - avarage train loss 0.00863995546764084  avarage test loss 0.012887598422821611\n",
      "Training: epoch 95 batch 0 loss 0.004083708859980106\n",
      "Training: epoch 95 batch 10 loss 0.007082849275320768\n",
      "Training: epoch 95 batch 20 loss 0.005187286529690027\n",
      "Test: epoch 95 batch 0 loss 0.003591708140447736\n",
      "epoch 95 finished - avarage train loss 0.008008398155778133  avarage test loss 0.01530221669236198\n",
      "Training: epoch 96 batch 0 loss 0.005380504764616489\n",
      "Training: epoch 96 batch 10 loss 0.007233201526105404\n",
      "Training: epoch 96 batch 20 loss 0.009916264563798904\n",
      "Test: epoch 96 batch 0 loss 0.010820994153618813\n",
      "epoch 96 finished - avarage train loss 0.008473471931085504  avarage test loss 0.02103656390681863\n",
      "Training: epoch 97 batch 0 loss 0.011249571107327938\n",
      "Training: epoch 97 batch 10 loss 0.010441734455525875\n",
      "Training: epoch 97 batch 20 loss 0.01239385548979044\n",
      "Test: epoch 97 batch 0 loss 0.018537838011980057\n",
      "epoch 97 finished - avarage train loss 0.015338035814207176  avarage test loss 0.02819260093383491\n",
      "Training: epoch 98 batch 0 loss 0.01936349645256996\n",
      "Training: epoch 98 batch 10 loss 0.014990940690040588\n",
      "Training: epoch 98 batch 20 loss 0.007058611139655113\n",
      "Test: epoch 98 batch 0 loss 0.00530976289883256\n",
      "epoch 98 finished - avarage train loss 0.010596198495477438  avarage test loss 0.01752311538439244\n",
      "Training: epoch 99 batch 0 loss 0.00930638425052166\n",
      "Training: epoch 99 batch 10 loss 0.005785386078059673\n",
      "Training: epoch 99 batch 20 loss 0.006625419482588768\n",
      "Test: epoch 99 batch 0 loss 0.004017766565084457\n",
      "epoch 99 finished - avarage train loss 0.008036900777369738  avarage test loss 0.014843172277323902\n",
      "Training: epoch 100 batch 0 loss 0.003554424038156867\n",
      "Training: epoch 100 batch 10 loss 0.007699260953813791\n",
      "Training: epoch 100 batch 20 loss 0.006027191877365112\n",
      "Test: epoch 100 batch 0 loss 0.004874053876847029\n",
      "epoch 100 finished - avarage train loss 0.00861821743531217  avarage test loss 0.016718311817385256\n",
      "Training: epoch 101 batch 0 loss 0.008208803832530975\n",
      "Training: epoch 101 batch 10 loss 0.009241277351975441\n",
      "Training: epoch 101 batch 20 loss 0.009781821630895138\n",
      "Test: epoch 101 batch 0 loss 0.00400986522436142\n",
      "epoch 101 finished - avarage train loss 0.00786518091978184  avarage test loss 0.0135026111965999\n",
      "Training: epoch 102 batch 0 loss 0.00809490866959095\n",
      "Training: epoch 102 batch 10 loss 0.00679471530020237\n",
      "Training: epoch 102 batch 20 loss 0.005828876048326492\n",
      "Test: epoch 102 batch 0 loss 0.004123573191463947\n",
      "epoch 102 finished - avarage train loss 0.00740989996923198  avarage test loss 0.013718963833525777\n",
      "Training: epoch 103 batch 0 loss 0.008630858734250069\n",
      "Training: epoch 103 batch 10 loss 0.006885168608278036\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training: epoch 103 batch 20 loss 0.00649838475510478\n",
      "Test: epoch 103 batch 0 loss 0.009928364306688309\n",
      "epoch 103 finished - avarage train loss 0.0084114292764972  avarage test loss 0.017735708970576525\n",
      "Training: epoch 104 batch 0 loss 0.01129467599093914\n",
      "Training: epoch 104 batch 10 loss 0.010046852752566338\n",
      "Training: epoch 104 batch 20 loss 0.014672503806650639\n",
      "Test: epoch 104 batch 0 loss 0.00709183793514967\n",
      "epoch 104 finished - avarage train loss 0.013272409985676921  avarage test loss 0.015725562465377152\n",
      "Training: epoch 105 batch 0 loss 0.007770335767418146\n",
      "Training: epoch 105 batch 10 loss 0.005925382021814585\n",
      "Training: epoch 105 batch 20 loss 0.008015886880457401\n",
      "Test: epoch 105 batch 0 loss 0.011584940366446972\n",
      "epoch 105 finished - avarage train loss 0.009748728480190039  avarage test loss 0.021211014362052083\n",
      "Training: epoch 106 batch 0 loss 0.007789887487888336\n",
      "Training: epoch 106 batch 10 loss 0.016237685456871986\n",
      "Training: epoch 106 batch 20 loss 0.00913244392722845\n",
      "Test: epoch 106 batch 0 loss 0.009785383939743042\n",
      "epoch 106 finished - avarage train loss 0.012681305135503924  avarage test loss 0.016213914612308145\n",
      "Training: epoch 107 batch 0 loss 0.012011473998427391\n",
      "Training: epoch 107 batch 10 loss 0.012024378404021263\n",
      "Training: epoch 107 batch 20 loss 0.011246968992054462\n",
      "Test: epoch 107 batch 0 loss 0.00534749636426568\n",
      "epoch 107 finished - avarage train loss 0.011787940426890192  avarage test loss 0.01526984665542841\n",
      "Training: epoch 108 batch 0 loss 0.006367783527821302\n",
      "Training: epoch 108 batch 10 loss 0.007686303928494453\n",
      "Training: epoch 108 batch 20 loss 0.007365990895777941\n",
      "Test: epoch 108 batch 0 loss 0.005015202797949314\n",
      "epoch 108 finished - avarage train loss 0.007537374602116901  avarage test loss 0.013357712770812213\n",
      "Training: epoch 109 batch 0 loss 0.013956832699477673\n",
      "Training: epoch 109 batch 10 loss 0.0067567783407866955\n",
      "Training: epoch 109 batch 20 loss 0.0072837891057133675\n",
      "Test: epoch 109 batch 0 loss 0.004139712546020746\n",
      "epoch 109 finished - avarage train loss 0.009034307392570993  avarage test loss 0.013491353834979236\n",
      "Training: epoch 110 batch 0 loss 0.0038577213417738676\n",
      "Training: epoch 110 batch 10 loss 0.004471690859645605\n",
      "Training: epoch 110 batch 20 loss 0.01632961817085743\n",
      "Test: epoch 110 batch 0 loss 0.004208530765026808\n",
      "epoch 110 finished - avarage train loss 0.007327070941442046  avarage test loss 0.014770341105759144\n",
      "Training: epoch 111 batch 0 loss 0.005862229038029909\n",
      "Training: epoch 111 batch 10 loss 0.00687059061601758\n",
      "Training: epoch 111 batch 20 loss 0.006771063432097435\n",
      "Test: epoch 111 batch 0 loss 0.00444822246208787\n",
      "epoch 111 finished - avarage train loss 0.008606581555294066  avarage test loss 0.015240299748256803\n",
      "Training: epoch 112 batch 0 loss 0.004563060123473406\n",
      "Training: epoch 112 batch 10 loss 0.01017918810248375\n",
      "Training: epoch 112 batch 20 loss 0.004329992458224297\n",
      "Test: epoch 112 batch 0 loss 0.004086107946932316\n",
      "epoch 112 finished - avarage train loss 0.007469246878513488  avarage test loss 0.01368225074838847\n",
      "Training: epoch 113 batch 0 loss 0.0070005059242248535\n",
      "Training: epoch 113 batch 10 loss 0.0048632132820785046\n",
      "Training: epoch 113 batch 20 loss 0.00905997771769762\n",
      "Test: epoch 113 batch 0 loss 0.004123024642467499\n",
      "epoch 113 finished - avarage train loss 0.007506770924825607  avarage test loss 0.012853380525484681\n",
      "Training: epoch 114 batch 0 loss 0.006030345801264048\n",
      "Training: epoch 114 batch 10 loss 0.004806627053767443\n",
      "Training: epoch 114 batch 20 loss 0.009059150703251362\n",
      "Test: epoch 114 batch 0 loss 0.005389163736253977\n",
      "epoch 114 finished - avarage train loss 0.008777161257277274  avarage test loss 0.013551014591939747\n",
      "Training: epoch 115 batch 0 loss 0.006875236518681049\n",
      "Training: epoch 115 batch 10 loss 0.0065309093333780766\n",
      "Training: epoch 115 batch 20 loss 0.00781724601984024\n",
      "Test: epoch 115 batch 0 loss 0.005404429975897074\n",
      "epoch 115 finished - avarage train loss 0.006140660470480035  avarage test loss 0.013641355792060494\n",
      "Training: epoch 116 batch 0 loss 0.004852404817938805\n",
      "Training: epoch 116 batch 10 loss 0.00742741534486413\n",
      "Training: epoch 116 batch 20 loss 0.004948700312525034\n",
      "Test: epoch 116 batch 0 loss 0.004516097251325846\n",
      "epoch 116 finished - avarage train loss 0.007881508313183641  avarage test loss 0.012822430115193129\n",
      "Training: epoch 117 batch 0 loss 0.01014759298413992\n",
      "Training: epoch 117 batch 10 loss 0.0035135115031152964\n",
      "Training: epoch 117 batch 20 loss 0.008849061094224453\n",
      "Test: epoch 117 batch 0 loss 0.00583207281306386\n",
      "epoch 117 finished - avarage train loss 0.00868294579137502  avarage test loss 0.015006984467618167\n",
      "Training: epoch 118 batch 0 loss 0.006524161901324987\n",
      "Training: epoch 118 batch 10 loss 0.009664683602750301\n",
      "Training: epoch 118 batch 20 loss 0.004723135381937027\n",
      "Test: epoch 118 batch 0 loss 0.0056776502169668674\n",
      "epoch 118 finished - avarage train loss 0.00834201658465739  avarage test loss 0.01308810873888433\n",
      "Training: epoch 119 batch 0 loss 0.006620605010539293\n",
      "Training: epoch 119 batch 10 loss 0.011795231141149998\n",
      "Training: epoch 119 batch 20 loss 0.0033468063920736313\n",
      "Test: epoch 119 batch 0 loss 0.005265331361442804\n",
      "epoch 119 finished - avarage train loss 0.008193735531049556  avarage test loss 0.014334478066302836\n",
      "Training: epoch 120 batch 0 loss 0.0034392294473946095\n",
      "Training: epoch 120 batch 10 loss 0.005040594842284918\n",
      "Training: epoch 120 batch 20 loss 0.010061869397759438\n",
      "Test: epoch 120 batch 0 loss 0.004785503260791302\n",
      "epoch 120 finished - avarage train loss 0.007890747522871042  avarage test loss 0.013390033389441669\n",
      "Training: epoch 121 batch 0 loss 0.007289384491741657\n",
      "Training: epoch 121 batch 10 loss 0.008303998038172722\n",
      "Training: epoch 121 batch 20 loss 0.0048136222176253796\n",
      "Test: epoch 121 batch 0 loss 0.0033798660151660442\n",
      "epoch 121 finished - avarage train loss 0.009143782329970393  avarage test loss 0.012723257066681981\n",
      "Training: epoch 122 batch 0 loss 0.006615953054279089\n",
      "Training: epoch 122 batch 10 loss 0.003001822391524911\n",
      "Training: epoch 122 batch 20 loss 0.0037261052057147026\n",
      "Test: epoch 122 batch 0 loss 0.0037845142651349306\n",
      "epoch 122 finished - avarage train loss 0.007487266380661006  avarage test loss 0.012670583499129862\n",
      "Training: epoch 123 batch 0 loss 0.005353971850126982\n",
      "Training: epoch 123 batch 10 loss 0.006506739184260368\n",
      "Training: epoch 123 batch 20 loss 0.005430046934634447\n",
      "Test: epoch 123 batch 0 loss 0.024679511785507202\n",
      "epoch 123 finished - avarage train loss 0.007957614717426998  avarage test loss 0.028960393741726875\n",
      "Training: epoch 124 batch 0 loss 0.028212692588567734\n",
      "Training: epoch 124 batch 10 loss 0.01057937927544117\n",
      "Training: epoch 124 batch 20 loss 0.02042355015873909\n",
      "Test: epoch 124 batch 0 loss 0.006708214990794659\n",
      "epoch 124 finished - avarage train loss 0.020728333355409318  avarage test loss 0.01669617067091167\n",
      "Training: epoch 125 batch 0 loss 0.007394122891128063\n",
      "Training: epoch 125 batch 10 loss 0.010354366153478622\n",
      "Training: epoch 125 batch 20 loss 0.005059343762695789\n",
      "Test: epoch 125 batch 0 loss 0.003982745110988617\n",
      "epoch 125 finished - avarage train loss 0.01063338569204869  avarage test loss 0.014444908476434648\n",
      "Training: epoch 126 batch 0 loss 0.006955002434551716\n",
      "Training: epoch 126 batch 10 loss 0.007146539632230997\n",
      "Training: epoch 126 batch 20 loss 0.004537531640380621\n",
      "Test: epoch 126 batch 0 loss 0.0033053553197532892\n",
      "epoch 126 finished - avarage train loss 0.007650862560318461  avarage test loss 0.012322423805017024\n",
      "Training: epoch 127 batch 0 loss 0.00577466981485486\n",
      "Training: epoch 127 batch 10 loss 0.009022565558552742\n",
      "Training: epoch 127 batch 20 loss 0.006158876232802868\n",
      "Test: epoch 127 batch 0 loss 0.003800205886363983\n",
      "epoch 127 finished - avarage train loss 0.00762950808837496  avarage test loss 0.01262053637765348\n",
      "Training: epoch 128 batch 0 loss 0.005880323704332113\n",
      "Training: epoch 128 batch 10 loss 0.00885841529816389\n",
      "Training: epoch 128 batch 20 loss 0.004352973308414221\n",
      "Test: epoch 128 batch 0 loss 0.004151646979153156\n",
      "epoch 128 finished - avarage train loss 0.009115926680508358  avarage test loss 0.01255325973033905\n",
      "Training: epoch 129 batch 0 loss 0.004317013081163168\n",
      "Training: epoch 129 batch 10 loss 0.006995788775384426\n",
      "Training: epoch 129 batch 20 loss 0.007590069435536861\n",
      "Test: epoch 129 batch 0 loss 0.0032833421137183905\n",
      "epoch 129 finished - avarage train loss 0.008672708171773059  avarage test loss 0.012273477215785533\n",
      "Training: epoch 130 batch 0 loss 0.005607725586742163\n",
      "Training: epoch 130 batch 10 loss 0.011774628423154354\n",
      "Training: epoch 130 batch 20 loss 0.014828454703092575\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test: epoch 130 batch 0 loss 0.0030570845119655132\n",
      "epoch 130 finished - avarage train loss 0.008366858265523252  avarage test loss 0.012377524864859879\n",
      "Training: epoch 131 batch 0 loss 0.004436907824128866\n",
      "Training: epoch 131 batch 10 loss 0.003408861579373479\n",
      "Training: epoch 131 batch 20 loss 0.0028995261527597904\n",
      "Test: epoch 131 batch 0 loss 0.003303558100014925\n",
      "epoch 131 finished - avarage train loss 0.008100756684896248  avarage test loss 0.012503455742262304\n",
      "Training: epoch 132 batch 0 loss 0.005101489368826151\n",
      "Training: epoch 132 batch 10 loss 0.004864922724664211\n",
      "Training: epoch 132 batch 20 loss 0.00438701780512929\n",
      "Test: epoch 132 batch 0 loss 0.004544748924672604\n",
      "epoch 132 finished - avarage train loss 0.0072557556619546534  avarage test loss 0.012782199075445533\n",
      "Training: epoch 133 batch 0 loss 0.0043072570115327835\n",
      "Training: epoch 133 batch 10 loss 0.0053386082872748375\n",
      "Training: epoch 133 batch 20 loss 0.005252717062830925\n",
      "Test: epoch 133 batch 0 loss 0.003969167359173298\n",
      "epoch 133 finished - avarage train loss 0.008386190121608048  avarage test loss 0.013014549273066223\n",
      "Training: epoch 134 batch 0 loss 0.0072531020268797874\n",
      "Training: epoch 134 batch 10 loss 0.006892706733196974\n",
      "Training: epoch 134 batch 20 loss 0.012140309438109398\n",
      "Test: epoch 134 batch 0 loss 0.003608080092817545\n",
      "epoch 134 finished - avarage train loss 0.006125866419410911  avarage test loss 0.0125823097769171\n",
      "Training: epoch 135 batch 0 loss 0.00768588250502944\n",
      "Training: epoch 135 batch 10 loss 0.005967677570879459\n",
      "Training: epoch 135 batch 20 loss 0.006009214092046022\n",
      "Test: epoch 135 batch 0 loss 0.005042826756834984\n",
      "epoch 135 finished - avarage train loss 0.007846328887510402  avarage test loss 0.01312650425825268\n",
      "Training: epoch 136 batch 0 loss 0.005901531782001257\n",
      "Training: epoch 136 batch 10 loss 0.006317229475826025\n",
      "Training: epoch 136 batch 20 loss 0.006048011127859354\n",
      "Test: epoch 136 batch 0 loss 0.003896474838256836\n",
      "epoch 136 finished - avarage train loss 0.009901168547442248  avarage test loss 0.012660366483032703\n",
      "Training: epoch 137 batch 0 loss 0.005741518456488848\n",
      "Training: epoch 137 batch 10 loss 0.003739840118214488\n",
      "Training: epoch 137 batch 20 loss 0.0031456369906663895\n",
      "Test: epoch 137 batch 0 loss 0.003809757065027952\n",
      "epoch 137 finished - avarage train loss 0.00835404740164763  avarage test loss 0.012278391979634762\n",
      "Training: epoch 138 batch 0 loss 0.0058146375231444836\n",
      "Training: epoch 138 batch 10 loss 0.0065963673405349255\n",
      "Training: epoch 138 batch 20 loss 0.007037003990262747\n",
      "Test: epoch 138 batch 0 loss 0.0035610711202025414\n",
      "epoch 138 finished - avarage train loss 0.007299159926458679  avarage test loss 0.012389623909257352\n",
      "Training: epoch 139 batch 0 loss 0.005948066711425781\n",
      "Training: epoch 139 batch 10 loss 0.005581032019108534\n",
      "Training: epoch 139 batch 20 loss 0.006342372857034206\n",
      "Test: epoch 139 batch 0 loss 0.003509167116135359\n",
      "epoch 139 finished - avarage train loss 0.007309031889549103  avarage test loss 0.013456494431011379\n",
      "Training: epoch 140 batch 0 loss 0.011463063769042492\n",
      "Training: epoch 140 batch 10 loss 0.013545578345656395\n",
      "Training: epoch 140 batch 20 loss 0.006061132997274399\n",
      "Test: epoch 140 batch 0 loss 0.00387836922891438\n",
      "epoch 140 finished - avarage train loss 0.008033708083154312  avarage test loss 0.013617960212286562\n",
      "Training: epoch 141 batch 0 loss 0.004224162083119154\n",
      "Training: epoch 141 batch 10 loss 0.004753698594868183\n",
      "Training: epoch 141 batch 20 loss 0.008073907345533371\n",
      "Test: epoch 141 batch 0 loss 0.0034710164181888103\n",
      "epoch 141 finished - avarage train loss 0.008055695453016409  avarage test loss 0.012504203477874398\n",
      "Training: epoch 142 batch 0 loss 0.008948618546128273\n",
      "Training: epoch 142 batch 10 loss 0.005963036324828863\n",
      "Training: epoch 142 batch 20 loss 0.00798376277089119\n",
      "Test: epoch 142 batch 0 loss 0.003356418339535594\n",
      "epoch 142 finished - avarage train loss 0.007807228971144249  avarage test loss 0.012334561615716666\n",
      "Training: epoch 143 batch 0 loss 0.003171038581058383\n",
      "Training: epoch 143 batch 10 loss 0.0035326220095157623\n",
      "Training: epoch 143 batch 20 loss 0.008929443545639515\n",
      "Test: epoch 143 batch 0 loss 0.003775176592171192\n",
      "epoch 143 finished - avarage train loss 0.007962670715140372  avarage test loss 0.014297388726845384\n",
      "Training: epoch 144 batch 0 loss 0.00786373857408762\n",
      "Training: epoch 144 batch 10 loss 0.002759661292657256\n",
      "Training: epoch 144 batch 20 loss 0.005157974548637867\n",
      "Test: epoch 144 batch 0 loss 0.005088808946311474\n",
      "epoch 144 finished - avarage train loss 0.006775876250246476  avarage test loss 0.013070313725620508\n",
      "Training: epoch 145 batch 0 loss 0.009125515818595886\n",
      "Training: epoch 145 batch 10 loss 0.005551828537136316\n",
      "Training: epoch 145 batch 20 loss 0.0064880079589784145\n",
      "Test: epoch 145 batch 0 loss 0.005877973511815071\n",
      "epoch 145 finished - avarage train loss 0.007688827957187234  avarage test loss 0.013565457775257528\n",
      "Training: epoch 146 batch 0 loss 0.0074868579395115376\n",
      "Training: epoch 146 batch 10 loss 0.017839541658759117\n",
      "Training: epoch 146 batch 20 loss 0.007780495099723339\n",
      "Test: epoch 146 batch 0 loss 0.005894818808883429\n",
      "epoch 146 finished - avarage train loss 0.008076172731495622  avarage test loss 0.013466234784573317\n",
      "Training: epoch 147 batch 0 loss 0.00883482862263918\n",
      "Training: epoch 147 batch 10 loss 0.0038270638324320316\n",
      "Training: epoch 147 batch 20 loss 0.005345460027456284\n",
      "Test: epoch 147 batch 0 loss 0.0034501133486628532\n",
      "epoch 147 finished - avarage train loss 0.008427128067304349  avarage test loss 0.012658194173127413\n",
      "Training: epoch 148 batch 0 loss 0.0044991676695644855\n",
      "Training: epoch 148 batch 10 loss 0.0020603227894753218\n",
      "Training: epoch 148 batch 20 loss 0.005386773496866226\n",
      "Test: epoch 148 batch 0 loss 0.005028979852795601\n",
      "epoch 148 finished - avarage train loss 0.008115123997538769  avarage test loss 0.015585809713229537\n",
      "Training: epoch 149 batch 0 loss 0.005724905524402857\n",
      "Training: epoch 149 batch 10 loss 0.008365294896066189\n",
      "Training: epoch 149 batch 20 loss 0.008997070603072643\n",
      "Test: epoch 149 batch 0 loss 0.011382992379367352\n",
      "epoch 149 finished - avarage train loss 0.01073290651728367  avarage test loss 0.023050846066325903\n",
      "Training: epoch 150 batch 0 loss 0.005407524295151234\n",
      "Training: epoch 150 batch 10 loss 0.015242103487253189\n",
      "Training: epoch 150 batch 20 loss 0.008984116837382317\n",
      "Test: epoch 150 batch 0 loss 0.008145369589328766\n",
      "epoch 150 finished - avarage train loss 0.009666981191599163  avarage test loss 0.019814619328826666\n",
      "Training: epoch 151 batch 0 loss 0.006514193490147591\n",
      "Training: epoch 151 batch 10 loss 0.011137844063341618\n",
      "Training: epoch 151 batch 20 loss 0.008285698480904102\n",
      "Test: epoch 151 batch 0 loss 0.003643203526735306\n",
      "epoch 151 finished - avarage train loss 0.008935588139012969  avarage test loss 0.012880744179710746\n",
      "Training: epoch 152 batch 0 loss 0.006356354337185621\n",
      "Training: epoch 152 batch 10 loss 0.005217874422669411\n",
      "Training: epoch 152 batch 20 loss 0.006992880254983902\n",
      "Test: epoch 152 batch 0 loss 0.0033543063327670097\n",
      "epoch 152 finished - avarage train loss 0.007615022056190105  avarage test loss 0.013099718606099486\n",
      "Training: epoch 153 batch 0 loss 0.006246116943657398\n",
      "Training: epoch 153 batch 10 loss 0.009874590672552586\n",
      "Training: epoch 153 batch 20 loss 0.00701966742053628\n",
      "Test: epoch 153 batch 0 loss 0.0033419819083064795\n",
      "epoch 153 finished - avarage train loss 0.007568201387365316  avarage test loss 0.013114102242980152\n",
      "Training: epoch 154 batch 0 loss 0.0073326947167515755\n",
      "Training: epoch 154 batch 10 loss 0.006386443041265011\n",
      "Training: epoch 154 batch 20 loss 0.0042761825025081635\n",
      "Test: epoch 154 batch 0 loss 0.003960442263633013\n",
      "epoch 154 finished - avarage train loss 0.009510052807886025  avarage test loss 0.012398099177516997\n",
      "Training: epoch 155 batch 0 loss 0.00787206832319498\n",
      "Training: epoch 155 batch 10 loss 0.0037931096740067005\n",
      "Training: epoch 155 batch 20 loss 0.014587277546525002\n",
      "Test: epoch 155 batch 0 loss 0.0033944426104426384\n",
      "epoch 155 finished - avarage train loss 0.007865992719949833  avarage test loss 0.012191358720883727\n",
      "Training: epoch 156 batch 0 loss 0.0049612135626375675\n",
      "Training: epoch 156 batch 10 loss 0.0028853476978838444\n",
      "Training: epoch 156 batch 20 loss 0.002532240469008684\n",
      "Test: epoch 156 batch 0 loss 0.0035328371450304985\n",
      "epoch 156 finished - avarage train loss 0.0070919515163605585  avarage test loss 0.01335821335669607\n",
      "Training: epoch 157 batch 0 loss 0.006559545174241066\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training: epoch 157 batch 10 loss 0.006065468303859234\n",
      "Training: epoch 157 batch 20 loss 0.007623712532222271\n",
      "Test: epoch 157 batch 0 loss 0.0030198253225535154\n",
      "epoch 157 finished - avarage train loss 0.00808319636492123  avarage test loss 0.01317644485970959\n",
      "Training: epoch 158 batch 0 loss 0.00699582789093256\n",
      "Training: epoch 158 batch 10 loss 0.006139424163848162\n",
      "Training: epoch 158 batch 20 loss 0.006569500081241131\n",
      "Test: epoch 158 batch 0 loss 0.0035507804714143276\n",
      "epoch 158 finished - avarage train loss 0.00868927673757847  avarage test loss 0.013882669853046536\n",
      "Training: epoch 159 batch 0 loss 0.010266716592013836\n",
      "Training: epoch 159 batch 10 loss 0.00639422656968236\n",
      "Training: epoch 159 batch 20 loss 0.007625577971339226\n",
      "Test: epoch 159 batch 0 loss 0.00321614695712924\n",
      "epoch 159 finished - avarage train loss 0.010129377077301514  avarage test loss 0.01243194576818496\n",
      "Training: epoch 160 batch 0 loss 0.004918332677334547\n",
      "Training: epoch 160 batch 10 loss 0.010014770552515984\n",
      "Training: epoch 160 batch 20 loss 0.004606312606483698\n",
      "Test: epoch 160 batch 0 loss 0.005159108433872461\n",
      "epoch 160 finished - avarage train loss 0.008770807485642105  avarage test loss 0.014046913362108171\n",
      "Training: epoch 161 batch 0 loss 0.013932887464761734\n",
      "Training: epoch 161 batch 10 loss 0.013978073373436928\n",
      "Training: epoch 161 batch 20 loss 0.009031740948557854\n",
      "Test: epoch 161 batch 0 loss 0.004087138455361128\n",
      "epoch 161 finished - avarage train loss 0.009277683987828165  avarage test loss 0.012870323029346764\n",
      "Training: epoch 162 batch 0 loss 0.010631206445395947\n",
      "Training: epoch 162 batch 10 loss 0.0029599270783364773\n",
      "Training: epoch 162 batch 20 loss 0.005991098936647177\n",
      "Test: epoch 162 batch 0 loss 0.0049701472744345665\n",
      "epoch 162 finished - avarage train loss 0.007232792129547432  avarage test loss 0.01418739277869463\n",
      "Training: epoch 163 batch 0 loss 0.007723596878349781\n",
      "Training: epoch 163 batch 10 loss 0.006651781965047121\n",
      "Training: epoch 163 batch 20 loss 0.007561604958027601\n",
      "Test: epoch 163 batch 0 loss 0.003891734406352043\n",
      "epoch 163 finished - avarage train loss 0.007882666934666962  avarage test loss 0.013355390168726444\n",
      "Training: epoch 164 batch 0 loss 0.011117137968540192\n",
      "Training: epoch 164 batch 10 loss 0.00471448665484786\n",
      "Training: epoch 164 batch 20 loss 0.006934520788490772\n",
      "Test: epoch 164 batch 0 loss 0.00346635514870286\n",
      "epoch 164 finished - avarage train loss 0.009056721059284333  avarage test loss 0.013722414267249405\n",
      "Training: epoch 165 batch 0 loss 0.006931125186383724\n",
      "Training: epoch 165 batch 10 loss 0.002873200224712491\n",
      "Training: epoch 165 batch 20 loss 0.006251439452171326\n",
      "Test: epoch 165 batch 0 loss 0.003275802358984947\n",
      "epoch 165 finished - avarage train loss 0.00691104274050429  avarage test loss 0.012811240856535733\n",
      "Training: epoch 166 batch 0 loss 0.005639423616230488\n",
      "Training: epoch 166 batch 10 loss 0.00725909136235714\n",
      "Training: epoch 166 batch 20 loss 0.012535344809293747\n",
      "Test: epoch 166 batch 0 loss 0.0032838659826666117\n",
      "epoch 166 finished - avarage train loss 0.00720000485407895  avarage test loss 0.013589652662631124\n",
      "Training: epoch 167 batch 0 loss 0.007239021826535463\n",
      "Training: epoch 167 batch 10 loss 0.006263083778321743\n",
      "Training: epoch 167 batch 20 loss 0.004970995709300041\n",
      "Test: epoch 167 batch 0 loss 0.004013748839497566\n",
      "epoch 167 finished - avarage train loss 0.0069123583299846485  avarage test loss 0.012571509112603962\n",
      "Training: epoch 168 batch 0 loss 0.009595679119229317\n",
      "Training: epoch 168 batch 10 loss 0.003136758226901293\n",
      "Training: epoch 168 batch 20 loss 0.007127430755645037\n",
      "Test: epoch 168 batch 0 loss 0.003893071785569191\n",
      "epoch 168 finished - avarage train loss 0.007099233848717192  avarage test loss 0.012727281311526895\n",
      "Training: epoch 169 batch 0 loss 0.007598706521093845\n",
      "Training: epoch 169 batch 10 loss 0.0027081663720309734\n",
      "Training: epoch 169 batch 20 loss 0.007850158959627151\n",
      "Test: epoch 169 batch 0 loss 0.0037966289091855288\n",
      "epoch 169 finished - avarage train loss 0.007045544001498613  avarage test loss 0.01349529082654044\n",
      "Training: epoch 170 batch 0 loss 0.006254428066313267\n",
      "Training: epoch 170 batch 10 loss 0.010439366102218628\n",
      "Training: epoch 170 batch 20 loss 0.00822401512414217\n",
      "Test: epoch 170 batch 0 loss 0.0053267222829163074\n",
      "epoch 170 finished - avarage train loss 0.00946607154890381  avarage test loss 0.014922417351044714\n",
      "Training: epoch 171 batch 0 loss 0.007900658994913101\n",
      "Training: epoch 171 batch 10 loss 0.006789226550608873\n",
      "Training: epoch 171 batch 20 loss 0.005422287620604038\n",
      "Test: epoch 171 batch 0 loss 0.004775777459144592\n",
      "epoch 171 finished - avarage train loss 0.008933740154165646  avarage test loss 0.01432551583275199\n",
      "Training: epoch 172 batch 0 loss 0.004871763754636049\n",
      "Training: epoch 172 batch 10 loss 0.014062231406569481\n",
      "Training: epoch 172 batch 20 loss 0.008449217304587364\n",
      "Test: epoch 172 batch 0 loss 0.004393694922327995\n",
      "epoch 172 finished - avarage train loss 0.009010984634595185  avarage test loss 0.014462887891568244\n",
      "Training: epoch 173 batch 0 loss 0.0028715315274894238\n",
      "Training: epoch 173 batch 10 loss 0.011559399776160717\n",
      "Training: epoch 173 batch 20 loss 0.0050653861835598946\n",
      "Test: epoch 173 batch 0 loss 0.003651808947324753\n",
      "epoch 173 finished - avarage train loss 0.007610665184670481  avarage test loss 0.012523332261480391\n",
      "Training: epoch 174 batch 0 loss 0.004781757947057486\n",
      "Training: epoch 174 batch 10 loss 0.007272487040609121\n",
      "Training: epoch 174 batch 20 loss 0.005977754946798086\n",
      "Test: epoch 174 batch 0 loss 0.003570869565010071\n",
      "epoch 174 finished - avarage train loss 0.008220975768977198  avarage test loss 0.012936087790876627\n",
      "Training: epoch 175 batch 0 loss 0.007231527008116245\n",
      "Training: epoch 175 batch 10 loss 0.00802876427769661\n",
      "Training: epoch 175 batch 20 loss 0.004166267346590757\n",
      "Test: epoch 175 batch 0 loss 0.004430468659847975\n",
      "epoch 175 finished - avarage train loss 0.009114290603661332  avarage test loss 0.013149144360795617\n",
      "Training: epoch 176 batch 0 loss 0.010038061998784542\n",
      "Training: epoch 176 batch 10 loss 0.005275622010231018\n",
      "Training: epoch 176 batch 20 loss 0.007097836118191481\n",
      "Test: epoch 176 batch 0 loss 0.004077489487826824\n",
      "epoch 176 finished - avarage train loss 0.007999876489991257  avarage test loss 0.015420500771142542\n",
      "Training: epoch 177 batch 0 loss 0.0048296526074409485\n",
      "Training: epoch 177 batch 10 loss 0.010708611458539963\n",
      "Training: epoch 177 batch 20 loss 0.006216614972800016\n",
      "Test: epoch 177 batch 0 loss 0.0055316658690571785\n",
      "epoch 177 finished - avarage train loss 0.008777582439883002  avarage test loss 0.01699116686359048\n",
      "Training: epoch 178 batch 0 loss 0.012082151137292385\n",
      "Training: epoch 178 batch 10 loss 0.006703533232212067\n",
      "Training: epoch 178 batch 20 loss 0.009723637253046036\n",
      "Test: epoch 178 batch 0 loss 0.0034352485090494156\n",
      "epoch 178 finished - avarage train loss 0.008147066105799428  avarage test loss 0.014300279202871025\n",
      "Training: epoch 179 batch 0 loss 0.005826523527503014\n",
      "Training: epoch 179 batch 10 loss 0.007639733143150806\n",
      "Training: epoch 179 batch 20 loss 0.008784888312220573\n",
      "Test: epoch 179 batch 0 loss 0.003735173027962446\n",
      "epoch 179 finished - avarage train loss 0.007612767308179675  avarage test loss 0.012598648318089545\n",
      "Training: epoch 180 batch 0 loss 0.005643605720251799\n",
      "Training: epoch 180 batch 10 loss 0.007773832883685827\n",
      "Training: epoch 180 batch 20 loss 0.006648643873631954\n",
      "Test: epoch 180 batch 0 loss 0.004476528614759445\n",
      "epoch 180 finished - avarage train loss 0.007703629747868098  avarage test loss 0.013032292015850544\n",
      "Training: epoch 181 batch 0 loss 0.007100082468241453\n",
      "Training: epoch 181 batch 10 loss 0.004359412472695112\n",
      "Training: epoch 181 batch 20 loss 0.003218746976926923\n",
      "Test: epoch 181 batch 0 loss 0.005466108676046133\n",
      "epoch 181 finished - avarage train loss 0.006815293177576928  avarage test loss 0.01408237183932215\n",
      "Training: epoch 182 batch 0 loss 0.0066070640459656715\n",
      "Training: epoch 182 batch 10 loss 0.01339985616505146\n",
      "Training: epoch 182 batch 20 loss 0.0066612339578568935\n",
      "Test: epoch 182 batch 0 loss 0.009501678869128227\n",
      "epoch 182 finished - avarage train loss 0.010588668599917457  avarage test loss 0.018524965504184365\n",
      "Training: epoch 183 batch 0 loss 0.01298721693456173\n",
      "Training: epoch 183 batch 10 loss 0.007194609381258488\n",
      "Training: epoch 183 batch 20 loss 0.0048542022705078125\n",
      "Test: epoch 183 batch 0 loss 0.004576240666210651\n",
      "epoch 183 finished - avarage train loss 0.00842638895044039  avarage test loss 0.013460828107781708\n",
      "Training: epoch 184 batch 0 loss 0.007739720866084099\n",
      "Training: epoch 184 batch 10 loss 0.007663538213819265\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training: epoch 184 batch 20 loss 0.00871351920068264\n",
      "Test: epoch 184 batch 0 loss 0.004503544885665178\n",
      "epoch 184 finished - avarage train loss 0.008547965999178845  avarage test loss 0.013437210815027356\n",
      "Training: epoch 185 batch 0 loss 0.004962117411196232\n",
      "Training: epoch 185 batch 10 loss 0.006512414198368788\n",
      "Training: epoch 185 batch 20 loss 0.0030209689866751432\n",
      "Test: epoch 185 batch 0 loss 0.004162154626101255\n",
      "epoch 185 finished - avarage train loss 0.00795400527241672  avarage test loss 0.01262727053835988\n",
      "Training: epoch 186 batch 0 loss 0.004080404527485371\n",
      "Training: epoch 186 batch 10 loss 0.010464703664183617\n",
      "Training: epoch 186 batch 20 loss 0.007728045340627432\n",
      "Test: epoch 186 batch 0 loss 0.0100389514118433\n",
      "epoch 186 finished - avarage train loss 0.008403223481607335  avarage test loss 0.018727009650319815\n",
      "Training: epoch 187 batch 0 loss 0.014522314071655273\n",
      "Training: epoch 187 batch 10 loss 0.013031743466854095\n",
      "Training: epoch 187 batch 20 loss 0.015330241061747074\n",
      "Test: epoch 187 batch 0 loss 0.008922611363232136\n",
      "epoch 187 finished - avarage train loss 0.01584499351808737  avarage test loss 0.018043552292510867\n",
      "Training: epoch 188 batch 0 loss 0.007545869331806898\n",
      "Training: epoch 188 batch 10 loss 0.005607311148196459\n",
      "Training: epoch 188 batch 20 loss 0.010699975304305553\n",
      "Test: epoch 188 batch 0 loss 0.0072058383375406265\n",
      "epoch 188 finished - avarage train loss 0.00954787047772572  avarage test loss 0.01558023679535836\n",
      "Training: epoch 189 batch 0 loss 0.006715038791298866\n",
      "Training: epoch 189 batch 10 loss 0.012808093801140785\n",
      "Training: epoch 189 batch 20 loss 0.009564644657075405\n",
      "Test: epoch 189 batch 0 loss 0.0056614950299263\n",
      "epoch 189 finished - avarage train loss 0.010829744876587185  avarage test loss 0.020784628810361028\n",
      "Training: epoch 190 batch 0 loss 0.005901564843952656\n",
      "Training: epoch 190 batch 10 loss 0.003642066614702344\n",
      "Training: epoch 190 batch 20 loss 0.0063635664992034435\n",
      "Test: epoch 190 batch 0 loss 0.003283632453531027\n",
      "epoch 190 finished - avarage train loss 0.009281970295605475  avarage test loss 0.01538691355381161\n",
      "Training: epoch 191 batch 0 loss 0.006710313726216555\n",
      "Training: epoch 191 batch 10 loss 0.008324586786329746\n",
      "Training: epoch 191 batch 20 loss 0.010547636076807976\n",
      "Test: epoch 191 batch 0 loss 0.005217328201979399\n",
      "epoch 191 finished - avarage train loss 0.007897624585392147  avarage test loss 0.01354047330096364\n",
      "Training: epoch 192 batch 0 loss 0.006377859506756067\n",
      "Training: epoch 192 batch 10 loss 0.0057640522718429565\n",
      "Training: epoch 192 batch 20 loss 0.0075419130735099316\n",
      "Test: epoch 192 batch 0 loss 0.003537125885486603\n",
      "epoch 192 finished - avarage train loss 0.008231171438920087  avarage test loss 0.014167943736538291\n",
      "Training: epoch 193 batch 0 loss 0.007087786681950092\n",
      "Training: epoch 193 batch 10 loss 0.008019940927624702\n",
      "Training: epoch 193 batch 20 loss 0.008316085673868656\n",
      "Test: epoch 193 batch 0 loss 0.05562761053442955\n",
      "epoch 193 finished - avarage train loss 0.008787945699717465  avarage test loss 0.0588418273255229\n",
      "Training: epoch 194 batch 0 loss 0.038223475217819214\n",
      "Training: epoch 194 batch 10 loss 0.02280551940202713\n",
      "Training: epoch 194 batch 20 loss 0.03668595477938652\n",
      "Test: epoch 194 batch 0 loss 0.022407513111829758\n",
      "epoch 194 finished - avarage train loss 0.02831777271910988  avarage test loss 0.031938109546899796\n",
      "Training: epoch 195 batch 0 loss 0.010394341312348843\n",
      "Training: epoch 195 batch 10 loss 0.010053716599941254\n",
      "Training: epoch 195 batch 20 loss 0.013947747647762299\n",
      "Test: epoch 195 batch 0 loss 0.020822493359446526\n",
      "epoch 195 finished - avarage train loss 0.01607801787683676  avarage test loss 0.03043280215933919\n",
      "Training: epoch 196 batch 0 loss 0.018829870969057083\n",
      "Training: epoch 196 batch 10 loss 0.013489150442183018\n",
      "Training: epoch 196 batch 20 loss 0.007047281600534916\n",
      "Test: epoch 196 batch 0 loss 0.01769823208451271\n",
      "epoch 196 finished - avarage train loss 0.014379812195768645  avarage test loss 0.028300307225435972\n",
      "Training: epoch 197 batch 0 loss 0.008249940350651741\n",
      "Training: epoch 197 batch 10 loss 0.014309733174741268\n",
      "Training: epoch 197 batch 20 loss 0.010011729784309864\n",
      "Test: epoch 197 batch 0 loss 0.010482314974069595\n",
      "epoch 197 finished - avarage train loss 0.011144979086158604  avarage test loss 0.023079385166056454\n",
      "Training: epoch 198 batch 0 loss 0.011666379868984222\n",
      "Training: epoch 198 batch 10 loss 0.008235031738877296\n",
      "Training: epoch 198 batch 20 loss 0.007902918383479118\n",
      "Test: epoch 198 batch 0 loss 0.011335664428770542\n",
      "epoch 198 finished - avarage train loss 0.008695642744865397  avarage test loss 0.02415778348222375\n",
      "Training: epoch 199 batch 0 loss 0.008055832237005234\n",
      "Training: epoch 199 batch 10 loss 0.010194284841418266\n",
      "Training: epoch 199 batch 20 loss 0.006879420485347509\n",
      "Test: epoch 199 batch 0 loss 0.008997926488518715\n",
      "epoch 199 finished - avarage train loss 0.008283779153536105  avarage test loss 0.02318361564539373\n",
      "Training: epoch 0 batch 0 loss 0.6854637265205383\n",
      "Training: epoch 0 batch 10 loss 0.5823178291320801\n",
      "Training: epoch 0 batch 20 loss 0.6030250787734985\n",
      "Test: epoch 0 batch 0 loss 0.43108445405960083\n",
      "epoch 0 finished - avarage train loss 0.5276935624665228  avarage test loss 0.5232784152030945\n",
      "Training: epoch 1 batch 0 loss 0.53154456615448\n",
      "Training: epoch 1 batch 10 loss 0.4318762421607971\n",
      "Training: epoch 1 batch 20 loss 0.6081993579864502\n",
      "Test: epoch 1 batch 0 loss 0.4373725652694702\n",
      "epoch 1 finished - avarage train loss 0.5227371351472263  avarage test loss 0.5199063196778297\n",
      "Training: epoch 2 batch 0 loss 0.39315998554229736\n",
      "Training: epoch 2 batch 10 loss 0.4451078772544861\n",
      "Training: epoch 2 batch 20 loss 0.5145097970962524\n",
      "Test: epoch 2 batch 0 loss 0.45198217034339905\n",
      "epoch 2 finished - avarage train loss 0.5127956846664692  avarage test loss 0.5156263411045074\n",
      "Training: epoch 3 batch 0 loss 0.5034038424491882\n",
      "Training: epoch 3 batch 10 loss 0.4441818594932556\n",
      "Training: epoch 3 batch 20 loss 0.37322473526000977\n",
      "Test: epoch 3 batch 0 loss 0.4231378734111786\n",
      "epoch 3 finished - avarage train loss 0.5113310741967169  avarage test loss 0.49161630123853683\n",
      "Training: epoch 4 batch 0 loss 0.4194732904434204\n",
      "Training: epoch 4 batch 10 loss 0.23414406180381775\n",
      "Training: epoch 4 batch 20 loss 0.14667150378227234\n",
      "Test: epoch 4 batch 0 loss 0.06722135096788406\n",
      "epoch 4 finished - avarage train loss 0.25688250953781194  avarage test loss 0.07177905831485987\n",
      "Training: epoch 5 batch 0 loss 0.060597073286771774\n",
      "Training: epoch 5 batch 10 loss 0.043158382177352905\n",
      "Training: epoch 5 batch 20 loss 0.029598647728562355\n",
      "Test: epoch 5 batch 0 loss 0.027851484715938568\n",
      "epoch 5 finished - avarage train loss 0.03771543396829531  avarage test loss 0.0383370378986001\n",
      "Training: epoch 6 batch 0 loss 0.02748960070312023\n",
      "Training: epoch 6 batch 10 loss 0.015426184982061386\n",
      "Training: epoch 6 batch 20 loss 0.01508778054267168\n",
      "Test: epoch 6 batch 0 loss 0.021771974861621857\n",
      "epoch 6 finished - avarage train loss 0.01864066260770477  avarage test loss 0.03014018153771758\n",
      "Training: epoch 7 batch 0 loss 0.010073515586555004\n",
      "Training: epoch 7 batch 10 loss 0.009325629100203514\n",
      "Training: epoch 7 batch 20 loss 0.00697430782020092\n",
      "Test: epoch 7 batch 0 loss 0.015239113941788673\n",
      "epoch 7 finished - avarage train loss 0.012478399877275887  avarage test loss 0.027181079145520926\n",
      "Training: epoch 8 batch 0 loss 0.017707983031868935\n",
      "Training: epoch 8 batch 10 loss 0.007761077955365181\n",
      "Training: epoch 8 batch 20 loss 0.0052299220114946365\n",
      "Test: epoch 8 batch 0 loss 0.006971842143684626\n",
      "epoch 8 finished - avarage train loss 0.009792222320262728  avarage test loss 0.018973480560816824\n",
      "Training: epoch 9 batch 0 loss 0.004940404556691647\n",
      "Training: epoch 9 batch 10 loss 0.007492955308407545\n",
      "Training: epoch 9 batch 20 loss 0.009045940823853016\n",
      "Test: epoch 9 batch 0 loss 0.00628150487318635\n",
      "epoch 9 finished - avarage train loss 0.008423774471056873  avarage test loss 0.01759289368055761\n",
      "Training: epoch 10 batch 0 loss 0.004384924657642841\n",
      "Training: epoch 10 batch 10 loss 0.0043935407884418964\n",
      "Training: epoch 10 batch 20 loss 0.006974441930651665\n",
      "Test: epoch 10 batch 0 loss 0.005435233470052481\n",
      "epoch 10 finished - avarage train loss 0.007693446080746322  avarage test loss 0.017166073666885495\n",
      "Training: epoch 11 batch 0 loss 0.0058219898492097855\n",
      "Training: epoch 11 batch 10 loss 0.009280035272240639\n",
      "Training: epoch 11 batch 20 loss 0.009994992054998875\n",
      "Test: epoch 11 batch 0 loss 0.005859817378222942\n",
      "epoch 11 finished - avarage train loss 0.01003700109391377  avarage test loss 0.018396811559796333\n",
      "Training: epoch 12 batch 0 loss 0.005826580338180065\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training: epoch 12 batch 10 loss 0.007568524684756994\n",
      "Training: epoch 12 batch 20 loss 0.004997228737920523\n",
      "Test: epoch 12 batch 0 loss 0.00467489892616868\n",
      "epoch 12 finished - avarage train loss 0.007488888190609628  avarage test loss 0.015900789061561227\n",
      "Training: epoch 13 batch 0 loss 0.0051192352548241615\n",
      "Training: epoch 13 batch 10 loss 0.011457091197371483\n",
      "Training: epoch 13 batch 20 loss 0.006360345985740423\n",
      "Test: epoch 13 batch 0 loss 0.0057091792114079\n",
      "epoch 13 finished - avarage train loss 0.008879952654563662  avarage test loss 0.016599595546722412\n",
      "Training: epoch 14 batch 0 loss 0.004960022401064634\n",
      "Training: epoch 14 batch 10 loss 0.005601928569376469\n",
      "Training: epoch 14 batch 20 loss 0.0027599716559052467\n",
      "Test: epoch 14 batch 0 loss 0.004710234701633453\n",
      "epoch 14 finished - avarage train loss 0.007717217680002595  avarage test loss 0.013427497702650726\n",
      "Training: epoch 15 batch 0 loss 0.008415423333644867\n",
      "Training: epoch 15 batch 10 loss 0.004025505390018225\n",
      "Training: epoch 15 batch 20 loss 0.013746335171163082\n",
      "Test: epoch 15 batch 0 loss 0.004876623395830393\n",
      "epoch 15 finished - avarage train loss 0.007756252901564385  avarage test loss 0.015236458508297801\n",
      "Training: epoch 16 batch 0 loss 0.011221961118280888\n",
      "Training: epoch 16 batch 10 loss 0.006741203833371401\n",
      "Training: epoch 16 batch 20 loss 0.006236530840396881\n",
      "Test: epoch 16 batch 0 loss 0.004928313195705414\n",
      "epoch 16 finished - avarage train loss 0.009516724883096999  avarage test loss 0.013552927877753973\n",
      "Training: epoch 17 batch 0 loss 0.00398947624489665\n",
      "Training: epoch 17 batch 10 loss 0.013137060217559338\n",
      "Training: epoch 17 batch 20 loss 0.00468165148049593\n",
      "Test: epoch 17 batch 0 loss 0.003995267674326897\n",
      "epoch 17 finished - avarage train loss 0.007405356720796433  avarage test loss 0.012396272097248584\n",
      "Training: epoch 18 batch 0 loss 0.003882849821820855\n",
      "Training: epoch 18 batch 10 loss 0.008084774017333984\n",
      "Training: epoch 18 batch 20 loss 0.0052971504628658295\n",
      "Test: epoch 18 batch 0 loss 0.004834759049117565\n",
      "epoch 18 finished - avarage train loss 0.008344670412419685  avarage test loss 0.013212733436375856\n",
      "Training: epoch 19 batch 0 loss 0.005442215595394373\n",
      "Training: epoch 19 batch 10 loss 0.004057004116475582\n",
      "Training: epoch 19 batch 20 loss 0.0035514747723937035\n",
      "Test: epoch 19 batch 0 loss 0.006508894730359316\n",
      "epoch 19 finished - avarage train loss 0.009316497851676982  avarage test loss 0.014386374270543456\n",
      "Training: epoch 20 batch 0 loss 0.009287224151194096\n",
      "Training: epoch 20 batch 10 loss 0.00744465971365571\n",
      "Training: epoch 20 batch 20 loss 0.006121690850704908\n",
      "Test: epoch 20 batch 0 loss 0.004852998070418835\n",
      "epoch 20 finished - avarage train loss 0.008125219180987313  avarage test loss 0.013998223235830665\n",
      "Training: epoch 21 batch 0 loss 0.005544947925955057\n",
      "Training: epoch 21 batch 10 loss 0.008019223809242249\n",
      "Training: epoch 21 batch 20 loss 0.00897490419447422\n",
      "Test: epoch 21 batch 0 loss 0.0060037970542907715\n",
      "epoch 21 finished - avarage train loss 0.008047926978304469  avarage test loss 0.015295326127670705\n",
      "Training: epoch 22 batch 0 loss 0.007725292816758156\n",
      "Training: epoch 22 batch 10 loss 0.008880036883056164\n",
      "Training: epoch 22 batch 20 loss 0.01259624119848013\n",
      "Test: epoch 22 batch 0 loss 0.010321738198399544\n",
      "epoch 22 finished - avarage train loss 0.011208187740553042  avarage test loss 0.01788664562627673\n",
      "Training: epoch 23 batch 0 loss 0.01115960069000721\n",
      "Training: epoch 23 batch 10 loss 0.011842712759971619\n",
      "Training: epoch 23 batch 20 loss 0.005199624225497246\n",
      "Test: epoch 23 batch 0 loss 0.004442989826202393\n",
      "epoch 23 finished - avarage train loss 0.008889953059882954  avarage test loss 0.014444099040701985\n",
      "Training: epoch 24 batch 0 loss 0.005787956994026899\n",
      "Training: epoch 24 batch 10 loss 0.008677814155817032\n",
      "Training: epoch 24 batch 20 loss 0.0042027500458061695\n",
      "Test: epoch 24 batch 0 loss 0.004327245056629181\n",
      "epoch 24 finished - avarage train loss 0.007527456426158033  avarage test loss 0.01355835550930351\n",
      "Training: epoch 25 batch 0 loss 0.004499173257499933\n",
      "Training: epoch 25 batch 10 loss 0.006677469704300165\n",
      "Training: epoch 25 batch 20 loss 0.0036184298805892467\n",
      "Test: epoch 25 batch 0 loss 0.005129379220306873\n",
      "epoch 25 finished - avarage train loss 0.007056279348787563  avarage test loss 0.016141173662617803\n",
      "Training: epoch 26 batch 0 loss 0.004006682429462671\n",
      "Training: epoch 26 batch 10 loss 0.006328045390546322\n",
      "Training: epoch 26 batch 20 loss 0.004230763763189316\n",
      "Test: epoch 26 batch 0 loss 0.010590044781565666\n",
      "epoch 26 finished - avarage train loss 0.007182252424736989  avarage test loss 0.021486243698745966\n",
      "Training: epoch 27 batch 0 loss 0.011777819134294987\n",
      "Training: epoch 27 batch 10 loss 0.010126505047082901\n",
      "Training: epoch 27 batch 20 loss 0.006047017872333527\n",
      "Test: epoch 27 batch 0 loss 0.008305084891617298\n",
      "epoch 27 finished - avarage train loss 0.01284295625599294  avarage test loss 0.021589705254882574\n",
      "Training: epoch 28 batch 0 loss 0.0067709749564528465\n",
      "Training: epoch 28 batch 10 loss 0.00745902257040143\n",
      "Training: epoch 28 batch 20 loss 0.010318232700228691\n",
      "Test: epoch 28 batch 0 loss 0.004855883773416281\n",
      "epoch 28 finished - avarage train loss 0.009444732058407932  avarage test loss 0.017011605319567025\n",
      "Training: epoch 29 batch 0 loss 0.004854233469814062\n",
      "Training: epoch 29 batch 10 loss 0.0086322957649827\n",
      "Training: epoch 29 batch 20 loss 0.009049716405570507\n",
      "Test: epoch 29 batch 0 loss 0.004148444160819054\n",
      "epoch 29 finished - avarage train loss 0.007585928277594262  avarage test loss 0.015596449957229197\n",
      "Training: epoch 30 batch 0 loss 0.0069137695245444775\n",
      "Training: epoch 30 batch 10 loss 0.007145576179027557\n",
      "Training: epoch 30 batch 20 loss 0.0033546278718858957\n",
      "Test: epoch 30 batch 0 loss 0.005323929712176323\n",
      "epoch 30 finished - avarage train loss 0.008170094557813016  avarage test loss 0.01461170893162489\n",
      "Training: epoch 31 batch 0 loss 0.005696197971701622\n",
      "Training: epoch 31 batch 10 loss 0.009548143483698368\n",
      "Training: epoch 31 batch 20 loss 0.006652879528701305\n",
      "Test: epoch 31 batch 0 loss 0.0038142227567732334\n",
      "epoch 31 finished - avarage train loss 0.008138181561411455  avarage test loss 0.01313669013325125\n",
      "Training: epoch 32 batch 0 loss 0.00827019289135933\n",
      "Training: epoch 32 batch 10 loss 0.004505080170929432\n",
      "Training: epoch 32 batch 20 loss 0.0074029709212481976\n",
      "Test: epoch 32 batch 0 loss 0.006244642194360495\n",
      "epoch 32 finished - avarage train loss 0.0071891522886038855  avarage test loss 0.014351709862239659\n",
      "Training: epoch 33 batch 0 loss 0.01056951005011797\n",
      "Training: epoch 33 batch 10 loss 0.007081762421876192\n",
      "Training: epoch 33 batch 20 loss 0.003927920013666153\n",
      "Test: epoch 33 batch 0 loss 0.0060214209370315075\n",
      "epoch 33 finished - avarage train loss 0.008767919855770367  avarage test loss 0.015017010271549225\n",
      "Training: epoch 34 batch 0 loss 0.015013176016509533\n",
      "Training: epoch 34 batch 10 loss 0.008172017522156239\n",
      "Training: epoch 34 batch 20 loss 0.009738724678754807\n",
      "Test: epoch 34 batch 0 loss 0.0050263903103768826\n",
      "epoch 34 finished - avarage train loss 0.009423722930509469  avarage test loss 0.013191064586862922\n",
      "Training: epoch 35 batch 0 loss 0.006535666063427925\n",
      "Training: epoch 35 batch 10 loss 0.004965532571077347\n",
      "Training: epoch 35 batch 20 loss 0.01143201719969511\n",
      "Test: epoch 35 batch 0 loss 0.006681423634290695\n",
      "epoch 35 finished - avarage train loss 0.007769475072814986  avarage test loss 0.014658975065685809\n",
      "Training: epoch 36 batch 0 loss 0.00695391558110714\n",
      "Training: epoch 36 batch 10 loss 0.010108078829944134\n",
      "Training: epoch 36 batch 20 loss 0.0058354344218969345\n",
      "Test: epoch 36 batch 0 loss 0.004970344249159098\n",
      "epoch 36 finished - avarage train loss 0.0070289230487983806  avarage test loss 0.012859773822128773\n",
      "Training: epoch 37 batch 0 loss 0.005214022938162088\n",
      "Training: epoch 37 batch 10 loss 0.008454120717942715\n",
      "Training: epoch 37 batch 20 loss 0.007092777173966169\n",
      "Test: epoch 37 batch 0 loss 0.011161369271576405\n",
      "epoch 37 finished - avarage train loss 0.008717885176683295  avarage test loss 0.01868904661387205\n",
      "Training: epoch 38 batch 0 loss 0.014912360347807407\n",
      "Training: epoch 38 batch 10 loss 0.004178513307124376\n",
      "Training: epoch 38 batch 20 loss 0.010749148204922676\n",
      "Test: epoch 38 batch 0 loss 0.004516522400081158\n",
      "epoch 38 finished - avarage train loss 0.009641257917572713  avarage test loss 0.014157428639009595\n",
      "Training: epoch 39 batch 0 loss 0.011740632355213165\n",
      "Training: epoch 39 batch 10 loss 0.00929829478263855\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training: epoch 39 batch 20 loss 0.003401175606995821\n",
      "Test: epoch 39 batch 0 loss 0.005369781516492367\n",
      "epoch 39 finished - avarage train loss 0.00914997435656601  avarage test loss 0.012995842611417174\n",
      "Training: epoch 40 batch 0 loss 0.00870998203754425\n",
      "Training: epoch 40 batch 10 loss 0.0036274439189583063\n",
      "Training: epoch 40 batch 20 loss 0.007556184194982052\n",
      "Test: epoch 40 batch 0 loss 0.007057487033307552\n",
      "epoch 40 finished - avarage train loss 0.008899670816829493  avarage test loss 0.014439299004152417\n",
      "Training: epoch 41 batch 0 loss 0.009332329966127872\n",
      "Training: epoch 41 batch 10 loss 0.0063127316534519196\n",
      "Training: epoch 41 batch 20 loss 0.008265581913292408\n",
      "Test: epoch 41 batch 0 loss 0.0051532043144106865\n",
      "epoch 41 finished - avarage train loss 0.008050090946449802  avarage test loss 0.013257662241812795\n",
      "Training: epoch 42 batch 0 loss 0.008079471066594124\n",
      "Training: epoch 42 batch 10 loss 0.009142580442130566\n",
      "Training: epoch 42 batch 20 loss 0.003712818492203951\n",
      "Test: epoch 42 batch 0 loss 0.0034506721422076225\n",
      "epoch 42 finished - avarage train loss 0.007806964740478273  avarage test loss 0.012663450208492577\n",
      "Training: epoch 43 batch 0 loss 0.004103641491383314\n",
      "Training: epoch 43 batch 10 loss 0.007155982311815023\n",
      "Training: epoch 43 batch 20 loss 0.006820901297032833\n",
      "Test: epoch 43 batch 0 loss 0.005519633647054434\n",
      "epoch 43 finished - avarage train loss 0.008038669615855506  avarage test loss 0.013575607212260365\n",
      "Training: epoch 44 batch 0 loss 0.004595574922859669\n",
      "Training: epoch 44 batch 10 loss 0.01668974943459034\n",
      "Training: epoch 44 batch 20 loss 0.008171126246452332\n",
      "Test: epoch 44 batch 0 loss 0.004083985462784767\n",
      "epoch 44 finished - avarage train loss 0.008709320252568558  avarage test loss 0.01430007943417877\n",
      "Training: epoch 45 batch 0 loss 0.009874781593680382\n",
      "Training: epoch 45 batch 10 loss 0.008483034558594227\n",
      "Training: epoch 45 batch 20 loss 0.005421799141913652\n",
      "Test: epoch 45 batch 0 loss 0.004585308022797108\n",
      "epoch 45 finished - avarage train loss 0.007237317281421916  avarage test loss 0.014305777964182198\n",
      "Training: epoch 46 batch 0 loss 0.004357574041932821\n",
      "Training: epoch 46 batch 10 loss 0.010197979398071766\n",
      "Training: epoch 46 batch 20 loss 0.004558623302727938\n",
      "Test: epoch 46 batch 0 loss 0.004164059180766344\n",
      "epoch 46 finished - avarage train loss 0.007652190497851577  avarage test loss 0.013752255705185235\n",
      "Training: epoch 47 batch 0 loss 0.007142209447920322\n",
      "Training: epoch 47 batch 10 loss 0.008074064739048481\n",
      "Training: epoch 47 batch 20 loss 0.006445811130106449\n",
      "Test: epoch 47 batch 0 loss 0.005792193114757538\n",
      "epoch 47 finished - avarage train loss 0.008731486669195623  avarage test loss 0.014404722838662565\n",
      "Training: epoch 48 batch 0 loss 0.00514158234000206\n",
      "Training: epoch 48 batch 10 loss 0.005848454777151346\n",
      "Training: epoch 48 batch 20 loss 0.00677512539550662\n",
      "Test: epoch 48 batch 0 loss 0.006254078354686499\n",
      "epoch 48 finished - avarage train loss 0.008000484635603839  avarage test loss 0.017690525739453733\n",
      "Training: epoch 49 batch 0 loss 0.00759127177298069\n",
      "Training: epoch 49 batch 10 loss 0.007784333545714617\n",
      "Training: epoch 49 batch 20 loss 0.007855313830077648\n",
      "Test: epoch 49 batch 0 loss 0.005538306664675474\n",
      "epoch 49 finished - avarage train loss 0.00862147096255473  avarage test loss 0.01677772938273847\n",
      "Training: epoch 50 batch 0 loss 0.008445273153483868\n",
      "Training: epoch 50 batch 10 loss 0.0033907704055309296\n",
      "Training: epoch 50 batch 20 loss 0.0073546660132706165\n",
      "Test: epoch 50 batch 0 loss 0.0043601724319159985\n",
      "epoch 50 finished - avarage train loss 0.008138364293323508  avarage test loss 0.013205757946707308\n",
      "Training: epoch 51 batch 0 loss 0.010872913524508476\n",
      "Training: epoch 51 batch 10 loss 0.00602516857907176\n",
      "Training: epoch 51 batch 20 loss 0.0066622169688344\n",
      "Test: epoch 51 batch 0 loss 0.006904520094394684\n",
      "epoch 51 finished - avarage train loss 0.008077265630508292  avarage test loss 0.015280419029295444\n",
      "Training: epoch 52 batch 0 loss 0.008786686696112156\n",
      "Training: epoch 52 batch 10 loss 0.006735002622008324\n",
      "Training: epoch 52 batch 20 loss 0.005122290458530188\n",
      "Test: epoch 52 batch 0 loss 0.005937865469604731\n",
      "epoch 52 finished - avarage train loss 0.008949793618300865  avarage test loss 0.016923550167120993\n",
      "Training: epoch 53 batch 0 loss 0.00788466352969408\n",
      "Training: epoch 53 batch 10 loss 0.011150332167744637\n",
      "Training: epoch 53 batch 20 loss 0.0037226658314466476\n",
      "Test: epoch 53 batch 0 loss 0.00572982057929039\n",
      "epoch 53 finished - avarage train loss 0.011804974742297983  avarage test loss 0.01597925741225481\n",
      "Training: epoch 54 batch 0 loss 0.0058797188103199005\n",
      "Training: epoch 54 batch 10 loss 0.006325320340692997\n",
      "Training: epoch 54 batch 20 loss 0.008149421773850918\n",
      "Test: epoch 54 batch 0 loss 0.004715859889984131\n",
      "epoch 54 finished - avarage train loss 0.008644351302164382  avarage test loss 0.014179601334035397\n",
      "Training: epoch 55 batch 0 loss 0.00737776467576623\n",
      "Training: epoch 55 batch 10 loss 0.005824986379593611\n",
      "Training: epoch 55 batch 20 loss 0.005253765266388655\n",
      "Test: epoch 55 batch 0 loss 0.0036586355417966843\n",
      "epoch 55 finished - avarage train loss 0.00849648829999155  avarage test loss 0.013359449221752584\n",
      "Training: epoch 56 batch 0 loss 0.015062465332448483\n",
      "Training: epoch 56 batch 10 loss 0.007173791527748108\n",
      "Training: epoch 56 batch 20 loss 0.004152906592935324\n",
      "Test: epoch 56 batch 0 loss 0.004833439830690622\n",
      "epoch 56 finished - avarage train loss 0.009646887457447833  avarage test loss 0.013511131051927805\n",
      "Training: epoch 57 batch 0 loss 0.010014227591454983\n",
      "Training: epoch 57 batch 10 loss 0.005068001337349415\n",
      "Training: epoch 57 batch 20 loss 0.00565190427005291\n",
      "Test: epoch 57 batch 0 loss 0.0031786097679287195\n",
      "epoch 57 finished - avarage train loss 0.008146890447121757  avarage test loss 0.013144783035386354\n",
      "Training: epoch 58 batch 0 loss 0.003897933755069971\n",
      "Training: epoch 58 batch 10 loss 0.005009660497307777\n",
      "Training: epoch 58 batch 20 loss 0.007751410827040672\n",
      "Test: epoch 58 batch 0 loss 0.004594696220010519\n",
      "epoch 58 finished - avarage train loss 0.0077994353266368655  avarage test loss 0.012920371838845313\n",
      "Training: epoch 59 batch 0 loss 0.0029679739382117987\n",
      "Training: epoch 59 batch 10 loss 0.003752537537366152\n",
      "Training: epoch 59 batch 20 loss 0.008318856358528137\n",
      "Test: epoch 59 batch 0 loss 0.003616941161453724\n",
      "epoch 59 finished - avarage train loss 0.007354480543591339  avarage test loss 0.013521097134798765\n",
      "Training: epoch 60 batch 0 loss 0.0071927327662706375\n",
      "Training: epoch 60 batch 10 loss 0.008233992382884026\n",
      "Training: epoch 60 batch 20 loss 0.008317738771438599\n",
      "Test: epoch 60 batch 0 loss 0.00374652910977602\n",
      "epoch 60 finished - avarage train loss 0.008299567736685276  avarage test loss 0.012821617187000811\n",
      "Training: epoch 61 batch 0 loss 0.008057466708123684\n",
      "Training: epoch 61 batch 10 loss 0.0038050960283726454\n",
      "Training: epoch 61 batch 20 loss 0.00787919107824564\n",
      "Test: epoch 61 batch 0 loss 0.0031731456983834505\n",
      "epoch 61 finished - avarage train loss 0.007230235897968042  avarage test loss 0.012658343941438943\n",
      "Training: epoch 62 batch 0 loss 0.005703433882445097\n",
      "Training: epoch 62 batch 10 loss 0.007600876037031412\n",
      "Training: epoch 62 batch 20 loss 0.00487959710881114\n",
      "Test: epoch 62 batch 0 loss 0.003643236355856061\n",
      "epoch 62 finished - avarage train loss 0.007353857801905994  avarage test loss 0.013892868941184133\n",
      "Training: epoch 63 batch 0 loss 0.005863431375473738\n",
      "Training: epoch 63 batch 10 loss 0.00924661010503769\n",
      "Training: epoch 63 batch 20 loss 0.004142820835113525\n",
      "Test: epoch 63 batch 0 loss 0.005264286883175373\n",
      "epoch 63 finished - avarage train loss 0.009476302103300032  avarage test loss 0.013850299175828695\n",
      "Training: epoch 64 batch 0 loss 0.008283809758722782\n",
      "Training: epoch 64 batch 10 loss 0.005918262992054224\n",
      "Training: epoch 64 batch 20 loss 0.004777716472744942\n",
      "Test: epoch 64 batch 0 loss 0.0038402946665883064\n",
      "epoch 64 finished - avarage train loss 0.007893198410627144  avarage test loss 0.013286251807585359\n",
      "Training: epoch 65 batch 0 loss 0.005129591096192598\n",
      "Training: epoch 65 batch 10 loss 0.005697251763194799\n",
      "Training: epoch 65 batch 20 loss 0.0033859838731586933\n",
      "Test: epoch 65 batch 0 loss 0.004021171014755964\n",
      "epoch 65 finished - avarage train loss 0.008126759853470942  avarage test loss 0.01292625314090401\n",
      "Training: epoch 66 batch 0 loss 0.006851596292108297\n",
      "Training: epoch 66 batch 10 loss 0.006235003471374512\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training: epoch 66 batch 20 loss 0.0047094374895095825\n",
      "Test: epoch 66 batch 0 loss 0.005456568207591772\n",
      "epoch 66 finished - avarage train loss 0.008531474781318986  avarage test loss 0.01578554220031947\n",
      "Training: epoch 67 batch 0 loss 0.0072574615478515625\n",
      "Training: epoch 67 batch 10 loss 0.006632059346884489\n",
      "Training: epoch 67 batch 20 loss 0.0035963817499578\n",
      "Test: epoch 67 batch 0 loss 0.003759884973987937\n",
      "epoch 67 finished - avarage train loss 0.008829908204618198  avarage test loss 0.013477074855472893\n",
      "Training: epoch 68 batch 0 loss 0.008465928956866264\n",
      "Training: epoch 68 batch 10 loss 0.0035871509462594986\n",
      "Training: epoch 68 batch 20 loss 0.005589754786342382\n",
      "Test: epoch 68 batch 0 loss 0.0035855777096003294\n",
      "epoch 68 finished - avarage train loss 0.007230104188081519  avarage test loss 0.013207300042267889\n",
      "Training: epoch 69 batch 0 loss 0.006008070427924395\n",
      "Training: epoch 69 batch 10 loss 0.007374532055109739\n",
      "Training: epoch 69 batch 20 loss 0.004262339789420366\n",
      "Test: epoch 69 batch 0 loss 0.0068884119391441345\n",
      "epoch 69 finished - avarage train loss 0.009789282150951952  avarage test loss 0.017216060310602188\n",
      "Training: epoch 70 batch 0 loss 0.01324802078306675\n",
      "Training: epoch 70 batch 10 loss 0.007792348973453045\n",
      "Training: epoch 70 batch 20 loss 0.004934902768582106\n",
      "Test: epoch 70 batch 0 loss 0.009183891117572784\n",
      "epoch 70 finished - avarage train loss 0.008614389418527997  avarage test loss 0.016957242507487535\n",
      "Training: epoch 71 batch 0 loss 0.010070059448480606\n",
      "Training: epoch 71 batch 10 loss 0.006102057173848152\n",
      "Training: epoch 71 batch 20 loss 0.0034966799430549145\n",
      "Test: epoch 71 batch 0 loss 0.009407911449670792\n",
      "epoch 71 finished - avarage train loss 0.008414927621147242  avarage test loss 0.01808854448609054\n",
      "Training: epoch 72 batch 0 loss 0.01169881783425808\n",
      "Training: epoch 72 batch 10 loss 0.01112620159983635\n",
      "Training: epoch 72 batch 20 loss 0.008054384961724281\n",
      "Test: epoch 72 batch 0 loss 0.004720430821180344\n",
      "epoch 72 finished - avarage train loss 0.008701190215952951  avarage test loss 0.01399874861817807\n",
      "Training: epoch 73 batch 0 loss 0.006420250050723553\n",
      "Training: epoch 73 batch 10 loss 0.0030977558344602585\n",
      "Training: epoch 73 batch 20 loss 0.00434780353680253\n",
      "Test: epoch 73 batch 0 loss 0.004413382150232792\n",
      "epoch 73 finished - avarage train loss 0.008513215415436646  avarage test loss 0.015334848780184984\n",
      "Training: epoch 74 batch 0 loss 0.00989861786365509\n",
      "Training: epoch 74 batch 10 loss 0.01539580151438713\n",
      "Training: epoch 74 batch 20 loss 0.0036980595905333757\n",
      "Test: epoch 74 batch 0 loss 0.007509645074605942\n",
      "epoch 74 finished - avarage train loss 0.010172370154860205  avarage test loss 0.01576566742733121\n",
      "Training: epoch 75 batch 0 loss 0.011407933197915554\n",
      "Training: epoch 75 batch 10 loss 0.007879135198891163\n",
      "Training: epoch 75 batch 20 loss 0.007187466602772474\n",
      "Test: epoch 75 batch 0 loss 0.0036149576772004366\n",
      "epoch 75 finished - avarage train loss 0.009123445681199945  avarage test loss 0.012963766290340573\n",
      "Training: epoch 76 batch 0 loss 0.007945460267364979\n",
      "Training: epoch 76 batch 10 loss 0.005395418033003807\n",
      "Training: epoch 76 batch 20 loss 0.004413537215441465\n",
      "Test: epoch 76 batch 0 loss 0.0060324231162667274\n",
      "epoch 76 finished - avarage train loss 0.009235179372902575  avarage test loss 0.015300208935514092\n",
      "Training: epoch 77 batch 0 loss 0.007171032950282097\n",
      "Training: epoch 77 batch 10 loss 0.003893556073307991\n",
      "Training: epoch 77 batch 20 loss 0.005645255092531443\n",
      "Test: epoch 77 batch 0 loss 0.004243065137416124\n",
      "epoch 77 finished - avarage train loss 0.008335717945712907  avarage test loss 0.01288335316348821\n",
      "Training: epoch 78 batch 0 loss 0.01576097495853901\n",
      "Training: epoch 78 batch 10 loss 0.010783882811665535\n",
      "Training: epoch 78 batch 20 loss 0.006840910296887159\n",
      "Test: epoch 78 batch 0 loss 0.0035918178036808968\n",
      "epoch 78 finished - avarage train loss 0.008835035994456246  avarage test loss 0.01374823902733624\n",
      "Training: epoch 79 batch 0 loss 0.005513142328709364\n",
      "Training: epoch 79 batch 10 loss 0.006450767163187265\n",
      "Training: epoch 79 batch 20 loss 0.013505028560757637\n",
      "Test: epoch 79 batch 0 loss 0.0044822124764323235\n",
      "epoch 79 finished - avarage train loss 0.007589743199661888  avarage test loss 0.013146708603017032\n",
      "Training: epoch 80 batch 0 loss 0.005127754528075457\n",
      "Training: epoch 80 batch 10 loss 0.008450889028608799\n",
      "Training: epoch 80 batch 20 loss 0.005425953306257725\n",
      "Test: epoch 80 batch 0 loss 0.0034038773737847805\n",
      "epoch 80 finished - avarage train loss 0.007207556555433006  avarage test loss 0.013140233233571053\n",
      "Training: epoch 81 batch 0 loss 0.008090506307780743\n",
      "Training: epoch 81 batch 10 loss 0.0034356657415628433\n",
      "Training: epoch 81 batch 20 loss 0.005369463469833136\n",
      "Test: epoch 81 batch 0 loss 0.0037082661874592304\n",
      "epoch 81 finished - avarage train loss 0.007862462565816682  avarage test loss 0.012816551490686834\n",
      "Training: epoch 82 batch 0 loss 0.003383275354281068\n",
      "Training: epoch 82 batch 10 loss 0.008322197012603283\n",
      "Training: epoch 82 batch 20 loss 0.00503688957542181\n",
      "Test: epoch 82 batch 0 loss 0.004244391806423664\n",
      "epoch 82 finished - avarage train loss 0.006701671759244697  avarage test loss 0.014983510132879019\n",
      "Training: epoch 83 batch 0 loss 0.007254005875438452\n",
      "Training: epoch 83 batch 10 loss 0.006763441488146782\n",
      "Training: epoch 83 batch 20 loss 0.010454343631863594\n",
      "Test: epoch 83 batch 0 loss 0.004684086423367262\n",
      "epoch 83 finished - avarage train loss 0.008465102383995364  avarage test loss 0.01336860831361264\n",
      "Training: epoch 84 batch 0 loss 0.007655664812773466\n",
      "Training: epoch 84 batch 10 loss 0.010898335836827755\n",
      "Training: epoch 84 batch 20 loss 0.006011564284563065\n",
      "Test: epoch 84 batch 0 loss 0.00896566640585661\n",
      "epoch 84 finished - avarage train loss 0.007672082945897147  avarage test loss 0.017196294153109193\n",
      "Training: epoch 85 batch 0 loss 0.007745925337076187\n",
      "Training: epoch 85 batch 10 loss 0.017370160669088364\n",
      "Training: epoch 85 batch 20 loss 0.014230235479772091\n",
      "Test: epoch 85 batch 0 loss 0.011901648715138435\n",
      "epoch 85 finished - avarage train loss 0.01596608436827002  avarage test loss 0.022909270133823156\n",
      "Training: epoch 86 batch 0 loss 0.0108242342248559\n",
      "Training: epoch 86 batch 10 loss 0.0035499087534844875\n",
      "Training: epoch 86 batch 20 loss 0.00880415365099907\n",
      "Test: epoch 86 batch 0 loss 0.004590175114572048\n",
      "epoch 86 finished - avarage train loss 0.009513902253118055  avarage test loss 0.015324488980695605\n",
      "Training: epoch 87 batch 0 loss 0.01026364229619503\n",
      "Training: epoch 87 batch 10 loss 0.009539899416267872\n",
      "Training: epoch 87 batch 20 loss 0.0037175328470766544\n",
      "Test: epoch 87 batch 0 loss 0.004542115144431591\n",
      "epoch 87 finished - avarage train loss 0.0072330486527162376  avarage test loss 0.013454195694066584\n",
      "Training: epoch 88 batch 0 loss 0.00589537201449275\n",
      "Training: epoch 88 batch 10 loss 0.00630776584148407\n",
      "Training: epoch 88 batch 20 loss 0.0051007941365242004\n",
      "Test: epoch 88 batch 0 loss 0.00439517991617322\n",
      "epoch 88 finished - avarage train loss 0.009587674931590927  avarage test loss 0.015618196688592434\n",
      "Training: epoch 89 batch 0 loss 0.0039833527989685535\n",
      "Training: epoch 89 batch 10 loss 0.005378081928938627\n",
      "Training: epoch 89 batch 20 loss 0.004756574518978596\n",
      "Test: epoch 89 batch 0 loss 0.0058150687254965305\n",
      "epoch 89 finished - avarage train loss 0.008850610236926326  avarage test loss 0.01574781583622098\n",
      "Training: epoch 90 batch 0 loss 0.007495211437344551\n",
      "Training: epoch 90 batch 10 loss 0.012178237549960613\n",
      "Training: epoch 90 batch 20 loss 0.006873600650578737\n",
      "Test: epoch 90 batch 0 loss 0.004830746445804834\n",
      "epoch 90 finished - avarage train loss 0.009017606458530343  avarage test loss 0.01634793623816222\n",
      "Training: epoch 91 batch 0 loss 0.003586453851312399\n",
      "Training: epoch 91 batch 10 loss 0.011862795799970627\n",
      "Training: epoch 91 batch 20 loss 0.0065915631130337715\n",
      "Test: epoch 91 batch 0 loss 0.003977513872087002\n",
      "epoch 91 finished - avarage train loss 0.008982325647154758  avarage test loss 0.014576214482076466\n",
      "Training: epoch 92 batch 0 loss 0.0054515027441084385\n",
      "Training: epoch 92 batch 10 loss 0.006324901711195707\n",
      "Training: epoch 92 batch 20 loss 0.00923217087984085\n",
      "Test: epoch 92 batch 0 loss 0.005284665152430534\n",
      "epoch 92 finished - avarage train loss 0.007393521569861934  avarage test loss 0.014474611612968147\n",
      "Training: epoch 93 batch 0 loss 0.006031518802046776\n",
      "Training: epoch 93 batch 10 loss 0.004343398381024599\n",
      "Training: epoch 93 batch 20 loss 0.009685540571808815\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test: epoch 93 batch 0 loss 0.004211612977087498\n",
      "epoch 93 finished - avarage train loss 0.00809794503810077  avarage test loss 0.01493831118568778\n",
      "Training: epoch 94 batch 0 loss 0.004440609365701675\n",
      "Training: epoch 94 batch 10 loss 0.004221213515847921\n",
      "Training: epoch 94 batch 20 loss 0.009171826764941216\n",
      "Test: epoch 94 batch 0 loss 0.00355815258808434\n",
      "epoch 94 finished - avarage train loss 0.007389359240388048  avarage test loss 0.01294874242739752\n",
      "Training: epoch 95 batch 0 loss 0.0104090990498662\n",
      "Training: epoch 95 batch 10 loss 0.005880217999219894\n",
      "Training: epoch 95 batch 20 loss 0.007129484787583351\n",
      "Test: epoch 95 batch 0 loss 0.0034484192728996277\n",
      "epoch 95 finished - avarage train loss 0.0083928019659401  avarage test loss 0.01339356426615268\n",
      "Training: epoch 96 batch 0 loss 0.004525111522525549\n",
      "Training: epoch 96 batch 10 loss 0.0048116352409124374\n",
      "Training: epoch 96 batch 20 loss 0.0038651274517178535\n",
      "Test: epoch 96 batch 0 loss 0.004357328172773123\n",
      "epoch 96 finished - avarage train loss 0.008451254082972119  avarage test loss 0.013243331224657595\n",
      "Training: epoch 97 batch 0 loss 0.004719689022749662\n",
      "Training: epoch 97 batch 10 loss 0.005207091569900513\n",
      "Training: epoch 97 batch 20 loss 0.0081930011510849\n",
      "Test: epoch 97 batch 0 loss 0.006270181853324175\n",
      "epoch 97 finished - avarage train loss 0.008316478868625287  avarage test loss 0.014553212793543935\n",
      "Training: epoch 98 batch 0 loss 0.010368837043642998\n",
      "Training: epoch 98 batch 10 loss 0.007117433939129114\n",
      "Training: epoch 98 batch 20 loss 0.0053817895241081715\n",
      "Test: epoch 98 batch 0 loss 0.006317092105746269\n",
      "epoch 98 finished - avarage train loss 0.011299919240690511  avarage test loss 0.016415301011875272\n",
      "Training: epoch 99 batch 0 loss 0.008925320580601692\n",
      "Training: epoch 99 batch 10 loss 0.004531520418822765\n",
      "Training: epoch 99 batch 20 loss 0.007030862383544445\n",
      "Test: epoch 99 batch 0 loss 0.005109202582389116\n",
      "epoch 99 finished - avarage train loss 0.009915617407010547  avarage test loss 0.01514372881501913\n",
      "Training: epoch 100 batch 0 loss 0.004586531314998865\n",
      "Training: epoch 100 batch 10 loss 0.008013999089598656\n",
      "Training: epoch 100 batch 20 loss 0.011214662343263626\n",
      "Test: epoch 100 batch 0 loss 0.003986733965575695\n",
      "epoch 100 finished - avarage train loss 0.008060272574296286  avarage test loss 0.014292275067418814\n",
      "Training: epoch 101 batch 0 loss 0.005720766726881266\n",
      "Training: epoch 101 batch 10 loss 0.0044645341113209724\n",
      "Training: epoch 101 batch 20 loss 0.006737089715898037\n",
      "Test: epoch 101 batch 0 loss 0.0037882260512560606\n",
      "epoch 101 finished - avarage train loss 0.0063019246986970815  avarage test loss 0.01287633931497112\n",
      "Training: epoch 102 batch 0 loss 0.004718140698969364\n",
      "Training: epoch 102 batch 10 loss 0.010493320412933826\n",
      "Training: epoch 102 batch 20 loss 0.008601359091699123\n",
      "Test: epoch 102 batch 0 loss 0.005408989265561104\n",
      "epoch 102 finished - avarage train loss 0.0067146876104304505  avarage test loss 0.014312556362710893\n",
      "Training: epoch 103 batch 0 loss 0.006583694368600845\n",
      "Training: epoch 103 batch 10 loss 0.0054941498674452305\n",
      "Training: epoch 103 batch 20 loss 0.0063041821122169495\n",
      "Test: epoch 103 batch 0 loss 0.004832353908568621\n",
      "epoch 103 finished - avarage train loss 0.009525122313663877  avarage test loss 0.013540369691327214\n",
      "Training: epoch 104 batch 0 loss 0.010799117386341095\n",
      "Training: epoch 104 batch 10 loss 0.004124979488551617\n",
      "Training: epoch 104 batch 20 loss 0.00282853189855814\n",
      "Test: epoch 104 batch 0 loss 0.0046256124041974545\n",
      "epoch 104 finished - avarage train loss 0.0076798473260012165  avarage test loss 0.014008396537974477\n",
      "Training: epoch 105 batch 0 loss 0.005783173255622387\n",
      "Training: epoch 105 batch 10 loss 0.010136119090020657\n",
      "Training: epoch 105 batch 20 loss 0.00439416104927659\n",
      "Test: epoch 105 batch 0 loss 0.003697332227602601\n",
      "epoch 105 finished - avarage train loss 0.006697882504748373  avarage test loss 0.013121939089614898\n",
      "Training: epoch 106 batch 0 loss 0.004266082774847746\n",
      "Training: epoch 106 batch 10 loss 0.003974663559347391\n",
      "Training: epoch 106 batch 20 loss 0.004948456771671772\n",
      "Test: epoch 106 batch 0 loss 0.0046355994418263435\n",
      "epoch 106 finished - avarage train loss 0.007256540694627269  avarage test loss 0.013349906192161143\n",
      "Training: epoch 107 batch 0 loss 0.004026208072900772\n",
      "Training: epoch 107 batch 10 loss 0.008892257697880268\n",
      "Training: epoch 107 batch 20 loss 0.00922088697552681\n",
      "Test: epoch 107 batch 0 loss 0.003633514977991581\n",
      "epoch 107 finished - avarage train loss 0.008201575621256027  avarage test loss 0.01355547271668911\n",
      "Training: epoch 108 batch 0 loss 0.0044663953594863415\n",
      "Training: epoch 108 batch 10 loss 0.011350228451192379\n",
      "Training: epoch 108 batch 20 loss 0.004493845161050558\n",
      "Test: epoch 108 batch 0 loss 0.00386748812161386\n",
      "epoch 108 finished - avarage train loss 0.008094613777537799  avarage test loss 0.014210780442226678\n",
      "Training: epoch 109 batch 0 loss 0.01598490960896015\n",
      "Training: epoch 109 batch 10 loss 0.005219622049480677\n",
      "Training: epoch 109 batch 20 loss 0.006238502915948629\n",
      "Test: epoch 109 batch 0 loss 0.004341050051152706\n",
      "epoch 109 finished - avarage train loss 0.007623840613162209  avarage test loss 0.013372136279940605\n",
      "Training: epoch 110 batch 0 loss 0.0030456981621682644\n",
      "Training: epoch 110 batch 10 loss 0.006251077633351088\n",
      "Training: epoch 110 batch 20 loss 0.010910065844655037\n",
      "Test: epoch 110 batch 0 loss 0.004328533075749874\n",
      "epoch 110 finished - avarage train loss 0.00846610857366488  avarage test loss 0.014535447349771857\n",
      "Training: epoch 111 batch 0 loss 0.01038309931755066\n",
      "Training: epoch 111 batch 10 loss 0.004392401780933142\n",
      "Training: epoch 111 batch 20 loss 0.003994214814156294\n",
      "Test: epoch 111 batch 0 loss 0.004061058163642883\n",
      "epoch 111 finished - avarage train loss 0.006138565845723296  avarage test loss 0.01441633130889386\n",
      "Training: epoch 112 batch 0 loss 0.008694452233612537\n",
      "Training: epoch 112 batch 10 loss 0.005993646569550037\n",
      "Training: epoch 112 batch 20 loss 0.006169739179313183\n",
      "Test: epoch 112 batch 0 loss 0.004565315321087837\n",
      "epoch 112 finished - avarage train loss 0.006976453868415335  avarage test loss 0.01487296400591731\n",
      "Training: epoch 113 batch 0 loss 0.005680781789124012\n",
      "Training: epoch 113 batch 10 loss 0.0046259262599051\n",
      "Training: epoch 113 batch 20 loss 0.006918703205883503\n",
      "Test: epoch 113 batch 0 loss 0.004978256765753031\n",
      "epoch 113 finished - avarage train loss 0.008375979108928606  avarage test loss 0.013614708092063665\n",
      "Training: epoch 114 batch 0 loss 0.008283093571662903\n",
      "Training: epoch 114 batch 10 loss 0.004941339138895273\n",
      "Training: epoch 114 batch 20 loss 0.011224376037716866\n",
      "Test: epoch 114 batch 0 loss 0.004035488702356815\n",
      "epoch 114 finished - avarage train loss 0.00884940932858093  avarage test loss 0.013499947730451822\n",
      "Training: epoch 115 batch 0 loss 0.0036129700019955635\n",
      "Training: epoch 115 batch 10 loss 0.006228246726095676\n",
      "Training: epoch 115 batch 20 loss 0.0066571771167218685\n",
      "Test: epoch 115 batch 0 loss 0.005043906159698963\n",
      "epoch 115 finished - avarage train loss 0.009202522643167397  avarage test loss 0.016874604276381433\n",
      "Training: epoch 116 batch 0 loss 0.010948109440505505\n",
      "Training: epoch 116 batch 10 loss 0.009287035092711449\n",
      "Training: epoch 116 batch 20 loss 0.009421958588063717\n",
      "Test: epoch 116 batch 0 loss 0.003949455451220274\n",
      "epoch 116 finished - avarage train loss 0.009049889567340243  avarage test loss 0.014823439065366983\n",
      "Training: epoch 117 batch 0 loss 0.007736614905297756\n",
      "Training: epoch 117 batch 10 loss 0.008723355829715729\n",
      "Training: epoch 117 batch 20 loss 0.00959073007106781\n",
      "Test: epoch 117 batch 0 loss 0.00511175487190485\n",
      "epoch 117 finished - avarage train loss 0.008610853138540325  avarage test loss 0.019078254234045744\n",
      "Training: epoch 118 batch 0 loss 0.0039042860735207796\n",
      "Training: epoch 118 batch 10 loss 0.005772266071289778\n",
      "Training: epoch 118 batch 20 loss 0.012419750913977623\n",
      "Test: epoch 118 batch 0 loss 0.006078227888792753\n",
      "epoch 118 finished - avarage train loss 0.007491473943508905  avarage test loss 0.014231351669877768\n",
      "Training: epoch 119 batch 0 loss 0.005623490549623966\n",
      "Training: epoch 119 batch 10 loss 0.004665406886488199\n",
      "Training: epoch 119 batch 20 loss 0.005711202975362539\n",
      "Test: epoch 119 batch 0 loss 0.003971488680690527\n",
      "epoch 119 finished - avarage train loss 0.0074056034530352415  avarage test loss 0.014412817894481122\n",
      "Training: epoch 120 batch 0 loss 0.007811298593878746\n",
      "Training: epoch 120 batch 10 loss 0.003297307062894106\n",
      "Training: epoch 120 batch 20 loss 0.004003975074738264\n",
      "Test: epoch 120 batch 0 loss 0.003646619850769639\n",
      "epoch 120 finished - avarage train loss 0.007206996364904375  avarage test loss 0.0140039183315821\n",
      "Training: epoch 121 batch 0 loss 0.00546497106552124\n",
      "Training: epoch 121 batch 10 loss 0.006111926399171352\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training: epoch 121 batch 20 loss 0.004034137353301048\n",
      "Test: epoch 121 batch 0 loss 0.003660363843664527\n",
      "epoch 121 finished - avarage train loss 0.007456624944662225  avarage test loss 0.014150731905829161\n",
      "Training: epoch 122 batch 0 loss 0.004290312062948942\n",
      "Training: epoch 122 batch 10 loss 0.006009768694639206\n",
      "Training: epoch 122 batch 20 loss 0.0033009429462254047\n",
      "Test: epoch 122 batch 0 loss 0.004056183155626059\n",
      "epoch 122 finished - avarage train loss 0.007545000078670424  avarage test loss 0.013512501725926995\n",
      "Training: epoch 123 batch 0 loss 0.004630516283214092\n",
      "Training: epoch 123 batch 10 loss 0.004537517204880714\n",
      "Training: epoch 123 batch 20 loss 0.008987994864583015\n",
      "Test: epoch 123 batch 0 loss 0.0053518726490437984\n",
      "epoch 123 finished - avarage train loss 0.00812857347572672  avarage test loss 0.013396454625762999\n",
      "Training: epoch 124 batch 0 loss 0.004455321468412876\n",
      "Training: epoch 124 batch 10 loss 0.012679954059422016\n",
      "Training: epoch 124 batch 20 loss 0.010154807940125465\n",
      "Test: epoch 124 batch 0 loss 0.0058171628043055534\n",
      "epoch 124 finished - avarage train loss 0.009198773042138281  avarage test loss 0.019248732598498464\n",
      "Training: epoch 125 batch 0 loss 0.007406060118228197\n",
      "Training: epoch 125 batch 10 loss 0.0048803118988871574\n",
      "Training: epoch 125 batch 20 loss 0.011264244094491005\n",
      "Test: epoch 125 batch 0 loss 0.00671555008739233\n",
      "epoch 125 finished - avarage train loss 0.00930077018987002  avarage test loss 0.014749614056199789\n",
      "Training: epoch 126 batch 0 loss 0.006887100171297789\n",
      "Training: epoch 126 batch 10 loss 0.012510507367551327\n",
      "Training: epoch 126 batch 20 loss 0.006714111194014549\n",
      "Test: epoch 126 batch 0 loss 0.0044714887626469135\n",
      "epoch 126 finished - avarage train loss 0.007234811072302018  avarage test loss 0.01296809478662908\n",
      "Training: epoch 127 batch 0 loss 0.006520924158394337\n",
      "Training: epoch 127 batch 10 loss 0.007739946711808443\n",
      "Training: epoch 127 batch 20 loss 0.007114907260984182\n",
      "Test: epoch 127 batch 0 loss 0.004315268713980913\n",
      "epoch 127 finished - avarage train loss 0.008851981175870731  avarage test loss 0.012967794784344733\n",
      "Training: epoch 128 batch 0 loss 0.004629358649253845\n",
      "Training: epoch 128 batch 10 loss 0.0033502187579870224\n",
      "Training: epoch 128 batch 20 loss 0.005336379166692495\n",
      "Test: epoch 128 batch 0 loss 0.003528247121721506\n",
      "epoch 128 finished - avarage train loss 0.00797795964372826  avarage test loss 0.013822668232023716\n",
      "Training: epoch 129 batch 0 loss 0.00726650794968009\n",
      "Training: epoch 129 batch 10 loss 0.005462339147925377\n",
      "Training: epoch 129 batch 20 loss 0.010772655718028545\n",
      "Test: epoch 129 batch 0 loss 0.009249645285308361\n",
      "epoch 129 finished - avarage train loss 0.007218082772632097  avarage test loss 0.01882873079739511\n",
      "Training: epoch 130 batch 0 loss 0.011164499446749687\n",
      "Training: epoch 130 batch 10 loss 0.00953433196991682\n",
      "Training: epoch 130 batch 20 loss 0.002892739837989211\n",
      "Test: epoch 130 batch 0 loss 0.003779914928600192\n",
      "epoch 130 finished - avarage train loss 0.010204838460375523  avarage test loss 0.013232205819804221\n",
      "Training: epoch 131 batch 0 loss 0.005592977628111839\n",
      "Training: epoch 131 batch 10 loss 0.007988976314663887\n",
      "Training: epoch 131 batch 20 loss 0.005770685616880655\n",
      "Test: epoch 131 batch 0 loss 0.005935295019298792\n",
      "epoch 131 finished - avarage train loss 0.008470806991681457  avarage test loss 0.0200318907154724\n",
      "Training: epoch 132 batch 0 loss 0.006926476024091244\n",
      "Training: epoch 132 batch 10 loss 0.0047828746028244495\n",
      "Training: epoch 132 batch 20 loss 0.004781074821949005\n",
      "Test: epoch 132 batch 0 loss 0.005799124017357826\n",
      "epoch 132 finished - avarage train loss 0.006732096293427307  avarage test loss 0.019240725436247885\n",
      "Training: epoch 133 batch 0 loss 0.008430288173258305\n",
      "Training: epoch 133 batch 10 loss 0.011513582430779934\n",
      "Training: epoch 133 batch 20 loss 0.005317693576216698\n",
      "Test: epoch 133 batch 0 loss 0.006675207056105137\n",
      "epoch 133 finished - avarage train loss 0.009380326116586039  avarage test loss 0.018540405319072306\n",
      "Training: epoch 134 batch 0 loss 0.008806531317532063\n",
      "Training: epoch 134 batch 10 loss 0.009896497242152691\n",
      "Training: epoch 134 batch 20 loss 0.008554190397262573\n",
      "Test: epoch 134 batch 0 loss 0.006071023177355528\n",
      "epoch 134 finished - avarage train loss 0.008493421510953841  avarage test loss 0.019865541718900204\n",
      "Training: epoch 135 batch 0 loss 0.0064805797301232815\n",
      "Training: epoch 135 batch 10 loss 0.00770551897585392\n",
      "Training: epoch 135 batch 20 loss 0.009154604747891426\n",
      "Test: epoch 135 batch 0 loss 0.0066161854192614555\n",
      "epoch 135 finished - avarage train loss 0.010654832348869792  avarage test loss 0.01899462891742587\n",
      "Training: epoch 136 batch 0 loss 0.0029462226666510105\n",
      "Training: epoch 136 batch 10 loss 0.005096843931823969\n",
      "Training: epoch 136 batch 20 loss 0.003683254588395357\n",
      "Test: epoch 136 batch 0 loss 0.0044931685552001\n",
      "epoch 136 finished - avarage train loss 0.006638495580710727  avarage test loss 0.016319365007802844\n",
      "Training: epoch 137 batch 0 loss 0.0076513588428497314\n",
      "Training: epoch 137 batch 10 loss 0.00521784508600831\n",
      "Training: epoch 137 batch 20 loss 0.006449268199503422\n",
      "Test: epoch 137 batch 0 loss 0.003914183471351862\n",
      "epoch 137 finished - avarage train loss 0.008093428886334958  avarage test loss 0.01343310356605798\n",
      "Training: epoch 138 batch 0 loss 0.00717519223690033\n",
      "Training: epoch 138 batch 10 loss 0.0030090934596955776\n",
      "Training: epoch 138 batch 20 loss 0.008647472597658634\n",
      "Test: epoch 138 batch 0 loss 0.005859970115125179\n",
      "epoch 138 finished - avarage train loss 0.007170792251568416  avarage test loss 0.017303377971984446\n",
      "Training: epoch 139 batch 0 loss 0.0063329278491437435\n",
      "Training: epoch 139 batch 10 loss 0.01045896578580141\n",
      "Training: epoch 139 batch 20 loss 0.009039720520377159\n",
      "Test: epoch 139 batch 0 loss 0.01805306039750576\n",
      "epoch 139 finished - avarage train loss 0.008426661964441681  avarage test loss 0.026337781455367804\n",
      "Training: epoch 140 batch 0 loss 0.01669958420097828\n",
      "Training: epoch 140 batch 10 loss 0.01108161173760891\n",
      "Training: epoch 140 batch 20 loss 0.009655601345002651\n",
      "Test: epoch 140 batch 0 loss 0.0038551674224436283\n",
      "epoch 140 finished - avarage train loss 0.00910448385723706  avarage test loss 0.013720687129534781\n",
      "Training: epoch 141 batch 0 loss 0.004395319148898125\n",
      "Training: epoch 141 batch 10 loss 0.005679210182279348\n",
      "Training: epoch 141 batch 20 loss 0.0040127974934875965\n",
      "Test: epoch 141 batch 0 loss 0.00444933632388711\n",
      "epoch 141 finished - avarage train loss 0.006883863985923858  avarage test loss 0.013333211187273264\n",
      "Training: epoch 142 batch 0 loss 0.006551562342792749\n",
      "Training: epoch 142 batch 10 loss 0.0042227632366120815\n",
      "Training: epoch 142 batch 20 loss 0.006008308846503496\n",
      "Test: epoch 142 batch 0 loss 0.003263586899265647\n",
      "epoch 142 finished - avarage train loss 0.007557535696582034  avarage test loss 0.012830902065616101\n",
      "Training: epoch 143 batch 0 loss 0.006123633589595556\n",
      "Training: epoch 143 batch 10 loss 0.007002480328083038\n",
      "Training: epoch 143 batch 20 loss 0.00927636306732893\n",
      "Test: epoch 143 batch 0 loss 0.0036690037231892347\n",
      "epoch 143 finished - avarage train loss 0.006551705983242598  avarage test loss 0.013092389737721533\n",
      "Training: epoch 144 batch 0 loss 0.003414297243580222\n",
      "Training: epoch 144 batch 10 loss 0.00559583306312561\n",
      "Training: epoch 144 batch 20 loss 0.0064383214339613914\n",
      "Test: epoch 144 batch 0 loss 0.00566822336986661\n",
      "epoch 144 finished - avarage train loss 0.00632472865379833  avarage test loss 0.013420111034065485\n",
      "Training: epoch 145 batch 0 loss 0.007904441095888615\n",
      "Training: epoch 145 batch 10 loss 0.0027294629253447056\n",
      "Training: epoch 145 batch 20 loss 0.006281446199864149\n",
      "Test: epoch 145 batch 0 loss 0.004835484083741903\n",
      "epoch 145 finished - avarage train loss 0.008401527759972317  avarage test loss 0.0187238467624411\n",
      "Training: epoch 146 batch 0 loss 0.004556308966130018\n",
      "Training: epoch 146 batch 10 loss 0.006464902311563492\n",
      "Training: epoch 146 batch 20 loss 0.006376513279974461\n",
      "Test: epoch 146 batch 0 loss 0.004560577217489481\n",
      "epoch 146 finished - avarage train loss 0.00763446310181813  avarage test loss 0.017096067662350833\n",
      "Training: epoch 147 batch 0 loss 0.004987566731870174\n",
      "Training: epoch 147 batch 10 loss 0.004678180441260338\n",
      "Training: epoch 147 batch 20 loss 0.005550048779696226\n",
      "Test: epoch 147 batch 0 loss 0.008110650815069675\n",
      "epoch 147 finished - avarage train loss 0.006933324199555249  avarage test loss 0.016136322170495987\n",
      "Training: epoch 148 batch 0 loss 0.010729094967246056\n",
      "Training: epoch 148 batch 10 loss 0.006253620609641075\n",
      "Training: epoch 148 batch 20 loss 0.010879798792302608\n",
      "Test: epoch 148 batch 0 loss 0.0056237708777189255\n",
      "epoch 148 finished - avarage train loss 0.009432542306403148  avarage test loss 0.015182225964963436\n",
      "Training: epoch 149 batch 0 loss 0.010328519158065319\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training: epoch 149 batch 10 loss 0.009839339181780815\n",
      "Training: epoch 149 batch 20 loss 0.005077040754258633\n",
      "Test: epoch 149 batch 0 loss 0.006331935524940491\n",
      "epoch 149 finished - avarage train loss 0.008605758033307463  avarage test loss 0.01906378217972815\n",
      "Training: epoch 150 batch 0 loss 0.006779951509088278\n",
      "Training: epoch 150 batch 10 loss 0.013448633253574371\n",
      "Training: epoch 150 batch 20 loss 0.00798923708498478\n",
      "Test: epoch 150 batch 0 loss 0.007148652337491512\n",
      "epoch 150 finished - avarage train loss 0.007969612849812055  avarage test loss 0.01636843360029161\n",
      "Training: epoch 151 batch 0 loss 0.006668157875537872\n",
      "Training: epoch 151 batch 10 loss 0.005374123342335224\n",
      "Training: epoch 151 batch 20 loss 0.0035156605299562216\n",
      "Test: epoch 151 batch 0 loss 0.00444988626986742\n",
      "epoch 151 finished - avarage train loss 0.007968138823093012  avarage test loss 0.015224688919261098\n",
      "Training: epoch 152 batch 0 loss 0.0073715983889997005\n",
      "Training: epoch 152 batch 10 loss 0.004351151175796986\n",
      "Training: epoch 152 batch 20 loss 0.00616795988753438\n",
      "Test: epoch 152 batch 0 loss 0.004386118613183498\n",
      "epoch 152 finished - avarage train loss 0.007568992658293453  avarage test loss 0.016473305062390864\n",
      "Training: epoch 153 batch 0 loss 0.003863656660541892\n",
      "Training: epoch 153 batch 10 loss 0.004928853362798691\n",
      "Training: epoch 153 batch 20 loss 0.007654756307601929\n",
      "Test: epoch 153 batch 0 loss 0.004258907865732908\n",
      "epoch 153 finished - avarage train loss 0.007295008788912974  avarage test loss 0.012613647035323083\n",
      "Training: epoch 154 batch 0 loss 0.006853486876934767\n",
      "Training: epoch 154 batch 10 loss 0.00481782853603363\n",
      "Training: epoch 154 batch 20 loss 0.005486566573381424\n",
      "Test: epoch 154 batch 0 loss 0.0047541591338813305\n",
      "epoch 154 finished - avarage train loss 0.007075751344833909  avarage test loss 0.014936040970496833\n",
      "Training: epoch 155 batch 0 loss 0.006105706095695496\n",
      "Training: epoch 155 batch 10 loss 0.0035016266629099846\n",
      "Training: epoch 155 batch 20 loss 0.009128300473093987\n",
      "Test: epoch 155 batch 0 loss 0.004534007515758276\n",
      "epoch 155 finished - avarage train loss 0.008340683773738044  avarage test loss 0.01480753195937723\n",
      "Training: epoch 156 batch 0 loss 0.005889554508030415\n",
      "Training: epoch 156 batch 10 loss 0.011786441318690777\n",
      "Training: epoch 156 batch 20 loss 0.008580978959798813\n",
      "Test: epoch 156 batch 0 loss 0.003999524749815464\n",
      "epoch 156 finished - avarage train loss 0.010120514619710117  avarage test loss 0.013663994846865535\n",
      "Training: epoch 157 batch 0 loss 0.0038257346022874117\n",
      "Training: epoch 157 batch 10 loss 0.005736622493714094\n",
      "Training: epoch 157 batch 20 loss 0.0055006276816129684\n",
      "Test: epoch 157 batch 0 loss 0.00454222084954381\n",
      "epoch 157 finished - avarage train loss 0.007533078659968129  avarage test loss 0.013883955078199506\n",
      "Training: epoch 158 batch 0 loss 0.007087176200002432\n",
      "Training: epoch 158 batch 10 loss 0.00357371405698359\n",
      "Training: epoch 158 batch 20 loss 0.011071023531258106\n",
      "Test: epoch 158 batch 0 loss 0.0043845572508871555\n",
      "epoch 158 finished - avarage train loss 0.007364290220470264  avarage test loss 0.013506601797416806\n",
      "Training: epoch 159 batch 0 loss 0.006159623619168997\n",
      "Training: epoch 159 batch 10 loss 0.010317300446331501\n",
      "Training: epoch 159 batch 20 loss 0.005851344205439091\n",
      "Test: epoch 159 batch 0 loss 0.00936991535127163\n",
      "epoch 159 finished - avarage train loss 0.009599432172574874  avarage test loss 0.021442621713504195\n",
      "Training: epoch 160 batch 0 loss 0.009865980595350266\n",
      "Training: epoch 160 batch 10 loss 0.012127053923904896\n",
      "Training: epoch 160 batch 20 loss 0.0037192946765571833\n",
      "Test: epoch 160 batch 0 loss 0.005389359313994646\n",
      "epoch 160 finished - avarage train loss 0.008093865572250095  avarage test loss 0.01760357664898038\n",
      "Training: epoch 161 batch 0 loss 0.00650464603677392\n",
      "Training: epoch 161 batch 10 loss 0.003687520045787096\n",
      "Training: epoch 161 batch 20 loss 0.006973839830607176\n",
      "Test: epoch 161 batch 0 loss 0.00693852361291647\n",
      "epoch 161 finished - avarage train loss 0.008268001722171903  avarage test loss 0.020930195692926645\n",
      "Training: epoch 162 batch 0 loss 0.008547530509531498\n",
      "Training: epoch 162 batch 10 loss 0.005475422367453575\n",
      "Training: epoch 162 batch 20 loss 0.003346345853060484\n",
      "Test: epoch 162 batch 0 loss 0.0053970692679286\n",
      "epoch 162 finished - avarage train loss 0.008700744521900498  avarage test loss 0.01520628237631172\n",
      "Training: epoch 163 batch 0 loss 0.005168880335986614\n",
      "Training: epoch 163 batch 10 loss 0.005736837163567543\n",
      "Training: epoch 163 batch 20 loss 0.007064091973006725\n",
      "Test: epoch 163 batch 0 loss 0.003980586305260658\n",
      "epoch 163 finished - avarage train loss 0.006850377656519413  avarage test loss 0.014006477547809482\n",
      "Training: epoch 164 batch 0 loss 0.006178774870932102\n",
      "Training: epoch 164 batch 10 loss 0.005309628322720528\n",
      "Training: epoch 164 batch 20 loss 0.004157169256359339\n",
      "Test: epoch 164 batch 0 loss 0.004169453401118517\n",
      "epoch 164 finished - avarage train loss 0.0065895785930855525  avarage test loss 0.012743599596433342\n",
      "Training: epoch 165 batch 0 loss 0.005060575436800718\n",
      "Training: epoch 165 batch 10 loss 0.003434685291722417\n",
      "Training: epoch 165 batch 20 loss 0.0062041934579610825\n",
      "Test: epoch 165 batch 0 loss 0.003471925389021635\n",
      "epoch 165 finished - avarage train loss 0.007682059112743571  avarage test loss 0.012855443404987454\n",
      "Training: epoch 166 batch 0 loss 0.007354607339948416\n",
      "Training: epoch 166 batch 10 loss 0.005800674203783274\n",
      "Training: epoch 166 batch 20 loss 0.0072202966548502445\n",
      "Test: epoch 166 batch 0 loss 0.0031699950341135263\n",
      "epoch 166 finished - avarage train loss 0.007541746720029363  avarage test loss 0.013071723224129528\n",
      "Training: epoch 167 batch 0 loss 0.003960174508392811\n",
      "Training: epoch 167 batch 10 loss 0.0041503566317260265\n",
      "Training: epoch 167 batch 20 loss 0.006400322075933218\n",
      "Test: epoch 167 batch 0 loss 0.0036533449310809374\n",
      "epoch 167 finished - avarage train loss 0.006840675194137569  avarage test loss 0.013251074647996575\n",
      "Training: epoch 168 batch 0 loss 0.004760551732033491\n",
      "Training: epoch 168 batch 10 loss 0.005106810946017504\n",
      "Training: epoch 168 batch 20 loss 0.008043922483921051\n",
      "Test: epoch 168 batch 0 loss 0.004336713347584009\n",
      "epoch 168 finished - avarage train loss 0.006568371920428914  avarage test loss 0.01781528873834759\n",
      "Training: epoch 169 batch 0 loss 0.008447369560599327\n",
      "Training: epoch 169 batch 10 loss 0.006499322131276131\n",
      "Training: epoch 169 batch 20 loss 0.0076928394846618176\n",
      "Test: epoch 169 batch 0 loss 0.004574183374643326\n",
      "epoch 169 finished - avarage train loss 0.007607096774053985  avarage test loss 0.01782009657472372\n",
      "Training: epoch 170 batch 0 loss 0.003218851750716567\n",
      "Training: epoch 170 batch 10 loss 0.009076820686459541\n",
      "Training: epoch 170 batch 20 loss 0.00688973069190979\n",
      "Test: epoch 170 batch 0 loss 0.006051923148334026\n",
      "epoch 170 finished - avarage train loss 0.007245396279954705  avarage test loss 0.014666429022327065\n",
      "Training: epoch 171 batch 0 loss 0.00528892083093524\n",
      "Training: epoch 171 batch 10 loss 0.004516820888966322\n",
      "Training: epoch 171 batch 20 loss 0.0052229780703783035\n",
      "Test: epoch 171 batch 0 loss 0.006566209718585014\n",
      "epoch 171 finished - avarage train loss 0.008498201355466554  avarage test loss 0.01384057686664164\n",
      "Training: epoch 172 batch 0 loss 0.004953044932335615\n",
      "Training: epoch 172 batch 10 loss 0.004943305626511574\n",
      "Training: epoch 172 batch 20 loss 0.006202253047376871\n",
      "Test: epoch 172 batch 0 loss 0.004887881223112345\n",
      "epoch 172 finished - avarage train loss 0.007130584992661044  avarage test loss 0.01679744478315115\n",
      "Training: epoch 173 batch 0 loss 0.010175837203860283\n",
      "Training: epoch 173 batch 10 loss 0.005125212483108044\n",
      "Training: epoch 173 batch 20 loss 0.005505148787051439\n",
      "Test: epoch 173 batch 0 loss 0.004162948112934828\n",
      "epoch 173 finished - avarage train loss 0.007148849239958258  avarage test loss 0.013212424470111728\n",
      "Training: epoch 174 batch 0 loss 0.009715499356389046\n",
      "Training: epoch 174 batch 10 loss 0.00584566593170166\n",
      "Training: epoch 174 batch 20 loss 0.004763612058013678\n",
      "Test: epoch 174 batch 0 loss 0.004696245305240154\n",
      "epoch 174 finished - avarage train loss 0.006161339548898154  avarage test loss 0.01892517285887152\n",
      "Training: epoch 175 batch 0 loss 0.005087736062705517\n",
      "Training: epoch 175 batch 10 loss 0.004439039155840874\n",
      "Training: epoch 175 batch 20 loss 0.005517329089343548\n",
      "Test: epoch 175 batch 0 loss 0.005076933186501265\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 175 finished - avarage train loss 0.007921013282611966  avarage test loss 0.018960056244395673\n",
      "Training: epoch 176 batch 0 loss 0.004660699050873518\n",
      "Training: epoch 176 batch 10 loss 0.0026539620012044907\n",
      "Training: epoch 176 batch 20 loss 0.007903225719928741\n",
      "Test: epoch 176 batch 0 loss 0.004182491451501846\n",
      "epoch 176 finished - avarage train loss 0.007491864167667669  avarage test loss 0.013021119986660779\n",
      "Training: epoch 177 batch 0 loss 0.003626953111961484\n",
      "Training: epoch 177 batch 10 loss 0.004662971943616867\n",
      "Training: epoch 177 batch 20 loss 0.006319697946310043\n",
      "Test: epoch 177 batch 0 loss 0.0038202910218387842\n",
      "epoch 177 finished - avarage train loss 0.0075856894415257305  avarage test loss 0.01299387781182304\n",
      "Training: epoch 178 batch 0 loss 0.0036308274138718843\n",
      "Training: epoch 178 batch 10 loss 0.008004224859178066\n",
      "Training: epoch 178 batch 20 loss 0.005256412085145712\n",
      "Test: epoch 178 batch 0 loss 0.004778990522027016\n",
      "epoch 178 finished - avarage train loss 0.006815621155667408  avarage test loss 0.018598037073388696\n",
      "Training: epoch 179 batch 0 loss 0.004847284406423569\n",
      "Training: epoch 179 batch 10 loss 0.010479245334863663\n",
      "Training: epoch 179 batch 20 loss 0.003110909601673484\n",
      "Test: epoch 179 batch 0 loss 0.004987234249711037\n",
      "epoch 179 finished - avarage train loss 0.008337018248657214  avarage test loss 0.01862444751895964\n",
      "Training: epoch 180 batch 0 loss 0.011313259601593018\n",
      "Training: epoch 180 batch 10 loss 0.006113410461694002\n",
      "Training: epoch 180 batch 20 loss 0.006739064119756222\n",
      "Test: epoch 180 batch 0 loss 0.004948056302964687\n",
      "epoch 180 finished - avarage train loss 0.006300991248532102  avarage test loss 0.01905423786956817\n",
      "Training: epoch 181 batch 0 loss 0.009602678939700127\n",
      "Training: epoch 181 batch 10 loss 0.005807540379464626\n",
      "Training: epoch 181 batch 20 loss 0.007413130719214678\n",
      "Test: epoch 181 batch 0 loss 0.004464279860258102\n",
      "epoch 181 finished - avarage train loss 0.007471858989447355  avarage test loss 0.018454484292306006\n",
      "Training: epoch 182 batch 0 loss 0.005493578035384417\n",
      "Training: epoch 182 batch 10 loss 0.006176880560815334\n",
      "Training: epoch 182 batch 20 loss 0.004355298355221748\n",
      "Test: epoch 182 batch 0 loss 0.006328387651592493\n",
      "epoch 182 finished - avarage train loss 0.007141075530571157  avarage test loss 0.017318054218776524\n",
      "Training: epoch 183 batch 0 loss 0.004957179538905621\n",
      "Training: epoch 183 batch 10 loss 0.006461573299020529\n",
      "Training: epoch 183 batch 20 loss 0.004350982140749693\n",
      "Test: epoch 183 batch 0 loss 0.006215828005224466\n",
      "epoch 183 finished - avarage train loss 0.007549101615260388  avarage test loss 0.015871113864704967\n",
      "Training: epoch 184 batch 0 loss 0.016109993681311607\n",
      "Training: epoch 184 batch 10 loss 0.010519906878471375\n",
      "Training: epoch 184 batch 20 loss 0.008402992971241474\n",
      "Test: epoch 184 batch 0 loss 0.016472555696964264\n",
      "epoch 184 finished - avarage train loss 0.00938012090447391  avarage test loss 0.029176607728004456\n",
      "Training: epoch 185 batch 0 loss 0.013864738866686821\n",
      "Training: epoch 185 batch 10 loss 0.0070231882855296135\n",
      "Training: epoch 185 batch 20 loss 0.00592899601906538\n",
      "Test: epoch 185 batch 0 loss 0.0052338349632918835\n",
      "epoch 185 finished - avarage train loss 0.00979544317092875  avarage test loss 0.019987699226476252\n",
      "Training: epoch 186 batch 0 loss 0.008910011500120163\n",
      "Training: epoch 186 batch 10 loss 0.0036524233873933554\n",
      "Training: epoch 186 batch 20 loss 0.004666667431592941\n",
      "Test: epoch 186 batch 0 loss 0.0053681619465351105\n",
      "epoch 186 finished - avarage train loss 0.0071286878323760525  avarage test loss 0.019110696739517152\n",
      "Training: epoch 187 batch 0 loss 0.010059149004518986\n",
      "Training: epoch 187 batch 10 loss 0.009608264081180096\n",
      "Training: epoch 187 batch 20 loss 0.009078098461031914\n",
      "Test: epoch 187 batch 0 loss 0.008371197618544102\n",
      "epoch 187 finished - avarage train loss 0.0089820246693903  avarage test loss 0.022877890150994062\n",
      "Training: epoch 188 batch 0 loss 0.006818716414272785\n",
      "Training: epoch 188 batch 10 loss 0.006072406657040119\n",
      "Training: epoch 188 batch 20 loss 0.008415930904448032\n",
      "Test: epoch 188 batch 0 loss 0.005504058673977852\n",
      "epoch 188 finished - avarage train loss 0.007403076289157416  avarage test loss 0.02020340843591839\n",
      "Training: epoch 189 batch 0 loss 0.004016507416963577\n",
      "Training: epoch 189 batch 10 loss 0.007769536226987839\n",
      "Training: epoch 189 batch 20 loss 0.005073461215943098\n",
      "Test: epoch 189 batch 0 loss 0.004573208279907703\n",
      "epoch 189 finished - avarage train loss 0.007509984011793959  avarage test loss 0.014967406983487308\n",
      "Training: epoch 190 batch 0 loss 0.005392598453909159\n",
      "Training: epoch 190 batch 10 loss 0.0045630489476025105\n",
      "Training: epoch 190 batch 20 loss 0.005531701259315014\n",
      "Test: epoch 190 batch 0 loss 0.007529572583734989\n",
      "epoch 190 finished - avarage train loss 0.007005933062012853  avarage test loss 0.021139314747415483\n",
      "Training: epoch 191 batch 0 loss 0.007975268177688122\n",
      "Training: epoch 191 batch 10 loss 0.004292839206755161\n",
      "Training: epoch 191 batch 20 loss 0.007640857715159655\n",
      "Test: epoch 191 batch 0 loss 0.005795327480882406\n",
      "epoch 191 finished - avarage train loss 0.008209860602650663  avarage test loss 0.016372036654502153\n",
      "Training: epoch 192 batch 0 loss 0.008108336478471756\n",
      "Training: epoch 192 batch 10 loss 0.021016592159867287\n",
      "Training: epoch 192 batch 20 loss 0.005934068001806736\n",
      "Test: epoch 192 batch 0 loss 0.006227653007954359\n",
      "epoch 192 finished - avarage train loss 0.013036311009963011  avarage test loss 0.01945913420058787\n",
      "Training: epoch 193 batch 0 loss 0.008270212449133396\n",
      "Training: epoch 193 batch 10 loss 0.005138914100825787\n",
      "Training: epoch 193 batch 20 loss 0.004853270016610622\n",
      "Test: epoch 193 batch 0 loss 0.008101902902126312\n",
      "epoch 193 finished - avarage train loss 0.009211516312869459  avarage test loss 0.02362188824918121\n",
      "Training: epoch 194 batch 0 loss 0.009824817068874836\n",
      "Training: epoch 194 batch 10 loss 0.00421553710475564\n",
      "Training: epoch 194 batch 20 loss 0.005261199548840523\n",
      "Test: epoch 194 batch 0 loss 0.004319537431001663\n",
      "epoch 194 finished - avarage train loss 0.00846996506804536  avarage test loss 0.016364436480216682\n",
      "Training: epoch 195 batch 0 loss 0.0067428844049572945\n",
      "Training: epoch 195 batch 10 loss 0.005228851456195116\n",
      "Training: epoch 195 batch 20 loss 0.00495706032961607\n",
      "Test: epoch 195 batch 0 loss 0.005831848364323378\n",
      "epoch 195 finished - avarage train loss 0.007583965964872262  avarage test loss 0.013647321611642838\n",
      "Training: epoch 196 batch 0 loss 0.008739640936255455\n",
      "Training: epoch 196 batch 10 loss 0.006521481554955244\n",
      "Training: epoch 196 batch 20 loss 0.006493966560810804\n",
      "Test: epoch 196 batch 0 loss 0.004624504595994949\n",
      "epoch 196 finished - avarage train loss 0.008326744652706487  avarage test loss 0.017843573354184628\n",
      "Training: epoch 197 batch 0 loss 0.007891158573329449\n",
      "Training: epoch 197 batch 10 loss 0.005819361656904221\n",
      "Training: epoch 197 batch 20 loss 0.0043188114650547504\n",
      "Test: epoch 197 batch 0 loss 0.004986518993973732\n",
      "epoch 197 finished - avarage train loss 0.007803754696365574  avarage test loss 0.01873475476168096\n",
      "Training: epoch 198 batch 0 loss 0.00857783854007721\n",
      "Training: epoch 198 batch 10 loss 0.006075030192732811\n",
      "Training: epoch 198 batch 20 loss 0.008540702983736992\n",
      "Test: epoch 198 batch 0 loss 0.0050162081606686115\n",
      "epoch 198 finished - avarage train loss 0.007897615818114117  avarage test loss 0.012683047098107636\n",
      "Training: epoch 199 batch 0 loss 0.005557871889322996\n",
      "Training: epoch 199 batch 10 loss 0.005007825326174498\n",
      "Training: epoch 199 batch 20 loss 0.004943731240928173\n",
      "Test: epoch 199 batch 0 loss 0.004377260338515043\n",
      "epoch 199 finished - avarage train loss 0.007479305668123837  avarage test loss 0.012914094724692404\n"
     ]
    }
   ],
   "source": [
    "median_loss_back, losses_back = get_final_median_loss(df[['x2', 'x1']], num_trials)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.012396272097248584"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "median_loss_back"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.01262878708075732,\n",
       " 0.012263277196325362,\n",
       " 0.012966789654456079,\n",
       " 0.012680116342380643,\n",
       " 0.012383790162857622,\n",
       " 0.012234172376338392,\n",
       " 0.012616099382285029,\n",
       " 0.012191358720883727,\n",
       " 0.012396272097248584]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "losses_back"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim = df.shape[1] - 1\n",
    "\n",
    "train, test = train_test_split(df, test_size=0.1, random_state=10, shuffle=True)\n",
    "\n",
    "train = np.array(train)\n",
    "test = np.array(test)\n",
    "\n",
    "train = MyDataset(train)\n",
    "test = MyDataset(test)\n",
    "\n",
    "train_loader = DataLoader(train, batch_size=batch_size, shuffle=True, \n",
    "                          num_workers=0, pin_memory=True)\n",
    "test_loader = DataLoader(test, batch_size=batch_size, shuffle=False,\n",
    "                         num_workers=0, pin_memory=True)\n",
    "\n",
    "train_loss_avgs, test_loss_avgs, min_loss = train_model(num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize=(12,8))\n",
    "ax.plot(train_loss_avgs, label='train')\n",
    "ax.plot(test_loss_avgs, label='test')\n",
    "ax.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize=(12,8))\n",
    "ax.plot(train_loss_avgs[100:], label='train')\n",
    "ax.plot(test_loss_avgs[100:], label='test')\n",
    "ax.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml1",
   "language": "python",
   "name": "ml1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
