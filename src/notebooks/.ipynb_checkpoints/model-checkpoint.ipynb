{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simulate_bivariate_abpnl(n: int) -> dict: \n",
    "    def f1(x: np.array) -> np.array:\n",
    "        return x**(-1) + 10*x\n",
    "    def f2(z: np.array) -> np.array:\n",
    "        return z**3\n",
    "    \n",
    "    x = np.random.uniform(0.1, 1.1, n)\n",
    "    noise = np.random.uniform(0, 5, n)\n",
    "    \n",
    "    z = f1(x) + noise\n",
    "    y = f2(z)\n",
    "    df = pd.DataFrame({'x1': x, 'x2': y})\n",
    "    sim_data = {'df': df, 'noise': noise}\n",
    "    \n",
    "    return sim_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def centering(M):\n",
    "    n = M.shape[0]\n",
    "    mat_ones = torch.ones((n, n))\n",
    "    idendity = torch.eye(n)\n",
    "    H = idendity - mat_ones/n\n",
    "    \n",
    "    C = torch.matmul(M, H)\n",
    "    return C\n",
    "    \n",
    "    \n",
    "def gaussian_grammat(x, sigma2=None):\n",
    "    xxT = torch.squeeze(torch.matmul(x, x.T))\n",
    "    x2 = torch.diag(xxT)\n",
    "    xnorm = x2 - xxT + (x2 - xxT).T\n",
    "    \n",
    "    if sigma2 is None:\n",
    "        sigma2 = torch.median(xnorm[xnorm != 0])\n",
    "        \n",
    "    if sigma2 == 0:\n",
    "        sigma2 += 1e-16\n",
    "        \n",
    "    Kx = torch.exp(-xnorm/sigma2)\n",
    "    \n",
    "    return Kx\n",
    "    \n",
    "def HSIC(x, y):\n",
    "    gram_x = gaussian_grammat(x)\n",
    "    gram_y = gaussian_grammat(y)\n",
    "    \n",
    "    c = x.shape[0]**2\n",
    "    hsic = torch.trace(torch.matmul(centering(gram_x), centering(gram_y)))/c\n",
    "    \n",
    "    return hsic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.0002)\n",
      "tensor(0.1009)\n"
     ]
    }
   ],
   "source": [
    "x = torch.Tensor(np.random.normal(size=1000).reshape((-1, 1, 1)))\n",
    "y = torch.Tensor(np.random.normal(size=1000).reshape((-1, 1, 1)))\n",
    "print(HSIC(x, y))\n",
    "print(HSIC(x, x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDataset(Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "        \n",
    "        data = self.data[idx, :]\n",
    "        \n",
    "        return data[:-1].reshape((-1, 1)), data[-1].reshape((-1, 1))\n",
    "    \n",
    "\n",
    "class Network(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super().__init__()\n",
    "        self.network = nn.Sequential(\n",
    "            nn.Linear(input_dim, 5),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Linear(5, 5),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Linear(5, 5),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Linear(5, 5),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Linear(5, input_dim)\n",
    "#             nn.LeakyReLU()\n",
    "        )\n",
    "        \n",
    "        self.encode = nn.Sequential(\n",
    "            nn.Linear(input_dim, 5),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Linear(5, 5),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Linear(5, 5),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Linear(5, 5),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Linear(5, input_dim)\n",
    "#             nn.LeakyReLU()\n",
    "        )\n",
    "        self.decode = nn.Sequential(\n",
    "            nn.Linear(input_dim, 5),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Linear(5, 5),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Linear(5, 5),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Linear(5, 5),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Linear(5, input_dim)\n",
    "#             nn.LeakyReLU()\n",
    "        )\n",
    "        \n",
    "    def forward(self, x, y):\n",
    "        g1_x = self.network(x)\n",
    "        g3_y = self.encode(y)\n",
    "        y_approx = self.decode(g3_y)\n",
    "        \n",
    "        assert y.shape == y_approx.shape\n",
    "        \n",
    "        return [g1_x, y_approx, g3_y]\n",
    "    \n",
    "def train_model(num_epochs, input_dim, log_every_batch = 10):\n",
    "    device = 0 if torch.cuda.is_available() else 'cpu'\n",
    "    model = Network(input_dim).to(device)\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.01, betas=(0.9, 0.999))\n",
    "\n",
    "    train_loss_avgs = []\n",
    "    test_loss_avgs = []\n",
    "    \n",
    "    min_loss = 10000\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        train_loss_trace = []\n",
    "\n",
    "        for batch, (x, y) in enumerate(train_loader):\n",
    "            x = x.to(device)\n",
    "            x = x.float()\n",
    "            y = y.to(device)\n",
    "            y = y.float()\n",
    "\n",
    "            g1_x, y_approx, g3_y = model.forward(x, y)\n",
    "            noise = g3_y - g1_x\n",
    "\n",
    "            loss = lamb*F.mse_loss(y_approx, y) + (1-lamb)*HSIC(x, noise)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            train_loss_trace.append(loss.detach().item())\n",
    "            if batch % log_every_batch == 0:\n",
    "                print(f'Training: epoch {epoch} batch {batch} loss {loss}')\n",
    "\n",
    "        model.eval()\n",
    "        test_loss_trace = []\n",
    "        for batch, (x, y) in enumerate(test_loader):\n",
    "            x = x.to(device)\n",
    "            x = x.float()\n",
    "            y = y.to(device)\n",
    "            y = y.float()\n",
    "\n",
    "            g1_x, y_approx, g3_y = model.forward(x, y)\n",
    "            noise = g3_y - g1_x\n",
    "\n",
    "            loss = lamb*F.mse_loss(y_approx, y) + (1-lamb)*HSIC(x, noise)\n",
    "\n",
    "            test_loss_trace.append(loss.detach().item())\n",
    "            if batch % log_every_batch == 0:\n",
    "                print(f'Test: epoch {epoch} batch {batch} loss {loss}')\n",
    "\n",
    "        train_avg = np.mean(train_loss_trace)\n",
    "        test_avg = np.mean(test_loss_trace)\n",
    "        \n",
    "        if test_avg < min_loss:\n",
    "            min_loss = test_avg\n",
    "\n",
    "        train_loss_avgs.append(train_avg)\n",
    "        test_loss_avgs.append(test_avg)\n",
    "        print(f'epoch {epoch} finished - avarage train loss {train_avg} ',\n",
    "             f'avarage test loss {test_avg}')\n",
    "        \n",
    "    return train_loss_avgs, test_loss_avgs, min_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 1000\n",
    "\n",
    "batch_size = 32\n",
    "lamb = 0.5\n",
    "num_epochs = 200\n",
    "\n",
    "num_trials = 9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = simulate_bivariate_abpnl(n)\n",
    "df = data['df']\n",
    "df = (df-df.mean())/df.std()\n",
    "noise = data['noise']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_final_median_loss(df, num_trials):\n",
    "    rand_seed = np.random.randint(0, 1000000)\n",
    "    random.seed(rand_seed)\n",
    "    np.random.seed(rand_seed)\n",
    "    torch.manual_seed(rand_seed)\n",
    "\n",
    "    input_dim = df.shape[1] - 1\n",
    "\n",
    "    train, test = train_test_split(df, test_size=0.1, random_state=10, shuffle=True)\n",
    "\n",
    "    train = np.array(train)\n",
    "    test = np.array(test)\n",
    "\n",
    "    train = MyDataset(train)\n",
    "    test = MyDataset(test)\n",
    "    \n",
    "    train_loader = DataLoader(train, batch_size=batch_size, shuffle=True, \n",
    "                              num_workers=0, pin_memory=True)\n",
    "    test_loader = DataLoader(test, batch_size=batch_size, shuffle=False,\n",
    "                             num_workers=0, pin_memory=True)\n",
    "    \n",
    "    losses = []\n",
    "    for trial in range(num_trials):\n",
    "        train_loss_avgs, test_loss_avgs, min_loss = train_model(num_epochs, input_dim)\n",
    "        losses.append(min_loss)\n",
    "    \n",
    "    median_loss = np.median(losses)\n",
    "    return median_loss, losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for b, (x, y) in enumerate(train_loader):\n",
    "#     print(b)\n",
    "#     print(x.shape)\n",
    "#     print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'input_dim' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-bcae1825aba0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmedian_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlosses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_final_median_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_trials\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-10-a406a1e78031>\u001b[0m in \u001b[0;36mget_final_median_loss\u001b[0;34m(df, num_trials)\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0mlosses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mtrial\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_trials\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m         \u001b[0mtrain_loss_avgs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_loss_avgs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmin_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m         \u001b[0mlosses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmin_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-7-bdad10013e43>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(num_epochs, log_every_batch)\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_epochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlog_every_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m     \u001b[0mdevice\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_available\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m'cpu'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mNetwork\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_dim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m     \u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAdam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.01\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbetas\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.9\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.999\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'input_dim' is not defined"
     ]
    }
   ],
   "source": [
    "median_loss, losses = get_final_median_loss(df, num_trials)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'median_loss' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-4f77ee4da54c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmedian_loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'median_loss' is not defined"
     ]
    }
   ],
   "source": [
    "median_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[['x2', 'x1']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "median_loss_back, losses_back = get_final_median_loss(df[['x2', 'x1']], num_trials)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "median_loss_back"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "losses_back"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim = df.shape[1] - 1\n",
    "\n",
    "train, test = train_test_split(df, test_size=0.1, random_state=10, shuffle=True)\n",
    "\n",
    "train = np.array(train)\n",
    "test = np.array(test)\n",
    "\n",
    "train = MyDataset(train)\n",
    "test = MyDataset(test)\n",
    "\n",
    "train_loader = DataLoader(train, batch_size=batch_size, shuffle=True, \n",
    "                          num_workers=0, pin_memory=True)\n",
    "test_loader = DataLoader(test, batch_size=batch_size, shuffle=False,\n",
    "                         num_workers=0, pin_memory=True)\n",
    "\n",
    "train_loss_avgs, test_loss_avgs, min_loss = train_model(num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize=(12,8))\n",
    "ax.plot(train_loss_avgs, label='train')\n",
    "ax.plot(test_loss_avgs, label='test')\n",
    "ax.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize=(12,8))\n",
    "ax.plot(train_loss_avgs[100:], label='train')\n",
    "ax.plot(test_loss_avgs[100:], label='test')\n",
    "ax.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml1",
   "language": "python",
   "name": "ml1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
